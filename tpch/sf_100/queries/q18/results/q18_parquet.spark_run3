 -- the query
insert into table q18_tmp_par
select 
  l_orderkey, sum(l_quantity) as t_sum_quantity
from 
  lineitem_par
group by l_orderkey;

insert into table q18_large_volume_customer_par
select 
  c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice,sum(l_quantity)
from 
  customer_par c join orders_par o 
  on 
    c.c_custkey = o.o_custkey
  join q18_tmp_par t 
  on 
    o.o_orderkey = t.l_orderkey and t.t_sum_quantity > 315
  join lineitem_par l 
  on 
    o.o_orderkey = l.l_orderkey
group by c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice
order by o_totalprice desc,o_orderdate
limit 100;
15/08/21 13:33:40 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 13:33:40 INFO metastore: Connected to metastore.
15/08/21 13:33:41 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/21 13:33:41 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:41 INFO SparkContext: Running Spark version 1.4.1
15/08/21 13:33:41 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:41 INFO SecurityManager: Changing view acls to: hive
15/08/21 13:33:41 INFO SecurityManager: Changing modify acls to: hive
15/08/21 13:33:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/21 13:33:43 INFO Slf4jLogger: Slf4jLogger started
15/08/21 13:33:43 INFO Remoting: Starting remoting
15/08/21 13:33:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:50747]
15/08/21 13:33:43 INFO Utils: Successfully started service 'sparkDriver' on port 50747.
15/08/21 13:33:43 INFO SparkEnv: Registering MapOutputTracker
15/08/21 13:33:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:43 INFO SparkEnv: Registering BlockManagerMaster
15/08/21 13:33:43 INFO DiskBlockManager: Created local directory at /tmp/spark-f0275731-dec3-43ab-821d-4d526d08464c/blockmgr-74647b95-16db-4827-8c41-296cefe8ca3b
15/08/21 13:33:43 INFO MemoryStore: MemoryStore started with capacity 20.7 GB
15/08/21 13:33:43 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:43 INFO HttpFileServer: HTTP File server directory is /tmp/spark-f0275731-dec3-43ab-821d-4d526d08464c/httpd-4fb44cf3-d860-42b8-883e-800ab0b2bfbe
15/08/21 13:33:43 INFO HttpServer: Starting HTTP Server
15/08/21 13:33:44 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 13:33:44 INFO AbstractConnector: Started SocketConnector@0.0.0.0:38923
15/08/21 13:33:44 INFO Utils: Successfully started service 'HTTP file server' on port 38923.
15/08/21 13:33:44 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/21 13:33:44 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 13:33:44 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/21 13:33:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/21 13:33:44 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/21 13:33:44 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:44 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:44 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:44 INFO Executor: Starting executor ID driver on host localhost
15/08/21 13:33:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45550.
15/08/21 13:33:44 INFO NettyBlockTransferService: Server created on 45550
15/08/21 13:33:44 INFO BlockManagerMaster: Trying to register BlockManager
15/08/21 13:33:44 INFO BlockManagerMasterEndpoint: Registering block manager localhost:45550 with 20.7 GB RAM, BlockManagerId(driver, localhost, 45550)
15/08/21 13:33:44 INFO BlockManagerMaster: Registered BlockManager
15/08/21 13:33:45 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 13:33:45 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/21 13:33:45 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/21 13:33:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/21 13:33:46 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 13:33:46 INFO metastore: Connected to metastore.
15/08/21 13:33:47 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/21 13:33:47 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/21 13:33:47 INFO ParseDriver: Parsing command: -- the query
insert into table q18_tmp_par
select 
  l_orderkey, sum(l_quantity) as t_sum_quantity
from 
  lineitem_par
group by l_orderkey
15/08/21 13:33:48 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/21 13:33:51 INFO MemoryStore: ensureFreeSpace(326528) called with curMem=0, maxMem=22226833244
15/08/21 13:33:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 13:33:51 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=326528, maxMem=22226833244
15/08/21 13:33:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 13:33:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:45550 (size: 22.3 KB, free: 20.7 GB)
15/08/21 13:33:51 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/21 13:33:51 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 13:33:52 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/21 13:33:52 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/21 13:33:52 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/21 13:33:52 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/21 13:33:52 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/21 13:33:52 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 13:33:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:33:52 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 13:33:52 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/21 13:33:52 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/21 13:33:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 13:33:52 INFO DAGScheduler: Registering RDD 3 (processCmd at CliDriver.java:423)
15/08/21 13:33:52 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 13:33:52 INFO DAGScheduler: Final stage: ResultStage 1(processCmd at CliDriver.java:423)
15/08/21 13:33:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15/08/21 13:33:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15/08/21 13:33:52 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 13:33:52 INFO MemoryStore: ensureFreeSpace(9208) called with curMem=349321, maxMem=22226833244
15/08/21 13:33:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 20.7 GB)
15/08/21 13:33:52 INFO MemoryStore: ensureFreeSpace(4552) called with curMem=358529, maxMem=22226833244
15/08/21 13:33:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 20.7 GB)
15/08/21 13:33:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:45550 (size: 4.4 KB, free: 20.7 GB)
15/08/21 13:33:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/08/21 13:33:52 INFO DAGScheduler: Submitting 170 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423)
15/08/21 13:33:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 170 tasks
15/08/21 13:33:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1758 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1770 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, ANY, 1757 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, ANY, 1769 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, ANY, 1757 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, ANY, 1771 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, ANY, 1758 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, ANY, 1773 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, ANY, 1758 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, ANY, 1770 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, localhost, ANY, 1757 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, localhost, ANY, 1767 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, localhost, ANY, 1757 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, localhost, ANY, 1772 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, localhost, ANY, 1758 bytes)
15/08/21 13:33:52 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, localhost, ANY, 1770 bytes)
15/08/21 13:33:53 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/08/21 13:33:53 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
15/08/21 13:33:53 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
15/08/21 13:33:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/21 13:33:53 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
15/08/21 13:33:53 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/21 13:33:53 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/08/21 13:33:53 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
15/08/21 13:33:53 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
15/08/21 13:33:53 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
15/08/21 13:33:53 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
15/08/21 13:33:53 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
15/08/21 13:33:53 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
15/08/21 13:33:53 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
15/08/21 13:33:53 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
15/08/21 13:33:53 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 134217728 end: 259746640 length: 125528912 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 134217728 end: 257477064 length: 123259336 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 134217728 end: 257420451 length: 123202723 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 134217728 end: 257329816 length: 123112088 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 134217728 end: 257425918 length: 123208190 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 134217728 end: 257171772 length: 122954044 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 134217728 end: 257463677 length: 123245949 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 134217728 end: 257568535 length: 123350807 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 13:33:53 INFO CodecPool: Got brand-new decompressor [.snappy]
21-Aug-2015 13:33:49 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 13:33:49 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 13:33:49 INFO: parquet.hadoop.ParquetFileReader: reading another 85 footers
21-Aug-2015 13:33:49 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573045 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572933 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501150 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501187 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627997 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572784 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573892 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501583 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572843 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574066 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573983 records.
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 223 ms. row count = 3501583
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 223 ms. row count = 3500100
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 224 ms. row count = 3500100
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 225 ms. row count = 3501150
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 224 ms. row count = 15/08/21 13:34:25 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 2125 bytes result sent to driver
15/08/21 13:34:25 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, localhost, ANY, 1755 bytes)
15/08/21 13:34:25 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
15/08/21 13:34:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:25 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 32933 ms on localhost (1/170)
15/08/21 13:34:27 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 2125 bytes result sent to driver
15/08/21 13:34:27 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, localhost, ANY, 1770 bytes)
15/08/21 13:34:27 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
15/08/21 13:34:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 134217728 end: 261799262 length: 127581534 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:27 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 34773 ms on localhost (2/170)
15/08/21 13:34:27 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 2125 bytes result sent to driver
15/08/21 13:34:27 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, localhost, ANY, 1758 bytes)
15/08/21 13:34:27 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
15/08/21 13:34:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:27 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 35023 ms on localhost (3/170)
15/08/21 13:34:28 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2125 bytes result sent to driver
15/08/21 13:34:28 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, localhost, ANY, 1769 bytes)
15/08/21 13:34:28 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
15/08/21 13:34:28 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 35770 ms on localhost (4/170)
15/08/21 13:34:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 134217728 end: 257435404 length: 123217676 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:29 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2125 bytes result sent to driver
15/08/21 13:34:29 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20, localhost, ANY, 1758 bytes)
15/08/21 13:34:29 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
15/08/21 13:34:29 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 36363 ms on localhost (5/170)
15/08/21 13:34:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:29 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2125 bytes result sent to driver
15/08/21 13:34:29 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21, localhost, ANY, 1772 bytes)
15/08/21 13:34:29 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
15/08/21 13:34:29 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 36911 ms on localhost (6/170)
15/08/21 13:34:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 134217728 end: 257347114 length: 123129386 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
3501187
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 227 ms. row count = 3503008
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 228 ms. row count = 3500100
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 229 ms. row count = 3500780
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 230 ms. row count = 3500100
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 228 ms. row count = 3501191
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 229 ms. row count = 3500100
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 231 ms. row count = 3502678
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 237 ms. row count = 3500100
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 237 ms. row count = 3500100
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 238 ms. row count = 3500100
21-Aug-2015 13:33:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 239 ms. row count = 3500939
21-Aug-2015 13:34:23 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29056 ms: 120.46049 rec/ms, 240.92097 cell/ms
21-Aug-2015 13:34:23 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (237 ms) and 99% processing (29056 ms)
21-Aug-2015 13:34:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:34:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 127897
21-Aug-2015 13:34:24 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 30851 ms: 113.45175 rec/ms, 226.9035 cell/ms
21-Aug-2015 13:34:24 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (228 ms) and 99% processing (30851 ms)
21-Aug-2015 13:34:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:34:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 73883
21-Aug-2015 13:34:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500939 records from 2 columns in 31732 ms: 110.32835 rec/ms, 220.6567 cell/ms
21-Aug-2015 13:34:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (239 ms) and 99% processing (31732 ms)
21-Aug-2015 13:34:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500939. reading next block
21-Aug-2015 13:34:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 71904
21-Aug-2015 13:34:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501183 records.
21-Aug-2015 13:34:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 48 ms. row count = 3501183
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 32434 ms: 108.004196 rec/ms, 216.00839 cell/ms
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (227 ms) and 99% processing (32434 ms)
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503008. reading next block
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 70884
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502678 records from 2 columns in 32490 ms: 107.80788 rec/ms, 215.61575 cell/ms
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (231 ms) and 99% processing (32490 ms)
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502678. reading next block
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 17 ms. row count = 70255
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32591 ms: 107.39468 rec/ms, 214.78935 cell/ms
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (230 ms) and 99% processing (32591 ms)
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:34:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 72945
21-Aug-2015 13:34:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3689969 records.
21-Aug-2015 13:34:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 3501351
21-Aug-2015 13:34:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:34:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 83 ms. row count = 3500100
21-Aug-2015 13:34:28 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 34544 ms: 101.32295 rec/ms, 202.6459 cell/ms
21-Aug-2015 13:34:28 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (224 ms) and 99% processing (34544 ms)
21-Aug-2015 13:34:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:34:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 23 ms. row count = 72684
21-Aug-2015 13:34:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573923 records.
21-Aug-2015 13:34:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 3501239
21-Aug-2015 13:34:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:34:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3500100
21-Aug-2015 13:34:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.T15/08/21 13:34:30 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 2125 bytes result sent to driver
15/08/21 13:34:30 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22, localhost, ANY, 1758 bytes)
15/08/21 13:34:30 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
15/08/21 13:34:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:30 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 37596 ms on localhost (7/170)
15/08/21 13:34:30 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2125 bytes result sent to driver
15/08/21 13:34:30 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23, localhost, ANY, 1769 bytes)
15/08/21 13:34:30 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
15/08/21 13:34:30 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 38076 ms on localhost (8/170)
15/08/21 13:34:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 134217728 end: 257844810 length: 123627082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:31 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2125 bytes result sent to driver
15/08/21 13:34:31 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24, localhost, ANY, 1758 bytes)
15/08/21 13:34:31 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
15/08/21 13:34:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:31 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 38207 ms on localhost (9/170)
15/08/21 13:34:39 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 2125 bytes result sent to driver
15/08/21 13:34:39 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25, localhost, ANY, 1768 bytes)
15/08/21 13:34:39 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
15/08/21 13:34:39 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 46731 ms on localhost (10/170)
15/08/21 13:34:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 134217728 end: 257827380 length: 123609652 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2125 bytes result sent to driver
15/08/21 13:34:39 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 2125 bytes result sent to driver
15/08/21 13:34:39 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26, localhost, ANY, 1758 bytes)
15/08/21 13:34:39 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
15/08/21 13:34:39 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27, localhost, ANY, 1770 bytes)
15/08/21 13:34:39 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
15/08/21 13:34:39 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 46986 ms on localhost (11/170)
15/08/21 13:34:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 134217728 end: 257458294 length: 123240566 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 47022 ms on localhost (12/170)
15/08/21 13:34:40 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 2125 bytes result sent to driver
15/08/21 13:34:40 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28, localhost, ANY, 1758 bytes)
15/08/21 13:34:40 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
15/08/21 13:34:40 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 47516 ms on localhost (13/170)
15/08/21 13:34:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:41 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 2125 bytes result sent to driver
15/08/21 13:34:41 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29, localhost, ANY, 1770 bytes)
15/08/21 13:34:41 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
15/08/21 13:34:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 134217728 end: 257369456 length: 123151728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:41 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 48347 ms on localhost (14/170)
15/08/21 13:34:42 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2125 bytes result sent to driver
15/08/21 13:34:42 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30, localhost, ANY, 1758 bytes)
15/08/21 13:34:42 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
15/08/21 13:34:42 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 49865 ms on localhost (15/170)
15/08/21 13:34:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:34:57 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 2125 bytes result sent to driver
15/08/21 13:34:57 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31, localhost, ANY, 1773 bytes)
15/08/21 13:34:57 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
15/08/21 13:34:57 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 64944 ms on localhost (16/170)
15/08/21 13:34:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 134217728 end: 257340601 length: 123122873 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:05 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 2125 bytes result sent to driver
15/08/21 13:35:05 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32, localhost, ANY, 1758 bytes)
15/08/21 13:35:05 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
15/08/21 13:35:05 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 40076 ms on localhost (17/170)
15/08/21 13:35:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
askAttemptContextImpl
21-Aug-2015 13:34:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
21-Aug-2015 13:34:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 76 ms. row count = 3500100
21-Aug-2015 13:34:30 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:30 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501423 records.
21-Aug-2015 13:34:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 93 ms. row count = 3501423
21-Aug-2015 13:34:30 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:30 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573035 records.
21-Aug-2015 13:34:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:34:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 160 ms. row count = 3500100
21-Aug-2015 13:34:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 111 ms. row count = 3500100
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500780 records from 2 columns in 45236 ms: 77.389244 rec/ms, 154.77849 cell/ms
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (229 ms) and 99% processing (45236 ms)
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500780. reading next block
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 73286
21-Aug-2015 13:34:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573921 records.
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 152 ms. row count = 3501305
21-Aug-2015 13:34:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501667 records.
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572747 records.
21-Aug-2015 13:34:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 109 ms. row count = 3500100
21-Aug-2015 13:34:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 175 ms. row count = 3501667
21-Aug-2015 13:34:40 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:40 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:34:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 104 ms. row count = 3500100
21-Aug-2015 13:34:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
21-Aug-2015 13:34:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500100
21-Aug-2015 13:34:42 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:42 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:34:42 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:34:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13423 ms. row count = 3500100
21-Aug-2015 13:34:57 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501351 records from 2 columns in 29482 ms: 118.76233 rec/ms, 237.52466 cell/ms
21-Aug-2015 13:34:57 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (42 ms) and 99% processing (29482 ms)
21-Aug-2015 13:34:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501351. reading next block
21-Aug-2015 13:34:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 188618
21-Aug-2015 13:34:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:34:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574247 records.
21-Aug-2015 13:34:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3363 ms. row count = 3500100
21-Aug-2015 13:35:05 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:35:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 74 ms. row count = 3500100
21-Aug-2015 13:35:06 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501239 records from 2 columns in 37367 ms: 93.69869 rec/ms, 187.39738 cell/ms
21-Aug-2015 13:35:06 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (50 ms) and 99% processing (37367 ms)
21-Aug-2015 13:35:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501239. reading next block
21-Aug-2015 1315/08/21 13:35:11 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 2125 bytes result sent to driver
15/08/21 13:35:11 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33, localhost, ANY, 1768 bytes)
15/08/21 13:35:11 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
15/08/21 13:35:11 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 43968 ms on localhost (18/170)
15/08/21 13:35:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 134217728 end: 259459156 length: 125241428 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:38 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 2125 bytes result sent to driver
15/08/21 13:35:38 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34, localhost, ANY, 1758 bytes)
15/08/21 13:35:38 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
15/08/21 13:35:38 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 69506 ms on localhost (19/170)
15/08/21 13:35:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:38 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 2125 bytes result sent to driver
15/08/21 13:35:38 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35, localhost, ANY, 1769 bytes)
15/08/21 13:35:38 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
15/08/21 13:35:39 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 70407 ms on localhost (20/170)
15/08/21 13:35:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 134217728 end: 257580347 length: 123362619 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:39 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 2125 bytes result sent to driver
15/08/21 13:35:39 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36, localhost, ANY, 1757 bytes)
15/08/21 13:35:39 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
15/08/21 13:35:39 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 71226 ms on localhost (21/170)
15/08/21 13:35:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:39 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 2125 bytes result sent to driver
15/08/21 13:35:39 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37, localhost, ANY, 1770 bytes)
15/08/21 13:35:39 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
15/08/21 13:35:39 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 68687 ms on localhost (22/170)
15/08/21 13:35:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 134217728 end: 257857124 length: 123639396 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:41 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 2125 bytes result sent to driver
15/08/21 13:35:41 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38, localhost, ANY, 1757 bytes)
15/08/21 13:35:41 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
15/08/21 13:35:41 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 61783 ms on localhost (23/170)
15/08/21 13:35:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:41 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 2125 bytes result sent to driver
15/08/21 13:35:41 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39, localhost, ANY, 1770 bytes)
15/08/21 13:35:41 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
15/08/21 13:35:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 134217728 end: 259199417 length: 124981689 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:41 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 71206 ms on localhost (24/170)
15/08/21 13:35:41 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 2125 bytes result sent to driver
15/08/21 13:35:41 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, ANY, 1758 bytes)
15/08/21 13:35:41 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
15/08/21 13:35:41 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 70858 ms on localhost (25/170)
15/08/21 13:35:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:41 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 2125 bytes result sent to driver
15/08/21 13:35:41 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41, localhost, ANY, 1769 bytes)
15/08/21 13:35:41 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
15/08/21 13:35:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 134217728 end: 257564901 length: 123347173 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:41 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 72118 ms on localhost (26/170)
15/08/21 13:35:44 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 2125 bytes result sent to driver
15/08/21 13:35:44 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42, localhost, ANY, 1757 bytes)
15/08/21 13:35:44 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
15/08/21 13:35:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:44 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 64626 ms on localhost (27/170)
:35:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 72684
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 40355 ms: 86.73275 rec/ms, 173.4655 cell/ms
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (160 ms) and 99% processing (40355 ms)
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 30 ms. row count = 72935
21-Aug-2015 13:35:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627517 records.
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 41744 ms: 83.84678 rec/ms, 167.69356 cell/ms
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (76 ms) and 99% processing (41744 ms)
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 74238
21-Aug-2015 13:35:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 146 ms. row count = 3500968
21-Aug-2015 13:35:12 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 31988 ms: 109.41916 rec/ms, 218.83832 cell/ms
21-Aug-2015 13:35:12 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (109 ms) and 99% processing (31988 ms)
21-Aug-2015 13:35:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:35:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 72647
21-Aug-2015 13:35:12 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501305 records from 2 columns in 32757 ms: 106.88723 rec/ms, 213.77446 cell/ms
21-Aug-2015 13:35:12 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (152 ms) and 99% processing (32757 ms)
21-Aug-2015 13:35:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501305. reading next block
21-Aug-2015 13:35:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 72616
21-Aug-2015 13:35:38 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 57372 ms: 61.00711 rec/ms, 122.01422 cell/ms
21-Aug-2015 13:35:38 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (61 ms) and 99% processing (57372 ms)
21-Aug-2015 13:35:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:35:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 74054
21-Aug-2015 13:35:38 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:38 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501180 records.
21-Aug-2015 13:35:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 216 ms. row count = 3501180
21-Aug-2015 13:35:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573027 records.
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 182 ms. row count = 3500932
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 151 ms. row count = 3500100
21-Aug-2015 13:35:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574216 records.
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 135 ms. row count = 3501165
21-Aug-2015 13:35:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626725 records.
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 118 ms. row count = 3501121
21-Aug-2015 13:35:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 102 ms. row count = 3502670
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501407 records.
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572791 records.
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 129 ms. row count = 3501407
21-Aug-2015 13:35:42 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 148 ms. row count = 3500841
21-Aug-2015 13:35:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:44 IN15/08/21 13:35:44 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 2125 bytes result sent to driver
15/08/21 13:35:44 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43, localhost, ANY, 1769 bytes)
15/08/21 13:35:44 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
15/08/21 13:35:44 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 65270 ms on localhost (28/170)
15/08/21 13:35:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 134217728 end: 257460000 length: 123242272 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:45 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 2125 bytes result sent to driver
15/08/21 13:35:45 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44, localhost, ANY, 1758 bytes)
15/08/21 13:35:45 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
15/08/21 13:35:45 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 63945 ms on localhost (29/170)
15/08/21 13:35:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:45 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 2125 bytes result sent to driver
15/08/21 13:35:45 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45, localhost, ANY, 1768 bytes)
15/08/21 13:35:45 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
15/08/21 13:35:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 134217728 end: 257842743 length: 123625015 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:48 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 67749 ms on localhost (30/170)
15/08/21 13:35:48 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 2125 bytes result sent to driver
15/08/21 13:35:48 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46, localhost, ANY, 1758 bytes)
15/08/21 13:35:48 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
15/08/21 13:35:48 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 66154 ms on localhost (31/170)
15/08/21 13:35:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:35:56 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 2125 bytes result sent to driver
15/08/21 13:35:56 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47, localhost, ANY, 1770 bytes)
15/08/21 13:35:56 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
15/08/21 13:35:56 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 58682 ms on localhost (32/170)
15/08/21 13:35:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 134217728 end: 257576472 length: 123358744 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:00 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 2125 bytes result sent to driver
15/08/21 13:36:00 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48, localhost, ANY, 1758 bytes)
15/08/21 13:36:00 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
15/08/21 13:36:00 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 54564 ms on localhost (33/170)
15/08/21 13:36:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:06 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 2125 bytes result sent to driver
15/08/21 13:36:06 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49, localhost, ANY, 1770 bytes)
15/08/21 13:36:06 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
15/08/21 13:36:06 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 27348 ms on localhost (34/170)
15/08/21 13:36:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 134217728 end: 256733155 length: 122515427 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:10 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 2125 bytes result sent to driver
15/08/21 13:36:10 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50, localhost, ANY, 1757 bytes)
15/08/21 13:36:10 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
15/08/21 13:36:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:10 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 31848 ms on localhost (35/170)
FO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501143 records.
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 145 ms. row count = 3501143
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 43367 ms: 80.70883 rec/ms, 161.41766 cell/ms
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 7% reading (3363 ms) and 92% processing (43367 ms)
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 74147
21-Aug-2015 13:35:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573047 records.
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 3500100
21-Aug-2015 13:35:45 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:45 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501260 records.
21-Aug-2015 13:35:45 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 71 ms. row count = 3501260
21-Aug-2015 13:35:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573207 records.
21-Aug-2015 13:35:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500100
21-Aug-2015 13:35:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501217 records.
21-Aug-2015 13:35:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 113 ms. row count = 3501217
21-Aug-2015 13:35:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:35:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573258 records.
21-Aug-2015 13:35:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:35:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3190 ms. row count = 3501508
21-Aug-2015 13:35:59 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500968 records from 2 columns in 48011 ms: 72.92012 rec/ms, 145.84024 cell/ms
21-Aug-2015 13:35:59 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (146 ms) and 99% processing (48011 ms)
21-Aug-2015 13:35:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500968. reading next block
21-Aug-2015 13:35:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 126549
21-Aug-2015 13:36:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501165 records from 2 columns in 20461 ms: 171.11407 rec/ms, 342.22815 cell/ms
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (135 ms) and 99% processing (20461 ms)
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501165. reading next block
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 73051
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503050 records.
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 123 ms. row count = 3503050
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500932 records from 2 columns in 21524 ms: 162.65248 rec/ms, 325.30496 cell/ms
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (182 ms) and 99% processing (21524 ms)
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500932. reading next block
21-Aug-2015 13:36:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 72095
21-Aug-2015 13:36:05 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500841 records from 2 columns in 23159 ms: 151.16547 rec/ms, 302.33093 cell/ms
21-Aug-2015 13:36:05 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (148 ms) and 99% processing (23159 ms)
21-Aug-2015 13:36:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500841. reading next block
21-Aug-2015 13:36:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 71950
21-Aug-2015 13:36:05 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502670 records from 2 columns in 23973 ms: 146.10896 rec/ms, 292.21793 cell/ms
21-Aug-2015 13:36:05 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (102 ms) and 99% processing (23973 ms)
21-Aug-2015 13:36:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502670. reading next block
21-Aug-2015 13:36:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 124055
21-Aug-2015 13:36:06 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:06 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3570986 records.
21-Aug-2015 13:36:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 118 ms. row count = 3503008
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22193 ms: 157.71188 rec/ms, 315.42377 cell/ms
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (101 ms) and 99% processing (22193 ms)
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 73107
21-Aug-2015 13:36:10 WARNING: parquet.hadoop.ParquetRecordReader: Can n15/08/21 13:36:10 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 2125 bytes result sent to driver
15/08/21 13:36:10 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51, localhost, ANY, 1768 bytes)
15/08/21 13:36:10 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
15/08/21 13:36:10 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 31199 ms on localhost (36/170)
15/08/21 13:36:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 134217728 end: 257568635 length: 123350907 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:10 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 2125 bytes result sent to driver
15/08/21 13:36:10 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52, localhost, ANY, 1757 bytes)
15/08/21 13:36:10 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
15/08/21 13:36:10 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 59377 ms on localhost (37/170)
15/08/21 13:36:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:11 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 2125 bytes result sent to driver
15/08/21 13:36:11 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53, localhost, ANY, 1768 bytes)
15/08/21 13:36:11 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
15/08/21 13:36:11 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 32330 ms on localhost (38/170)
15/08/21 13:36:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 134217728 end: 257584480 length: 123366752 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:11 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 2125 bytes result sent to driver
15/08/21 13:36:11 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54, localhost, ANY, 1757 bytes)
15/08/21 13:36:11 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
15/08/21 13:36:11 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 30228 ms on localhost (39/170)
15/08/21 13:36:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:11 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 2125 bytes result sent to driver
15/08/21 13:36:12 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55, localhost, ANY, 1769 bytes)
15/08/21 13:36:12 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
15/08/21 13:36:12 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 30218 ms on localhost (40/170)
15/08/21 13:36:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 134217728 end: 259050527 length: 124832799 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:12 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 2125 bytes result sent to driver
15/08/21 13:36:12 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56, localhost, ANY, 1758 bytes)
15/08/21 13:36:12 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
15/08/21 13:36:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:12 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 30346 ms on localhost (41/170)
15/08/21 13:36:15 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 2125 bytes result sent to driver
15/08/21 13:36:15 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57, localhost, ANY, 1772 bytes)
15/08/21 13:36:15 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
15/08/21 13:36:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 134217728 end: 257461890 length: 123244162 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:15 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 34302 ms on localhost (42/170)
15/08/21 13:36:16 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 2125 bytes result sent to driver
15/08/21 13:36:16 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58, localhost, ANY, 1758 bytes)
15/08/21 13:36:16 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
15/08/21 13:36:16 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 31955 ms on localhost (43/170)
15/08/21 13:36:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:16 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 2125 bytes result sent to driver
15/08/21 13:36:16 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59, localhost, ANY, 1773 bytes)
15/08/21 13:36:16 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
15/08/21 13:36:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 134217728 end: 257416343 length: 123198615 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:16 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 31103 ms on localhost (44/170)
15/08/21 13:36:16 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 2125 bytes result sent to driver
15/08/21 13:36:16 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60, localhost, ANY, 1758 bytes)
15/08/21 13:36:16 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
15/08/21 13:36:16 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 31509 ms on localhost (45/170)
15/08/21 13:36:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:29 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 2125 bytes result sent to driver
15/08/21 13:36:29 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61, localhost, ANY, 1770 bytes)
15/08/21 13:36:29 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
15/08/21 13:36:29 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 40620 ms on localhost (46/170)
15/08/21 13:36:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 134217728 end: 257883468 length: 123665740 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:30 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 2125 bytes result sent to driver
15/08/21 13:36:30 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62, localhost, ANY, 1757 bytes)
15/08/21 13:36:30 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
15/08/21 13:36:30 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 45197 ms on localhost (47/170)
15/08/21 13:36:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
ot initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501200 records.
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 72 ms. row count = 3501200
21-Aug-2015 13:36:10 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573168 records.
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:10 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500823 records.
21-Aug-2015 13:36:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 71 ms. row count = 3501341
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 26087 ms: 134.17027 rec/ms, 268.34055 cell/ms
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (51 ms) and 99% processing (26087 ms)
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 87 ms. row count = 3500823
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 72947
21-Aug-2015 13:36:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573143 records.
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 59 ms. row count = 3501035
21-Aug-2015 13:36:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503144 records.
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 3503144
21-Aug-2015 13:36:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3624571 records.
21-Aug-2015 13:36:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:36:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 100 ms. row count = 3500100
21-Aug-2015 13:36:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
21-Aug-2015 13:36:15 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573988 records.
21-Aug-2015 13:36:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 44 ms. row count = 3500824
21-Aug-2015 13:36:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:36:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 55 ms. row count = 3500100
21-Aug-2015 13:36:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574223 records.
21-Aug-2015 13:36:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 38 ms. row count = 3501614
21-Aug-2015 13:36:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
21-Aug-2015 13:36:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12806 ms. row count = 3501191
21-Aug-2015 13:36:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573362 records.
21-Aug-2015 13:36:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500100
21-Aug-2015 13:36:29 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501508 records from 2 columns in 29986 ms: 116.77142 rec/ms, 233.54285 cell/ms
21-Aug-2015 13:36:29 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 9% reading (3190 ms) and 90% processing (29986 ms)
21-Aug-2015 13:36:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501508. reading next block
21-Aug-2015 13:36:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 71750
21-Aug-2015 13:36:30 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:30 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will 15/08/21 13:36:37 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 2125 bytes result sent to driver
15/08/21 13:36:37 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63, localhost, ANY, 1772 bytes)
15/08/21 13:36:37 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
15/08/21 13:36:37 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 41303 ms on localhost (48/170)
15/08/21 13:36:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 134217728 end: 257592803 length: 123375075 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:47 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 2125 bytes result sent to driver
15/08/21 13:36:47 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64, localhost, ANY, 1758 bytes)
15/08/21 13:36:47 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
15/08/21 13:36:47 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 46804 ms on localhost (49/170)
15/08/21 13:36:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:51 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 2125 bytes result sent to driver
15/08/21 13:36:51 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65, localhost, ANY, 1770 bytes)
15/08/21 13:36:51 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
15/08/21 13:36:51 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 40674 ms on localhost (50/170)
15/08/21 13:36:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 134217728 end: 258145407 length: 123927679 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:52 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 2125 bytes result sent to driver
15/08/21 13:36:52 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66, localhost, ANY, 1757 bytes)
15/08/21 13:36:52 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 41949 ms on localhost (51/170)
15/08/21 13:36:52 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
15/08/21 13:36:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:52 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 2125 bytes result sent to driver
15/08/21 13:36:52 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67, localhost, ANY, 1769 bytes)
15/08/21 13:36:52 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
15/08/21 13:36:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 134217728 end: 259344407 length: 125126679 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:52 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 40891 ms on localhost (52/170)
15/08/21 13:36:53 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 2125 bytes result sent to driver
15/08/21 13:36:53 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68, localhost, ANY, 1758 bytes)
15/08/21 13:36:53 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
15/08/21 13:36:53 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 42540 ms on localhost (53/170)
15/08/21 13:36:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:54 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 2125 bytes result sent to driver
15/08/21 13:36:54 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69, localhost, ANY, 1770 bytes)
15/08/21 13:36:54 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
15/08/21 13:36:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 134217728 end: 257768338 length: 123550610 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:54 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 47728 ms on localhost (54/170)
15/08/21 13:36:54 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 2125 bytes result sent to driver
15/08/21 13:36:54 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70, localhost, ANY, 1758 bytes)
15/08/21 13:36:54 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
15/08/21 13:36:54 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 42461 ms on localhost (55/170)
15/08/21 13:36:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
read a total of 3501059 records.
21-Aug-2015 13:36:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 172 ms. row count = 3501059
21-Aug-2015 13:36:37 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:37 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573291 records.
21-Aug-2015 13:36:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 56 ms. row count = 3501221
21-Aug-2015 13:36:41 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501341 records from 2 columns in 30614 ms: 114.37058 rec/ms, 228.74117 cell/ms
21-Aug-2015 13:36:41 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (71 ms) and 99% processing (30614 ms)
21-Aug-2015 13:36:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501341. reading next block
21-Aug-2015 13:36:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 71827
21-Aug-2015 13:36:42 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 35989 ms: 97.33552 rec/ms, 194.67104 cell/ms
21-Aug-2015 13:36:42 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (118 ms) and 99% processing (35989 ms)
21-Aug-2015 13:36:42 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503008. reading next block
21-Aug-2015 13:36:42 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 67978
21-Aug-2015 13:36:46 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501035 records from 2 columns in 34757 ms: 100.72892 rec/ms, 201.45784 cell/ms
21-Aug-2015 13:36:46 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (59 ms) and 99% processing (34757 ms)
21-Aug-2015 13:36:46 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501035. reading next block
21-Aug-2015 13:36:46 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 72108
21-Aug-2015 13:36:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:47 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:36:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 117 ms. row count = 3500100
21-Aug-2015 13:36:47 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 35176 ms: 99.5025 rec/ms, 199.005 cell/ms
21-Aug-2015 13:36:47 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (100 ms) and 99% processing (35176 ms)
21-Aug-2015 13:36:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:36:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 124471
21-Aug-2015 13:36:51 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500824 records from 2 columns in 35519 ms: 98.56201 rec/ms, 197.12402 cell/ms
21-Aug-2015 13:36:51 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (44 ms) and 99% processing (35519 ms)
21-Aug-2015 13:36:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500824. reading next block
21-Aug-2015 13:36:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 73164
21-Aug-2015 13:36:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574053 records.
21-Aug-2015 13:36:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 158 ms. row count = 3500100
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22741 ms: 153.91144 rec/ms, 307.82288 cell/ms
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (68 ms) and 99% processing (22741 ms)
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 73262
21-Aug-2015 13:36:52 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 114 ms. row count = 3500100
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501614 records from 2 columns in 36219 ms: 96.678925 rec/ms, 193.35785 cell/ms
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (38 ms) and 99% processing (36219 ms)
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501614. reading next block
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 72609
21-Aug-2015 13:36:52 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627630 records.
21-Aug-2015 13:36:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 47 ms. row count = 3500100
21-Aug-2015 13:36:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:36:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 62 ms. row count = 3500100
21-Aug-2015 13:36:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574075 records.
21-Aug-2015 13:36:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 94 ms. row count = 3500100
21-Aug-2015 13:36:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is or15/08/21 13:36:58 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 2125 bytes result sent to driver
15/08/21 13:36:58 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71, localhost, ANY, 1771 bytes)
15/08/21 13:36:58 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
15/08/21 13:36:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 134217728 end: 257102581 length: 122884853 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:58 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 46958 ms on localhost (56/170)
15/08/21 13:36:58 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 2125 bytes result sent to driver
15/08/21 13:36:58 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72, localhost, ANY, 1758 bytes)
15/08/21 13:36:58 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
15/08/21 13:36:58 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 46674 ms on localhost (57/170)
15/08/21 13:36:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:59 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 2125 bytes result sent to driver
15/08/21 13:36:59 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73, localhost, ANY, 1772 bytes)
15/08/21 13:36:59 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
15/08/21 13:36:59 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 43802 ms on localhost (58/170)
15/08/21 13:36:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 134217728 end: 257873629 length: 123655901 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:36:59 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 2125 bytes result sent to driver
15/08/21 13:36:59 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74, localhost, ANY, 1757 bytes)
15/08/21 13:36:59 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
15/08/21 13:36:59 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 43375 ms on localhost (59/170)
15/08/21 13:36:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:00 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 2125 bytes result sent to driver
15/08/21 13:37:00 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75, localhost, ANY, 1767 bytes)
15/08/21 13:37:00 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
15/08/21 13:37:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 134217728 end: 259470450 length: 125252722 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:00 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 43773 ms on localhost (60/170)
15/08/21 13:37:03 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 2125 bytes result sent to driver
15/08/21 13:37:03 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76, localhost, ANY, 1757 bytes)
15/08/21 13:37:03 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
15/08/21 13:37:03 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 33843 ms on localhost (61/170)
15/08/21 13:37:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:04 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 2125 bytes result sent to driver
15/08/21 13:37:04 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77, localhost, ANY, 1771 bytes)
15/08/21 13:37:04 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
15/08/21 13:37:04 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 34602 ms on localhost (62/170)
15/08/21 13:37:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 134217728 end: 259842205 length: 125624477 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:04 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 2125 bytes result sent to driver
15/08/21 13:37:04 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78, localhost, ANY, 1758 bytes)
15/08/21 13:37:04 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
15/08/21 13:37:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:04 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 48154 ms on localhost (63/170)
15/08/21 13:37:21 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 2125 bytes result sent to driver
15/08/21 13:37:21 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79, localhost, ANY, 1773 bytes)
15/08/21 13:37:21 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
15/08/21 13:37:21 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 44207 ms on localhost (64/170)
15/08/21 13:37:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 134217728 end: 257071082 length: 122853354 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
g.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502727 records.
21-Aug-2015 13:36:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 41 ms. row count = 3502727
21-Aug-2015 13:36:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571405 records.
21-Aug-2015 13:36:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 138 ms. row count = 3500100
21-Aug-2015 13:36:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501229 records.
21-Aug-2015 13:36:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 116 ms. row count = 3501229
21-Aug-2015 13:36:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:36:59 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572739 records.
21-Aug-2015 13:36:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:36:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 67 ms. row count = 3500100
21-Aug-2015 13:36:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500100
21-Aug-2015 13:37:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627768 records.
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501221 records from 2 columns in 22354 ms: 156.62614 rec/ms, 313.2523 cell/ms
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (56 ms) and 99% processing (22354 ms)
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501221. reading next block
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 35 ms. row count = 72070
21-Aug-2015 13:37:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 127 ms. row count = 3501462
21-Aug-2015 13:37:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 94 ms. row count = 3500100
21-Aug-2015 13:37:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627712 records.
21-Aug-2015 13:37:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 61 ms. row count = 3501302
21-Aug-2015 13:37:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503221 records.
21-Aug-2015 13:37:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 58 ms. row count = 3503221
21-Aug-2015 13:37:14 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 23042 ms: 151.90088 rec/ms, 303.80176 cell/ms
21-Aug-2015 13:37:14 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (158 ms) and 99% processing (23042 ms)
21-Aug-2015 13:37:14 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:37:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 73953
21-Aug-2015 13:37:20 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 26985 ms: 129.7054 rec/ms, 259.4108 cell/ms
21-Aug-2015 13:37:20 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (47 ms) and 99% processing (26985 ms)
21-Aug-2015 13:37:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:37:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 127530
21-Aug-2015 13:37:21 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 27229 ms: 128.54309 rec/ms, 257.08618 cell/ms
21-Aug-2015 13:37:21 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (94 ms) and 99% processing (27229 ms)
21-Aug-2015 13:37:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:37:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 73975
21-Aug-2015 13:37:21 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 23350 ms: 149.89722 rec/ms, 299.79443 cell/ms
21-Aug-2015 13:37:21 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (138 ms) and 99% processing (23350 ms)
21-Aug-2015 13:37:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:37:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 71305
21-Aug-2015 13:37:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572089 records.
21-Aug-2015 13:37:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:26 INFO: parquet.hadoop.InternalParquetRec15/08/21 13:37:26 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 2125 bytes result sent to driver
15/08/21 13:37:26 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80, localhost, ANY, 1758 bytes)
15/08/21 13:37:26 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
15/08/21 13:37:26 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 39816 ms on localhost (65/170)
15/08/21 13:37:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:27 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 2125 bytes result sent to driver
15/08/21 13:37:27 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81, localhost, ANY, 1770 bytes)
15/08/21 13:37:27 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
15/08/21 13:37:27 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 35877 ms on localhost (66/170)
15/08/21 13:37:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 134217728 end: 257075001 length: 122857273 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:28 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 2125 bytes result sent to driver
15/08/21 13:37:28 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82, localhost, ANY, 1758 bytes)
15/08/21 13:37:28 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
15/08/21 13:37:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:28 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 35516 ms on localhost (67/170)
15/08/21 13:37:28 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 2125 bytes result sent to driver
15/08/21 13:37:28 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83, localhost, ANY, 1770 bytes)
15/08/21 13:37:28 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
15/08/21 13:37:28 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 35339 ms on localhost (68/170)
15/08/21 13:37:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 134217728 end: 257427527 length: 123209799 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:28 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 2125 bytes result sent to driver
15/08/21 13:37:28 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84, localhost, ANY, 1757 bytes)
15/08/21 13:37:28 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
15/08/21 13:37:28 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 36458 ms on localhost (69/170)
15/08/21 13:37:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:29 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 2125 bytes result sent to driver
15/08/21 13:37:29 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85, localhost, ANY, 1771 bytes)
15/08/21 13:37:29 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
15/08/21 13:37:29 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 35338 ms on localhost (70/170)
15/08/21 13:37:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 134217728 end: 257349334 length: 123131606 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:29 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 2125 bytes result sent to driver
15/08/21 13:37:29 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 2125 bytes result sent to driver
15/08/21 13:37:29 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86, localhost, ANY, 1757 bytes)
15/08/21 13:37:29 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87, localhost, ANY, 1770 bytes)
15/08/21 13:37:29 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
15/08/21 13:37:29 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
15/08/21 13:37:29 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 31463 ms on localhost (71/170)
15/08/21 13:37:29 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 31012 ms on localhost (72/170)
15/08/21 13:37:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 134217728 end: 257709471 length: 123491743 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:47 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 2125 bytes result sent to driver
15/08/21 13:37:47 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88, localhost, ANY, 1757 bytes)
15/08/21 13:37:47 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
15/08/21 13:37:47 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 47410 ms on localhost (73/170)
15/08/21 13:37:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:47 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 2125 bytes result sent to driver
15/08/21 13:37:47 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89, localhost, ANY, 1769 bytes)
15/08/21 13:37:47 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
15/08/21 13:37:47 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 43727 ms on localhost (74/170)
15/08/21 13:37:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 134217728 end: 259741355 length: 125523627 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:47 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 2125 bytes result sent to driver
15/08/21 13:37:47 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90, localhost, ANY, 1758 bytes)
15/08/21 13:37:47 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
15/08/21 13:37:47 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 47595 ms on localhost (75/170)
15/08/21 13:37:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:47 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 2125 bytes result sent to driver
15/08/21 13:37:47 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91, localhost, ANY, 1773 bytes)
15/08/21 13:37:47 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
15/08/21 13:37:47 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 53567 ms on localhost (76/170)
15/08/21 13:37:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 134217728 end: 257333395 length: 123115667 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
ordReader: block read in memory in 53 ms. row count = 3500100
21-Aug-2015 13:37:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 107 ms. row count = 3500100
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501462 records from 2 columns in 26917 ms: 130.08366 rec/ms, 260.16733 cell/ms
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (127 ms) and 99% processing (26917 ms)
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501462. reading next block
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 126306
21-Aug-2015 13:37:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574581 records.
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 86 ms. row count = 3503231
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 28017 ms: 124.92772 rec/ms, 249.85544 cell/ms
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (67 ms) and 99% processing (28017 ms)
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:37:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 72639
21-Aug-2015 13:37:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3500100
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501302 records from 2 columns in 24573 ms: 142.48573 rec/ms, 284.97147 cell/ms
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (61 ms) and 99% processing (24573 ms)
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501302. reading next block
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 126410
21-Aug-2015 13:37:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574316 records.
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 96 ms. row count = 3503274
21-Aug-2015 13:37:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3500100
21-Aug-2015 13:37:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574256 records.
21-Aug-2015 13:37:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 60 ms. row count = 3500100
21-Aug-2015 13:37:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
21-Aug-2015 13:37:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 17451 ms. row count = 3500100
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 108 ms. row count = 3500100
21-Aug-2015 13:37:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 79 ms. row count = 3500100
21-Aug-2015 13:37:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627697 records.
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500100
21-Aug-2015 13:37:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 64 ms. row count = 3500100
21-Aug-2015 13:37:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl15/08/21 13:37:48 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 2125 bytes result sent to driver
15/08/21 13:37:48 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92, localhost, ANY, 1758 bytes)
15/08/21 13:37:48 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
15/08/21 13:37:48 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 44079 ms on localhost (77/170)
15/08/21 13:37:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:48 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 2125 bytes result sent to driver
15/08/21 13:37:48 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93, localhost, ANY, 1773 bytes)
15/08/21 13:37:48 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
15/08/21 13:37:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 134217728 end: 257330329 length: 123112601 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:48 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 49261 ms on localhost (78/170)
15/08/21 13:37:51 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 2125 bytes result sent to driver
15/08/21 13:37:51 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94, localhost, ANY, 1758 bytes)
15/08/21 13:37:51 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
15/08/21 13:37:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:37:51 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 47756 ms on localhost (79/170)
15/08/21 13:38:10 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 2125 bytes result sent to driver
15/08/21 13:38:10 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95, localhost, ANY, 1768 bytes)
15/08/21 13:38:10 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 48707 ms on localhost (80/170)
15/08/21 13:38:10 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
15/08/21 13:38:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 134217728 end: 257756022 length: 123538294 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:10 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 2125 bytes result sent to driver
15/08/21 13:38:10 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96, localhost, ANY, 1757 bytes)
15/08/21 13:38:10 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
15/08/21 13:38:10 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 43964 ms on localhost (81/170)
15/08/21 13:38:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:11 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 2125 bytes result sent to driver
15/08/21 13:38:11 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97, localhost, ANY, 1771 bytes)
15/08/21 13:38:11 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
15/08/21 13:38:11 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 43860 ms on localhost (82/170)
15/08/21 13:38:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 134217728 end: 257832393 length: 123614665 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:11 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 2125 bytes result sent to driver
15/08/21 13:38:11 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98, localhost, ANY, 1758 bytes)
15/08/21 13:38:11 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
15/08/21 13:38:11 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 43501 ms on localhost (83/170)
15/08/21 13:38:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}

21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573967 records.
21-Aug-2015 13:37:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 144 ms. row count = 3500100
21-Aug-2015 13:37:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 80 ms. row count = 3500100
21-Aug-2015 13:37:48 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:48 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574008 records.
21-Aug-2015 13:37:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 80 ms. row count = 3500100
21-Aug-2015 13:37:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:37:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:37:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:37:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 85 ms. row count = 3500100
21-Aug-2015 13:37:57 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 31040 ms: 112.760956 rec/ms, 225.52191 cell/ms
21-Aug-2015 13:37:57 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (53 ms) and 99% processing (31040 ms)
21-Aug-2015 13:37:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:37:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 71989
21-Aug-2015 13:38:03 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503231 records from 2 columns in 35986 ms: 97.34983 rec/ms, 194.69966 cell/ms
21-Aug-2015 13:38:03 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (86 ms) and 99% processing (35986 ms)
21-Aug-2015 13:38:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503231. reading next block
21-Aug-2015 13:38:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 71350
21-Aug-2015 13:38:04 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503274 records from 2 columns in 35582 ms: 98.45635 rec/ms, 196.9127 cell/ms
21-Aug-2015 13:38:04 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (96 ms) and 99% processing (35582 ms)
21-Aug-2015 13:38:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503274. reading next block
21-Aug-2015 13:38:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 71042
21-Aug-2015 13:38:09 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22499 ms: 155.56691 rec/ms, 311.13382 cell/ms
21-Aug-2015 13:38:09 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 43% reading (17451 ms) and 56% processing (22499 ms)
21-Aug-2015 13:38:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 74054
21-Aug-2015 13:38:09 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 40325 ms: 86.79727 rec/ms, 173.59454 cell/ms
21-Aug-2015 13:38:09 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (60 ms) and 99% processing (40325 ms)
21-Aug-2015 13:38:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 74156
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22289 ms: 157.03262 rec/ms, 314.06525 cell/ms
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (144 ms) and 99% processing (22289 ms)
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 73867
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22658 ms: 154.47523 rec/ms, 308.95047 cell/ms
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (69 ms) and 99% processing (22658 ms)
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 127597
21-Aug-2015 13:38:10 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574308 records.
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500100
21-Aug-2015 13:38:10 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:38:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22027 ms: 158.90044 rec/ms, 317.80087 cell/ms
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (80 ms) and 99% processing (22027 ms)
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 73908
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 217 ms. row count = 3500100
21-Aug-2015 13:38:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573690 records.
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 3500612
21-Aug-2015 13:38:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter15/08/21 13:38:12 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 2125 bytes result sent to driver
15/08/21 13:38:12 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99, localhost, ANY, 1771 bytes)
15/08/21 13:38:12 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
15/08/21 13:38:12 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 43521 ms on localhost (84/170)
15/08/21 13:38:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 134217728 end: 257486081 length: 123268353 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:12 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 2125 bytes result sent to driver
15/08/21 13:38:12 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100, localhost, ANY, 1757 bytes)
15/08/21 13:38:12 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
15/08/21 13:38:12 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 42697 ms on localhost (85/170)
15/08/21 13:38:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:12 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 2125 bytes result sent to driver
15/08/21 13:38:12 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101, localhost, ANY, 1769 bytes)
15/08/21 13:38:12 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
15/08/21 13:38:12 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 43182 ms on localhost (86/170)
15/08/21 13:38:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 134217728 end: 257368738 length: 123151010 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:12 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 2125 bytes result sent to driver
15/08/21 13:38:12 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102, localhost, ANY, 1758 bytes)
15/08/21 13:38:12 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
15/08/21 13:38:12 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 43839 ms on localhost (87/170)
15/08/21 13:38:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:12 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 2125 bytes result sent to driver
15/08/21 13:38:12 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103, localhost, ANY, 1770 bytes)
15/08/21 13:38:12 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
15/08/21 13:38:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 134217728 end: 257317174 length: 123099446 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:12 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 25085 ms on localhost (88/170)
15/08/21 13:38:16 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 2125 bytes result sent to driver
15/08/21 13:38:16 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104, localhost, ANY, 1758 bytes)
15/08/21 13:38:16 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
15/08/21 13:38:16 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 29274 ms on localhost (89/170)
15/08/21 13:38:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:16 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 2125 bytes result sent to driver
15/08/21 13:38:16 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105, localhost, ANY, 1769 bytes)
15/08/21 13:38:16 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
15/08/21 13:38:16 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 47095 ms on localhost (90/170)
15/08/21 13:38:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 134217728 end: 257566042 length: 123348314 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:16 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 2125 bytes result sent to driver
15/08/21 13:38:16 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106, localhost, ANY, 1758 bytes)
15/08/21 13:38:16 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
15/08/21 13:38:16 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 29067 ms on localhost (91/170)
15/08/21 13:38:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:16 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 2125 bytes result sent to driver
15/08/21 13:38:16 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107, localhost, ANY, 1771 bytes)
15/08/21 13:38:16 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
15/08/21 13:38:16 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 29355 ms on localhost (92/170)
15/08/21 13:38:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 134217728 end: 257458240 length: 123240512 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:16 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 2125 bytes result sent to driver
15/08/21 13:38:16 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108, localhost, ANY, 1758 bytes)
15/08/21 13:38:16 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
15/08/21 13:38:16 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 28231 ms on localhost (93/170)
15/08/21 13:38:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:17 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 2125 bytes result sent to driver
15/08/21 13:38:17 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109, localhost, ANY, 1770 bytes)
15/08/21 13:38:17 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
15/08/21 13:38:17 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 28271 ms on localhost (94/170)
15/08/21 13:38:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 134217728 end: 257458739 length: 123241011 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:17 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 2125 bytes result sent to driver
15/08/21 13:38:17 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110, localhost, ANY, 1757 bytes)
15/08/21 13:38:17 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
15/08/21 13:38:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:17 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 25646 ms on localhost (95/170)
 due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501124 records.
21-Aug-2015 13:38:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 70 ms. row count = 3501124
21-Aug-2015 13:38:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573157 records.
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 3500100
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 3500100
21-Aug-2015 13:38:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 64 ms. row count = 3500100
21-Aug-2015 13:38:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573826 records.
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500100
21-Aug-2015 13:38:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500786 records.
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 59 ms. row count = 3500786
21-Aug-2015 13:38:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573221 records.
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 35 ms. row count = 3501171
21-Aug-2015 13:38:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502806 records.
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571385 records.
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 45 ms. row count = 3502806
21-Aug-2015 13:38:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:38:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 65 ms. row count = 3500100
21-Aug-2015 13:38:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 114 ms. row count = 3500100
21-Aug-2015 13:38:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574219 records.
21-Aug-2015 13:38:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 52 ms. row count = 3501364
21-Aug-2015 13:38:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501448 records.
21-Aug-2015 13:38:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 57 ms. row count = 3501448
21-Aug-2015 13:38:27 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 16705 ms: 209.5241 rec/ms, 419.0482 cell/ms
21-Aug-2015 13:38:27 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (16705 ms)
21-Aug-2015 13:38:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 74208
21-Aug-2015 13:38:50 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500612 records from 2 columns in 39274 ms: 89.133064 rec/ms, 178.26613 cell/ms
21-Aug-2015 13:38:50 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (51 ms) and 99% processing (39274 ms15/08/21 13:38:51 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 2125 bytes result sent to driver
15/08/21 13:38:51 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111, localhost, ANY, 1771 bytes)
15/08/21 13:38:51 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
15/08/21 13:38:51 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 40434 ms on localhost (96/170)
15/08/21 13:38:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 134217728 end: 257439181 length: 123221453 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:54 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 2125 bytes result sent to driver
15/08/21 13:38:54 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112, localhost, ANY, 1757 bytes)
15/08/21 13:38:54 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
15/08/21 13:38:54 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 43757 ms on localhost (97/170)
15/08/21 13:38:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:55 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 2125 bytes result sent to driver
15/08/21 13:38:55 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113, localhost, ANY, 1770 bytes)
15/08/21 13:38:55 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
15/08/21 13:38:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 134217728 end: 181459518 length: 47241790 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:55 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 43230 ms on localhost (98/170)
15/08/21 13:38:55 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 2125 bytes result sent to driver
15/08/21 13:38:55 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114, localhost, ANY, 1757 bytes)
15/08/21 13:38:55 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
15/08/21 13:38:55 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 44437 ms on localhost (99/170)
15/08/21 13:38:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:56 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 2125 bytes result sent to driver
15/08/21 13:38:56 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115, localhost, ANY, 1768 bytes)
15/08/21 13:38:56 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
15/08/21 13:38:56 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 43370 ms on localhost (100/170)
15/08/21 13:38:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 134217728 end: 259458210 length: 125240482 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:56 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 2125 bytes result sent to driver
15/08/21 13:38:56 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116, localhost, ANY, 1758 bytes)
15/08/21 13:38:56 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
15/08/21 13:38:56 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 44073 ms on localhost (101/170)
15/08/21 13:38:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:56 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 2125 bytes result sent to driver
15/08/21 13:38:56 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117, localhost, ANY, 1769 bytes)
15/08/21 13:38:56 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
15/08/21 13:38:56 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 43812 ms on localhost (102/170)
15/08/21 13:38:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 134217728 end: 257467186 length: 123249458 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:56 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 2125 bytes result sent to driver
15/08/21 13:38:56 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118, localhost, ANY, 1756 bytes)
15/08/21 13:38:56 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
15/08/21 13:38:56 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 39670 ms on localhost (103/170)
15/08/21 13:38:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:38:56 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 2125 bytes result sent to driver
15/08/21 13:38:56 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119, localhost, ANY, 1769 bytes)
15/08/21 13:38:56 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
15/08/21 13:38:56 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 40065 ms on localhost (104/170)
15/08/21 13:38:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 134217728 end: 260141700 length: 125923972 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
)
21-Aug-2015 13:38:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500612. reading next block
21-Aug-2015 13:38:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 73078
21-Aug-2015 13:38:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572926 records.
21-Aug-2015 13:38:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 72 ms. row count = 3500100
21-Aug-2015 13:38:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 37049 ms: 94.47218 rec/ms, 188.94437 cell/ms
21-Aug-2015 13:38:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (68 ms) and 99% processing (37049 ms)
21-Aug-2015 13:38:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 73726
21-Aug-2015 13:38:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 41503 ms: 84.333664 rec/ms, 168.66733 cell/ms
21-Aug-2015 13:38:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (42 ms) and 99% processing (41503 ms)
21-Aug-2015 13:38:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 73057
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501171 records from 2 columns in 37218 ms: 94.07198 rec/ms, 188.14397 cell/ms
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (35 ms) and 99% processing (37218 ms)
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501171. reading next block
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 72050
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 41503 ms: 84.333664 rec/ms, 168.66733 cell/ms
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (64 ms) and 99% processing (41503 ms)
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 73871
21-Aug-2015 13:38:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 133 ms. row count = 3500100
21-Aug-2015 13:38:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1466882 records.
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 1466882
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501364 records from 2 columns in 38152 ms: 91.77406 rec/ms, 183.54813 cell/ms
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (52 ms) and 99% processing (38152 ms)
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501364. reading next block
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 72855
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 38640 ms: 90.5823 rec/ms, 181.1646 cell/ms
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (114 ms) and 99% processing (38640 ms)
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4 ms. row count = 71285
21-Aug-2015 13:38:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 67 ms. row count = 3500100
21-Aug-2015 13:38:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627682 records.
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 85 ms. row count = 3503132
21-Aug-2015 13:38:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 47 ms. row count = 3500100
21-Aug-2015 13:38:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574192 records.
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500631 records.
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 101 ms. row count = 3501130
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 120 ms. row count = 3500631
21-Aug-2015 13:38:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is 15/08/21 13:39:01 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 2125 bytes result sent to driver
15/08/21 13:39:01 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120, localhost, ANY, 1756 bytes)
15/08/21 13:39:01 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
15/08/21 13:39:01 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 48565 ms on localhost (105/170)
15/08/21 13:39:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:01 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 2125 bytes result sent to driver
15/08/21 13:39:01 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121, localhost, ANY, 1769 bytes)
15/08/21 13:39:01 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
15/08/21 13:39:01 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 48868 ms on localhost (106/170)
15/08/21 13:39:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 134217728 end: 259582440 length: 125364712 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:01 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 2125 bytes result sent to driver
15/08/21 13:39:01 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122, localhost, ANY, 1758 bytes)
15/08/21 13:39:01 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
15/08/21 13:39:01 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 43914 ms on localhost (107/170)
15/08/21 13:39:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:01 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 2125 bytes result sent to driver
15/08/21 13:39:01 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123, localhost, ANY, 1770 bytes)
15/08/21 13:39:01 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
15/08/21 13:39:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 134217728 end: 257838232 length: 123620504 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:01 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 44625 ms on localhost (108/170)
15/08/21 13:39:01 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 2125 bytes result sent to driver
15/08/21 13:39:01 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124, localhost, ANY, 1757 bytes)
15/08/21 13:39:01 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
15/08/21 13:39:01 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 44989 ms on localhost (109/170)
15/08/21 13:39:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:02 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 2125 bytes result sent to driver
15/08/21 13:39:02 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125, localhost, ANY, 1770 bytes)
15/08/21 13:39:02 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
15/08/21 13:39:02 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 44964 ms on localhost (110/170)
15/08/21 13:39:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 134217728 end: 257849235 length: 123631507 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:02 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 2125 bytes result sent to driver
15/08/21 13:39:02 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126, localhost, ANY, 1758 bytes)
15/08/21 13:39:02 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
15/08/21 13:39:02 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 45542 ms on localhost (111/170)
15/08/21 13:39:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:06 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 2125 bytes result sent to driver
15/08/21 13:39:06 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127, localhost, ANY, 1770 bytes)
15/08/21 13:39:06 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
15/08/21 13:39:06 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 11336 ms on localhost (112/170)
15/08/21 13:39:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 134217728 end: 257181348 length: 122963620 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:16 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 2125 bytes result sent to driver
15/08/21 13:39:16 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128, localhost, ANY, 1758 bytes)
15/08/21 13:39:16 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
15/08/21 13:39:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:16 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 25818 ms on localhost (113/170)
org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3648307 records.
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:38:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 67 ms. row count = 3500100
21-Aug-2015 13:39:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501115 records.
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626900 records.
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3501235
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 136 ms. row count = 3501115
21-Aug-2015 13:39:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574036 records.
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 117 ms. row count = 3500100
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 45 ms. row count = 3501487
21-Aug-2015 13:39:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501235 records.
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 90 ms. row count = 3501235
21-Aug-2015 13:39:02 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:02 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573259 records.
21-Aug-2015 13:39:02 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:02 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 106 ms. row count = 3500100
21-Aug-2015 13:39:02 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:02 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502933 records.
21-Aug-2015 13:39:02 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:02 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 3502933
21-Aug-2015 13:39:06 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:06 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571471 records.
21-Aug-2015 13:39:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 49 ms. row count = 3501332
21-Aug-2015 13:39:06 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 15624 ms: 224.02074 rec/ms, 448.04147 cell/ms
21-Aug-2015 13:39:06 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (72 ms) and 99% processing (15624 ms)
21-Aug-2015 13:39:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:39:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 72826
21-Aug-2015 13:39:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503132 records from 2 columns in 20465 ms: 171.17674 rec/ms, 342.3535 cell/ms
21-Aug-2015 13:39:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (85 ms) and 99% processing (20465 ms)
21-Aug-2015 13:39:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503132. reading next block
21-Aug-2015 13:39:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14 ms. row count = 124550
21-Aug-2015 13:39:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500833 records.
21-Aug-2015 13:39:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 120 ms. row count = 3500833
21-Aug-2015 13:39:17 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501235 records from 2 columns in 16344 ms: 214.22142 rec/ms, 428.44284 cell/ms
21-Aug-2015 13:39:17 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (46 ms) and 99% processing (16344 ms)
21-Aug-2015 13:39:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501235. reading next block
21-Aug-2015 13:39:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 125665
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501130 records from 2 columns in 25661 ms: 136.43779 rec/ms, 272.87558 cell/ms
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (101 ms) and 99% processing (25661 ms)
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501130. reading next block
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 73062
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 25568 ms: 136.89377 rec/ms, 273.78754 cell/ms
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (67 ms) and 99% processing (25568 ms)
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParq15/08/21 13:39:22 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 2125 bytes result sent to driver
15/08/21 13:39:22 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129, localhost, ANY, 1769 bytes)
15/08/21 13:39:22 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
15/08/21 13:39:22 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 28307 ms on localhost (114/170)
15/08/21 13:39:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 134217728 end: 257473792 length: 123256064 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:24 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 2125 bytes result sent to driver
15/08/21 13:39:24 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130, localhost, ANY, 1757 bytes)
15/08/21 13:39:24 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
15/08/21 13:39:24 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 27471 ms on localhost (115/170)
15/08/21 13:39:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:24 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 2125 bytes result sent to driver
15/08/21 13:39:24 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131, localhost, ANY, 1770 bytes)
15/08/21 13:39:24 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
15/08/21 13:39:24 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 28044 ms on localhost (116/170)
15/08/21 13:39:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 134217728 end: 257455806 length: 123238078 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:24 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 2125 bytes result sent to driver
15/08/21 13:39:24 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132, localhost, ANY, 1758 bytes)
15/08/21 13:39:24 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
15/08/21 13:39:24 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 27849 ms on localhost (117/170)
15/08/21 13:39:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:24 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 2125 bytes result sent to driver
15/08/21 13:39:24 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133, localhost, ANY, 1773 bytes)
15/08/21 13:39:24 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
15/08/21 13:39:24 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 28603 ms on localhost (118/170)
15/08/21 13:39:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 134217728 end: 257547934 length: 123330206 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:24 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 2125 bytes result sent to driver
15/08/21 13:39:24 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134, localhost, ANY, 1758 bytes)
15/08/21 13:39:24 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
15/08/21 13:39:24 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 23284 ms on localhost (119/170)
15/08/21 13:39:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:24 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 2125 bytes result sent to driver
15/08/21 13:39:24 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135, localhost, ANY, 1769 bytes)
15/08/21 13:39:24 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
15/08/21 13:39:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 134217728 end: 257790576 length: 123572848 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:24 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 23530 ms on localhost (120/170)
15/08/21 13:39:24 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 2125 bytes result sent to driver
15/08/21 13:39:24 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136, localhost, ANY, 1757 bytes)
15/08/21 13:39:24 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
15/08/21 13:39:24 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 28412 ms on localhost (121/170)
15/08/21 13:39:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:25 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 2125 bytes result sent to driver
15/08/21 13:39:25 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137, localhost, ANY, 1770 bytes)
15/08/21 13:39:25 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
15/08/21 13:39:25 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 28350 ms on localhost (122/170)
15/08/21 13:39:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 134217728 end: 257571649 length: 123353921 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:41 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 2125 bytes result sent to driver
15/08/21 13:39:41 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138, localhost, ANY, 1758 bytes)
15/08/21 13:39:41 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
15/08/21 13:39:41 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 40239 ms on localhost (123/170)
15/08/21 13:39:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:41 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 2125 bytes result sent to driver
15/08/21 13:39:41 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139, localhost, ANY, 1770 bytes)
15/08/21 13:39:41 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
15/08/21 13:39:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 134217728 end: 257573201 length: 123355473 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:41 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 39787 ms on localhost (124/170)
15/08/21 13:39:41 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 2125 bytes result sent to driver
15/08/21 13:39:41 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140, localhost, ANY, 1757 bytes)
15/08/21 13:39:41 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
15/08/21 13:39:41 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 39570 ms on localhost (125/170)
15/08/21 13:39:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
uetRecordReader: block read in memory in 10 ms. row count = 148207
21-Aug-2015 13:39:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501487 records from 2 columns in 21436 ms: 163.3461 rec/ms, 326.6922 cell/ms
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (45 ms) and 99% processing (21436 ms)
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501487. reading next block
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573329 records.
21-Aug-2015 13:39:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 35 ms. row count = 72549
21-Aug-2015 13:39:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 57 ms. row count = 3500100
21-Aug-2015 13:39:23 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 21155 ms: 165.45024 rec/ms, 330.90048 cell/ms
21-Aug-2015 13:39:23 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (106 ms) and 99% processing (21155 ms)
21-Aug-2015 13:39:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:39:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 73159
21-Aug-2015 13:39:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502559 records.
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571559 records.
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501332 records from 2 columns in 17575 ms: 199.2223 rec/ms, 398.4446 cell/ms
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (49 ms) and 99% processing (17575 ms)
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501332. reading next block
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 70139
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500100
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3502559
21-Aug-2015 13:39:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501043 records.
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573366 records.
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 3501043
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3501786
21-Aug-2015 13:39:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 36 ms. row count = 3500100
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574145 records.
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500100
21-Aug-2015 13:39:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501169 records.
21-Aug-2015 13:39:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 82 ms. row count = 3501169
21-Aug-2015 13:39:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573313 records.
21-Aug-2015 13:39:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 58 ms. row count = 3501584
21-Aug-2015 13:39:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501195 records.
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572913 records.
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 67 ms. row count = 3501195
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 59 ms. row count = 3500728
21-Aug-2015 13:39:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Au15/08/21 13:39:41 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 2125 bytes result sent to driver
15/08/21 13:39:41 INFO TaskSetManager: Starting task 141.0 in stage 0.0 (TID 141, localhost, ANY, 1770 bytes)
15/08/21 13:39:41 INFO Executor: Running task 141.0 in stage 0.0 (TID 141)
15/08/21 13:39:41 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 40389 ms on localhost (126/170)
15/08/21 13:39:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 134217728 end: 257466118 length: 123248390 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:41 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 2125 bytes result sent to driver
15/08/21 13:39:41 INFO TaskSetManager: Starting task 142.0 in stage 0.0 (TID 142, localhost, ANY, 1758 bytes)
15/08/21 13:39:41 INFO Executor: Running task 142.0 in stage 0.0 (TID 142)
15/08/21 13:39:41 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 39531 ms on localhost (127/170)
15/08/21 13:39:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:39:42 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 2125 bytes result sent to driver
15/08/21 13:39:42 INFO TaskSetManager: Starting task 143.0 in stage 0.0 (TID 143, localhost, ANY, 1770 bytes)
15/08/21 13:39:42 INFO Executor: Running task 143.0 in stage 0.0 (TID 143)
15/08/21 13:39:42 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 36329 ms on localhost (128/170)
15/08/21 13:39:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 134217728 end: 257888240 length: 123670512 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:02 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 2125 bytes result sent to driver
15/08/21 13:40:02 INFO TaskSetManager: Starting task 144.0 in stage 0.0 (TID 144, localhost, ANY, 1757 bytes)
15/08/21 13:40:03 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 46152 ms on localhost (129/170)
15/08/21 13:40:03 INFO Executor: Running task 144.0 in stage 0.0 (TID 144)
15/08/21 13:40:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:03 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 2125 bytes result sent to driver
15/08/21 13:40:03 INFO TaskSetManager: Starting task 145.0 in stage 0.0 (TID 145, localhost, ANY, 1768 bytes)
15/08/21 13:40:03 INFO Executor: Running task 145.0 in stage 0.0 (TID 145)
15/08/21 13:40:03 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 39015 ms on localhost (130/170)
15/08/21 13:40:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 134217728 end: 258178393 length: 123960665 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:03 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 2125 bytes result sent to driver
15/08/21 13:40:03 INFO TaskSetManager: Starting task 146.0 in stage 0.0 (TID 146, localhost, ANY, 1758 bytes)
15/08/21 13:40:03 INFO Executor: Running task 146.0 in stage 0.0 (TID 146)
15/08/21 13:40:03 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 39107 ms on localhost (131/170)
15/08/21 13:40:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:03 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 2125 bytes result sent to driver
15/08/21 13:40:03 INFO TaskSetManager: Starting task 147.0 in stage 0.0 (TID 147, localhost, ANY, 1771 bytes)
15/08/21 13:40:03 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 40793 ms on localhost (132/170)
15/08/21 13:40:03 INFO Executor: Running task 147.0 in stage 0.0 (TID 147)
15/08/21 13:40:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 134217728 end: 257798680 length: 123580952 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:03 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 2125 bytes result sent to driver
15/08/21 13:40:03 INFO TaskSetManager: Starting task 148.0 in stage 0.0 (TID 148, localhost, ANY, 1757 bytes)
15/08/21 13:40:03 INFO Executor: Running task 148.0 in stage 0.0 (TID 148)
15/08/21 13:40:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:03 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 39645 ms on localhost (133/170)
g-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501339 records.
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 68 ms. row count = 3501339
21-Aug-2015 13:39:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572985 records.
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 3500100
21-Aug-2015 13:39:41 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:42 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:39:42 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:42 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 71 ms. row count = 3500100
21-Aug-2015 13:39:42 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:39:42 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
21-Aug-2015 13:39:42 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:39:42 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3501076
21-Aug-2015 13:39:55 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 31707 ms: 110.38887 rec/ms, 220.77774 cell/ms
21-Aug-2015 13:39:55 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (31707 ms)
21-Aug-2015 13:39:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:39:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 71459
21-Aug-2015 13:39:56 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 33298 ms: 105.11442 rec/ms, 210.22884 cell/ms
21-Aug-2015 13:39:56 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (57 ms) and 99% processing (33298 ms)
21-Aug-2015 13:39:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:39:56 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501786 records from 2 columns in 31999 ms: 109.434235 rec/ms, 218.86847 cell/ms
21-Aug-2015 13:39:56 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (31999 ms)
21-Aug-2015 13:39:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501786. reading next block
21-Aug-2015 13:39:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 71580
21-Aug-2015 13:39:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 22 ms. row count = 73229
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32370 ms: 108.1279 rec/ms, 216.2558 cell/ms
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (32370 ms)
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 74045
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501584 records from 2 columns in 32373 ms: 108.16372 rec/ms, 216.32744 cell/ms
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (58 ms) and 99% processing (32373 ms)
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501584. reading next block
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 71729
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500728 records from 2 columns in 16442 ms: 212.91376 rec/ms, 425.8275 cell/ms
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (59 ms) and 99% processing (16442 ms)
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500728. reading next block
21-Aug-2015 13:39:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 72185
21-Aug-2015 13:40:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573846 records.
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 125 ms. row count = 3500100
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 108 ms. row count = 3500100
21-Aug-2015 13:40:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501395 records.
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501076 records from 2 columns in 20627 ms: 169.73268 rec/ms, 339.46536 cell/ms
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (53 ms) and 99% processing (20627 ms)
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501076. reading next block
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 3501395
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 73262
21-Aug-2015 13:40:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572381 records.
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 112 ms. row count = 3500100
21-Aug-2015 13:40:03 WARNING: parquet.hadoop.ParquetRecordReader: Ca15/08/21 13:40:04 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 2125 bytes result sent to driver
15/08/21 13:40:04 INFO TaskSetManager: Starting task 149.0 in stage 0.0 (TID 149, localhost, ANY, 1771 bytes)
15/08/21 13:40:04 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 39512 ms on localhost (134/170)
15/08/21 13:40:04 INFO Executor: Running task 149.0 in stage 0.0 (TID 149)
15/08/21 13:40:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 134217728 end: 257837778 length: 123620050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:04 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 2125 bytes result sent to driver
15/08/21 13:40:04 INFO TaskSetManager: Starting task 150.0 in stage 0.0 (TID 150, localhost, ANY, 1757 bytes)
15/08/21 13:40:04 INFO Executor: Running task 150.0 in stage 0.0 (TID 150)
15/08/21 13:40:04 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 23561 ms on localhost (135/170)
15/08/21 13:40:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:05 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 2125 bytes result sent to driver
15/08/21 13:40:05 INFO TaskSetManager: Starting task 151.0 in stage 0.0 (TID 151, localhost, ANY, 1771 bytes)
15/08/21 13:40:05 INFO Executor: Running task 151.0 in stage 0.0 (TID 151)
15/08/21 13:40:05 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 40524 ms on localhost (136/170)
15/08/21 13:40:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 134217728 end: 257748250 length: 123530522 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:05 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 2125 bytes result sent to driver
15/08/21 13:40:05 INFO TaskSetManager: Starting task 152.0 in stage 0.0 (TID 152, localhost, ANY, 1757 bytes)
15/08/21 13:40:05 INFO Executor: Running task 152.0 in stage 0.0 (TID 152)
15/08/21 13:40:05 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 41297 ms on localhost (137/170)
15/08/21 13:40:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:05 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 2125 bytes result sent to driver
15/08/21 13:40:05 INFO TaskSetManager: Starting task 153.0 in stage 0.0 (TID 153, localhost, ANY, 1772 bytes)
15/08/21 13:40:05 INFO Executor: Running task 153.0 in stage 0.0 (TID 153)
15/08/21 13:40:05 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 24191 ms on localhost (138/170)
15/08/21 13:40:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 134217728 end: 257331238 length: 123113510 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:06 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 2125 bytes result sent to driver
15/08/21 13:40:06 INFO TaskSetManager: Starting task 154.0 in stage 0.0 (TID 154, localhost, ANY, 1758 bytes)
15/08/21 13:40:06 INFO Executor: Running task 154.0 in stage 0.0 (TID 154)
15/08/21 13:40:06 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 40992 ms on localhost (139/170)
15/08/21 13:40:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:06 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 2125 bytes result sent to driver
15/08/21 13:40:06 INFO TaskSetManager: Starting task 155.0 in stage 0.0 (TID 155, localhost, ANY, 1771 bytes)
15/08/21 13:40:06 INFO Executor: Running task 155.0 in stage 0.0 (TID 155)
15/08/21 13:40:06 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 24893 ms on localhost (140/170)
15/08/21 13:40:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 134217728 end: 257176539 length: 122958811 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:06 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 2125 bytes result sent to driver
15/08/21 13:40:06 INFO TaskSetManager: Starting task 156.0 in stage 0.0 (TID 156, localhost, ANY, 1758 bytes)
15/08/21 13:40:06 INFO Executor: Running task 156.0 in stage 0.0 (TID 156)
15/08/21 13:40:06 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 41867 ms on localhost (141/170)
15/08/21 13:40:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:12 INFO Executor: Finished task 143.0 in stage 0.0 (TID 143). 2125 bytes result sent to driver
15/08/21 13:40:12 INFO TaskSetManager: Starting task 157.0 in stage 0.0 (TID 157, localhost, ANY, 1771 bytes)
15/08/21 13:40:12 INFO Executor: Running task 157.0 in stage 0.0 (TID 157)
15/08/21 13:40:12 INFO TaskSetManager: Finished task 143.0 in stage 0.0 (TID 143) in 29546 ms on localhost (142/170)
15/08/21 13:40:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 134217728 end: 257446174 length: 123228446 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:13 INFO Executor: Finished task 141.0 in stage 0.0 (TID 141). 2125 bytes result sent to driver
15/08/21 13:40:13 INFO TaskSetManager: Starting task 158.0 in stage 0.0 (TID 158, localhost, ANY, 1758 bytes)
15/08/21 13:40:13 INFO Executor: Running task 158.0 in stage 0.0 (TID 158)
15/08/21 13:40:13 INFO TaskSetManager: Finished task 141.0 in stage 0.0 (TID 141) in 31639 ms on localhost (143/170)
15/08/21 13:40:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:13 INFO Executor: Finished task 142.0 in stage 0.0 (TID 142). 2125 bytes result sent to driver
15/08/21 13:40:13 INFO TaskSetManager: Starting task 159.0 in stage 0.0 (TID 159, localhost, ANY, 1769 bytes)
15/08/21 13:40:13 INFO Executor: Running task 159.0 in stage 0.0 (TID 159)
15/08/21 13:40:13 INFO TaskSetManager: Finished task 142.0 in stage 0.0 (TID 142) in 31557 ms on localhost (144/170)
15/08/21 13:40:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 134217728 end: 259884384 length: 125666656 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
n not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
21-Aug-2015 13:40:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 69 ms. row count = 3501121
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22406 ms: 156.21262 rec/ms, 312.42523 cell/ms
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (42 ms) and 99% processing (22406 ms)
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 72885
21-Aug-2015 13:40:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573120 records.
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 108 ms. row count = 3500100
21-Aug-2015 13:40:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:40:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 103 ms. row count = 3500100
21-Aug-2015 13:40:05 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573993 records.
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 111 ms. row count = 3500100
21-Aug-2015 13:40:05 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500100
21-Aug-2015 13:40:05 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 3500100
21-Aug-2015 13:40:06 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503161 records.
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 73 ms. row count = 3503161
21-Aug-2015 13:40:06 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571357 records.
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3501317
21-Aug-2015 13:40:06 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501689 records.
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3501689
21-Aug-2015 13:40:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572541 records.
21-Aug-2015 13:40:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 143 ms. row count = 3500100
21-Aug-2015 13:40:13 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:13 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:13 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501046 records.
21-Aug-2015 13:40:13 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:13 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626733 records.
21-Aug-2015 13:40:13 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:13 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 209 ms. row count = 3501046
21-Aug-2015 13:40:13 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 263 ms. row count = 3500100
21-Aug-2015 13:40:26 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 23462 ms: 149.18166 rec/ms, 298.3633 cell/ms
21-Aug-2015 13:40:26 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (108 ms) and 99% processing (23462 ms)
21-Aug-2015 13:40:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:40:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 73746
21-Aug-2015 13:40:28 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 24465 ms: 143.0656 rec/ms, 286.1312 cell/ms
21-Aug-2015 13:40:28 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (112 ms) and 99% processing (24465 ms)15/08/21 13:40:53 INFO Executor: Finished task 145.0 in stage 0.0 (TID 145). 2125 bytes result sent to driver
15/08/21 13:40:53 INFO TaskSetManager: Starting task 160.0 in stage 0.0 (TID 160, localhost, ANY, 1758 bytes)
15/08/21 13:40:53 INFO Executor: Running task 160.0 in stage 0.0 (TID 160)
15/08/21 13:40:53 INFO TaskSetManager: Finished task 145.0 in stage 0.0 (TID 145) in 50354 ms on localhost (145/170)
15/08/21 13:40:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:53 INFO Executor: Finished task 146.0 in stage 0.0 (TID 146). 2125 bytes result sent to driver
15/08/21 13:40:53 INFO TaskSetManager: Starting task 161.0 in stage 0.0 (TID 161, localhost, ANY, 1771 bytes)
15/08/21 13:40:53 INFO Executor: Running task 161.0 in stage 0.0 (TID 161)
15/08/21 13:40:53 INFO TaskSetManager: Finished task 146.0 in stage 0.0 (TID 146) in 50620 ms on localhost (146/170)
15/08/21 13:40:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 134217728 end: 259183553 length: 124965825 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:54 INFO Executor: Finished task 144.0 in stage 0.0 (TID 144). 2125 bytes result sent to driver
15/08/21 13:40:54 INFO TaskSetManager: Starting task 162.0 in stage 0.0 (TID 162, localhost, ANY, 1757 bytes)
15/08/21 13:40:54 INFO Executor: Running task 162.0 in stage 0.0 (TID 162)
15/08/21 13:40:54 INFO TaskSetManager: Finished task 144.0 in stage 0.0 (TID 144) in 51428 ms on localhost (147/170)
15/08/21 13:40:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:54 INFO Executor: Finished task 147.0 in stage 0.0 (TID 147). 2125 bytes result sent to driver
15/08/21 13:40:54 INFO TaskSetManager: Starting task 163.0 in stage 0.0 (TID 163, localhost, ANY, 1771 bytes)
15/08/21 13:40:54 INFO Executor: Running task 163.0 in stage 0.0 (TID 163)
15/08/21 13:40:54 INFO TaskSetManager: Finished task 147.0 in stage 0.0 (TID 147) in 50998 ms on localhost (148/170)
15/08/21 13:40:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 134217728 end: 257504450 length: 123286722 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:55 INFO Executor: Finished task 148.0 in stage 0.0 (TID 148). 2125 bytes result sent to driver
15/08/21 13:40:55 INFO TaskSetManager: Starting task 164.0 in stage 0.0 (TID 164, localhost, ANY, 1758 bytes)
15/08/21 13:40:55 INFO Executor: Running task 164.0 in stage 0.0 (TID 164)
15/08/21 13:40:55 INFO TaskSetManager: Finished task 148.0 in stage 0.0 (TID 148) in 51105 ms on localhost (149/170)
15/08/21 13:40:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:58 INFO Executor: Finished task 149.0 in stage 0.0 (TID 149). 2125 bytes result sent to driver
15/08/21 13:40:58 INFO TaskSetManager: Starting task 165.0 in stage 0.0 (TID 165, localhost, ANY, 1769 bytes)
15/08/21 13:40:58 INFO Executor: Running task 165.0 in stage 0.0 (TID 165)
15/08/21 13:40:58 INFO TaskSetManager: Finished task 149.0 in stage 0.0 (TID 149) in 53721 ms on localhost (150/170)
15/08/21 13:40:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 134217728 end: 257173847 length: 122956119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:58 INFO Executor: Finished task 150.0 in stage 0.0 (TID 150). 2125 bytes result sent to driver
15/08/21 13:40:58 INFO TaskSetManager: Starting task 166.0 in stage 0.0 (TID 166, localhost, ANY, 1758 bytes)
15/08/21 13:40:58 INFO Executor: Running task 166.0 in stage 0.0 (TID 166)
15/08/21 13:40:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:58 INFO TaskSetManager: Finished task 150.0 in stage 0.0 (TID 150) in 53430 ms on localhost (151/170)
15/08/21 13:40:58 INFO Executor: Finished task 151.0 in stage 0.0 (TID 151). 2125 bytes result sent to driver
15/08/21 13:40:58 INFO TaskSetManager: Starting task 167.0 in stage 0.0 (TID 167, localhost, ANY, 1770 bytes)
15/08/21 13:40:58 INFO Executor: Running task 167.0 in stage 0.0 (TID 167)
15/08/21 13:40:58 INFO TaskSetManager: Finished task 151.0 in stage 0.0 (TID 151) in 53133 ms on localhost (152/170)
15/08/21 13:40:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 134217728 end: 257494956 length: 123277228 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:58 INFO Executor: Finished task 155.0 in stage 0.0 (TID 155). 2125 bytes result sent to driver
15/08/21 13:40:58 INFO TaskSetManager: Starting task 168.0 in stage 0.0 (TID 168, localhost, ANY, 1757 bytes)
15/08/21 13:40:58 INFO Executor: Running task 168.0 in stage 0.0 (TID 168)
15/08/21 13:40:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:58 INFO TaskSetManager: Finished task 155.0 in stage 0.0 (TID 155) in 52641 ms on localhost (153/170)

21-Aug-2015 13:40:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:40:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 12 ms. row count = 72281
21-Aug-2015 13:40:52 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 47555 ms: 73.6011 rec/ms, 147.2022 cell/ms
21-Aug-2015 13:40:52 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (108 ms) and 99% processing (47555 ms)
21-Aug-2015 13:40:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:40:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 73020
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 47663 ms: 73.43432 rec/ms, 146.86864 cell/ms
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (111 ms) and 99% processing (47663 ms)
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 73893
21-Aug-2015 13:40:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501221 records.
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 101 ms. row count = 3501221
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501317 records from 2 columns in 47578 ms: 73.591095 rec/ms, 147.18219 cell/ms
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (53 ms) and 99% processing (47578 ms)
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501317. reading next block
21-Aug-2015 13:40:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 70040
21-Aug-2015 13:40:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627071 records.
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 48142 ms: 72.70367 rec/ms, 145.40733 cell/ms
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (54 ms) and 99% processing (48142 ms)
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 73871
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 109 ms. row count = 3503161
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 41605 ms: 84.12691 rec/ms, 168.25381 cell/ms
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (143 ms) and 99% processing (41605 ms)
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 72441
21-Aug-2015 13:40:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 66 ms. row count = 3500100
21-Aug-2015 13:40:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573920 records.
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 94 ms. row count = 3500779
21-Aug-2015 13:40:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501110 records.
21-Aug-2015 13:40:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 90 ms. row count = 3501110
21-Aug-2015 13:40:57 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 44023 ms: 79.506165 rec/ms, 159.01233 cell/ms
21-Aug-2015 13:40:57 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (263 ms) and 99% processing (44023 ms)
21-Aug-2015 13:40:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 13:40:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 126633
21-Aug-2015 13:40:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573142 records.
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 34 ms. row count = 3502917
21-Aug-2015 13:40:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573577 records.
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500519 records.
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 48 ms. row count = 3500519
21-Aug-2015 13:40:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 83 ms. row count = 3500100
21-Aug-2015 13:40:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but i15/08/21 13:40:59 INFO Executor: Finished task 157.0 in stage 0.0 (TID 157). 2125 bytes result sent to driver
15/08/21 13:40:59 INFO TaskSetManager: Starting task 169.0 in stage 0.0 (TID 169, localhost, ANY, 1771 bytes)
15/08/21 13:40:59 INFO Executor: Running task 169.0 in stage 0.0 (TID 169)
15/08/21 13:40:59 INFO TaskSetManager: Finished task 157.0 in stage 0.0 (TID 157) in 46820 ms on localhost (154/170)
15/08/21 13:40:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 134217728 end: 257035718 length: 122817990 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:40:59 INFO Executor: Finished task 154.0 in stage 0.0 (TID 154). 2125 bytes result sent to driver
15/08/21 13:40:59 INFO Executor: Finished task 156.0 in stage 0.0 (TID 156). 2125 bytes result sent to driver
15/08/21 13:40:59 INFO TaskSetManager: Finished task 156.0 in stage 0.0 (TID 156) in 52854 ms on localhost (155/170)
15/08/21 13:40:59 INFO TaskSetManager: Finished task 154.0 in stage 0.0 (TID 154) in 53176 ms on localhost (156/170)
15/08/21 13:40:59 INFO Executor: Finished task 152.0 in stage 0.0 (TID 152). 2125 bytes result sent to driver
15/08/21 13:40:59 INFO TaskSetManager: Finished task 152.0 in stage 0.0 (TID 152) in 53954 ms on localhost (157/170)
15/08/21 13:40:59 INFO Executor: Finished task 153.0 in stage 0.0 (TID 153). 2125 bytes result sent to driver
15/08/21 13:40:59 INFO TaskSetManager: Finished task 153.0 in stage 0.0 (TID 153) in 53745 ms on localhost (158/170)
15/08/21 13:41:04 INFO Executor: Finished task 159.0 in stage 0.0 (TID 159). 2125 bytes result sent to driver
15/08/21 13:41:04 INFO TaskSetManager: Finished task 159.0 in stage 0.0 (TID 159) in 50734 ms on localhost (159/170)
15/08/21 13:41:04 INFO Executor: Finished task 158.0 in stage 0.0 (TID 158). 2125 bytes result sent to driver
15/08/21 13:41:04 INFO TaskSetManager: Finished task 158.0 in stage 0.0 (TID 158) in 51500 ms on localhost (160/170)
15/08/21 13:41:12 INFO Executor: Finished task 160.0 in stage 0.0 (TID 160). 2125 bytes result sent to driver
15/08/21 13:41:12 INFO TaskSetManager: Finished task 160.0 in stage 0.0 (TID 160) in 18838 ms on localhost (161/170)
15/08/21 13:41:12 INFO Executor: Finished task 162.0 in stage 0.0 (TID 162). 2125 bytes result sent to driver
15/08/21 13:41:12 INFO TaskSetManager: Finished task 162.0 in stage 0.0 (TID 162) in 18229 ms on localhost (162/170)
15/08/21 13:41:12 INFO Executor: Finished task 161.0 in stage 0.0 (TID 161). 2125 bytes result sent to driver
15/08/21 13:41:12 INFO TaskSetManager: Finished task 161.0 in stage 0.0 (TID 161) in 18966 ms on localhost (163/170)
15/08/21 13:41:13 INFO Executor: Finished task 163.0 in stage 0.0 (TID 163). 2125 bytes result sent to driver
15/08/21 13:41:13 INFO TaskSetManager: Finished task 163.0 in stage 0.0 (TID 163) in 18468 ms on localhost (164/170)
15/08/21 13:41:13 INFO Executor: Finished task 164.0 in stage 0.0 (TID 164). 2125 bytes result sent to driver
15/08/21 13:41:13 INFO TaskSetManager: Finished task 164.0 in stage 0.0 (TID 164) in 18294 ms on localhost (165/170)
15/08/21 13:41:14 INFO Executor: Finished task 166.0 in stage 0.0 (TID 166). 2125 bytes result sent to driver
15/08/21 13:41:14 INFO TaskSetManager: Finished task 166.0 in stage 0.0 (TID 166) in 15773 ms on localhost (166/170)
15/08/21 13:41:14 INFO Executor: Finished task 167.0 in stage 0.0 (TID 167). 2125 bytes result sent to driver
15/08/21 13:41:14 INFO TaskSetManager: Finished task 167.0 in stage 0.0 (TID 167) in 16117 ms on localhost (167/170)
15/08/21 13:41:14 INFO Executor: Finished task 165.0 in stage 0.0 (TID 165). 2125 bytes result sent to driver
15/08/21 13:41:14 INFO TaskSetManager: Finished task 165.0 in stage 0.0 (TID 165) in 16430 ms on localhost (168/170)
15/08/21 13:41:14 INFO Executor: Finished task 168.0 in stage 0.0 (TID 168). 2125 bytes result sent to driver
15/08/21 13:41:14 INFO TaskSetManager: Finished task 168.0 in stage 0.0 (TID 168) in 15829 ms on localhost (169/170)
15/08/21 13:41:15 INFO Executor: Finished task 169.0 in stage 0.0 (TID 169). 2125 bytes result sent to driver
15/08/21 13:41:15 INFO TaskSetManager: Finished task 169.0 in stage 0.0 (TID 169) in 16140 ms on localhost (170/170)
15/08/21 13:41:15 INFO DAGScheduler: ShuffleMapStage 0 (processCmd at CliDriver.java:423) finished in 442.420 s
15/08/21 13:41:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/21 13:41:15 INFO DAGScheduler: looking for newly runnable stages
15/08/21 13:41:15 INFO DAGScheduler: running: Set()
15/08/21 13:41:15 INFO DAGScheduler: waiting: Set(ResultStage 1)
15/08/21 13:41:15 INFO DAGScheduler: failed: Set()
15/08/21 13:41:15 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4e21fd57
15/08/21 13:41:15 INFO DAGScheduler: Missing parents for ResultStage 1: List()
15/08/21 13:41:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 13:41:15 INFO StatsReportListener: task runtime:(count: 170, mean: 41079.217647, stdev: 12594.664573, max: 72118.000000, min: 11336.000000)
15/08/21 13:41:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:41:15 INFO StatsReportListener: 	11.3 s	18.5 s	25.6 s	31.6 s	41.3 s	47.6 s	54.6 s	1.1 min	1.2 min
15/08/21 13:41:15 INFO StatsReportListener: shuffle bytes written:(count: 170, mean: 9629242.888235, stdev: 468850.402633, max: 10377140.000000, min: 4098365.000000)
15/08/21 13:41:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:41:15 INFO StatsReportListener: 	3.9 MB	9.0 MB	9.0 MB	9.1 MB	9.2 MB	9.3 MB	9.5 MB	9.6 MB	9.9 MB
15/08/21 13:41:15 INFO StatsReportListener: task result size:(count: 170, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 13:41:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:41:15 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 13:41:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 170, mean: 99.660179, stdev: 0.898231, max: 99.948104, min: 90.319667)
15/08/21 13:41:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:41:15 INFO StatsReportListener: 	90 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 13:41:15 INFO StatsReportListener: other time pct: (count: 170, mean: 0.339821, stdev: 0.898231, max: 9.680333, min: 0.051896)
15/08/21 13:41:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:41:15 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	10 %
15/08/21 13:41:15 INFO MemoryStore: ensureFreeSpace(78888) called with curMem=363081, maxMem=22226833244
15/08/21 13:41:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 77.0 KB, free 20.7 GB)
15/08/21 13:41:15 INFO MemoryStore: ensureFreeSpace(30054) called with curMem=441969, maxMem=22226833244
15/08/21 13:41:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.3 KB, free 20.7 GB)
15/08/21 13:41:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:45550 (size: 29.3 KB, free: 20.7 GB)
15/08/21 13:41:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
15/08/21 13:41:15 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423)
15/08/21 13:41:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
15/08/21 13:41:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:41:15 INFO Executor: Running task 3.0 in stage 1.0 (TID 173)
15/08/21 13:41:15 INFO Executor: Running task 1.0 in stage 1.0 (TID 171)
15/08/21 13:41:15 INFO Executor: Running task 5.0 in stage 1.0 (TID 175)
15/08/21 13:41:15 INFO Executor: Running task 4.0 in stage 1.0 (TID 174)
15/08/21 13:41:15 INFO Executor: Running task 2.0 in stage 1.0 (TID 172)
15/08/21 13:41:15 INFO Executor: Running task 6.0 in stage 1.0 (TID 176)
15/08/21 13:41:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 170)
15/08/21 13:41:15 INFO Executor: Running task 7.0 in stage 1.0 (TID 177)
15/08/21 13:41:15 INFO Executor: Running task 8.0 in stage 1.0 (TID 178)
15/08/21 13:41:15 INFO Executor: Running task 10.0 in stage 1.0 (TID 180)
15/08/21 13:41:15 INFO Executor: Running task 12.0 in stage 1.0 (TID 182)
15/08/21 13:41:15 INFO Executor: Running task 9.0 in stage 1.0 (TID 179)
15/08/21 13:41:15 INFO Executor: Running task 11.0 in stage 1.0 (TID 181)
15/08/21 13:41:15 INFO Executor: Running task 13.0 in stage 1.0 (TID 183)
15/08/21 13:41:15 INFO Executor: Running task 15.0 in stage 1.0 (TID 185)
15/08/21 13:41:15 INFO Executor: Running task 14.0 in stage 1.0 (TID 184)
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:41:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:41:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:45550 in memory (size: 4.4 KB, free: 20.7 GB)
15/08/21 13:41:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:52 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:52 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:52 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/21 13:41:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:52 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:52 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:52 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:52 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:59 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:59 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:41:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:41:59 INFO CodecConfig: Compression: GZIP
15/08/21 13:41:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:41:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:41:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:41:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:41:59 INFO ParquetOutputFormat: Validation is off
15/08/21 13:41:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:41:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,681
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,665,013B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,894B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 833,233B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,951B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 320 entries, 2,560B raw, 320B comp}
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,665,833B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,714B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 832,984B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,702B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,664,920B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,801B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 833,650B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,368B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,671,085B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,966B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 833,328B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,046B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,670,778B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,659B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 834,080B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,798B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,670,872B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,753B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 834,621B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,339B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,664,829B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,710B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 833,548B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,266B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,671,125B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,006B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 836,802B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 836,520B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000002
15/08/21 13:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000008
15/08/21 13:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000015
15/08/21 13:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000010
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000012
15/08/21 13:42:01 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000002_0: Committed
15/08/21 13:42:01 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000008_0: Committed
15/08/21 13:42:01 INFO Executor: Finished task 2.0 in stage 1.0 (TID 172). 843 bytes result sent to driver
15/08/21 13:42:01 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000012_0: Committed
15/08/21 13:42:01 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000015_0: Committed
15/08/21 13:42:01 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:01 INFO Executor: Finished task 12.0 in stage 1.0 (TID 182). 843 bytes result sent to driver
15/08/21 13:42:01 INFO Executor: Finished task 8.0 in stage 1.0 (TID 178). 843 bytes result sent to driver
15/08/21 13:42:01 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000010_0: Committed
15/08/21 13:42:01 INFO Executor: Running task 16.0 in stage 1.0 (TID 186)
15/08/21 13:42:01 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:01 INFO Executor: Finished task 10.0 in stage 1.0 (TID 180). 843 bytes result sent to driver
15/08/21 13:42:01 INFO Executor: Finished task 15.0 in stage 1.0 (TID 185). 843 bytes result sent to driver
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,665,058B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,939B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 172) in 46445 ms on localhost (1/200)
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 832,232B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,950B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:42:01 INFO Executor: Running task 17.0 in stage 1.0 (TID 187)
15/08/21 13:42:01 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 182) in 46445 ms on localhost (2/200)
15/08/21 13:42:01 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:01 INFO Executor: Running task 18.0 in stage 1.0 (TID 188)
15/08/21 13:42:01 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:01 INFO Executor: Running task 19.0 in stage 1.0 (TID 189)
15/08/21 13:42:01 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:01 INFO Executor: Running task 20.0 in stage 1.0 (TID 190)
15/08/21 13:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000007
15/08/21 13:42:01 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000007_0: Committed
15/08/21 13:42:01 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 178) in 46463 ms on localhost (3/200)
15/08/21 13:42:01 INFO Executor: Finished task 7.0 in stage 1.0 (TID 177). 843 bytes result sent to driver
15/08/21 13:42:01 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000000
15/08/21 13:42:01 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000000_0: Committed
15/08/21 13:42:01 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 177) in 46470 ms on localhost (4/200)
15/08/21 13:42:01 INFO Executor: Running task 21.0 in stage 1.0 (TID 191)
15/08/21 13:42:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000009
15/08/21 13:42:01 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 185) in 46466 ms on localhost (5/200)
15/08/21 13:42:01 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000009_0: Committed
15/08/21 13:42:01 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 180) in 46471 ms on localhost (6/200)
15/08/21 13:42:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 170). 843 bytes result sent to driver
15/08/21 13:42:01 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:01 INFO Executor: Running task 22.0 in stage 1.0 (TID 192)
15/08/21 13:42:01 INFO Executor: Finished task 9.0 in stage 1.0 (TID 179). 843 bytes result sent to driver
15/08/21 13:42:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:42:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 170) in 46488 ms on localhost (7/200)
15/08/21 13:42:01 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:01 INFO Executor: Running task 23.0 in stage 1.0 (TID 193)
15/08/21 13:42:01 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 179) in 46485 ms on localhost (8/200)
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 2,671,049B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,930B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:01 INFO ColumnChunkPageWriteStore: written 834,267B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,985B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:42:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000011
15/08/21 13:42:02 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000011_0: Committed
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:02 INFO Executor: Finished task 11.0 in stage 1.0 (TID 181). 843 bytes result sent to driver
15/08/21 13:42:02 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:02 INFO Executor: Running task 24.0 in stage 1.0 (TID 194)
15/08/21 13:42:02 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 181) in 46645 ms on localhost (9/200)
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/21 13:42:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000005
15/08/21 13:42:02 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000005_0: Committed
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 2,664,834B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,715B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:02 INFO Executor: Finished task 5.0 in stage 1.0 (TID 175). 843 bytes result sent to driver
15/08/21 13:42:02 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:02 INFO Executor: Running task 25.0 in stage 1.0 (TID 195)
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 833,045B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,763B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:42:02 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 175) in 46682 ms on localhost (10/200)
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:42:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 13:42:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000003
15/08/21 13:42:02 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000003_0: Committed
15/08/21 13:42:02 INFO Executor: Finished task 3.0 in stage 1.0 (TID 173). 843 bytes result sent to driver
15/08/21 13:42:02 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:02 INFO Executor: Running task 26.0 in stage 1.0 (TID 196)
15/08/21 13:42:02 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 173) in 46806 ms on localhost (11/200)
15/08/21 13:42:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 2,670,799B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,680B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 832,054B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,772B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 2,664,882B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,763B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 833,967B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,685B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 2,665,860B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,741B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 833,117B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,835B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:42:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:42:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000006
15/08/21 13:42:02 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000006_0: Committed
15/08/21 13:42:02 INFO Executor: Finished task 6.0 in stage 1.0 (TID 176). 843 bytes result sent to driver
15/08/21 13:42:02 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:02 INFO Executor: Running task 27.0 in stage 1.0 (TID 197)
15/08/21 13:42:02 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 176) in 46995 ms on localhost (12/200)
15/08/21 13:42:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000001
15/08/21 13:42:02 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000001_0: Committed
15/08/21 13:42:02 INFO Executor: Finished task 1.0 in stage 1.0 (TID 171). 843 bytes result sent to driver
15/08/21 13:42:02 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:02 INFO Executor: Running task 28.0 in stage 1.0 (TID 198)
15/08/21 13:42:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000004
15/08/21 13:42:02 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000004_0: Committed
15/08/21 13:42:02 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 171) in 47027 ms on localhost (13/200)
15/08/21 13:42:02 INFO Executor: Finished task 4.0 in stage 1.0 (TID 174). 843 bytes result sent to driver
15/08/21 13:42:02 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:02 INFO Executor: Running task 29.0 in stage 1.0 (TID 199)
15/08/21 13:42:02 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 174) in 47040 ms on localhost (14/200)
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 2,671,003B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,884B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 833,807B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,525B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:42:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000013
15/08/21 13:42:02 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000013_0: Committed
15/08/21 13:42:02 INFO Executor: Finished task 13.0 in stage 1.0 (TID 183). 843 bytes result sent to driver
15/08/21 13:42:02 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:02 INFO Executor: Running task 30.0 in stage 1.0 (TID 200)
15/08/21 13:42:02 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 183) in 47177 ms on localhost (15/200)
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 2,671,226B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,107B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:02 INFO ColumnChunkPageWriteStore: written 833,273B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,991B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:42:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211341_0001_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211341_0001_m_000014
15/08/21 13:42:02 INFO SparkHadoopMapRedUtil: attempt_201508211341_0001_m_000014_0: Committed
15/08/21 13:42:02 INFO Executor: Finished task 14.0 in stage 1.0 (TID 184). 843 bytes result sent to driver
15/08/21 13:42:02 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:02 INFO Executor: Running task 31.0 in stage 1.0 (TID 201)
15/08/21 13:42:02 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 184) in 47384 ms on localhost (16/200)
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:42:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:21 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:21 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:21 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:21 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:42:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:42:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:42:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:42:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:42:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:42:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:42:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:42:28 INFO ColumnChunkPageWriteStore: written 2,664,826B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,707B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:28 INFO ColumnChunkPageWriteStore: written 832,355B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,073B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:42:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000026
15/08/21 13:42:28 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000026_0: Committed
15/08/21 13:42:28 INFO ColumnChunkPageWriteStore: written 2,671,426B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,307B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:28 INFO Executor: Finished task 26.0 in stage 1.0 (TID 196). 843 bytes result sent to driver
15/08/21 13:42:28 INFO ColumnChunkPageWriteStore: written 834,577B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,295B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:42:28 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:28 INFO Executor: Running task 32.0 in stage 1.0 (TID 202)
15/08/21 13:42:28 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 196) in 26410 ms on localhost (17/200)
15/08/21 13:42:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:42:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000016
15/08/21 13:42:28 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000016_0: Committed
15/08/21 13:42:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:28 INFO Executor: Finished task 16.0 in stage 1.0 (TID 186). 843 bytes result sent to driver
15/08/21 13:42:28 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:28 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 186) in 26868 ms on localhost (18/200)
15/08/21 13:42:28 INFO Executor: Running task 33.0 in stage 1.0 (TID 203)
15/08/21 13:42:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:42:28 INFO ColumnChunkPageWriteStore: written 2,665,097B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,978B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:28 INFO ColumnChunkPageWriteStore: written 833,850B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,568B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:42:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:28 INFO ColumnChunkPageWriteStore: written 2,671,512B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,393B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:28 INFO ColumnChunkPageWriteStore: written 834,404B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,122B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:42:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000018
15/08/21 13:42:28 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000018_0: Committed
15/08/21 13:42:28 INFO Executor: Finished task 18.0 in stage 1.0 (TID 188). 843 bytes result sent to driver
15/08/21 13:42:28 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:28 INFO Executor: Running task 34.0 in stage 1.0 (TID 204)
15/08/21 13:42:28 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 188) in 27021 ms on localhost (19/200)
15/08/21 13:42:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000023
15/08/21 13:42:28 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000023_0: Committed
15/08/21 13:42:28 INFO Executor: Finished task 23.0 in stage 1.0 (TID 193). 843 bytes result sent to driver
15/08/21 13:42:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:28 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:28 INFO Executor: Running task 35.0 in stage 1.0 (TID 205)
15/08/21 13:42:28 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 193) in 27052 ms on localhost (20/200)
15/08/21 13:42:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 2,671,237B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,118B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 833,850B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,568B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:42:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:42:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:42:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:42:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000021
15/08/21 13:42:29 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000021_0: Committed
15/08/21 13:42:29 INFO Executor: Finished task 21.0 in stage 1.0 (TID 191). 843 bytes result sent to driver
15/08/21 13:42:29 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:29 INFO Executor: Running task 36.0 in stage 1.0 (TID 206)
15/08/21 13:42:29 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 191) in 27363 ms on localhost (21/200)
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 2,671,314B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,195B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 830,496B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,214B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:42:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 2,670,946B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,827B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 833,075B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,793B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:42:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000024
15/08/21 13:42:29 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000024_0: Committed
15/08/21 13:42:29 INFO Executor: Finished task 24.0 in stage 1.0 (TID 194). 843 bytes result sent to driver
15/08/21 13:42:29 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:29 INFO Executor: Running task 37.0 in stage 1.0 (TID 207)
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:29 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 194) in 27321 ms on localhost (22/200)
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 2,664,899B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,780B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 2,665,155B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,036B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 833,825B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,543B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 834,265B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,983B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:42:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000025
15/08/21 13:42:29 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000025_0: Committed
15/08/21 13:42:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000029
15/08/21 13:42:29 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000029_0: Committed
15/08/21 13:42:29 INFO Executor: Finished task 25.0 in stage 1.0 (TID 195). 843 bytes result sent to driver
15/08/21 13:42:29 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 208, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:29 INFO Executor: Running task 38.0 in stage 1.0 (TID 208)
15/08/21 13:42:29 INFO Executor: Finished task 29.0 in stage 1.0 (TID 199). 843 bytes result sent to driver
15/08/21 13:42:29 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 209, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:29 INFO Executor: Running task 39.0 in stage 1.0 (TID 209)
15/08/21 13:42:29 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 195) in 27401 ms on localhost (23/200)
15/08/21 13:42:29 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 199) in 27040 ms on localhost (24/200)
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000017
15/08/21 13:42:29 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000017_0: Committed
15/08/21 13:42:29 INFO Executor: Finished task 17.0 in stage 1.0 (TID 187). 843 bytes result sent to driver
15/08/21 13:42:29 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 210, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:29 INFO Executor: Running task 40.0 in stage 1.0 (TID 210)
15/08/21 13:42:29 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 187) in 27699 ms on localhost (25/200)
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:42:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 2,664,955B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,836B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 834,916B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,634B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:42:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 2,670,944B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,825B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 834,137B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,855B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 2,666,072B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,953B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:29 INFO ColumnChunkPageWriteStore: written 833,917B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,635B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:42:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000027
15/08/21 13:42:29 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000027_0: Committed
15/08/21 13:42:29 INFO Executor: Finished task 27.0 in stage 1.0 (TID 197). 843 bytes result sent to driver
15/08/21 13:42:29 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 211, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:29 INFO Executor: Running task 41.0 in stage 1.0 (TID 211)
15/08/21 13:42:29 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 197) in 27578 ms on localhost (26/200)
15/08/21 13:42:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000030
15/08/21 13:42:30 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000030_0: Committed
15/08/21 13:42:30 INFO Executor: Finished task 30.0 in stage 1.0 (TID 200). 843 bytes result sent to driver
15/08/21 13:42:30 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 212, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:30 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 200) in 27432 ms on localhost (27/200)
15/08/21 13:42:30 INFO Executor: Running task 42.0 in stage 1.0 (TID 212)
15/08/21 13:42:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000028
15/08/21 13:42:30 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000028_0: Committed
15/08/21 13:42:30 INFO Executor: Finished task 28.0 in stage 1.0 (TID 198). 843 bytes result sent to driver
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:30 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 213, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:30 INFO Executor: Running task 43.0 in stage 1.0 (TID 213)
15/08/21 13:42:30 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 198) in 27607 ms on localhost (28/200)
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:42:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:42:30 INFO ColumnChunkPageWriteStore: written 2,671,180B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,061B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:30 INFO ColumnChunkPageWriteStore: written 834,246B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,964B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:42:30 INFO ColumnChunkPageWriteStore: written 2,665,197B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,078B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:30 INFO ColumnChunkPageWriteStore: written 835,387B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,105B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:42:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000022
15/08/21 13:42:30 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000022_0: Committed
15/08/21 13:42:30 INFO Executor: Finished task 22.0 in stage 1.0 (TID 192). 843 bytes result sent to driver
15/08/21 13:42:30 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 214, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:42:30 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 192) in 28451 ms on localhost (29/200)
15/08/21 13:42:30 INFO Executor: Running task 44.0 in stage 1.0 (TID 214)
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:42:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000019
15/08/21 13:42:30 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000019_0: Committed
15/08/21 13:42:30 INFO Executor: Finished task 19.0 in stage 1.0 (TID 189). 843 bytes result sent to driver
15/08/21 13:42:30 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 215, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:30 INFO Executor: Running task 45.0 in stage 1.0 (TID 215)
15/08/21 13:42:30 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 189) in 28558 ms on localhost (30/200)
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:42:30 INFO ColumnChunkPageWriteStore: written 2,671,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,926B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:30 INFO ColumnChunkPageWriteStore: written 835,320B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,038B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:42:30 INFO ColumnChunkPageWriteStore: written 2,666,255B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,136B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:42:30 INFO ColumnChunkPageWriteStore: written 834,498B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,216B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:42:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000031
15/08/21 13:42:30 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000031_0: Committed
15/08/21 13:42:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211342_0001_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211342_0001_m_000020
15/08/21 13:42:30 INFO SparkHadoopMapRedUtil: attempt_201508211342_0001_m_000020_0: Committed
15/08/21 13:42:30 INFO Executor: Finished task 31.0 in stage 1.0 (TID 201). 843 bytes result sent to driver
15/08/21 13:42:30 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 216, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:30 INFO Executor: Running task 46.0 in stage 1.0 (TID 216)
15/08/21 13:42:30 INFO Executor: Finished task 20.0 in stage 1.0 (TID 190). 843 bytes result sent to driver
15/08/21 13:42:30 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 217, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:42:30 INFO Executor: Running task 47.0 in stage 1.0 (TID 217)
15/08/21 13:42:30 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 201) in 27834 ms on localhost (31/200)
15/08/21 13:42:30 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 190) in 28762 ms on localhost (32/200)
15/08/21 13:42:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:42:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:42:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:04 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:04 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:05 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:05 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:05 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:05 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:08 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:08 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:08 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:08 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:08 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:08 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:08 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:08 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:08 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:08 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:10 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:10 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:43:10 INFO ColumnChunkPageWriteStore: written 2,664,808B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,689B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:10 INFO ColumnChunkPageWriteStore: written 832,830B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,548B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:43:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000034
15/08/21 13:43:10 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000034_0: Committed
15/08/21 13:43:10 INFO Executor: Finished task 34.0 in stage 1.0 (TID 204). 843 bytes result sent to driver
15/08/21 13:43:10 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 218, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:10 INFO Executor: Running task 48.0 in stage 1.0 (TID 218)
15/08/21 13:43:10 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 204) in 42088 ms on localhost (33/200)
15/08/21 13:43:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,665,178B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,059B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 834,058B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,776B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:43:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000041
15/08/21 13:43:11 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000041_0: Committed
15/08/21 13:43:11 INFO Executor: Finished task 41.0 in stage 1.0 (TID 211). 843 bytes result sent to driver
15/08/21 13:43:11 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 219, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:11 INFO Executor: Running task 49.0 in stage 1.0 (TID 219)
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,670,777B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,658B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 833,951B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,669B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:43:11 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 211) in 41190 ms on localhost (34/200)
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,664,725B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,606B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 833,737B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,455B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,664,648B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,529B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000037
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,665,042B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,923B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000037_0: Committed
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 832,856B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,574B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,671,004B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,885B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 832,636B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,354B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 834,687B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,405B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:43:11 INFO Executor: Finished task 37.0 in stage 1.0 (TID 207). 843 bytes result sent to driver
15/08/21 13:43:11 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 220, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:11 INFO Executor: Running task 50.0 in stage 1.0 (TID 220)
15/08/21 13:43:11 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 207) in 41929 ms on localhost (35/200)
15/08/21 13:43:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000035
15/08/21 13:43:11 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000035_0: Committed
15/08/21 13:43:11 INFO Executor: Finished task 35.0 in stage 1.0 (TID 205). 843 bytes result sent to driver
15/08/21 13:43:11 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 221, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:11 INFO Executor: Running task 51.0 in stage 1.0 (TID 221)
15/08/21 13:43:11 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 205) in 42422 ms on localhost (36/200)
15/08/21 13:43:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000033
15/08/21 13:43:11 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000033_0: Committed
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:43:11 INFO Executor: Finished task 33.0 in stage 1.0 (TID 203). 843 bytes result sent to driver
15/08/21 13:43:11 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 222, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:11 INFO Executor: Running task 52.0 in stage 1.0 (TID 222)
15/08/21 13:43:11 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 203) in 42688 ms on localhost (37/200)
15/08/21 13:43:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000042
15/08/21 13:43:11 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000042_0: Committed
15/08/21 13:43:11 INFO Executor: Finished task 42.0 in stage 1.0 (TID 212). 843 bytes result sent to driver
15/08/21 13:43:11 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 223, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:11 INFO Executor: Running task 53.0 in stage 1.0 (TID 223)
15/08/21 13:43:11 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 212) in 41393 ms on localhost (38/200)
15/08/21 13:43:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000032
15/08/21 13:43:11 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000032_0: Committed
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:11 INFO Executor: Finished task 32.0 in stage 1.0 (TID 202). 843 bytes result sent to driver
15/08/21 13:43:11 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 224, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:11 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 202) in 42815 ms on localhost (39/200)
15/08/21 13:43:11 INFO Executor: Running task 54.0 in stage 1.0 (TID 224)
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,670,885B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,766B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 834,145B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,863B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:43:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000038
15/08/21 13:43:11 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000038_0: Committed
15/08/21 13:43:11 INFO Executor: Finished task 38.0 in stage 1.0 (TID 208). 843 bytes result sent to driver
15/08/21 13:43:11 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 225, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:11 INFO Executor: Running task 55.0 in stage 1.0 (TID 225)
15/08/21 13:43:11 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 208) in 42132 ms on localhost (40/200)
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,665,950B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,831B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 833,179B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,897B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:43:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 2,670,816B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,697B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:11 INFO ColumnChunkPageWriteStore: written 834,052B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,770B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:43:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000036
15/08/21 13:43:12 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000036_0: Committed
15/08/21 13:43:12 INFO Executor: Finished task 36.0 in stage 1.0 (TID 206). 843 bytes result sent to driver
15/08/21 13:43:12 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 226, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:12 INFO Executor: Running task 56.0 in stage 1.0 (TID 226)
15/08/21 13:43:12 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 206) in 42793 ms on localhost (41/200)
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 2,665,054B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,935B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 836,409B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 836,127B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:43:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:43:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000039
15/08/21 13:43:12 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000039_0: Committed
15/08/21 13:43:12 INFO Executor: Finished task 39.0 in stage 1.0 (TID 209). 843 bytes result sent to driver
15/08/21 13:43:12 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 227, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:12 INFO Executor: Running task 57.0 in stage 1.0 (TID 227)
15/08/21 13:43:12 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 209) in 42637 ms on localhost (42/200)
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000043
15/08/21 13:43:12 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000043_0: Committed
15/08/21 13:43:12 INFO Executor: Finished task 43.0 in stage 1.0 (TID 213). 843 bytes result sent to driver
15/08/21 13:43:12 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 228, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:12 INFO Executor: Running task 58.0 in stage 1.0 (TID 228)
15/08/21 13:43:12 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 213) in 42139 ms on localhost (43/200)
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 2,671,186B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,067B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 833,435B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,153B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000040
15/08/21 13:43:12 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000040_0: Committed
15/08/21 13:43:12 INFO Executor: Finished task 40.0 in stage 1.0 (TID 210). 843 bytes result sent to driver
15/08/21 13:43:12 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 229, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:12 INFO Executor: Running task 59.0 in stage 1.0 (TID 229)
15/08/21 13:43:12 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 210) in 42878 ms on localhost (44/200)
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 2,665,941B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,822B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 833,292B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,010B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:43:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:43:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:43:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000044
15/08/21 13:43:12 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000044_0: Committed
15/08/21 13:43:12 INFO Executor: Finished task 44.0 in stage 1.0 (TID 214). 843 bytes result sent to driver
15/08/21 13:43:12 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 230, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:12 INFO Executor: Running task 60.0 in stage 1.0 (TID 230)
15/08/21 13:43:12 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 214) in 42423 ms on localhost (45/200)
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 2,671,276B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,157B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 832,176B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,894B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 2,671,149B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,030B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:12 INFO ColumnChunkPageWriteStore: written 833,990B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,708B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000045
15/08/21 13:43:12 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000045_0: Committed
15/08/21 13:43:12 INFO Executor: Finished task 45.0 in stage 1.0 (TID 215). 843 bytes result sent to driver
15/08/21 13:43:12 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 231, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000047
15/08/21 13:43:12 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000047_0: Committed
15/08/21 13:43:12 INFO Executor: Running task 61.0 in stage 1.0 (TID 231)
15/08/21 13:43:12 INFO Executor: Finished task 47.0 in stage 1.0 (TID 217). 843 bytes result sent to driver
15/08/21 13:43:12 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 215) in 42507 ms on localhost (46/200)
15/08/21 13:43:12 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 232, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:12 INFO Executor: Running task 62.0 in stage 1.0 (TID 232)
15/08/21 13:43:12 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 217) in 42305 ms on localhost (47/200)
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:43:18 INFO ColumnChunkPageWriteStore: written 2,671,206B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,087B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:18 INFO ColumnChunkPageWriteStore: written 835,007B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,725B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:43:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000046
15/08/21 13:43:19 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000046_0: Committed
15/08/21 13:43:19 INFO Executor: Finished task 46.0 in stage 1.0 (TID 216). 843 bytes result sent to driver
15/08/21 13:43:19 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 233, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:19 INFO Executor: Running task 63.0 in stage 1.0 (TID 233)
15/08/21 13:43:19 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 216) in 48487 ms on localhost (48/200)
15/08/21 13:43:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:33 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:33 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:34 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:34 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:35 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:35 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:40 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:40 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:40 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:40 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:40 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:40 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:40 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:40 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:40 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:40 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:41 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:41 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:41 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:43:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:41 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:41 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:41 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:41 INFO ColumnChunkPageWriteStore: written 2,671,229B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,110B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:41 INFO ColumnChunkPageWriteStore: written 834,384B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,102B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:43:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000048
15/08/21 13:43:41 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000048_0: Committed
15/08/21 13:43:41 INFO Executor: Finished task 48.0 in stage 1.0 (TID 218). 843 bytes result sent to driver
15/08/21 13:43:41 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 234, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:41 INFO Executor: Running task 64.0 in stage 1.0 (TID 234)
15/08/21 13:43:41 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 218) in 30893 ms on localhost (49/200)
15/08/21 13:43:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 2,671,231B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,112B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 836,159B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,877B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:43:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:43:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000054
15/08/21 13:43:42 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000054_0: Committed
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 2,664,687B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,568B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 833,617B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,335B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:43:42 INFO Executor: Finished task 54.0 in stage 1.0 (TID 224). 843 bytes result sent to driver
15/08/21 13:43:42 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 235, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:42 INFO Executor: Running task 65.0 in stage 1.0 (TID 235)
15/08/21 13:43:42 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 224) in 31086 ms on localhost (50/200)
15/08/21 13:43:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:43:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000059
15/08/21 13:43:42 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000059_0: Committed
15/08/21 13:43:42 INFO Executor: Finished task 59.0 in stage 1.0 (TID 229). 843 bytes result sent to driver
15/08/21 13:43:42 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:42 INFO Executor: Running task 66.0 in stage 1.0 (TID 236)
15/08/21 13:43:42 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 229) in 30159 ms on localhost (51/200)
15/08/21 13:43:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:43:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:43:42 INFO CodecConfig: Compression: GZIP
15/08/21 13:43:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:43:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:43:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:43:42 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:43:42 INFO ParquetOutputFormat: Validation is off
15/08/21 13:43:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:43:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 2,665,307B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,188B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 834,754B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,472B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:43:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:43:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:43:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000050
15/08/21 13:43:42 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000050_0: Committed
15/08/21 13:43:42 INFO Executor: Finished task 50.0 in stage 1.0 (TID 220). 843 bytes result sent to driver
15/08/21 13:43:42 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 237, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:42 INFO Executor: Running task 67.0 in stage 1.0 (TID 237)
15/08/21 13:43:42 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 220) in 31544 ms on localhost (52/200)
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 2,664,734B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,615B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 835,655B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,373B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:43:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:43:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:43:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000057
15/08/21 13:43:42 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000057_0: Committed
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 2,666,261B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,142B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:42 INFO Executor: Finished task 57.0 in stage 1.0 (TID 227). 843 bytes result sent to driver
15/08/21 13:43:42 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 238, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:42 INFO Executor: Running task 68.0 in stage 1.0 (TID 238)
15/08/21 13:43:42 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 227) in 30837 ms on localhost (53/200)
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 834,132B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,850B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 2,664,718B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,599B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:42 INFO ColumnChunkPageWriteStore: written 831,971B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,689B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:43:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 2,671,319B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,200B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 831,020B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,738B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:43:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000052
15/08/21 13:43:43 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000052_0: Committed
15/08/21 13:43:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000058
15/08/21 13:43:43 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000058_0: Committed
15/08/21 13:43:43 INFO Executor: Finished task 52.0 in stage 1.0 (TID 222). 843 bytes result sent to driver
15/08/21 13:43:43 INFO Executor: Finished task 58.0 in stage 1.0 (TID 228). 843 bytes result sent to driver
15/08/21 13:43:43 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 239, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:43 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 240, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:43 INFO Executor: Running task 69.0 in stage 1.0 (TID 239)
15/08/21 13:43:43 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 222) in 31714 ms on localhost (54/200)
15/08/21 13:43:43 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 228) in 30936 ms on localhost (55/200)
15/08/21 13:43:43 INFO Executor: Running task 70.0 in stage 1.0 (TID 240)
15/08/21 13:43:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:43:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000056
15/08/21 13:43:43 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000056_0: Committed
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:43 INFO Executor: Finished task 56.0 in stage 1.0 (TID 226). 843 bytes result sent to driver
15/08/21 13:43:43 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 241, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:43 INFO Executor: Running task 71.0 in stage 1.0 (TID 241)
15/08/21 13:43:43 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 226) in 31171 ms on localhost (56/200)
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 2,665,201B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,082B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 833,868B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,586B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:43:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 2,671,541B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,422B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 833,666B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,384B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:43:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000051
15/08/21 13:43:43 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000051_0: Committed
15/08/21 13:43:43 INFO Executor: Finished task 51.0 in stage 1.0 (TID 221). 843 bytes result sent to driver
15/08/21 13:43:43 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 242, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:43 INFO Executor: Running task 72.0 in stage 1.0 (TID 242)
15/08/21 13:43:43 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 221) in 31969 ms on localhost (57/200)
15/08/21 13:43:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000053
15/08/21 13:43:43 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000053_0: Committed
15/08/21 13:43:43 INFO Executor: Finished task 53.0 in stage 1.0 (TID 223). 843 bytes result sent to driver
15/08/21 13:43:43 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 243, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:43 INFO Executor: Running task 73.0 in stage 1.0 (TID 243)
15/08/21 13:43:43 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 223) in 32030 ms on localhost (58/200)
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 2,670,932B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,813B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 830,638B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,356B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000062
15/08/21 13:43:43 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000062_0: Committed
15/08/21 13:43:43 INFO Executor: Finished task 62.0 in stage 1.0 (TID 232). 843 bytes result sent to driver
15/08/21 13:43:43 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 244, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:43 INFO Executor: Running task 74.0 in stage 1.0 (TID 244)
15/08/21 13:43:43 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 232) in 30703 ms on localhost (59/200)
15/08/21 13:43:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:43:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 2,666,043B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,924B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 836,248B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,966B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 2,664,912B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,793B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:43 INFO ColumnChunkPageWriteStore: written 835,880B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,598B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:43:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000060
15/08/21 13:43:43 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000060_0: Committed
15/08/21 13:43:43 INFO Executor: Finished task 60.0 in stage 1.0 (TID 230). 843 bytes result sent to driver
15/08/21 13:43:43 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 245, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:43 INFO Executor: Running task 75.0 in stage 1.0 (TID 245)
15/08/21 13:43:43 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 230) in 31149 ms on localhost (60/200)
15/08/21 13:43:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000049
15/08/21 13:43:43 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000049_0: Committed
15/08/21 13:43:43 INFO Executor: Finished task 49.0 in stage 1.0 (TID 219). 843 bytes result sent to driver
15/08/21 13:43:43 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 246, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:43 INFO Executor: Running task 76.0 in stage 1.0 (TID 246)
15/08/21 13:43:43 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 219) in 32814 ms on localhost (61/200)
15/08/21 13:43:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:43:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:43:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:44 INFO ColumnChunkPageWriteStore: written 2,670,898B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,779B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:44 INFO ColumnChunkPageWriteStore: written 834,402B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,120B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:43:44 INFO ColumnChunkPageWriteStore: written 2,671,403B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,284B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:44 INFO ColumnChunkPageWriteStore: written 833,349B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,067B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:43:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000055
15/08/21 13:43:44 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000055_0: Committed
15/08/21 13:43:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000061
15/08/21 13:43:44 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000061_0: Committed
15/08/21 13:43:44 INFO Executor: Finished task 61.0 in stage 1.0 (TID 231). 843 bytes result sent to driver
15/08/21 13:43:44 INFO Executor: Finished task 55.0 in stage 1.0 (TID 225). 843 bytes result sent to driver
15/08/21 13:43:44 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 247, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:44 INFO Executor: Running task 77.0 in stage 1.0 (TID 247)
15/08/21 13:43:44 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:44 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 231) in 31316 ms on localhost (62/200)
15/08/21 13:43:44 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 225) in 32637 ms on localhost (63/200)
15/08/21 13:43:44 INFO Executor: Running task 78.0 in stage 1.0 (TID 248)
15/08/21 13:43:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:43:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:43:44 INFO ColumnChunkPageWriteStore: written 2,670,957B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,838B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:43:44 INFO ColumnChunkPageWriteStore: written 831,272B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,990B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:43:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211343_0001_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211343_0001_m_000063
15/08/21 13:43:45 INFO SparkHadoopMapRedUtil: attempt_201508211343_0001_m_000063_0: Committed
15/08/21 13:43:45 INFO Executor: Finished task 63.0 in stage 1.0 (TID 233). 843 bytes result sent to driver
15/08/21 13:43:45 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 249, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:43:45 INFO Executor: Running task 79.0 in stage 1.0 (TID 249)
15/08/21 13:43:45 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 233) in 25921 ms on localhost (64/200)
15/08/21 13:43:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:43:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 13:44:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:08 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:08 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:12 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:12 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:12 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:12 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:12 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:12 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:12 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:44:12 INFO ColumnChunkPageWriteStore: written 2,671,438B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,319B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:12 INFO ColumnChunkPageWriteStore: written 831,981B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,699B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:44:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000064
15/08/21 13:44:13 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000064_0: Committed
15/08/21 13:44:13 INFO Executor: Finished task 64.0 in stage 1.0 (TID 234). 843 bytes result sent to driver
15/08/21 13:44:13 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 250, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:13 INFO Executor: Running task 80.0 in stage 1.0 (TID 250)
15/08/21 13:44:13 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 234) in 31229 ms on localhost (65/200)
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:13 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:13 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:13 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:14 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:14 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:44:14 INFO ColumnChunkPageWriteStore: written 2,664,716B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,597B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:14 INFO ColumnChunkPageWriteStore: written 833,304B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,022B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:44:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000066
15/08/21 13:44:14 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000066_0: Committed
15/08/21 13:44:14 INFO Executor: Finished task 66.0 in stage 1.0 (TID 236). 843 bytes result sent to driver
15/08/21 13:44:14 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 251, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:14 INFO Executor: Running task 81.0 in stage 1.0 (TID 251)
15/08/21 13:44:14 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 236) in 31947 ms on localhost (66/200)
15/08/21 13:44:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:44:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:14 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:14 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:14 INFO ColumnChunkPageWriteStore: written 2,664,507B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,388B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:14 INFO ColumnChunkPageWriteStore: written 834,830B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,548B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:44:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000065
15/08/21 13:44:14 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000065_0: Committed
15/08/21 13:44:14 INFO Executor: Finished task 65.0 in stage 1.0 (TID 235). 843 bytes result sent to driver
15/08/21 13:44:14 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 252, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:14 INFO Executor: Running task 82.0 in stage 1.0 (TID 252)
15/08/21 13:44:14 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 235) in 32372 ms on localhost (67/200)
15/08/21 13:44:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:44:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:44:19 INFO ColumnChunkPageWriteStore: written 2,664,740B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,621B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:19 INFO ColumnChunkPageWriteStore: written 835,568B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,286B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:44:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000073
15/08/21 13:44:20 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000073_0: Committed
15/08/21 13:44:20 INFO Executor: Finished task 73.0 in stage 1.0 (TID 243). 843 bytes result sent to driver
15/08/21 13:44:20 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 253, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:20 INFO Executor: Running task 83.0 in stage 1.0 (TID 253)
15/08/21 13:44:20 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 243) in 36640 ms on localhost (68/200)
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 2,671,025B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,906B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 832,782B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,500B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:44:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:44:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000071
15/08/21 13:44:20 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000071_0: Committed
15/08/21 13:44:20 INFO Executor: Finished task 71.0 in stage 1.0 (TID 241). 843 bytes result sent to driver
15/08/21 13:44:20 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 254, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:20 INFO Executor: Running task 84.0 in stage 1.0 (TID 254)
15/08/21 13:44:20 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 241) in 37209 ms on localhost (69/200)
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 2,664,870B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,751B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 834,960B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,678B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:44:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000067
15/08/21 13:44:20 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000067_0: Committed
15/08/21 13:44:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:44:20 INFO Executor: Finished task 67.0 in stage 1.0 (TID 237). 843 bytes result sent to driver
15/08/21 13:44:20 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 255, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:20 INFO Executor: Running task 85.0 in stage 1.0 (TID 255)
15/08/21 13:44:20 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 237) in 37718 ms on localhost (70/200)
15/08/21 13:44:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 2,670,691B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,572B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 835,083B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,801B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 2,664,772B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,653B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 834,014B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,732B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:44:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000069
15/08/21 13:44:20 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000069_0: Committed
15/08/21 13:44:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:44:20 INFO Executor: Finished task 69.0 in stage 1.0 (TID 239). 843 bytes result sent to driver
15/08/21 13:44:20 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 256, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:20 INFO Executor: Running task 86.0 in stage 1.0 (TID 256)
15/08/21 13:44:20 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 239) in 37623 ms on localhost (71/200)
15/08/21 13:44:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000074
15/08/21 13:44:20 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000074_0: Committed
15/08/21 13:44:20 INFO Executor: Finished task 74.0 in stage 1.0 (TID 244). 843 bytes result sent to driver
15/08/21 13:44:20 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 257, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:20 INFO Executor: Running task 87.0 in stage 1.0 (TID 257)
15/08/21 13:44:20 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 244) in 37112 ms on localhost (72/200)
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 2,671,325B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,206B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 833,913B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,631B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:44:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000072
15/08/21 13:44:20 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000072_0: Committed
15/08/21 13:44:20 INFO Executor: Finished task 72.0 in stage 1.0 (TID 242). 843 bytes result sent to driver
15/08/21 13:44:20 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 258, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:20 INFO Executor: Running task 88.0 in stage 1.0 (TID 258)
15/08/21 13:44:20 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 242) in 37577 ms on localhost (73/200)
15/08/21 13:44:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 2,665,867B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,748B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:20 INFO ColumnChunkPageWriteStore: written 833,718B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,436B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:44:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000068
15/08/21 13:44:21 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000068_0: Committed
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 2,671,148B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,029B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 833,406B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,124B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:44:21 INFO Executor: Finished task 68.0 in stage 1.0 (TID 238). 843 bytes result sent to driver
15/08/21 13:44:21 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 259, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:21 INFO Executor: Running task 89.0 in stage 1.0 (TID 259)
15/08/21 13:44:21 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 238) in 38083 ms on localhost (74/200)
15/08/21 13:44:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:44:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000077
15/08/21 13:44:21 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000077_0: Committed
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 2,665,533B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,414B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 833,356B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,074B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:44:21 INFO Executor: Finished task 77.0 in stage 1.0 (TID 247). 843 bytes result sent to driver
15/08/21 13:44:21 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 260, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:21 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 247) in 36909 ms on localhost (75/200)
15/08/21 13:44:21 INFO Executor: Running task 90.0 in stage 1.0 (TID 260)
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:44:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000076
15/08/21 13:44:21 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000076_0: Committed
15/08/21 13:44:21 INFO Executor: Finished task 76.0 in stage 1.0 (TID 246). 843 bytes result sent to driver
15/08/21 13:44:21 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 261, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:21 INFO Executor: Running task 91.0 in stage 1.0 (TID 261)
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 2,670,847B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,728B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 831,987B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,705B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:44:21 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 246) in 37240 ms on localhost (76/200)
15/08/21 13:44:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:44:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000070
15/08/21 13:44:21 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000070_0: Committed
15/08/21 13:44:21 INFO Executor: Finished task 70.0 in stage 1.0 (TID 240). 843 bytes result sent to driver
15/08/21 13:44:21 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 262, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:44:21 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 240) in 38187 ms on localhost (77/200)
15/08/21 13:44:21 INFO Executor: Running task 92.0 in stage 1.0 (TID 262)
15/08/21 13:44:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 2,664,859B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,740B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 831,489B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,207B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:44:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000075
15/08/21 13:44:21 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000075_0: Committed
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 2,671,149B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,030B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:21 INFO Executor: Finished task 75.0 in stage 1.0 (TID 245). 843 bytes result sent to driver
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 833,093B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,811B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:44:21 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 263, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:21 INFO Executor: Running task 93.0 in stage 1.0 (TID 263)
15/08/21 13:44:21 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 245) in 37562 ms on localhost (78/200)
15/08/21 13:44:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000078
15/08/21 13:44:21 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000078_0: Committed
15/08/21 13:44:21 INFO Executor: Finished task 78.0 in stage 1.0 (TID 248). 843 bytes result sent to driver
15/08/21 13:44:21 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 264, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:44:21 INFO Executor: Running task 94.0 in stage 1.0 (TID 264)
15/08/21 13:44:21 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 248) in 37313 ms on localhost (79/200)
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 2,671,144B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,025B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:21 INFO ColumnChunkPageWriteStore: written 832,813B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,531B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:44:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000079
15/08/21 13:44:22 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000079_0: Committed
15/08/21 13:44:22 INFO Executor: Finished task 79.0 in stage 1.0 (TID 249). 843 bytes result sent to driver
15/08/21 13:44:22 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 265, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:22 INFO Executor: Running task 95.0 in stage 1.0 (TID 265)
15/08/21 13:44:22 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 249) in 37034 ms on localhost (80/200)
15/08/21 13:44:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:29 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:29 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:44:36 INFO ColumnChunkPageWriteStore: written 2,671,198B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,079B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:36 INFO ColumnChunkPageWriteStore: written 836,069B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,787B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:44:36 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000080
15/08/21 13:44:36 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000080_0: Committed
15/08/21 13:44:36 INFO Executor: Finished task 80.0 in stage 1.0 (TID 250). 843 bytes result sent to driver
15/08/21 13:44:36 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 266, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:36 INFO Executor: Running task 96.0 in stage 1.0 (TID 266)
15/08/21 13:44:36 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 250) in 23704 ms on localhost (81/200)
15/08/21 13:44:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 13:44:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:37 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:37 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:37 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:37 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:38 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:38 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:43 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:43 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:43 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:43 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:44 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:44 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:44 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:44 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:44 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:44 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:44:44 INFO ColumnChunkPageWriteStore: written 2,665,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,926B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:44 INFO ColumnChunkPageWriteStore: written 834,589B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,307B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:44:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000081
15/08/21 13:44:44 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000081_0: Committed
15/08/21 13:44:44 INFO Executor: Finished task 81.0 in stage 1.0 (TID 251). 843 bytes result sent to driver
15/08/21 13:44:44 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 267, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:44 INFO Executor: Running task 97.0 in stage 1.0 (TID 267)
15/08/21 13:44:44 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 251) in 30381 ms on localhost (82/200)
15/08/21 13:44:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:45 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:45 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:44:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:45 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:45 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:45 INFO ColumnChunkPageWriteStore: written 2,665,242B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,123B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:45 INFO ColumnChunkPageWriteStore: written 834,874B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,592B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:44:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:45 INFO ColumnChunkPageWriteStore: written 2,666,194B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,075B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:45 INFO ColumnChunkPageWriteStore: written 831,501B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,219B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:44:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000082
15/08/21 13:44:45 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000082_0: Committed
15/08/21 13:44:45 INFO Executor: Finished task 82.0 in stage 1.0 (TID 252). 843 bytes result sent to driver
15/08/21 13:44:45 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 268, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:45 INFO Executor: Running task 98.0 in stage 1.0 (TID 268)
15/08/21 13:44:45 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 252) in 30502 ms on localhost (83/200)
15/08/21 13:44:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:45 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:45 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000084
15/08/21 13:44:45 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000084_0: Committed
15/08/21 13:44:45 INFO Executor: Finished task 84.0 in stage 1.0 (TID 254). 843 bytes result sent to driver
15/08/21 13:44:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:45 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 269, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:45 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 254) in 25055 ms on localhost (84/200)
15/08/21 13:44:45 INFO Executor: Running task 99.0 in stage 1.0 (TID 269)
15/08/21 13:44:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:44:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:45 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:45 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:44:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:45 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:45 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:46 INFO ColumnChunkPageWriteStore: written 2,671,098B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,979B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:46 INFO ColumnChunkPageWriteStore: written 832,460B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,178B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:44:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000085
15/08/21 13:44:46 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000085_0: Committed
15/08/21 13:44:46 INFO Executor: Finished task 85.0 in stage 1.0 (TID 255). 843 bytes result sent to driver
15/08/21 13:44:46 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 270, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:46 INFO Executor: Running task 100.0 in stage 1.0 (TID 270)
15/08/21 13:44:46 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 255) in 25614 ms on localhost (85/200)
15/08/21 13:44:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:44:46 INFO ColumnChunkPageWriteStore: written 2,665,382B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,263B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:46 INFO ColumnChunkPageWriteStore: written 832,836B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,554B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:44:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:44:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000083
15/08/21 13:44:46 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000083_0: Committed
15/08/21 13:44:46 INFO ColumnChunkPageWriteStore: written 2,671,438B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,319B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:46 INFO ColumnChunkPageWriteStore: written 834,780B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,498B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:44:46 INFO Executor: Finished task 83.0 in stage 1.0 (TID 253). 843 bytes result sent to driver
15/08/21 13:44:46 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 271, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:46 INFO Executor: Running task 101.0 in stage 1.0 (TID 271)
15/08/21 13:44:46 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 253) in 26309 ms on localhost (86/200)
15/08/21 13:44:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:44:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000086
15/08/21 13:44:46 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000086_0: Committed
15/08/21 13:44:46 INFO Executor: Finished task 86.0 in stage 1.0 (TID 256). 843 bytes result sent to driver
15/08/21 13:44:46 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 272, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:46 INFO Executor: Running task 102.0 in stage 1.0 (TID 272)
15/08/21 13:44:46 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 256) in 25776 ms on localhost (87/200)
15/08/21 13:44:46 INFO ColumnChunkPageWriteStore: written 2,671,429B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,310B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:46 INFO ColumnChunkPageWriteStore: written 833,569B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,287B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:44:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:44:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000087
15/08/21 13:44:46 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000087_0: Committed
15/08/21 13:44:46 INFO Executor: Finished task 87.0 in stage 1.0 (TID 257). 843 bytes result sent to driver
15/08/21 13:44:46 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 273, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:46 INFO Executor: Running task 103.0 in stage 1.0 (TID 273)
15/08/21 13:44:46 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 257) in 25960 ms on localhost (88/200)
15/08/21 13:44:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:44:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:44:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:46 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:46 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:47 INFO ColumnChunkPageWriteStore: written 2,665,894B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,775B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:47 INFO ColumnChunkPageWriteStore: written 833,016B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,734B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:44:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000092
15/08/21 13:44:47 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000092_0: Committed
15/08/21 13:44:47 INFO Executor: Finished task 92.0 in stage 1.0 (TID 262). 843 bytes result sent to driver
15/08/21 13:44:47 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 274, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:47 INFO Executor: Running task 104.0 in stage 1.0 (TID 274)
15/08/21 13:44:47 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 262) in 25848 ms on localhost (89/200)
15/08/21 13:44:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:44:47 INFO ColumnChunkPageWriteStore: written 2,664,909B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,790B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:47 INFO ColumnChunkPageWriteStore: written 830,715B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,433B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:44:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:44:47 INFO CodecConfig: Compression: GZIP
15/08/21 13:44:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:44:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:44:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:44:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:44:47 INFO ParquetOutputFormat: Validation is off
15/08/21 13:44:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:44:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:44:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,673
15/08/21 13:44:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000090
15/08/21 13:44:52 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000090_0: Committed
15/08/21 13:44:52 INFO Executor: Finished task 90.0 in stage 1.0 (TID 260). 843 bytes result sent to driver
15/08/21 13:44:52 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 275, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:52 INFO Executor: Running task 105.0 in stage 1.0 (TID 275)
15/08/21 13:44:52 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 260) in 31067 ms on localhost (90/200)
15/08/21 13:44:52 INFO ColumnChunkPageWriteStore: written 2,671,325B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,206B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:52 INFO ColumnChunkPageWriteStore: written 835,794B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,512B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 319 entries, 2,552B raw, 319B comp}
15/08/21 13:44:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000088
15/08/21 13:44:52 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000088_0: Committed
15/08/21 13:44:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:52 INFO Executor: Finished task 88.0 in stage 1.0 (TID 258). 843 bytes result sent to driver
15/08/21 13:44:52 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 276, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:52 INFO Executor: Running task 106.0 in stage 1.0 (TID 276)
15/08/21 13:44:52 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 258) in 31485 ms on localhost (91/200)
15/08/21 13:44:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:44:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:44:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 13:44:52 INFO ColumnChunkPageWriteStore: written 2,665,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,926B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:52 INFO ColumnChunkPageWriteStore: written 832,530B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,248B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:44:52 INFO ColumnChunkPageWriteStore: written 2,665,088B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,969B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:52 INFO ColumnChunkPageWriteStore: written 834,046B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,764B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 13:44:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000091
15/08/21 13:44:52 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000091_0: Committed
15/08/21 13:44:52 INFO Executor: Finished task 91.0 in stage 1.0 (TID 261). 843 bytes result sent to driver
15/08/21 13:44:52 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 277, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000089
15/08/21 13:44:52 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000089_0: Committed
15/08/21 13:44:52 INFO Executor: Running task 107.0 in stage 1.0 (TID 277)
15/08/21 13:44:52 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 261) in 31541 ms on localhost (92/200)
15/08/21 13:44:52 INFO Executor: Finished task 89.0 in stage 1.0 (TID 259). 843 bytes result sent to driver
15/08/21 13:44:52 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 278, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:52 INFO Executor: Running task 108.0 in stage 1.0 (TID 278)
15/08/21 13:44:52 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 259) in 31727 ms on localhost (93/200)
15/08/21 13:44:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:44:53 INFO ColumnChunkPageWriteStore: written 2,670,977B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,858B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:53 INFO ColumnChunkPageWriteStore: written 832,600B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,318B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:44:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000094
15/08/21 13:44:53 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000094_0: Committed
15/08/21 13:44:53 INFO Executor: Finished task 94.0 in stage 1.0 (TID 264). 843 bytes result sent to driver
15/08/21 13:44:53 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 279, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:53 INFO Executor: Running task 109.0 in stage 1.0 (TID 279)
15/08/21 13:44:53 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 264) in 32217 ms on localhost (94/200)
15/08/21 13:44:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:44:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,585
15/08/21 13:44:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:44:54 INFO ColumnChunkPageWriteStore: written 2,671,043B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,924B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:54 INFO ColumnChunkPageWriteStore: written 834,855B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,573B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 308 entries, 2,464B raw, 308B comp}
15/08/21 13:44:54 INFO ColumnChunkPageWriteStore: written 2,670,798B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,679B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:44:54 INFO ColumnChunkPageWriteStore: written 833,138B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,856B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:44:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000093
15/08/21 13:44:54 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000093_0: Committed
15/08/21 13:44:54 INFO Executor: Finished task 93.0 in stage 1.0 (TID 263). 843 bytes result sent to driver
15/08/21 13:44:54 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 280, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:54 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 263) in 33214 ms on localhost (95/200)
15/08/21 13:44:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211344_0001_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211344_0001_m_000095
15/08/21 13:44:54 INFO SparkHadoopMapRedUtil: attempt_201508211344_0001_m_000095_0: Committed
15/08/21 13:44:54 INFO Executor: Running task 110.0 in stage 1.0 (TID 280)
15/08/21 13:44:54 INFO Executor: Finished task 95.0 in stage 1.0 (TID 265). 843 bytes result sent to driver
15/08/21 13:44:54 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 281, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:44:54 INFO Executor: Running task 111.0 in stage 1.0 (TID 281)
15/08/21 13:44:54 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 265) in 32635 ms on localhost (96/200)
15/08/21 13:44:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:44:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:44:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:10 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:10 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:45:10 INFO ColumnChunkPageWriteStore: written 2,671,367B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,248B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:10 INFO ColumnChunkPageWriteStore: written 835,262B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,980B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:45:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000096
15/08/21 13:45:11 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000096_0: Committed
15/08/21 13:45:11 INFO Executor: Finished task 96.0 in stage 1.0 (TID 266). 843 bytes result sent to driver
15/08/21 13:45:11 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 282, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:11 INFO Executor: Running task 112.0 in stage 1.0 (TID 282)
15/08/21 13:45:11 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 266) in 34309 ms on localhost (97/200)
15/08/21 13:45:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,673
15/08/21 13:45:13 INFO ColumnChunkPageWriteStore: written 2,665,205B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,086B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:14 INFO ColumnChunkPageWriteStore: written 833,315B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,033B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 319 entries, 2,552B raw, 319B comp}
15/08/21 13:45:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:14 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:14 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:14 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000097
15/08/21 13:45:14 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000097_0: Committed
15/08/21 13:45:14 INFO Executor: Finished task 97.0 in stage 1.0 (TID 267). 843 bytes result sent to driver
15/08/21 13:45:14 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 283, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:14 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 267) in 29251 ms on localhost (98/200)
15/08/21 13:45:14 INFO Executor: Running task 113.0 in stage 1.0 (TID 283)
15/08/21 13:45:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:14 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:14 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:14 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:14 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:14 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:14 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:15 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:15 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:15 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:15 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:15 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:15 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:15 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:15 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:45:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:20 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:20 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:20 INFO ColumnChunkPageWriteStore: written 2,664,831B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,712B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:20 INFO ColumnChunkPageWriteStore: written 834,250B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,968B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:45:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:20 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:20 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:20 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:20 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000098
15/08/21 13:45:20 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000098_0: Committed
15/08/21 13:45:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:20 INFO Executor: Finished task 98.0 in stage 1.0 (TID 268). 843 bytes result sent to driver
15/08/21 13:45:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:20 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 284, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:20 INFO Executor: Running task 114.0 in stage 1.0 (TID 284)
15/08/21 13:45:20 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 268) in 35570 ms on localhost (99/200)
15/08/21 13:45:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:21 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:21 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:45:21 INFO ColumnChunkPageWriteStore: written 2,664,905B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,786B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:21 INFO ColumnChunkPageWriteStore: written 834,064B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,782B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:45:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000099
15/08/21 13:45:21 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000099_0: Committed
15/08/21 13:45:21 INFO Executor: Finished task 99.0 in stage 1.0 (TID 269). 843 bytes result sent to driver
15/08/21 13:45:21 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 285, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:21 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 269) in 36371 ms on localhost (100/200)
15/08/21 13:45:21 INFO Executor: Running task 115.0 in stage 1.0 (TID 285)
15/08/21 13:45:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:45:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 2,671,218B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,099B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 831,764B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,482B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:45:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:45:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:22 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:22 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:45:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000103
15/08/21 13:45:22 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000103_0: Committed
15/08/21 13:45:22 INFO Executor: Finished task 103.0 in stage 1.0 (TID 273). 843 bytes result sent to driver
15/08/21 13:45:22 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 286, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:22 INFO Executor: Running task 116.0 in stage 1.0 (TID 286)
15/08/21 13:45:22 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 273) in 35641 ms on localhost (101/200)
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 2,665,707B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,588B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 833,973B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,691B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 2,671,228B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,109B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 837,388B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 837,106B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:45:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:45:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000100
15/08/21 13:45:22 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000100_0: Committed
15/08/21 13:45:22 INFO Executor: Finished task 100.0 in stage 1.0 (TID 270). 843 bytes result sent to driver
15/08/21 13:45:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000101
15/08/21 13:45:22 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000101_0: Committed
15/08/21 13:45:22 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 287, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:22 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 270) in 36338 ms on localhost (102/200)
15/08/21 13:45:22 INFO Executor: Running task 117.0 in stage 1.0 (TID 287)
15/08/21 13:45:22 INFO Executor: Finished task 101.0 in stage 1.0 (TID 271). 843 bytes result sent to driver
15/08/21 13:45:22 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 288, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:22 INFO Executor: Running task 118.0 in stage 1.0 (TID 288)
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 2,671,439B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,320B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:22 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 271) in 36143 ms on localhost (103/200)
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 834,740B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,458B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000104
15/08/21 13:45:22 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000104_0: Committed
15/08/21 13:45:22 INFO Executor: Finished task 104.0 in stage 1.0 (TID 274). 843 bytes result sent to driver
15/08/21 13:45:22 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 289, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:22 INFO Executor: Running task 119.0 in stage 1.0 (TID 289)
15/08/21 13:45:22 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 274) in 35534 ms on localhost (104/200)
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 2,670,961B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,842B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 834,751B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,469B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:45:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:45:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000102
15/08/21 13:45:22 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000102_0: Committed
15/08/21 13:45:22 INFO Executor: Finished task 102.0 in stage 1.0 (TID 272). 843 bytes result sent to driver
15/08/21 13:45:22 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 290, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:22 INFO Executor: Running task 120.0 in stage 1.0 (TID 290)
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 2,666,321B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,202B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:22 INFO ColumnChunkPageWriteStore: written 833,519B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,237B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:45:22 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 272) in 36427 ms on localhost (105/200)
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000108
15/08/21 13:45:23 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000108_0: Committed
15/08/21 13:45:23 INFO Executor: Finished task 108.0 in stage 1.0 (TID 278). 843 bytes result sent to driver
15/08/21 13:45:23 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 291, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:23 INFO Executor: Running task 121.0 in stage 1.0 (TID 291)
15/08/21 13:45:23 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 278) in 30269 ms on localhost (106/200)
15/08/21 13:45:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:45:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:23 INFO ColumnChunkPageWriteStore: written 2,664,933B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,814B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:23 INFO ColumnChunkPageWriteStore: written 834,256B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,974B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:45:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:45:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000105
15/08/21 13:45:23 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000105_0: Committed
15/08/21 13:45:23 INFO Executor: Finished task 105.0 in stage 1.0 (TID 275). 843 bytes result sent to driver
15/08/21 13:45:23 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 292, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:23 INFO Executor: Running task 122.0 in stage 1.0 (TID 292)
15/08/21 13:45:23 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 275) in 30994 ms on localhost (107/200)
15/08/21 13:45:23 INFO ColumnChunkPageWriteStore: written 2,665,132B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,013B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:23 INFO ColumnChunkPageWriteStore: written 833,912B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,630B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:45:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:45:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000107
15/08/21 13:45:23 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000107_0: Committed
15/08/21 13:45:23 INFO Executor: Finished task 107.0 in stage 1.0 (TID 277). 843 bytes result sent to driver
15/08/21 13:45:23 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 293, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:23 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 277) in 30632 ms on localhost (108/200)
15/08/21 13:45:23 INFO Executor: Running task 123.0 in stage 1.0 (TID 293)
15/08/21 13:45:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:23 INFO ColumnChunkPageWriteStore: written 2,665,021B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,902B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:23 INFO ColumnChunkPageWriteStore: written 832,356B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,074B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:45:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000106
15/08/21 13:45:23 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000106_0: Committed
15/08/21 13:45:23 INFO Executor: Finished task 106.0 in stage 1.0 (TID 276). 843 bytes result sent to driver
15/08/21 13:45:23 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 294, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:23 INFO Executor: Running task 124.0 in stage 1.0 (TID 294)
15/08/21 13:45:23 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 276) in 31150 ms on localhost (109/200)
15/08/21 13:45:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:45:24 INFO ColumnChunkPageWriteStore: written 2,671,095B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,976B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:24 INFO ColumnChunkPageWriteStore: written 830,535B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,253B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:45:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000109
15/08/21 13:45:24 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000109_0: Committed
15/08/21 13:45:24 INFO Executor: Finished task 109.0 in stage 1.0 (TID 279). 843 bytes result sent to driver
15/08/21 13:45:24 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 295, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:24 INFO Executor: Running task 125.0 in stage 1.0 (TID 295)
15/08/21 13:45:24 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 279) in 30332 ms on localhost (110/200)
15/08/21 13:45:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:24 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:24 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 13:45:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:45:30 INFO ColumnChunkPageWriteStore: written 2,671,231B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,112B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:30 INFO ColumnChunkPageWriteStore: written 834,861B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,579B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:45:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:30 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:30 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000110
15/08/21 13:45:30 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000110_0: Committed
15/08/21 13:45:30 INFO Executor: Finished task 110.0 in stage 1.0 (TID 280). 843 bytes result sent to driver
15/08/21 13:45:30 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 296, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:30 INFO Executor: Running task 126.0 in stage 1.0 (TID 296)
15/08/21 13:45:30 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 280) in 35636 ms on localhost (111/200)
15/08/21 13:45:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:45:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:31 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:31 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:45:37 INFO ColumnChunkPageWriteStore: written 2,671,480B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,361B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:37 INFO ColumnChunkPageWriteStore: written 834,612B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,330B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:45:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000111
15/08/21 13:45:37 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000111_0: Committed
15/08/21 13:45:37 INFO Executor: Finished task 111.0 in stage 1.0 (TID 281). 843 bytes result sent to driver
15/08/21 13:45:37 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 297, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:37 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 281) in 43094 ms on localhost (112/200)
15/08/21 13:45:37 INFO Executor: Running task 127.0 in stage 1.0 (TID 297)
15/08/21 13:45:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:45:38 INFO ColumnChunkPageWriteStore: written 2,665,196B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,077B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:38 INFO ColumnChunkPageWriteStore: written 835,647B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,365B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:45:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000113
15/08/21 13:45:38 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000113_0: Committed
15/08/21 13:45:38 INFO Executor: Finished task 113.0 in stage 1.0 (TID 283). 843 bytes result sent to driver
15/08/21 13:45:38 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 298, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:38 INFO Executor: Running task 128.0 in stage 1.0 (TID 298)
15/08/21 13:45:38 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 283) in 24528 ms on localhost (113/200)
15/08/21 13:45:38 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:39 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:39 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:39 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:45:40 INFO ColumnChunkPageWriteStore: written 2,671,213B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,094B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:40 INFO ColumnChunkPageWriteStore: written 833,615B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,333B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:45:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000112
15/08/21 13:45:40 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000112_0: Committed
15/08/21 13:45:40 INFO Executor: Finished task 112.0 in stage 1.0 (TID 282). 843 bytes result sent to driver
15/08/21 13:45:40 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 299, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:40 INFO Executor: Running task 129.0 in stage 1.0 (TID 299)
15/08/21 13:45:40 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 282) in 29307 ms on localhost (114/200)
15/08/21 13:45:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:46 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:46 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:45:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:46 INFO ColumnChunkPageWriteStore: written 2,665,084B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,965B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:46 INFO ColumnChunkPageWriteStore: written 834,882B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,600B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:45:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:46 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:46 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000114
15/08/21 13:45:46 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000114_0: Committed
15/08/21 13:45:46 INFO Executor: Finished task 114.0 in stage 1.0 (TID 284). 843 bytes result sent to driver
15/08/21 13:45:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:46 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 300, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:46 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:46 INFO Executor: Running task 130.0 in stage 1.0 (TID 300)
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:46 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:46 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 284) in 25499 ms on localhost (115/200)
15/08/21 13:45:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:46 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:46 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:46 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:47 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:47 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:47 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:47 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:47 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:47 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:47 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:47 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:47 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:47 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:47 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:47 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:45:48 INFO ColumnChunkPageWriteStore: written 2,671,112B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,993B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:48 INFO ColumnChunkPageWriteStore: written 832,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,713B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:45:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000117
15/08/21 13:45:48 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000117_0: Committed
15/08/21 13:45:48 INFO Executor: Finished task 117.0 in stage 1.0 (TID 287). 843 bytes result sent to driver
15/08/21 13:45:48 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 301, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:48 INFO Executor: Running task 131.0 in stage 1.0 (TID 301)
15/08/21 13:45:48 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 287) in 26036 ms on localhost (116/200)
15/08/21 13:45:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:45:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:48 INFO ColumnChunkPageWriteStore: written 2,665,047B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,928B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:48 INFO ColumnChunkPageWriteStore: written 834,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,713B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:45:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000115
15/08/21 13:45:48 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000115_0: Committed
15/08/21 13:45:48 INFO Executor: Finished task 115.0 in stage 1.0 (TID 285). 843 bytes result sent to driver
15/08/21 13:45:48 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 302, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:48 INFO Executor: Running task 132.0 in stage 1.0 (TID 302)
15/08/21 13:45:48 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 285) in 26972 ms on localhost (117/200)
15/08/21 13:45:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:45:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:48 INFO ColumnChunkPageWriteStore: written 2,666,031B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,912B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:48 INFO ColumnChunkPageWriteStore: written 837,303B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 837,021B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:45:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000116
15/08/21 13:45:53 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000116_0: Committed
15/08/21 13:45:53 INFO Executor: Finished task 116.0 in stage 1.0 (TID 286). 843 bytes result sent to driver
15/08/21 13:45:53 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 303, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:53 INFO Executor: Running task 133.0 in stage 1.0 (TID 303)
15/08/21 13:45:53 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 286) in 31674 ms on localhost (118/200)
15/08/21 13:45:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:45:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:45:54 INFO ColumnChunkPageWriteStore: written 2,671,005B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,886B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:54 INFO ColumnChunkPageWriteStore: written 834,168B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,886B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:45:54 INFO ColumnChunkPageWriteStore: written 2,665,196B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,077B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:54 INFO ColumnChunkPageWriteStore: written 834,711B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,429B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:45:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000119
15/08/21 13:45:54 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000119_0: Committed
15/08/21 13:45:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000123
15/08/21 13:45:54 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000123_0: Committed
15/08/21 13:45:54 INFO Executor: Finished task 123.0 in stage 1.0 (TID 293). 843 bytes result sent to driver
15/08/21 13:45:54 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 304, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:54 INFO Executor: Finished task 119.0 in stage 1.0 (TID 289). 843 bytes result sent to driver
15/08/21 13:45:54 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 305, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:54 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 293) in 31222 ms on localhost (119/200)
15/08/21 13:45:54 INFO Executor: Running task 134.0 in stage 1.0 (TID 304)
15/08/21 13:45:54 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 289) in 31938 ms on localhost (120/200)
15/08/21 13:45:54 INFO Executor: Running task 135.0 in stage 1.0 (TID 305)
15/08/21 13:45:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:45:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:45:54 INFO ColumnChunkPageWriteStore: written 2,671,269B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,150B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:54 INFO ColumnChunkPageWriteStore: written 834,770B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,488B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:45:54 INFO ColumnChunkPageWriteStore: written 2,664,913B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,794B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:54 INFO ColumnChunkPageWriteStore: written 833,998B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,716B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:45:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:45:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000118
15/08/21 13:45:54 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000118_0: Committed
15/08/21 13:45:54 INFO Executor: Finished task 118.0 in stage 1.0 (TID 288). 843 bytes result sent to driver
15/08/21 13:45:54 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 306, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:54 INFO Executor: Running task 136.0 in stage 1.0 (TID 306)
15/08/21 13:45:54 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 288) in 32460 ms on localhost (121/200)
15/08/21 13:45:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000122
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:55 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000122_0: Committed
15/08/21 13:45:55 INFO Executor: Finished task 122.0 in stage 1.0 (TID 292). 843 bytes result sent to driver
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 13:45:55 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 307, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:55 INFO Executor: Running task 137.0 in stage 1.0 (TID 307)
15/08/21 13:45:55 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 292) in 31864 ms on localhost (122/200)
15/08/21 13:45:55 INFO ColumnChunkPageWriteStore: written 2,671,208B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,089B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:55 INFO ColumnChunkPageWriteStore: written 832,665B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,383B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:45:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000120
15/08/21 13:45:55 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000120_0: Committed
15/08/21 13:45:55 INFO Executor: Finished task 120.0 in stage 1.0 (TID 290). 843 bytes result sent to driver
15/08/21 13:45:55 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 308, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:55 INFO Executor: Running task 138.0 in stage 1.0 (TID 308)
15/08/21 13:45:55 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 290) in 32293 ms on localhost (123/200)
15/08/21 13:45:55 INFO ColumnChunkPageWriteStore: written 2,665,084B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,965B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:55 INFO ColumnChunkPageWriteStore: written 836,016B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,734B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:45:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000121
15/08/21 13:45:55 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000121_0: Committed
15/08/21 13:45:55 INFO Executor: Finished task 121.0 in stage 1.0 (TID 291). 843 bytes result sent to driver
15/08/21 13:45:55 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 309, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:55 INFO Executor: Running task 139.0 in stage 1.0 (TID 309)
15/08/21 13:45:55 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 291) in 32316 ms on localhost (124/200)
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:45:55 INFO ColumnChunkPageWriteStore: written 2,665,944B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,825B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:55 INFO ColumnChunkPageWriteStore: written 835,183B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,901B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:45:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000124
15/08/21 13:45:55 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000124_0: Committed
15/08/21 13:45:55 INFO Executor: Finished task 124.0 in stage 1.0 (TID 294). 843 bytes result sent to driver
15/08/21 13:45:55 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 310, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:55 INFO Executor: Running task 140.0 in stage 1.0 (TID 310)
15/08/21 13:45:55 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 294) in 32098 ms on localhost (125/200)
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:45:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:56 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:56 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:45:56 INFO CodecConfig: Compression: GZIP
15/08/21 13:45:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:45:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:45:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:45:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:45:56 INFO ParquetOutputFormat: Validation is off
15/08/21 13:45:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:45:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:45:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:45:56 INFO ColumnChunkPageWriteStore: written 2,671,062B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,943B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:56 INFO ColumnChunkPageWriteStore: written 834,555B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,273B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:45:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:45:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000126
15/08/21 13:45:57 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000126_0: Committed
15/08/21 13:45:57 INFO Executor: Finished task 126.0 in stage 1.0 (TID 296). 843 bytes result sent to driver
15/08/21 13:45:57 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 311, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:45:57 INFO Executor: Running task 141.0 in stage 1.0 (TID 311)
15/08/21 13:45:57 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 296) in 26874 ms on localhost (126/200)
15/08/21 13:45:57 INFO ColumnChunkPageWriteStore: written 2,671,132B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,013B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:45:57 INFO ColumnChunkPageWriteStore: written 833,511B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,229B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:45:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:45:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 13:45:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000125
15/08/21 13:45:57 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000125_0: Committed
15/08/21 13:46:02 INFO Executor: Finished task 125.0 in stage 1.0 (TID 295). 843 bytes result sent to driver
15/08/21 13:46:02 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 312, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:02 INFO Executor: Running task 142.0 in stage 1.0 (TID 312)
15/08/21 13:46:02 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 295) in 38493 ms on localhost (127/200)
15/08/21 13:46:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:46:03 INFO ColumnChunkPageWriteStore: written 2,671,683B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,564B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:03 INFO ColumnChunkPageWriteStore: written 834,278B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,996B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:46:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:03 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:03 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000128
15/08/21 13:46:03 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000128_0: Committed
15/08/21 13:46:03 INFO Executor: Finished task 128.0 in stage 1.0 (TID 298). 843 bytes result sent to driver
15/08/21 13:46:03 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 313, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:03 INFO Executor: Running task 143.0 in stage 1.0 (TID 313)
15/08/21 13:46:03 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 298) in 25323 ms on localhost (128/200)
15/08/21 13:46:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:46:04 INFO ColumnChunkPageWriteStore: written 2,671,421B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,302B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:04 INFO ColumnChunkPageWriteStore: written 832,191B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,909B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:46:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508211345_0001_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211345_0001_m_000127
15/08/21 13:46:04 INFO SparkHadoopMapRedUtil: attempt_201508211345_0001_m_000127_0: Committed
15/08/21 13:46:04 INFO Executor: Finished task 127.0 in stage 1.0 (TID 297). 843 bytes result sent to driver
15/08/21 13:46:04 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 314, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:04 INFO Executor: Running task 144.0 in stage 1.0 (TID 314)
15/08/21 13:46:04 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 297) in 26507 ms on localhost (129/200)
15/08/21 13:46:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:20 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:20 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:46:21 INFO ColumnChunkPageWriteStore: written 2,665,074B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,955B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:21 INFO ColumnChunkPageWriteStore: written 831,680B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,398B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:46:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000129
15/08/21 13:46:21 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000129_0: Committed
15/08/21 13:46:21 INFO Executor: Finished task 129.0 in stage 1.0 (TID 299). 843 bytes result sent to driver
15/08/21 13:46:21 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 315, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:21 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 299) in 41336 ms on localhost (130/200)
15/08/21 13:46:21 INFO Executor: Running task 145.0 in stage 1.0 (TID 315)
15/08/21 13:46:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:22 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:22 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:22 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:22 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:46:23 INFO ColumnChunkPageWriteStore: written 2,664,800B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,681B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:23 INFO ColumnChunkPageWriteStore: written 834,840B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,558B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:46:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000130
15/08/21 13:46:23 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000130_0: Committed
15/08/21 13:46:23 INFO Executor: Finished task 130.0 in stage 1.0 (TID 300). 843 bytes result sent to driver
15/08/21 13:46:23 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 316, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:23 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 300) in 36857 ms on localhost (131/200)
15/08/21 13:46:23 INFO Executor: Running task 146.0 in stage 1.0 (TID 316)
15/08/21 13:46:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:24 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:24 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:46:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:27 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:27 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:27 INFO ColumnChunkPageWriteStore: written 2,671,115B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,996B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:27 INFO ColumnChunkPageWriteStore: written 833,766B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,484B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:46:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000133
15/08/21 13:46:28 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000133_0: Committed
15/08/21 13:46:28 INFO Executor: Finished task 133.0 in stage 1.0 (TID 303). 843 bytes result sent to driver
15/08/21 13:46:28 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 317, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:28 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 303) in 34093 ms on localhost (132/200)
15/08/21 13:46:28 INFO Executor: Running task 147.0 in stage 1.0 (TID 317)
15/08/21 13:46:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:28 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:28 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:46:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:28 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:28 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:28 INFO ColumnChunkPageWriteStore: written 2,665,010B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,891B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:28 INFO ColumnChunkPageWriteStore: written 835,357B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,075B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:46:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000131
15/08/21 13:46:28 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000131_0: Committed
15/08/21 13:46:28 INFO Executor: Finished task 131.0 in stage 1.0 (TID 301). 843 bytes result sent to driver
15/08/21 13:46:28 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 318, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:28 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 301) in 39997 ms on localhost (133/200)
15/08/21 13:46:28 INFO Executor: Running task 148.0 in stage 1.0 (TID 318)
15/08/21 13:46:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:46:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 13:46:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:46:29 INFO ColumnChunkPageWriteStore: written 2,665,953B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,834B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:29 INFO ColumnChunkPageWriteStore: written 835,522B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,240B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 13:46:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:29 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:29 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:29 INFO ColumnChunkPageWriteStore: written 2,670,779B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,660B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:29 INFO ColumnChunkPageWriteStore: written 834,758B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,476B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:46:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000132
15/08/21 13:46:29 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000132_0: Committed
15/08/21 13:46:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000134
15/08/21 13:46:29 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000134_0: Committed
15/08/21 13:46:29 INFO Executor: Finished task 132.0 in stage 1.0 (TID 302). 843 bytes result sent to driver
15/08/21 13:46:29 INFO Executor: Finished task 134.0 in stage 1.0 (TID 304). 843 bytes result sent to driver
15/08/21 13:46:29 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 319, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:29 INFO Executor: Running task 149.0 in stage 1.0 (TID 319)
15/08/21 13:46:29 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 320, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:29 INFO Executor: Running task 150.0 in stage 1.0 (TID 320)
15/08/21 13:46:29 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 302) in 40985 ms on localhost (134/200)
15/08/21 13:46:29 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 304) in 35177 ms on localhost (135/200)
15/08/21 13:46:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:46:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:29 INFO ColumnChunkPageWriteStore: written 2,670,949B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,830B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:29 INFO ColumnChunkPageWriteStore: written 834,040B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,758B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:46:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:46:30 INFO ColumnChunkPageWriteStore: written 2,671,262B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,143B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:30 INFO ColumnChunkPageWriteStore: written 833,200B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,918B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:46:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000136
15/08/21 13:46:30 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000136_0: Committed
15/08/21 13:46:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:30 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:30 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:30 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:30 INFO Executor: Finished task 136.0 in stage 1.0 (TID 306). 843 bytes result sent to driver
15/08/21 13:46:30 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 321, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:30 INFO Executor: Running task 151.0 in stage 1.0 (TID 321)
15/08/21 13:46:30 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 306) in 35126 ms on localhost (136/200)
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:46:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000135
15/08/21 13:46:30 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000135_0: Committed
15/08/21 13:46:30 INFO Executor: Finished task 135.0 in stage 1.0 (TID 305). 843 bytes result sent to driver
15/08/21 13:46:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:46:30 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 322, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:30 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 305) in 35625 ms on localhost (137/200)
15/08/21 13:46:30 INFO Executor: Running task 152.0 in stage 1.0 (TID 322)
15/08/21 13:46:30 INFO ColumnChunkPageWriteStore: written 2,665,071B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,952B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:30 INFO ColumnChunkPageWriteStore: written 835,821B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,539B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:46:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000139
15/08/21 13:46:30 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000139_0: Committed
15/08/21 13:46:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:46:30 INFO Executor: Finished task 139.0 in stage 1.0 (TID 309). 843 bytes result sent to driver
15/08/21 13:46:30 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 323, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:30 INFO Executor: Running task 153.0 in stage 1.0 (TID 323)
15/08/21 13:46:30 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 309) in 35069 ms on localhost (138/200)
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:30 INFO ColumnChunkPageWriteStore: written 2,665,032B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,913B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:30 INFO ColumnChunkPageWriteStore: written 832,389B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,107B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:46:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:46:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000137
15/08/21 13:46:30 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000137_0: Committed
15/08/21 13:46:30 INFO Executor: Finished task 137.0 in stage 1.0 (TID 307). 843 bytes result sent to driver
15/08/21 13:46:30 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 324, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:30 INFO Executor: Running task 154.0 in stage 1.0 (TID 324)
15/08/21 13:46:30 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 307) in 35594 ms on localhost (139/200)
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:46:30 INFO ColumnChunkPageWriteStore: written 2,665,955B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,836B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:30 INFO ColumnChunkPageWriteStore: written 832,522B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,240B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:46:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000140
15/08/21 13:46:30 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000140_0: Committed
15/08/21 13:46:30 INFO Executor: Finished task 140.0 in stage 1.0 (TID 310). 843 bytes result sent to driver
15/08/21 13:46:30 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 325, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:30 INFO Executor: Running task 155.0 in stage 1.0 (TID 325)
15/08/21 13:46:30 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 310) in 35216 ms on localhost (140/200)
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:46:36 INFO ColumnChunkPageWriteStore: written 2,665,250B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,131B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:36 INFO ColumnChunkPageWriteStore: written 831,478B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,196B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:46:36 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000138
15/08/21 13:46:36 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000138_0: Committed
15/08/21 13:46:36 INFO Executor: Finished task 138.0 in stage 1.0 (TID 308). 843 bytes result sent to driver
15/08/21 13:46:36 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 326, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:36 INFO Executor: Running task 156.0 in stage 1.0 (TID 326)
15/08/21 13:46:36 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 308) in 41815 ms on localhost (141/200)
15/08/21 13:46:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 13:46:37 INFO ColumnChunkPageWriteStore: written 2,670,980B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,861B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:37 INFO ColumnChunkPageWriteStore: written 834,170B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,888B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 13:46:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000141
15/08/21 13:46:37 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000141_0: Committed
15/08/21 13:46:37 INFO Executor: Finished task 141.0 in stage 1.0 (TID 311). 843 bytes result sent to driver
15/08/21 13:46:37 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 327, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:37 INFO Executor: Running task 157.0 in stage 1.0 (TID 327)
15/08/21 13:46:37 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 311) in 40239 ms on localhost (142/200)
15/08/21 13:46:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:37 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:37 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:46:38 INFO ColumnChunkPageWriteStore: written 2,671,252B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,133B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:38 INFO ColumnChunkPageWriteStore: written 833,254B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,972B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:46:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:38 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:38 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:38 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000142
15/08/21 13:46:38 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000142_0: Committed
15/08/21 13:46:38 INFO Executor: Finished task 142.0 in stage 1.0 (TID 312). 843 bytes result sent to driver
15/08/21 13:46:38 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 328, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:38 INFO Executor: Running task 158.0 in stage 1.0 (TID 328)
15/08/21 13:46:38 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 312) in 35651 ms on localhost (143/200)
15/08/21 13:46:38 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:46:39 INFO ColumnChunkPageWriteStore: written 2,671,240B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,121B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:39 INFO ColumnChunkPageWriteStore: written 832,390B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,108B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:46:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000143
15/08/21 13:46:39 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000143_0: Committed
15/08/21 13:46:39 INFO Executor: Finished task 143.0 in stage 1.0 (TID 313). 843 bytes result sent to driver
15/08/21 13:46:39 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 329, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:39 INFO Executor: Running task 159.0 in stage 1.0 (TID 329)
15/08/21 13:46:39 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 313) in 35715 ms on localhost (144/200)
15/08/21 13:46:39 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:45 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:45 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:46:45 INFO ColumnChunkPageWriteStore: written 2,671,505B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,386B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:45 INFO ColumnChunkPageWriteStore: written 834,710B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,428B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:46:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000144
15/08/21 13:46:45 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000144_0: Committed
15/08/21 13:46:45 INFO Executor: Finished task 144.0 in stage 1.0 (TID 314). 843 bytes result sent to driver
15/08/21 13:46:45 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 330, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:45 INFO Executor: Running task 160.0 in stage 1.0 (TID 330)
15/08/21 13:46:45 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 314) in 41370 ms on localhost (145/200)
15/08/21 13:46:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:45 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:45 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:45 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:46:46 INFO ColumnChunkPageWriteStore: written 2,664,977B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,858B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:46 INFO ColumnChunkPageWriteStore: written 832,215B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,933B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:46:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000145
15/08/21 13:46:46 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000145_0: Committed
15/08/21 13:46:46 INFO Executor: Finished task 145.0 in stage 1.0 (TID 315). 843 bytes result sent to driver
15/08/21 13:46:47 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 331, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:47 INFO Executor: Running task 161.0 in stage 1.0 (TID 331)
15/08/21 13:46:47 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 315) in 25305 ms on localhost (146/200)
15/08/21 13:46:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:46:47 INFO ColumnChunkPageWriteStore: written 2,665,008B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,889B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:47 INFO ColumnChunkPageWriteStore: written 835,046B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,764B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:46:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000146
15/08/21 13:46:47 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000146_0: Committed
15/08/21 13:46:48 INFO Executor: Finished task 146.0 in stage 1.0 (TID 316). 843 bytes result sent to driver
15/08/21 13:46:48 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 332, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:48 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 316) in 24750 ms on localhost (147/200)
15/08/21 13:46:48 INFO Executor: Running task 162.0 in stage 1.0 (TID 332)
15/08/21 13:46:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:46:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:53 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:53 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:53 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:46:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:46:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:46:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:46:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:46:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:46:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:46:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:46:55 INFO ColumnChunkPageWriteStore: written 2,671,251B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,132B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:55 INFO ColumnChunkPageWriteStore: written 833,479B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,197B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:46:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:46:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000150
15/08/21 13:46:55 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000150_0: Committed
15/08/21 13:46:55 INFO Executor: Finished task 150.0 in stage 1.0 (TID 320). 843 bytes result sent to driver
15/08/21 13:46:55 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 333, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:55 INFO Executor: Running task 163.0 in stage 1.0 (TID 333)
15/08/21 13:46:55 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 320) in 25498 ms on localhost (148/200)
15/08/21 13:46:55 INFO ColumnChunkPageWriteStore: written 2,664,987B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,868B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:46:55 INFO ColumnChunkPageWriteStore: written 833,748B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,466B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:46:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:46:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:46:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000147
15/08/21 13:46:59 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000147_0: Committed
15/08/21 13:46:59 INFO Executor: Finished task 147.0 in stage 1.0 (TID 317). 843 bytes result sent to driver
15/08/21 13:46:59 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 334, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:46:59 INFO Executor: Running task 164.0 in stage 1.0 (TID 334)
15/08/21 13:47:00 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 317) in 31929 ms on localhost (149/200)
15/08/21 13:47:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:00 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:00 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:00 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:00 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:00 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:00 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:47:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:00 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:00 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:00 INFO ColumnChunkPageWriteStore: written 2,671,374B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,255B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:00 INFO ColumnChunkPageWriteStore: written 833,179B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,897B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:47:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:47:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000149
15/08/21 13:47:00 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000149_0: Committed
15/08/21 13:47:00 INFO Executor: Finished task 149.0 in stage 1.0 (TID 319). 843 bytes result sent to driver
15/08/21 13:47:00 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 335, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:00 INFO Executor: Running task 165.0 in stage 1.0 (TID 335)
15/08/21 13:47:00 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 319) in 30959 ms on localhost (150/200)
15/08/21 13:47:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:00 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:00 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:00 INFO ColumnChunkPageWriteStore: written 2,666,190B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,071B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:00 INFO ColumnChunkPageWriteStore: written 831,826B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,544B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:47:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000148
15/08/21 13:47:00 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000148_0: Committed
15/08/21 13:47:00 INFO Executor: Finished task 148.0 in stage 1.0 (TID 318). 843 bytes result sent to driver
15/08/21 13:47:00 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 336, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:00 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 318) in 32408 ms on localhost (151/200)
15/08/21 13:47:00 INFO Executor: Running task 166.0 in stage 1.0 (TID 336)
15/08/21 13:47:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:01 INFO ColumnChunkPageWriteStore: written 2,665,303B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,184B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:01 INFO ColumnChunkPageWriteStore: written 834,375B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,093B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:01 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:01 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000154
15/08/21 13:47:01 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000154_0: Committed
15/08/21 13:47:01 INFO Executor: Finished task 154.0 in stage 1.0 (TID 324). 843 bytes result sent to driver
15/08/21 13:47:01 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 337, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:01 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 324) in 31013 ms on localhost (152/200)
15/08/21 13:47:01 INFO Executor: Running task 167.0 in stage 1.0 (TID 337)
15/08/21 13:47:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:47:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:01 INFO ColumnChunkPageWriteStore: written 2,671,152B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,033B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:01 INFO ColumnChunkPageWriteStore: written 831,567B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,285B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:47:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:01 INFO ColumnChunkPageWriteStore: written 2,671,444B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,325B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:01 INFO ColumnChunkPageWriteStore: written 832,700B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,418B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000151
15/08/21 13:47:01 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000151_0: Committed
15/08/21 13:47:01 INFO Executor: Finished task 151.0 in stage 1.0 (TID 321). 843 bytes result sent to driver
15/08/21 13:47:01 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 338, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:01 INFO Executor: Running task 168.0 in stage 1.0 (TID 338)
15/08/21 13:47:01 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 321) in 31868 ms on localhost (153/200)
15/08/21 13:47:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508211346_0001_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211346_0001_m_000152
15/08/21 13:47:01 INFO SparkHadoopMapRedUtil: attempt_201508211346_0001_m_000152_0: Committed
15/08/21 13:47:01 INFO Executor: Finished task 152.0 in stage 1.0 (TID 322). 843 bytes result sent to driver
15/08/21 13:47:01 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 339, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:01 INFO Executor: Running task 169.0 in stage 1.0 (TID 339)
15/08/21 13:47:01 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 322) in 31781 ms on localhost (154/200)
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:47:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:02 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:02 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:02 INFO ColumnChunkPageWriteStore: written 2,670,940B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,821B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:02 INFO ColumnChunkPageWriteStore: written 833,949B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,667B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:47:02 INFO ColumnChunkPageWriteStore: written 2,665,144B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,025B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:02 INFO ColumnChunkPageWriteStore: written 832,740B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,458B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000157
15/08/21 13:47:02 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000157_0: Committed
15/08/21 13:47:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000155
15/08/21 13:47:02 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000155_0: Committed
15/08/21 13:47:02 INFO Executor: Finished task 155.0 in stage 1.0 (TID 325). 843 bytes result sent to driver
15/08/21 13:47:02 INFO Executor: Finished task 157.0 in stage 1.0 (TID 327). 843 bytes result sent to driver
15/08/21 13:47:02 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 340, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:02 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 341, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:02 INFO Executor: Running task 170.0 in stage 1.0 (TID 340)
15/08/21 13:47:02 INFO Executor: Running task 171.0 in stage 1.0 (TID 341)
15/08/21 13:47:02 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 325) in 31696 ms on localhost (155/200)
15/08/21 13:47:02 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 327) in 25129 ms on localhost (156/200)
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:47:02 INFO ColumnChunkPageWriteStore: written 2,665,172B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,053B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:02 INFO ColumnChunkPageWriteStore: written 833,923B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,641B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:47:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000153
15/08/21 13:47:02 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000153_0: Committed
15/08/21 13:47:02 INFO Executor: Finished task 153.0 in stage 1.0 (TID 323). 843 bytes result sent to driver
15/08/21 13:47:02 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 342, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:02 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 323) in 32430 ms on localhost (157/200)
15/08/21 13:47:02 INFO Executor: Running task 172.0 in stage 1.0 (TID 342)
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:47:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:47:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:47:03 INFO ColumnChunkPageWriteStore: written 2,666,080B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,961B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:03 INFO ColumnChunkPageWriteStore: written 834,306B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,024B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:47:03 INFO ColumnChunkPageWriteStore: written 2,671,373B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,254B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:03 INFO ColumnChunkPageWriteStore: written 830,066B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 829,784B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:47:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000158
15/08/21 13:47:03 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000158_0: Committed
15/08/21 13:47:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000156
15/08/21 13:47:03 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000156_0: Committed
15/08/21 13:47:03 INFO Executor: Finished task 156.0 in stage 1.0 (TID 326). 843 bytes result sent to driver
15/08/21 13:47:03 INFO Executor: Finished task 158.0 in stage 1.0 (TID 328). 843 bytes result sent to driver
15/08/21 13:47:03 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 343, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:03 INFO Executor: Running task 173.0 in stage 1.0 (TID 343)
15/08/21 13:47:03 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 344, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:03 INFO Executor: Running task 174.0 in stage 1.0 (TID 344)
15/08/21 13:47:03 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 326) in 26433 ms on localhost (158/200)
15/08/21 13:47:03 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 328) in 25221 ms on localhost (159/200)
15/08/21 13:47:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 13:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/21 13:47:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:47:19 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:19 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:19 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:19 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:19 INFO ColumnChunkPageWriteStore: written 2,671,157B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,038B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:19 INFO ColumnChunkPageWriteStore: written 833,432B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,150B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:47:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000159
15/08/21 13:47:19 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000159_0: Committed
15/08/21 13:47:19 INFO Executor: Finished task 159.0 in stage 1.0 (TID 329). 843 bytes result sent to driver
15/08/21 13:47:19 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 345, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:19 INFO Executor: Running task 175.0 in stage 1.0 (TID 345)
15/08/21 13:47:19 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 329) in 40017 ms on localhost (160/200)
15/08/21 13:47:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:47:20 INFO ColumnChunkPageWriteStore: written 2,671,419B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,300B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:20 INFO ColumnChunkPageWriteStore: written 831,056B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,774B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:47:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000160
15/08/21 13:47:20 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000160_0: Committed
15/08/21 13:47:20 INFO Executor: Finished task 160.0 in stage 1.0 (TID 330). 843 bytes result sent to driver
15/08/21 13:47:20 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 346, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:20 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 330) in 34637 ms on localhost (161/200)
15/08/21 13:47:20 INFO Executor: Running task 176.0 in stage 1.0 (TID 346)
15/08/21 13:47:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:20 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:20 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:47:21 INFO ColumnChunkPageWriteStore: written 2,664,814B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,695B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:21 INFO ColumnChunkPageWriteStore: written 833,829B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,547B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:47:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000161
15/08/21 13:47:21 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000161_0: Committed
15/08/21 13:47:21 INFO Executor: Finished task 161.0 in stage 1.0 (TID 331). 843 bytes result sent to driver
15/08/21 13:47:21 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 347, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:21 INFO Executor: Running task 177.0 in stage 1.0 (TID 347)
15/08/21 13:47:21 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 331) in 34805 ms on localhost (162/200)
15/08/21 13:47:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:25 INFO ColumnChunkPageWriteStore: written 2,665,004B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,885B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:25 INFO ColumnChunkPageWriteStore: written 832,594B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,312B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:25 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000162
15/08/21 13:47:25 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000162_0: Committed
15/08/21 13:47:25 INFO Executor: Finished task 162.0 in stage 1.0 (TID 332). 843 bytes result sent to driver
15/08/21 13:47:25 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 348, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:25 INFO Executor: Running task 178.0 in stage 1.0 (TID 348)
15/08/21 13:47:25 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 332) in 37246 ms on localhost (163/200)
15/08/21 13:47:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:25 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:25 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:32 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:32 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:32 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:32 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:32 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:32 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:47:33 INFO ColumnChunkPageWriteStore: written 2,665,184B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,065B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:33 INFO ColumnChunkPageWriteStore: written 833,514B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,232B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:47:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000163
15/08/21 13:47:33 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000163_0: Committed
15/08/21 13:47:33 INFO Executor: Finished task 163.0 in stage 1.0 (TID 333). 843 bytes result sent to driver
15/08/21 13:47:33 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 349, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:33 INFO Executor: Running task 179.0 in stage 1.0 (TID 349)
15/08/21 13:47:33 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 333) in 37893 ms on localhost (164/200)
15/08/21 13:47:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:33 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:33 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:33 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:33 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:34 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:34 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:34 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:34 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:34 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:34 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:39 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:39 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:39 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:39 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:39 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:39 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:39 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:39 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:39 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:39 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:39 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:47:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:39 INFO ColumnChunkPageWriteStore: written 2,665,102B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,983B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:39 INFO ColumnChunkPageWriteStore: written 830,633B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,351B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:47:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000169
15/08/21 13:47:39 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000169_0: Committed
15/08/21 13:47:39 INFO Executor: Finished task 169.0 in stage 1.0 (TID 339). 843 bytes result sent to driver
15/08/21 13:47:39 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 350, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:39 INFO Executor: Running task 180.0 in stage 1.0 (TID 350)
15/08/21 13:47:39 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 339) in 37950 ms on localhost (165/200)
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 2,665,751B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,632B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 833,909B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,627B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 2,670,794B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,675B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 832,013B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,731B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000164
15/08/21 13:47:40 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000164_0: Committed
15/08/21 13:47:40 INFO Executor: Finished task 164.0 in stage 1.0 (TID 334). 843 bytes result sent to driver
15/08/21 13:47:40 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 351, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:40 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 334) in 40399 ms on localhost (166/200)
15/08/21 13:47:40 INFO Executor: Running task 181.0 in stage 1.0 (TID 351)
15/08/21 13:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:47:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000166
15/08/21 13:47:40 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000166_0: Committed
15/08/21 13:47:40 INFO Executor: Finished task 166.0 in stage 1.0 (TID 336). 843 bytes result sent to driver
15/08/21 13:47:40 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 352, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:40 INFO Executor: Running task 182.0 in stage 1.0 (TID 352)
15/08/21 13:47:40 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 336) in 39598 ms on localhost (167/200)
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 2,671,029B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,910B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 836,258B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,976B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 2,671,233B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,114B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 834,702B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,420B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 2,666,198B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,079B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:40 INFO ColumnChunkPageWriteStore: written 834,962B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,680B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000165
15/08/21 13:47:40 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000165_0: Committed
15/08/21 13:47:40 INFO Executor: Finished task 165.0 in stage 1.0 (TID 335). 843 bytes result sent to driver
15/08/21 13:47:40 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 353, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:40 INFO Executor: Running task 183.0 in stage 1.0 (TID 353)
15/08/21 13:47:40 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 335) in 39951 ms on localhost (168/200)
15/08/21 13:47:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000167
15/08/21 13:47:40 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000167_0: Committed
15/08/21 13:47:40 INFO Executor: Finished task 167.0 in stage 1.0 (TID 337). 843 bytes result sent to driver
15/08/21 13:47:40 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 354, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:40 INFO Executor: Running task 184.0 in stage 1.0 (TID 354)
15/08/21 13:47:40 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 337) in 39047 ms on localhost (169/200)
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000172
15/08/21 13:47:40 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000172_0: Committed
15/08/21 13:47:40 INFO Executor: Finished task 172.0 in stage 1.0 (TID 342). 843 bytes result sent to driver
15/08/21 13:47:40 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 355, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:40 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 342) in 37929 ms on localhost (170/200)
15/08/21 13:47:40 INFO Executor: Running task 185.0 in stage 1.0 (TID 355)
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 2,665,329B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,210B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 832,235B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,953B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:47:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:47:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000170
15/08/21 13:47:41 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000170_0: Committed
15/08/21 13:47:41 INFO Executor: Finished task 170.0 in stage 1.0 (TID 340). 843 bytes result sent to driver
15/08/21 13:47:41 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 356, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:41 INFO Executor: Running task 186.0 in stage 1.0 (TID 356)
15/08/21 13:47:41 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 340) in 38596 ms on localhost (171/200)
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 2,665,025B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,906B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 831,175B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,893B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:41 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:41 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:41 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 13:47:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 2,671,396B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,277B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 836,094B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,812B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 13:47:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 13:47:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000173
15/08/21 13:47:41 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000173_0: Committed
15/08/21 13:47:41 INFO Executor: Finished task 173.0 in stage 1.0 (TID 343). 843 bytes result sent to driver
15/08/21 13:47:41 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 357, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:41 INFO Executor: Running task 187.0 in stage 1.0 (TID 357)
15/08/21 13:47:41 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 343) in 38037 ms on localhost (172/200)
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 2,671,213B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,094B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 832,436B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,154B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 13:47:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000175
15/08/21 13:47:41 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000175_0: Committed
15/08/21 13:47:41 INFO Executor: Finished task 175.0 in stage 1.0 (TID 345). 843 bytes result sent to driver
15/08/21 13:47:41 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 358, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:41 INFO Executor: Running task 188.0 in stage 1.0 (TID 358)
15/08/21 13:47:41 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 345) in 21903 ms on localhost (173/200)
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 2,671,218B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,099B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 832,689B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,407B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000171
15/08/21 13:47:41 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000171_0: Committed
15/08/21 13:47:41 INFO Executor: Finished task 171.0 in stage 1.0 (TID 341). 843 bytes result sent to driver
15/08/21 13:47:41 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 359, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:41 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 341) in 39146 ms on localhost (174/200)
15/08/21 13:47:41 INFO Executor: Running task 189.0 in stage 1.0 (TID 359)
15/08/21 13:47:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:47:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000168
15/08/21 13:47:41 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000168_0: Committed
15/08/21 13:47:41 INFO Executor: Finished task 168.0 in stage 1.0 (TID 338). 843 bytes result sent to driver
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:41 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 360, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:41 INFO Executor: Running task 190.0 in stage 1.0 (TID 360)
15/08/21 13:47:41 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 338) in 39794 ms on localhost (175/200)
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 2,671,155B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,036B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 831,754B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,472B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000176
15/08/21 13:47:41 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000176_0: Committed
15/08/21 13:47:41 INFO Executor: Finished task 176.0 in stage 1.0 (TID 346). 843 bytes result sent to driver
15/08/21 13:47:41 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 361, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:41 INFO Executor: Running task 191.0 in stage 1.0 (TID 361)
15/08/21 13:47:41 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 346) in 21564 ms on localhost (176/200)
15/08/21 13:47:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 2,671,354B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,235B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:41 INFO ColumnChunkPageWriteStore: written 833,246B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,964B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000174
15/08/21 13:47:42 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000174_0: Committed
15/08/21 13:47:42 INFO Executor: Finished task 174.0 in stage 1.0 (TID 344). 843 bytes result sent to driver
15/08/21 13:47:42 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 362, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:42 INFO Executor: Running task 192.0 in stage 1.0 (TID 362)
15/08/21 13:47:42 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 344) in 38669 ms on localhost (177/200)
15/08/21 13:47:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:47:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:47 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:47 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:47:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:47:47 INFO ColumnChunkPageWriteStore: written 2,665,184B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,065B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:47 INFO ColumnChunkPageWriteStore: written 835,844B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,562B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:47:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000177
15/08/21 13:47:47 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000177_0: Committed
15/08/21 13:47:47 INFO Executor: Finished task 177.0 in stage 1.0 (TID 347). 843 bytes result sent to driver
15/08/21 13:47:47 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 363, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:47 INFO Executor: Running task 193.0 in stage 1.0 (TID 363)
15/08/21 13:47:47 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 347) in 25950 ms on localhost (178/200)
15/08/21 13:47:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:47:49 INFO ColumnChunkPageWriteStore: written 2,665,190B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,071B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:47:49 INFO ColumnChunkPageWriteStore: written 833,011B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,729B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:47:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000178
15/08/21 13:47:49 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000178_0: Committed
15/08/21 13:47:49 INFO Executor: Finished task 178.0 in stage 1.0 (TID 348). 843 bytes result sent to driver
15/08/21 13:47:49 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 364, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:47:49 INFO Executor: Running task 194.0 in stage 1.0 (TID 364)
15/08/21 13:47:49 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 348) in 24106 ms on localhost (179/200)
15/08/21 13:47:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:47:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:47:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:47:54 INFO CodecConfig: Compression: GZIP
15/08/21 13:47:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:47:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:47:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:47:54 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:47:54 INFO ParquetOutputFormat: Validation is off
15/08/21 13:47:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:47:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:01 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:01 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:48:02 INFO ColumnChunkPageWriteStore: written 2,665,124B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,005B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:02 INFO ColumnChunkPageWriteStore: written 834,872B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,590B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:48:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508211347_0001_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211347_0001_m_000179
15/08/21 13:48:02 INFO SparkHadoopMapRedUtil: attempt_201508211347_0001_m_000179_0: Committed
15/08/21 13:48:02 INFO Executor: Finished task 179.0 in stage 1.0 (TID 349). 843 bytes result sent to driver
15/08/21 13:48:02 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 365, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:48:02 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 349) in 29431 ms on localhost (180/200)
15/08/21 13:48:02 INFO Executor: Running task 195.0 in stage 1.0 (TID 365)
15/08/21 13:48:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:48:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:48:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:03 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:03 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:03 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:03 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 13:48:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:03 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:03 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:03 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:03 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:03 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:03 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:03 INFO ColumnChunkPageWriteStore: written 2,666,139B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,020B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:03 INFO ColumnChunkPageWriteStore: written 833,416B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,134B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 13:48:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:03 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:03 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:03 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:03 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000180
15/08/21 13:48:03 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000180_0: Committed
15/08/21 13:48:03 INFO Executor: Finished task 180.0 in stage 1.0 (TID 350). 843 bytes result sent to driver
15/08/21 13:48:03 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 366, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:48:03 INFO Executor: Running task 196.0 in stage 1.0 (TID 366)
15/08/21 13:48:03 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 350) in 23919 ms on localhost (181/200)
15/08/21 13:48:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:48:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:48:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:04 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:04 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:09 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:09 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:10 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:10 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:48:10 INFO ColumnChunkPageWriteStore: written 2,671,236B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,117B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:10 INFO ColumnChunkPageWriteStore: written 833,872B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,590B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:48:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:48:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000182
15/08/21 13:48:10 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000182_0: Committed
15/08/21 13:48:10 INFO Executor: Finished task 182.0 in stage 1.0 (TID 352). 843 bytes result sent to driver
15/08/21 13:48:10 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 367, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:48:10 INFO Executor: Running task 197.0 in stage 1.0 (TID 367)
15/08/21 13:48:10 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 352) in 30141 ms on localhost (182/200)
15/08/21 13:48:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:48:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:48:10 INFO ColumnChunkPageWriteStore: written 2,671,529B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,410B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:10 INFO ColumnChunkPageWriteStore: written 833,348B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,066B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:48:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:48:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000191
15/08/21 13:48:10 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000191_0: Committed
15/08/21 13:48:10 INFO Executor: Finished task 191.0 in stage 1.0 (TID 361). 843 bytes result sent to driver
15/08/21 13:48:10 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 368, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:48:10 INFO Executor: Running task 198.0 in stage 1.0 (TID 368)
15/08/21 13:48:10 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 361) in 28948 ms on localhost (183/200)
15/08/21 13:48:10 INFO ColumnChunkPageWriteStore: written 2,671,450B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,331B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:10 INFO ColumnChunkPageWriteStore: written 831,022B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,740B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:48:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:48:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:48:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000189
15/08/21 13:48:10 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000189_0: Committed
15/08/21 13:48:10 INFO Executor: Finished task 189.0 in stage 1.0 (TID 359). 843 bytes result sent to driver
15/08/21 13:48:10 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 369, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 13:48:10 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 359) in 29230 ms on localhost (184/200)
15/08/21 13:48:10 INFO Executor: Running task 199.0 in stage 1.0 (TID 369)
15/08/21 13:48:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:48:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 2,671,442B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,323B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 833,079B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,797B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000190
15/08/21 13:48:11 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000190_0: Committed
15/08/21 13:48:11 INFO Executor: Finished task 190.0 in stage 1.0 (TID 360). 843 bytes result sent to driver
15/08/21 13:48:11 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 360) in 29494 ms on localhost (185/200)
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 2,671,394B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,275B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 833,536B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,254B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:48:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:11 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:11 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000183
15/08/21 13:48:11 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000183_0: Committed
15/08/21 13:48:11 INFO Executor: Finished task 183.0 in stage 1.0 (TID 353). 843 bytes result sent to driver
15/08/21 13:48:11 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 353) in 30658 ms on localhost (186/200)
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 2,671,432B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,313B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 830,734B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,452B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000181
15/08/21 13:48:11 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000181_0: Committed
15/08/21 13:48:11 INFO Executor: Finished task 181.0 in stage 1.0 (TID 351). 843 bytes result sent to driver
15/08/21 13:48:11 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 351) in 30981 ms on localhost (187/200)
15/08/21 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 2,671,254B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,135B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 835,132B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,850B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 2,665,090B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,971B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 832,815B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,533B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000184
15/08/21 13:48:11 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000184_0: Committed
15/08/21 13:48:11 INFO Executor: Finished task 184.0 in stage 1.0 (TID 354). 843 bytes result sent to driver
15/08/21 13:48:11 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 354) in 30899 ms on localhost (188/200)
15/08/21 13:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000185
15/08/21 13:48:11 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000185_0: Committed
15/08/21 13:48:11 INFO Executor: Finished task 185.0 in stage 1.0 (TID 355). 843 bytes result sent to driver
15/08/21 13:48:11 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 355) in 30881 ms on localhost (189/200)
15/08/21 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 2,666,015B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,896B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 834,611B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,329B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:48:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000188
15/08/21 13:48:11 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000188_0: Committed
15/08/21 13:48:11 INFO Executor: Finished task 188.0 in stage 1.0 (TID 358). 843 bytes result sent to driver
15/08/21 13:48:11 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 358) in 30229 ms on localhost (190/200)
15/08/21 13:48:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:48:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:11 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:11 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:11 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 2,664,952B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,833B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:11 INFO ColumnChunkPageWriteStore: written 835,727B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,445B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:48:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:48:12 INFO ColumnChunkPageWriteStore: written 2,665,171B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,052B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:12 INFO ColumnChunkPageWriteStore: written 834,877B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,595B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:48:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000187
15/08/21 13:48:12 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000187_0: Committed
15/08/21 13:48:12 INFO Executor: Finished task 187.0 in stage 1.0 (TID 357). 843 bytes result sent to driver
15/08/21 13:48:12 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 357) in 30773 ms on localhost (191/200)
15/08/21 13:48:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 13:48:12 INFO ColumnChunkPageWriteStore: written 2,671,177B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,058B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:12 INFO ColumnChunkPageWriteStore: written 835,289B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,007B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 13:48:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000186
15/08/21 13:48:12 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000186_0: Committed
15/08/21 13:48:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000192
15/08/21 13:48:12 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000192_0: Committed
15/08/21 13:48:12 INFO Executor: Finished task 186.0 in stage 1.0 (TID 356). 843 bytes result sent to driver
15/08/21 13:48:12 INFO Executor: Finished task 192.0 in stage 1.0 (TID 362). 843 bytes result sent to driver
15/08/21 13:48:12 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 356) in 31270 ms on localhost (192/200)
15/08/21 13:48:12 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 362) in 30282 ms on localhost (193/200)
15/08/21 13:48:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:48:13 INFO ColumnChunkPageWriteStore: written 2,664,917B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,798B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:13 INFO ColumnChunkPageWriteStore: written 833,921B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,639B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:48:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000193
15/08/21 13:48:13 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000193_0: Committed
15/08/21 13:48:13 INFO Executor: Finished task 193.0 in stage 1.0 (TID 363). 843 bytes result sent to driver
15/08/21 13:48:13 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 363) in 25951 ms on localhost (194/200)
15/08/21 13:48:14 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 13:48:14 INFO ColumnChunkPageWriteStore: written 2,665,340B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,221B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:14 INFO ColumnChunkPageWriteStore: written 831,556B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,274B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 13:48:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000194
15/08/21 13:48:26 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000194_0: Committed
15/08/21 13:48:26 INFO Executor: Finished task 194.0 in stage 1.0 (TID 364). 843 bytes result sent to driver
15/08/21 13:48:26 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 364) in 36953 ms on localhost (195/200)
15/08/21 13:48:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:26 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:26 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:48:28 INFO ColumnChunkPageWriteStore: written 2,665,052B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,933B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:28 INFO ColumnChunkPageWriteStore: written 834,559B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,277B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:48:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000195
15/08/21 13:48:28 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000195_0: Committed
15/08/21 13:48:28 INFO Executor: Finished task 195.0 in stage 1.0 (TID 365). 843 bytes result sent to driver
15/08/21 13:48:28 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 365) in 25767 ms on localhost (196/200)
15/08/21 13:48:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 13:48:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:28 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:28 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:28 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:28 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:28 INFO ColumnChunkPageWriteStore: written 2,666,316B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,197B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:28 INFO ColumnChunkPageWriteStore: written 832,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,713B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 13:48:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:48:28 INFO CodecConfig: Compression: GZIP
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:48:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:48:28 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:48:28 INFO ParquetOutputFormat: Validation is off
15/08/21 13:48:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:48:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000196
15/08/21 13:48:28 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000196_0: Committed
15/08/21 13:48:28 INFO Executor: Finished task 196.0 in stage 1.0 (TID 366). 843 bytes result sent to driver
15/08/21 13:48:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:48:28 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 366) in 24769 ms on localhost (197/200)
15/08/21 13:48:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 13:48:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 13:48:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 13:48:30 INFO ColumnChunkPageWriteStore: written 2,671,110B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,991B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:30 INFO ColumnChunkPageWriteStore: written 2,671,199B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,080B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:30 INFO ColumnChunkPageWriteStore: written 832,892B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,610B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 13:48:30 INFO ColumnChunkPageWriteStore: written 832,452B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,170B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 13:48:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000199
15/08/21 13:48:30 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000199_0: Committed
15/08/21 13:48:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000197
15/08/21 13:48:30 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000197_0: Committed
15/08/21 13:48:30 INFO Executor: Finished task 199.0 in stage 1.0 (TID 369). 843 bytes result sent to driver
15/08/21 13:48:30 INFO ColumnChunkPageWriteStore: written 2,671,458B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,339B comp, 3 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:48:30 INFO Executor: Finished task 197.0 in stage 1.0 (TID 367). 843 bytes result sent to driver
15/08/21 13:48:30 INFO ColumnChunkPageWriteStore: written 833,958B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,676B comp, 6 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 13:48:30 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 369) in 19634 ms on localhost (198/200)
15/08/21 13:48:30 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 367) in 19883 ms on localhost (199/200)
15/08/21 13:48:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508211348_0001_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508211348_0001_m_000198
15/08/21 13:48:30 INFO SparkHadoopMapRedUtil: attempt_201508211348_0001_m_000198_0: Committed
15/08/21 13:48:30 INFO Executor: Finished task 198.0 in stage 1.0 (TID 368). 843 bytes result sent to driver
15/08/21 13:48:30 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 368) in 19797 ms on localhost (200/200)
15/08/21 13:48:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/21 13:48:30 INFO DAGScheduler: ResultStage 1 (processCmd at CliDriver.java:423) finished in 435.157 s
15/08/21 13:48:30 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@26d31d87
15/08/21 13:48:30 INFO StatsReportListener: task runtime:(count: 200, mean: 33844.440000, stdev: 6665.500650, max: 48487.000000, min: 19634.000000)
15/08/21 13:48:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:30 INFO StatsReportListener: 	19.6 s	24.8 s	25.8 s	29.3 s	32.2 s	38.2 s	42.8 s	46.5 s	48.5 s
15/08/21 13:48:30 INFO StatsReportListener: fetch wait time:(count: 200, mean: 1.365000, stdev: 2.356220, max: 22.000000, min: 0.000000)
15/08/21 13:48:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:30 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	4.0 ms	5.0 ms	22.0 ms
15/08/21 13:48:30 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 13:48:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:30 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 13:48:30 INFO DAGScheduler: Job 0 finished: processCmd at CliDriver.java:423, took 878.346538 s
15/08/21 13:48:30 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 13:48:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:30 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 13:48:30 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 98.841791, stdev: 4.753750, max: 99.903312, min: 58.632384)
15/08/21 13:48:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:30 INFO StatsReportListener: 	59 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 13:48:30 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.003966, stdev: 0.007176, max: 0.071115, min: 0.000000)
15/08/21 13:48:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:30 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 13:48:30 INFO StatsReportListener: other time pct: (count: 200, mean: 1.154243, stdev: 4.753979, max: 41.367616, min: 0.096688)
15/08/21 13:48:30 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:30 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	41 %
15/08/21 13:48:32 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:48:33 INFO DefaultWriterContainer: Job job_201508211333_0000 committed.
15/08/21 13:48:33 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:48:33 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_common_metadata
15/08/21 13:48:33 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 13:48:33 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 13:48:33 INFO DAGScheduler: Final stage: ResultStage 2(processCmd at CliDriver.java:423)
15/08/21 13:48:33 INFO DAGScheduler: Parents of final stage: List()
15/08/21 13:48:33 INFO DAGScheduler: Missing parents: List()
15/08/21 13:48:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 13:48:33 INFO MemoryStore: ensureFreeSpace(2976) called with curMem=458263, maxMem=22226833244
15/08/21 13:48:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 13:48:33 INFO MemoryStore: ensureFreeSpace(1784) called with curMem=461239, maxMem=22226833244
15/08/21 13:48:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1784.0 B, free 20.7 GB)
15/08/21 13:48:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:45550 (size: 1784.0 B, free: 20.7 GB)
15/08/21 13:48:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/21 13:48:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423)
15/08/21 13:48:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/21 13:48:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 370, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 13:48:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 370)
15/08/21 13:48:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 370). 606 bytes result sent to driver
15/08/21 13:48:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 370) in 65 ms on localhost (1/1)
15/08/21 13:48:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/21 13:48:33 INFO DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:423) finished in 0.066 s
15/08/21 13:48:33 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3ea71413
15/08/21 13:48:33 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 0.098194 s
15/08/21 13:48:33 INFO StatsReportListener: task runtime:(count: 1, mean: 65.000000, stdev: 0.000000, max: 65.000000, min: 65.000000)
15/08/21 13:48:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:33 INFO StatsReportListener: 	65.0 ms	65.0 ms	65.0 ms	65.0 ms	65.0 ms	65.0 ms	65.0 ms	65.0 ms	65.0 ms
15/08/21 13:48:33 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 13:48:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:33 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 13:48:33 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 18.461538, stdev: 0.000000, max: 18.461538, min: 18.461538)
15/08/21 13:48:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:33 INFO StatsReportListener: 	18 %	18 %	18 %	18 %	18 %	18 %	18 %	18 %	18 %
15/08/21 13:48:33 INFO StatsReportListener: other time pct: (count: 1, mean: 81.538462, stdev: 0.000000, max: 81.538462, min: 81.538462)
15/08/21 13:48:33 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:33 INFO StatsReportListener: 	82 %	82 %	82 %	82 %	82 %	82 %	82 %	82 %	82 %
Time taken: 886.238 seconds
15/08/21 13:48:33 INFO CliDriver: Time taken: 886.238 seconds
15/08/21 13:48:33 INFO ParseDriver: Parsing command: insert into table q18_large_volume_customer_par
select 
  c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice,sum(l_quantity)
from 
  customer_par c join orders_par o 
  on 
    c.c_custkey = o.o_custkey
  join q18_tmp_par t 
  on 
    o.o_orderkey = t.l_orderkey and t.t_sum_quantity > 315
  join lineitem_par l 
  on 
    o.o_orderkey = l.l_orderkey
group by c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice
order by o_totalprice desc,o_orderdate
limit 100
15/08/21 13:48:33 INFO ParseDriver: Parse Completed
15/08/21 13:48:34 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:48:34 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:48:34 INFO ParquetFileReader: reading another 10 footers
15/08/21 13:48:34 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:48:34 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:48:34 INFO ParquetFileReader: reading another 19 footers
15/08/21 13:48:34 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:48:35 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=463023, maxMem=22226833244
15/08/21 13:48:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 13:48:35 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=789631, maxMem=22226833244
15/08/21 13:48:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 13:48:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:45550 (size: 22.3 KB, free: 20.7 GB)
15/08/21 13:48:35 INFO SparkContext: Created broadcast 4 from processCmd at CliDriver.java:423
15/08/21 13:48:35 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=812424, maxMem=22226833244
15/08/21 13:48:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 13:48:35 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1139032, maxMem=22226833244
15/08/21 13:48:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 13:48:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:45550 (size: 22.3 KB, free: 20.7 GB)
15/08/21 13:48:35 INFO SparkContext: Created broadcast 5 from processCmd at CliDriver.java:423
15/08/21 13:48:35 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1161825, maxMem=22226833244
15/08/21 13:48:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 13:48:35 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1488433, maxMem=22226833244
15/08/21 13:48:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 13:48:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:45550 (size: 22.3 KB, free: 20.7 GB)
15/08/21 13:48:35 INFO SparkContext: Created broadcast 6 from processCmd at CliDriver.java:423
15/08/21 13:48:35 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1511226, maxMem=22226833244
15/08/21 13:48:35 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 13:48:35 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1837834, maxMem=22226833244
15/08/21 13:48:35 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 13:48:35 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45550 (size: 22.3 KB, free: 20.7 GB)
15/08/21 13:48:35 INFO SparkContext: Created broadcast 7 from processCmd at CliDriver.java:423
15/08/21 13:48:35 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 13:48:35 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 13:48:35 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 13:48:35 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 13:48:35 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 13:48:35 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 13:48:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 13:48:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 13:48:35 INFO DAGScheduler: Registering RDD 25 (processCmd at CliDriver.java:423)
15/08/21 13:48:35 INFO DAGScheduler: Registering RDD 28 (processCmd at CliDriver.java:423)
15/08/21 13:48:35 INFO DAGScheduler: Registering RDD 33 (processCmd at CliDriver.java:423)
15/08/21 13:48:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 13:48:36 INFO DAGScheduler: Registering RDD 22 (processCmd at CliDriver.java:423)
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 13:48:36 INFO DAGScheduler: Registering RDD 17 (processCmd at CliDriver.java:423)
15/08/21 13:48:36 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 13:48:36 INFO DAGScheduler: Final stage: ResultStage 8(processCmd at CliDriver.java:423)
15/08/21 13:48:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 13:48:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 13:48:36 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 13:48:36 INFO MemoryStore: ensureFreeSpace(6632) called with curMem=1860627, maxMem=22226833244
15/08/21 13:48:36 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.5 KB, free 20.7 GB)
15/08/21 13:48:36 INFO MemoryStore: ensureFreeSpace(3624) called with curMem=1867259, maxMem=22226833244
15/08/21 13:48:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.5 KB, free 20.7 GB)
15/08/21 13:48:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:45550 (size: 3.5 KB, free: 20.7 GB)
15/08/21 13:48:36 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874
15/08/21 13:48:36 INFO DAGScheduler: Submitting 19 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423)
15/08/21 13:48:36 INFO TaskSchedulerImpl: Adding task set 3.0 with 19 tasks
15/08/21 13:48:36 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 371, localhost, ANY, 1706 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 372, localhost, ANY, 1715 bytes)
15/08/21 13:48:36 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 13:48:36 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 373, localhost, ANY, 1709 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 374, localhost, ANY, 1706 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 375, localhost, ANY, 1714 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 376, localhost, ANY, 1706 bytes)
15/08/21 13:48:36 INFO MemoryStore: ensureFreeSpace(6728) called with curMem=1870883, maxMem=22226833244
15/08/21 13:48:36 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 6.6 KB, free 20.7 GB)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 377, localhost, ANY, 1714 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 378, localhost, ANY, 1706 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 379, localhost, ANY, 1713 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 380, localhost, ANY, 1705 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 381, localhost, ANY, 1715 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 382, localhost, ANY, 1706 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 383, localhost, ANY, 1716 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 384, localhost, ANY, 1706 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 385, localhost, ANY, 1713 bytes)
15/08/21 13:48:36 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 386, localhost, ANY, 1706 bytes)
15/08/21 13:48:36 INFO Executor: Running task 0.0 in stage 3.0 (TID 371)
15/08/21 13:48:36 INFO Executor: Running task 1.0 in stage 3.0 (TID 372)
15/08/21 13:48:36 INFO Executor: Running task 3.0 in stage 3.0 (TID 374)
15/08/21 13:48:36 INFO Executor: Running task 5.0 in stage 3.0 (TID 376)
15/08/21 13:48:36 INFO Executor: Running task 2.0 in stage 3.0 (TID 373)
15/08/21 13:48:36 INFO Executor: Running task 4.0 in stage 3.0 (TID 375)
15/08/21 13:48:36 INFO MemoryStore: ensureFreeSpace(3650) called with curMem=1877611, maxMem=22226833244
15/08/21 13:48:36 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.6 KB, free 20.7 GB)
15/08/21 13:48:36 INFO Executor: Running task 6.0 in stage 3.0 (TID 377)
15/08/21 13:48:36 INFO Executor: Running task 7.0 in stage 3.0 (TID 378)
15/08/21 13:48:36 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:45550 (size: 3.6 KB, free: 20.7 GB)
15/08/21 13:48:36 INFO Executor: Running task 8.0 in stage 3.0 (TID 379)
15/08/21 13:48:36 INFO Executor: Running task 9.0 in stage 3.0 (TID 380)
15/08/21 13:48:36 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874
15/08/21 13:48:36 INFO Executor: Running task 10.0 in stage 3.0 (TID 381)
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000004_0 start: 134217728 end: 138024796 length: 3807068 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000001_0 start: 134217728 end: 138025614 length: 3807886 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000005_0 start: 134217728 end: 138026641 length: 3808913 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO Executor: Running task 12.0 in stage 3.0 (TID 383)
15/08/21 13:48:36 INFO Executor: Running task 11.0 in stage 3.0 (TID 382)
15/08/21 13:48:36 INFO Executor: Running task 13.0 in stage 3.0 (TID 384)
15/08/21 13:48:36 INFO Executor: Running task 14.0 in stage 3.0 (TID 385)
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000000_0 start: 134217728 end: 138596223 length: 4378495 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 INFO Executor: Running task 15.0 in stage 3.0 (TID 386)
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000009_0 start: 0 end: 24310349 length: 24310349 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000007_0 start: 134217728 end: 137178560 length: 2960832 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000006_0 start: 134217728 end: 137279350 length: 3061622 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000002_0 start: 134217728 end: 138011074 length: 3793346 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO DAGScheduler: Submitting 57 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423)
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 57 tasks
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76962 records.
15/08/21 13:48:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 77014 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 288335 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 83694 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 68086 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1559896 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 67074 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76861 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560100 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560128 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560100 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76866 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560462 records.
15/08/21 13:48:36 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[22] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560144 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 77014
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 76962
15/08/21 13:48:36 INFO MemoryStore: ensureFreeSpace(7320) called with curMem=1881261, maxMem=22226833244
15/08/21 13:48:36 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.1 KB, free 20.7 GB)
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 288335
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560031 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560003 records.
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 83694
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 76861
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO MemoryStore: ensureFreeSpace(3874) called with curMem=1888581, maxMem=22226833244
15/08/21 13:48:36 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.8 KB, free 20.7 GB)
15/08/21 13:48:36 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:45550 (size: 3.8 KB, free: 20.7 GB)
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:874
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 67074
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 68086
15/08/21 13:48:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 76866
15/08/21 13:48:36 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[22] at processCmd at CliDriver.java:423)
15/08/21 13:48:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 200 tasks
15/08/21 13:48:36 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[17] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 151 ms. row count = 1560462
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 146 ms. row count = 1560100
15/08/21 13:48:36 INFO MemoryStore: ensureFreeSpace(6952) called with curMem=1892455, maxMem=22226833244
15/08/21 13:48:36 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 6.8 KB, free 20.7 GB)
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 222 ms. row count = 1559896
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 138 ms. row count = 1560003
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 182 ms. row count = 1560100
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 190 ms. row count = 1560128
15/08/21 13:48:36 INFO InternalParquetRecordReader: block read in memory in 205 ms. row count = 1560031
15/08/21 13:48:37 INFO MemoryStore: ensureFreeSpace(3749) called with curMem=1899407, maxMem=22226833244
15/08/21 13:48:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KB, free 20.7 GB)
15/08/21 13:48:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:45550 (size: 3.7 KB, free: 20.7 GB)
15/08/21 13:48:37 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:874
15/08/21 13:48:37 INFO InternalParquetRecordReader: block read in memory in 238 ms. row count = 1560144
15/08/21 13:48:37 INFO DAGScheduler: Submitting 170 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[17] at processCmd at CliDriver.java:423)
15/08/21 13:48:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 170 tasks
15/08/21 13:48:37 INFO Executor: Finished task 14.0 in stage 3.0 (TID 385). 2125 bytes result sent to driver
15/08/21 13:48:37 INFO Executor: Finished task 6.0 in stage 3.0 (TID 377). 2125 bytes result sent to driver
15/08/21 13:48:37 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 387, localhost, ANY, 1714 bytes)
15/08/21 13:48:37 INFO Executor: Running task 16.0 in stage 3.0 (TID 387)
15/08/21 13:48:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000008_0 start: 134217728 end: 137173148 length: 2955420 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:37 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 388, localhost, ANY, 1706 bytes)
15/08/21 13:48:37 INFO Executor: Running task 17.0 in stage 3.0 (TID 388)
15/08/21 13:48:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:37 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 385) in 1037 ms on localhost (1/19)
15/08/21 13:48:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 66882 records.
15/08/21 13:48:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:37 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 377) in 1065 ms on localhost (2/19)
15/08/21 13:48:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560365 records.
15/08/21 13:48:37 INFO Executor: Finished task 1.0 in stage 3.0 (TID 372). 2125 bytes result sent to driver
15/08/21 13:48:37 INFO Executor: Finished task 4.0 in stage 3.0 (TID 375). 2125 bytes result sent to driver
15/08/21 13:48:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:37 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 66882
15/08/21 13:48:37 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 389, localhost, ANY, 1712 bytes)
15/08/21 13:48:37 INFO Executor: Running task 18.0 in stage 3.0 (TID 389)
15/08/21 13:48:37 INFO Executor: Finished task 12.0 in stage 3.0 (TID 383). 2125 bytes result sent to driver
15/08/21 13:48:37 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 390, localhost, ANY, 1740 bytes)
15/08/21 13:48:37 INFO Executor: Running task 0.0 in stage 4.0 (TID 390)
15/08/21 13:48:37 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 391, localhost, ANY, 1746 bytes)
15/08/21 13:48:37 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 372) in 1106 ms on localhost (3/19)
15/08/21 13:48:37 INFO Executor: Running task 1.0 in stage 4.0 (TID 391)
15/08/21 13:48:37 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 383) in 1100 ms on localhost (4/19)
15/08/21 13:48:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:37 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 375) in 1108 ms on localhost (5/19)
15/08/21 13:48:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000003_0 start: 134217728 end: 138036519 length: 3818791 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:37 INFO Executor: Finished task 8.0 in stage 3.0 (TID 379). 2125 bytes result sent to driver
15/08/21 13:48:37 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 392, localhost, ANY, 1752 bytes)
15/08/21 13:48:37 INFO Executor: Running task 2.0 in stage 4.0 (TID 392)
15/08/21 13:48:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 268435456 end: 340165230 length: 71729774 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:37 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 379) in 1168 ms on localhost (6/19)
15/08/21 13:48:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76997 records.
15/08/21 13:48:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061701 records.
15/08/21 13:48:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061449 records.
15/08/21 13:48:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1785023 records.
15/08/21 13:48:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:37 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 76997
15/08/21 13:48:37 INFO Executor: Finished task 10.0 in stage 3.0 (TID 381). 2125 bytes result sent to driver
15/08/21 13:48:37 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 393, localhost, ANY, 1739 bytes)
15/08/21 13:48:37 INFO Executor: Running task 3.0 in stage 4.0 (TID 393)
15/08/21 13:48:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:37 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 381) in 1229 ms on localhost (7/19)
15/08/21 13:48:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061593 records.
15/08/21 13:48:37 INFO InternalParquetRecordReader: block read in memory in 198 ms. row count = 1560365
15/08/21 13:48:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:38 INFO InternalParquetRecordReader: block read in memory in 214 ms. row count = 1785023
15/08/21 13:48:38 INFO InternalParquetRecordReader: block read in memory in 329 ms. row count = 3061701
15/08/21 13:48:38 INFO InternalParquetRecordReader: block read in memory in 359 ms. row count = 3061449
15/08/21 13:48:38 INFO InternalParquetRecordReader: block read in memory in 379 ms. row count = 3061593
15/08/21 13:48:38 INFO Executor: Finished task 2.0 in stage 3.0 (TID 373). 2125 bytes result sent to driver
15/08/21 13:48:38 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 394, localhost, ANY, 1745 bytes)
15/08/21 13:48:38 INFO Executor: Running task 4.0 in stage 4.0 (TID 394)
15/08/21 13:48:38 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 373) in 1728 ms on localhost (8/19)
15/08/21 13:48:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061600 records.
15/08/21 13:48:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:38 INFO Executor: Finished task 18.0 in stage 3.0 (TID 389). 2125 bytes result sent to driver
15/08/21 13:48:38 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 395, localhost, ANY, 1756 bytes)
15/08/21 13:48:38 INFO Executor: Running task 5.0 in stage 4.0 (TID 395)
15/08/21 13:48:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 268435456 end: 344047583 length: 75612127 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:38 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 389) in 756 ms on localhost (9/19)
15/08/21 13:48:38 INFO Executor: Finished task 16.0 in stage 3.0 (TID 387). 2125 bytes result sent to driver
15/08/21 13:48:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1875761 records.
15/08/21 13:48:38 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 396, localhost, ANY, 1741 bytes)
15/08/21 13:48:38 INFO Executor: Running task 6.0 in stage 4.0 (TID 396)
15/08/21 13:48:38 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 387) in 869 ms on localhost (10/19)
15/08/21 13:48:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061554 records.
15/08/21 13:48:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:38 INFO InternalParquetRecordReader: block read in memory in 251 ms. row count = 3061600
15/08/21 13:48:38 INFO InternalParquetRecordReader: block read in memory in 197 ms. row count = 1875761
15/08/21 13:48:38 INFO InternalParquetRecordReader: block read in memory in 215 ms. row count = 3061554
15/08/21 13:48:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:45550 in memory (size: 1784.0 B, free: 20.7 GB)
15/08/21 13:48:42 INFO Executor: Finished task 3.0 in stage 3.0 (TID 374). 2125 bytes result sent to driver
15/08/21 13:48:42 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 397, localhost, ANY, 1746 bytes)
15/08/21 13:48:42 INFO Executor: Running task 7.0 in stage 4.0 (TID 397)
15/08/21 13:48:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:42 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 374) in 5666 ms on localhost (11/19)
15/08/21 13:48:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061231 records.
15/08/21 13:48:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:42 INFO InternalParquetRecordReader: block read in memory in 214 ms. row count = 3061231
15/08/21 13:48:42 INFO Executor: Finished task 15.0 in stage 3.0 (TID 386). 2125 bytes result sent to driver
15/08/21 13:48:42 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 398, localhost, ANY, 1752 bytes)
15/08/21 13:48:42 INFO Executor: Running task 8.0 in stage 4.0 (TID 398)
15/08/21 13:48:42 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 386) in 6191 ms on localhost (12/19)
15/08/21 13:48:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 268435456 end: 340526195 length: 72090739 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1793520 records.
15/08/21 13:48:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:42 INFO Executor: Finished task 9.0 in stage 3.0 (TID 380). 2125 bytes result sent to driver
15/08/21 13:48:42 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 399, localhost, ANY, 1742 bytes)
15/08/21 13:48:42 INFO Executor: Running task 9.0 in stage 4.0 (TID 399)
15/08/21 13:48:42 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 380) in 6327 ms on localhost (13/19)
15/08/21 13:48:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061301 records.
15/08/21 13:48:42 INFO InternalParquetRecordReader: block read in memory in 130 ms. row count = 1793520
15/08/21 13:48:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:43 INFO Executor: Finished task 13.0 in stage 3.0 (TID 384). 2125 bytes result sent to driver
15/08/21 13:48:43 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 400, localhost, ANY, 1747 bytes)
15/08/21 13:48:43 INFO Executor: Running task 10.0 in stage 4.0 (TID 400)
15/08/21 13:48:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:43 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 384) in 6516 ms on localhost (14/19)
15/08/21 13:48:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061650 records.
15/08/21 13:48:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 371). 2125 bytes result sent to driver
15/08/21 13:48:43 INFO InternalParquetRecordReader: block read in memory in 272 ms. row count = 3061301
15/08/21 13:48:43 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 401, localhost, ANY, 1753 bytes)
15/08/21 13:48:43 INFO Executor: Running task 11.0 in stage 4.0 (TID 401)
15/08/21 13:48:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 371) in 6668 ms on localhost (15/19)
15/08/21 13:48:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 268435456 end: 340154417 length: 71718961 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784493 records.
15/08/21 13:48:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:43 INFO Executor: Finished task 11.0 in stage 3.0 (TID 382). 2125 bytes result sent to driver
15/08/21 13:48:43 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 402, localhost, ANY, 1742 bytes)
15/08/21 13:48:43 INFO Executor: Running task 12.0 in stage 4.0 (TID 402)
15/08/21 13:48:43 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 382) in 6715 ms on localhost (16/19)
15/08/21 13:48:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061813 records.
15/08/21 13:48:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:43 INFO Executor: Finished task 7.0 in stage 3.0 (TID 378). 2125 bytes result sent to driver
15/08/21 13:48:43 INFO InternalParquetRecordReader: block read in memory in 238 ms. row count = 3061650
15/08/21 13:48:43 INFO Executor: Finished task 17.0 in stage 3.0 (TID 388). 2125 bytes result sent to driver
15/08/21 13:48:43 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 403, localhost, ANY, 1747 bytes)
15/08/21 13:48:43 INFO Executor: Running task 13.0 in stage 4.0 (TID 403)
15/08/21 13:48:43 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 404, localhost, ANY, 1754 bytes)
15/08/21 13:48:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:43 INFO Executor: Running task 14.0 in stage 4.0 (TID 404)
15/08/21 13:48:43 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 388) in 5791 ms on localhost (17/19)
15/08/21 13:48:43 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 378) in 6805 ms on localhost (18/19)
15/08/21 13:48:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 268435456 end: 340147334 length: 71711878 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:43 INFO InternalParquetRecordReader: block read in memory in 133 ms. row count = 1784493
15/08/21 13:48:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061669 records.
15/08/21 13:48:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784858 records.
15/08/21 13:48:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:43 INFO InternalParquetRecordReader: block read in memory in 217 ms. row count = 3061813
15/08/21 13:48:43 INFO Executor: Finished task 5.0 in stage 3.0 (TID 376). 2125 bytes result sent to driver
15/08/21 13:48:43 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 405, localhost, ANY, 1740 bytes)
15/08/21 13:48:43 INFO Executor: Running task 15.0 in stage 4.0 (TID 405)
15/08/21 13:48:43 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 376) in 6997 ms on localhost (19/19)
15/08/21 13:48:43 INFO DAGScheduler: ShuffleMapStage 3 (processCmd at CliDriver.java:423) finished in 7.007 s
15/08/21 13:48:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/21 13:48:43 INFO DAGScheduler: looking for newly runnable stages
15/08/21 13:48:43 INFO DAGScheduler: running: Set(ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 4)
15/08/21 13:48:43 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/21 13:48:43 INFO DAGScheduler: failed: Set()
15/08/21 13:48:43 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@2dc3ed
15/08/21 13:48:43 INFO StatsReportListener: task runtime:(count: 19, mean: 3623.263158, stdev: 2664.597764, max: 6997.000000, min: 756.000000)
15/08/21 13:48:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:43 INFO StatsReportListener: 	756.0 ms	756.0 ms	869.0 ms	1.1 s	1.7 s	6.5 s	6.8 s	7.0 s	7.0 s
15/08/21 13:48:43 INFO StatsReportListener: shuffle bytes written:(count: 19, mean: 8918860.526316, stdev: 8262561.827176, max: 17626413.000000, min: 769785.000000)
15/08/21 13:48:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:43 INFO StatsReportListener: 	751.7 KB	751.7 KB	753.5 KB	860.8 KB	3.1 MB	16.8 MB	16.8 MB	16.8 MB	16.8 MB
15/08/21 13:48:43 INFO StatsReportListener: task result size:(count: 19, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 13:48:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:43 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 13:48:43 INFO StatsReportListener: executor (non-fetch) time pct: (count: 19, mean: 95.975700, stdev: 3.484159, max: 99.292005, min: 88.717454)
15/08/21 13:48:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:43 INFO StatsReportListener: 	89 %	89 %	89 %	94 %	97 %	99 %	99 %	99 %	99 %
15/08/21 13:48:43 INFO StatsReportListener: other time pct: (count: 19, mean: 4.024300, stdev: 3.484159, max: 11.282546, min: 0.707995)
15/08/21 13:48:43 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:48:43 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 3 %	 6 %	11 %	11 %	11 %
15/08/21 13:48:43 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List(ShuffleMapStage 4)
15/08/21 13:48:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061498 records.
15/08/21 13:48:43 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 13:48:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:43 INFO InternalParquetRecordReader: block read in memory in 198 ms. row count = 1784858
15/08/21 13:48:43 INFO InternalParquetRecordReader: block read in memory in 436 ms. row count = 3061669
15/08/21 13:48:43 INFO InternalParquetRecordReader: block read in memory in 280 ms. row count = 3061498
15/08/21 13:48:45 INFO Executor: Finished task 2.0 in stage 4.0 (TID 392). 2125 bytes result sent to driver
15/08/21 13:48:45 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 406, localhost, ANY, 1746 bytes)
15/08/21 13:48:45 INFO Executor: Running task 16.0 in stage 4.0 (TID 406)
15/08/21 13:48:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:45 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 392) in 7562 ms on localhost (1/57)
15/08/21 13:48:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061799 records.
15/08/21 13:48:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:45 INFO InternalParquetRecordReader: block read in memory in 209 ms. row count = 3061799
15/08/21 13:48:48 INFO Executor: Finished task 5.0 in stage 4.0 (TID 395). 2125 bytes result sent to driver
15/08/21 13:48:48 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 407, localhost, ANY, 1752 bytes)
15/08/21 13:48:48 INFO Executor: Running task 17.0 in stage 4.0 (TID 407)
15/08/21 13:48:48 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 395) in 10004 ms on localhost (2/57)
15/08/21 13:48:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 268435456 end: 343033905 length: 74598449 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1851904 records.
15/08/21 13:48:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:48 INFO InternalParquetRecordReader: block read in memory in 115 ms. row count = 1851904
15/08/21 13:48:49 INFO Executor: Finished task 8.0 in stage 4.0 (TID 398). 2125 bytes result sent to driver
15/08/21 13:48:49 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 408, localhost, ANY, 1741 bytes)
15/08/21 13:48:49 INFO Executor: Running task 18.0 in stage 4.0 (TID 408)
15/08/21 13:48:49 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 398) in 7191 ms on localhost (3/57)
15/08/21 13:48:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061581 records.
15/08/21 13:48:50 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:50 INFO InternalParquetRecordReader: block read in memory in 232 ms. row count = 3061581
15/08/21 13:48:51 INFO Executor: Finished task 11.0 in stage 4.0 (TID 401). 2125 bytes result sent to driver
15/08/21 13:48:51 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 409, localhost, ANY, 1747 bytes)
15/08/21 13:48:51 INFO Executor: Running task 19.0 in stage 4.0 (TID 409)
15/08/21 13:48:51 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 401) in 8134 ms on localhost (4/57)
15/08/21 13:48:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:51 INFO Executor: Finished task 14.0 in stage 4.0 (TID 404). 2125 bytes result sent to driver
15/08/21 13:48:51 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 410, localhost, ANY, 1753 bytes)
15/08/21 13:48:51 INFO Executor: Running task 20.0 in stage 4.0 (TID 410)
15/08/21 13:48:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061649 records.
15/08/21 13:48:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 268435456 end: 340167695 length: 71732239 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:51 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 404) in 8012 ms on localhost (5/57)
15/08/21 13:48:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784882 records.
15/08/21 13:48:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:51 INFO Executor: Finished task 6.0 in stage 4.0 (TID 396). 2125 bytes result sent to driver
15/08/21 13:48:51 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 411, localhost, ANY, 1741 bytes)
15/08/21 13:48:51 INFO Executor: Running task 21.0 in stage 4.0 (TID 411)
15/08/21 13:48:51 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 396) in 13354 ms on localhost (6/57)
15/08/21 13:48:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:51 INFO InternalParquetRecordReader: block read in memory in 417 ms. row count = 1784882
15/08/21 13:48:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061580 records.
15/08/21 13:48:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:52 INFO InternalParquetRecordReader: block read in memory in 631 ms. row count = 3061649
15/08/21 13:48:52 INFO Executor: Finished task 1.0 in stage 4.0 (TID 391). 2125 bytes result sent to driver
15/08/21 13:48:52 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 412, localhost, ANY, 1747 bytes)
15/08/21 13:48:52 INFO Executor: Running task 22.0 in stage 4.0 (TID 412)
15/08/21 13:48:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:52 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 391) in 14396 ms on localhost (7/57)
15/08/21 13:48:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061754 records.
15/08/21 13:48:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:52 INFO InternalParquetRecordReader: block read in memory in 269 ms. row count = 3061580
15/08/21 13:48:52 INFO Executor: Finished task 4.0 in stage 4.0 (TID 394). 2125 bytes result sent to driver
15/08/21 13:48:52 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 413, localhost, ANY, 1754 bytes)
15/08/21 13:48:52 INFO Executor: Running task 23.0 in stage 4.0 (TID 413)
15/08/21 13:48:52 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 394) in 13959 ms on localhost (8/57)
15/08/21 13:48:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 268435456 end: 340141911 length: 71706455 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784589 records.
15/08/21 13:48:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:52 INFO InternalParquetRecordReader: block read in memory in 202 ms. row count = 3061754
15/08/21 13:48:52 INFO Executor: Finished task 0.0 in stage 4.0 (TID 390). 2125 bytes result sent to driver
15/08/21 13:48:52 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 414, localhost, ANY, 1742 bytes)
15/08/21 13:48:52 INFO Executor: Running task 24.0 in stage 4.0 (TID 414)
15/08/21 13:48:52 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 390) in 14670 ms on localhost (9/57)
15/08/21 13:48:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061826 records.
15/08/21 13:48:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:52 INFO Executor: Finished task 3.0 in stage 4.0 (TID 393). 2125 bytes result sent to driver
15/08/21 13:48:52 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 415, localhost, ANY, 1747 bytes)
15/08/21 13:48:52 INFO Executor: Running task 25.0 in stage 4.0 (TID 415)
15/08/21 13:48:52 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 393) in 14598 ms on localhost (10/57)
15/08/21 13:48:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:52 INFO InternalParquetRecordReader: block read in memory in 124 ms. row count = 1784589
15/08/21 13:48:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061906 records.
15/08/21 13:48:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:52 INFO InternalParquetRecordReader: block read in memory in 257 ms. row count = 3061826
15/08/21 13:48:52 INFO InternalParquetRecordReader: block read in memory in 229 ms. row count = 3061906
15/08/21 13:48:53 INFO Executor: Finished task 9.0 in stage 4.0 (TID 399). 2125 bytes result sent to driver
15/08/21 13:48:53 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 416, localhost, ANY, 1755 bytes)
15/08/21 13:48:53 INFO Executor: Running task 26.0 in stage 4.0 (TID 416)
15/08/21 13:48:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 268435456 end: 340152459 length: 71717003 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:53 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 399) in 10486 ms on localhost (11/57)
15/08/21 13:48:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784988 records.
15/08/21 13:48:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:53 INFO InternalParquetRecordReader: block read in memory in 162 ms. row count = 1784988
15/08/21 13:48:54 INFO Executor: Finished task 10.0 in stage 4.0 (TID 400). 2125 bytes result sent to driver
15/08/21 13:48:54 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 417, localhost, ANY, 1741 bytes)
15/08/21 13:48:54 INFO Executor: Running task 27.0 in stage 4.0 (TID 417)
15/08/21 13:48:54 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 400) in 11148 ms on localhost (12/57)
15/08/21 13:48:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061659 records.
15/08/21 13:48:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:54 INFO InternalParquetRecordReader: block read in memory in 242 ms. row count = 3061659
15/08/21 13:48:54 INFO Executor: Finished task 17.0 in stage 4.0 (TID 407). 2125 bytes result sent to driver
15/08/21 13:48:54 INFO Executor: Finished task 7.0 in stage 4.0 (TID 397). 2125 bytes result sent to driver
15/08/21 13:48:54 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 418, localhost, ANY, 1747 bytes)
15/08/21 13:48:54 INFO Executor: Running task 28.0 in stage 4.0 (TID 418)
15/08/21 13:48:54 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 419, localhost, ANY, 1754 bytes)
15/08/21 13:48:54 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 407) in 6416 ms on localhost (13/57)
15/08/21 13:48:54 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 397) in 12610 ms on localhost (14/57)
15/08/21 13:48:54 INFO Executor: Running task 29.0 in stage 4.0 (TID 419)
15/08/21 13:48:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 268435456 end: 340149052 length: 71713596 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061649 records.
15/08/21 13:48:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784790 records.
15/08/21 13:48:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:54 INFO Executor: Finished task 12.0 in stage 4.0 (TID 402). 2125 bytes result sent to driver
15/08/21 13:48:54 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 420, localhost, ANY, 1742 bytes)
15/08/21 13:48:54 INFO Executor: Running task 30.0 in stage 4.0 (TID 420)
15/08/21 13:48:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:54 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 402) in 11584 ms on localhost (15/57)
15/08/21 13:48:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061804 records.
15/08/21 13:48:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:55 INFO InternalParquetRecordReader: block read in memory in 154 ms. row count = 1784790
15/08/21 13:48:55 INFO Executor: Finished task 15.0 in stage 4.0 (TID 405). 2125 bytes result sent to driver
15/08/21 13:48:55 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 421, localhost, ANY, 1747 bytes)
15/08/21 13:48:55 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 405) in 12051 ms on localhost (16/57)
15/08/21 13:48:55 INFO Executor: Running task 31.0 in stage 4.0 (TID 421)
15/08/21 13:48:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061148 records.
15/08/21 13:48:55 INFO InternalParquetRecordReader: block read in memory in 797 ms. row count = 3061649
15/08/21 13:48:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:55 INFO Executor: Finished task 13.0 in stage 4.0 (TID 403). 2125 bytes result sent to driver
15/08/21 13:48:55 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 422, localhost, ANY, 1754 bytes)
15/08/21 13:48:55 INFO Executor: Running task 32.0 in stage 4.0 (TID 422)
15/08/21 13:48:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 268435456 end: 340153987 length: 71718531 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:55 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 403) in 12362 ms on localhost (17/57)
15/08/21 13:48:55 INFO InternalParquetRecordReader: block read in memory in 835 ms. row count = 3061804
15/08/21 13:48:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784820 records.
15/08/21 13:48:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:55 INFO InternalParquetRecordReader: block read in memory in 204 ms. row count = 3061148
15/08/21 13:48:56 INFO InternalParquetRecordReader: block read in memory in 248 ms. row count = 1784820
15/08/21 13:48:56 INFO Executor: Finished task 16.0 in stage 4.0 (TID 406). 2125 bytes result sent to driver
15/08/21 13:48:56 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 423, localhost, ANY, 1741 bytes)
15/08/21 13:48:56 INFO Executor: Running task 33.0 in stage 4.0 (TID 423)
15/08/21 13:48:56 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 406) in 11479 ms on localhost (18/57)
15/08/21 13:48:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061090 records.
15/08/21 13:48:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:57 INFO InternalParquetRecordReader: block read in memory in 342 ms. row count = 3061090
15/08/21 13:48:57 INFO Executor: Finished task 20.0 in stage 4.0 (TID 410). 2125 bytes result sent to driver
15/08/21 13:48:57 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 424, localhost, ANY, 1747 bytes)
15/08/21 13:48:57 INFO Executor: Running task 34.0 in stage 4.0 (TID 424)
15/08/21 13:48:57 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 410) in 5951 ms on localhost (19/57)
15/08/21 13:48:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061657 records.
15/08/21 13:48:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:57 INFO Executor: Finished task 23.0 in stage 4.0 (TID 413). 2125 bytes result sent to driver
15/08/21 13:48:57 INFO InternalParquetRecordReader: block read in memory in 382 ms. row count = 3061657
15/08/21 13:48:57 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 425, localhost, ANY, 1755 bytes)
15/08/21 13:48:57 INFO Executor: Running task 35.0 in stage 4.0 (TID 425)
15/08/21 13:48:57 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 413) in 5501 ms on localhost (20/57)
15/08/21 13:48:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 268435456 end: 340154038 length: 71718582 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784628 records.
15/08/21 13:48:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:58 INFO InternalParquetRecordReader: block read in memory in 219 ms. row count = 1784628
15/08/21 13:48:58 INFO Executor: Finished task 26.0 in stage 4.0 (TID 416). 2125 bytes result sent to driver
15/08/21 13:48:58 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 426, localhost, ANY, 1741 bytes)
15/08/21 13:48:58 INFO Executor: Running task 36.0 in stage 4.0 (TID 426)
15/08/21 13:48:58 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 416) in 5589 ms on localhost (21/57)
15/08/21 13:48:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:48:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:48:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061565 records.
15/08/21 13:48:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:48:59 INFO InternalParquetRecordReader: block read in memory in 335 ms. row count = 3061565
15/08/21 13:49:00 INFO Executor: Finished task 18.0 in stage 4.0 (TID 408). 2125 bytes result sent to driver
15/08/21 13:49:00 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 427, localhost, ANY, 1747 bytes)
15/08/21 13:49:00 INFO Executor: Running task 37.0 in stage 4.0 (TID 427)
15/08/21 13:49:00 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 408) in 10080 ms on localhost (22/57)
15/08/21 13:49:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061553 records.
15/08/21 13:49:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:01 INFO InternalParquetRecordReader: block read in memory in 940 ms. row count = 3061553
15/08/21 13:49:01 INFO Executor: Finished task 29.0 in stage 4.0 (TID 419). 2125 bytes result sent to driver
15/08/21 13:49:01 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 428, localhost, ANY, 1753 bytes)
15/08/21 13:49:01 INFO Executor: Running task 38.0 in stage 4.0 (TID 428)
15/08/21 13:49:01 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 419) in 7018 ms on localhost (23/57)
15/08/21 13:49:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 268435456 end: 340156978 length: 71721522 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1785031 records.
15/08/21 13:49:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:02 INFO Executor: Finished task 32.0 in stage 4.0 (TID 422). 2125 bytes result sent to driver
15/08/21 13:49:02 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 429, localhost, ANY, 1741 bytes)
15/08/21 13:49:02 INFO Executor: Running task 39.0 in stage 4.0 (TID 429)
15/08/21 13:49:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:02 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 422) in 6425 ms on localhost (24/57)
15/08/21 13:49:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:02 INFO InternalParquetRecordReader: block read in memory in 296 ms. row count = 1785031
15/08/21 13:49:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061805 records.
15/08/21 13:49:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:02 INFO InternalParquetRecordReader: block read in memory in 561 ms. row count = 3061805
15/08/21 13:49:03 INFO Executor: Finished task 22.0 in stage 4.0 (TID 412). 2125 bytes result sent to driver
15/08/21 13:49:03 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 430, localhost, ANY, 1747 bytes)
15/08/21 13:49:03 INFO Executor: Running task 40.0 in stage 4.0 (TID 430)
15/08/21 13:49:03 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 412) in 10975 ms on localhost (25/57)
15/08/21 13:49:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061589 records.
15/08/21 13:49:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:03 INFO Executor: Finished task 19.0 in stage 4.0 (TID 409). 2125 bytes result sent to driver
15/08/21 13:49:03 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 431, localhost, ANY, 1754 bytes)
15/08/21 13:49:03 INFO Executor: Running task 41.0 in stage 4.0 (TID 431)
15/08/21 13:49:03 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 409) in 11937 ms on localhost (26/57)
15/08/21 13:49:03 INFO Executor: Finished task 25.0 in stage 4.0 (TID 415). 2125 bytes result sent to driver
15/08/21 13:49:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 268435456 end: 340173405 length: 71737949 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:03 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 432, localhost, ANY, 1741 bytes)
15/08/21 13:49:03 INFO Executor: Running task 42.0 in stage 4.0 (TID 432)
15/08/21 13:49:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:03 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 415) in 10915 ms on localhost (27/57)
15/08/21 13:49:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784699 records.
15/08/21 13:49:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061319 records.
15/08/21 13:49:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:03 INFO Executor: Finished task 24.0 in stage 4.0 (TID 414). 2125 bytes result sent to driver
15/08/21 13:49:03 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 433, localhost, ANY, 1747 bytes)
15/08/21 13:49:03 INFO Executor: Running task 43.0 in stage 4.0 (TID 433)
15/08/21 13:49:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:03 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 414) in 11143 ms on localhost (28/57)
15/08/21 13:49:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061685 records.
15/08/21 13:49:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:03 INFO InternalParquetRecordReader: block read in memory in 541 ms. row count = 3061589
15/08/21 13:49:03 INFO InternalParquetRecordReader: block read in memory in 524 ms. row count = 1784699
15/08/21 13:49:03 INFO Executor: Finished task 35.0 in stage 4.0 (TID 425). 2125 bytes result sent to driver
15/08/21 13:49:04 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 434, localhost, ANY, 1752 bytes)
15/08/21 13:49:04 INFO Executor: Running task 44.0 in stage 4.0 (TID 434)
15/08/21 13:49:04 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 425) in 6245 ms on localhost (29/57)
15/08/21 13:49:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 268435456 end: 340161363 length: 71725907 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:04 INFO Executor: Finished task 21.0 in stage 4.0 (TID 411). 2125 bytes result sent to driver
15/08/21 13:49:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784830 records.
15/08/21 13:49:04 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 435, localhost, ANY, 1741 bytes)
15/08/21 13:49:04 INFO Executor: Running task 45.0 in stage 4.0 (TID 435)
15/08/21 13:49:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:04 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 411) in 12251 ms on localhost (30/57)
15/08/21 13:49:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061466 records.
15/08/21 13:49:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:04 INFO InternalParquetRecordReader: block read in memory in 1003 ms. row count = 3061319
15/08/21 13:49:04 INFO Executor: Finished task 27.0 in stage 4.0 (TID 417). 2125 bytes result sent to driver
15/08/21 13:49:04 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 436, localhost, ANY, 1747 bytes)
15/08/21 13:49:04 INFO Executor: Running task 46.0 in stage 4.0 (TID 436)
15/08/21 13:49:04 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 417) in 10205 ms on localhost (31/57)
15/08/21 13:49:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061853 records.
15/08/21 13:49:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:04 INFO InternalParquetRecordReader: block read in memory in 1144 ms. row count = 3061685
15/08/21 13:49:04 INFO InternalParquetRecordReader: block read in memory in 927 ms. row count = 1784830
15/08/21 13:49:05 INFO InternalParquetRecordReader: block read in memory in 1427 ms. row count = 3061466
15/08/21 13:49:05 INFO InternalParquetRecordReader: block read in memory in 1107 ms. row count = 3061853
15/08/21 13:49:06 INFO Executor: Finished task 31.0 in stage 4.0 (TID 421). 2125 bytes result sent to driver
15/08/21 13:49:06 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 437, localhost, ANY, 1755 bytes)
15/08/21 13:49:06 INFO Executor: Running task 47.0 in stage 4.0 (TID 437)
15/08/21 13:49:06 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 421) in 10384 ms on localhost (32/57)
15/08/21 13:49:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 268435456 end: 319211896 length: 50776440 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1297836 records.
15/08/21 13:49:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:06 INFO InternalParquetRecordReader: block read in memory in 263 ms. row count = 1297836
15/08/21 13:49:06 INFO Executor: Finished task 30.0 in stage 4.0 (TID 420). 2125 bytes result sent to driver
15/08/21 13:49:06 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 438, localhost, ANY, 1741 bytes)
15/08/21 13:49:06 INFO Executor: Running task 48.0 in stage 4.0 (TID 438)
15/08/21 13:49:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:06 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 420) in 11671 ms on localhost (33/57)
15/08/21 13:49:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062023 records.
15/08/21 13:49:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:07 INFO Executor: Finished task 28.0 in stage 4.0 (TID 418). 2125 bytes result sent to driver
15/08/21 13:49:07 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 439, localhost, ANY, 1747 bytes)
15/08/21 13:49:07 INFO Executor: Running task 49.0 in stage 4.0 (TID 439)
15/08/21 13:49:07 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 418) in 12232 ms on localhost (34/57)
15/08/21 13:49:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061647 records.
15/08/21 13:49:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:07 INFO InternalParquetRecordReader: block read in memory in 726 ms. row count = 3062023
15/08/21 13:49:07 INFO InternalParquetRecordReader: block read in memory in 566 ms. row count = 3061647
15/08/21 13:49:10 INFO Executor: Finished task 33.0 in stage 4.0 (TID 423). 2125 bytes result sent to driver
15/08/21 13:49:10 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 440, localhost, ANY, 1755 bytes)
15/08/21 13:49:10 INFO Executor: Running task 50.0 in stage 4.0 (TID 440)
15/08/21 13:49:10 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 423) in 13757 ms on localhost (35/57)
15/08/21 13:49:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 268435456 end: 340156068 length: 71720612 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:10 INFO Executor: Finished task 34.0 in stage 4.0 (TID 424). 2125 bytes result sent to driver
15/08/21 13:49:10 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 441, localhost, ANY, 1741 bytes)
15/08/21 13:49:10 INFO Executor: Running task 51.0 in stage 4.0 (TID 441)
15/08/21 13:49:10 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 424) in 13203 ms on localhost (36/57)
15/08/21 13:49:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784477 records.
15/08/21 13:49:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062000 records.
15/08/21 13:49:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:10 INFO InternalParquetRecordReader: block read in memory in 369 ms. row count = 1784477
15/08/21 13:49:10 INFO Executor: Finished task 41.0 in stage 4.0 (TID 431). 2125 bytes result sent to driver
15/08/21 13:49:10 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 442, localhost, ANY, 1746 bytes)
15/08/21 13:49:10 INFO Executor: Running task 52.0 in stage 4.0 (TID 442)
15/08/21 13:49:10 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 431) in 7691 ms on localhost (37/57)
15/08/21 13:49:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062082 records.
15/08/21 13:49:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:11 INFO Executor: Finished task 38.0 in stage 4.0 (TID 428). 2125 bytes result sent to driver
15/08/21 13:49:11 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 443, localhost, ANY, 1754 bytes)
15/08/21 13:49:11 INFO Executor: Running task 53.0 in stage 4.0 (TID 443)
15/08/21 13:49:11 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 428) in 9212 ms on localhost (38/57)
15/08/21 13:49:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 268435456 end: 343036825 length: 74601369 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1851764 records.
15/08/21 13:49:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:11 INFO InternalParquetRecordReader: block read in memory in 505 ms. row count = 3062000
15/08/21 13:49:11 INFO InternalParquetRecordReader: block read in memory in 612 ms. row count = 1851764
15/08/21 13:49:11 INFO InternalParquetRecordReader: block read in memory in 812 ms. row count = 3062082
15/08/21 13:49:12 INFO Executor: Finished task 36.0 in stage 4.0 (TID 426). 2125 bytes result sent to driver
15/08/21 13:49:12 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 444, localhost, ANY, 1741 bytes)
15/08/21 13:49:12 INFO Executor: Running task 54.0 in stage 4.0 (TID 444)
15/08/21 13:49:12 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 426) in 13320 ms on localhost (39/57)
15/08/21 13:49:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061869 records.
15/08/21 13:49:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:12 INFO Executor: Finished task 47.0 in stage 4.0 (TID 437). 2125 bytes result sent to driver
15/08/21 13:49:12 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 445, localhost, ANY, 1747 bytes)
15/08/21 13:49:12 INFO Executor: Running task 55.0 in stage 4.0 (TID 445)
15/08/21 13:49:12 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 437) in 6732 ms on localhost (40/57)
15/08/21 13:49:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062071 records.
15/08/21 13:49:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:12 INFO InternalParquetRecordReader: block read in memory in 631 ms. row count = 3061869
15/08/21 13:49:13 INFO InternalParquetRecordReader: block read in memory in 1054 ms. row count = 3062071
15/08/21 13:49:13 INFO Executor: Finished task 44.0 in stage 4.0 (TID 434). 2125 bytes result sent to driver
15/08/21 13:49:13 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 446, localhost, ANY, 1753 bytes)
15/08/21 13:49:13 INFO Executor: Running task 56.0 in stage 4.0 (TID 446)
15/08/21 13:49:13 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 434) in 9984 ms on localhost (41/57)
15/08/21 13:49:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 268435456 end: 340157695 length: 71722239 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784419 records.
15/08/21 13:49:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:14 INFO InternalParquetRecordReader: block read in memory in 368 ms. row count = 1784419
15/08/21 13:49:14 INFO Executor: Finished task 37.0 in stage 4.0 (TID 427). 2125 bytes result sent to driver
15/08/21 13:49:14 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 447, localhost, ANY, 1690 bytes)
15/08/21 13:49:14 INFO Executor: Running task 0.0 in stage 6.0 (TID 447)
15/08/21 13:49:14 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 427) in 14645 ms on localhost (42/57)
15/08/21 13:49:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00008-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506690 length: 3506690 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:14 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:14 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 13:49:15 INFO Executor: Finished task 0.0 in stage 6.0 (TID 447). 2125 bytes result sent to driver
15/08/21 13:49:15 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 448, localhost, ANY, 1692 bytes)
15/08/21 13:49:15 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 447) in 581 ms on localhost (1/200)
15/08/21 13:49:15 INFO Executor: Running task 1.0 in stage 6.0 (TID 448)
15/08/21 13:49:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00188-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501816 length: 3501816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:15 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 13:49:15 INFO Executor: Finished task 1.0 in stage 6.0 (TID 448). 2125 bytes result sent to driver
15/08/21 13:49:15 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 449, localhost, ANY, 1692 bytes)
15/08/21 13:49:15 INFO Executor: Running task 2.0 in stage 6.0 (TID 449)
15/08/21 13:49:15 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 448) in 602 ms on localhost (2/200)
15/08/21 13:49:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00070-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504014 length: 3504014 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:15 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 750000
15/08/21 13:49:16 INFO Executor: Finished task 2.0 in stage 6.0 (TID 449). 2125 bytes result sent to driver
15/08/21 13:49:16 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 450, localhost, ANY, 1691 bytes)
15/08/21 13:49:16 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 449) in 996 ms on localhost (3/200)
15/08/21 13:49:16 INFO Executor: Running task 3.0 in stage 6.0 (TID 450)
15/08/21 13:49:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00150-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505922 length: 3505922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:16 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 13:49:17 INFO Executor: Finished task 39.0 in stage 4.0 (TID 429). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 451, localhost, ANY, 1693 bytes)
15/08/21 13:49:17 INFO Executor: Running task 4.0 in stage 6.0 (TID 451)
15/08/21 13:49:17 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 429) in 14943 ms on localhost (43/57)
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00180-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500745 length: 3500745 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 750000
15/08/21 13:49:17 INFO Executor: Finished task 40.0 in stage 4.0 (TID 430). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 452, localhost, ANY, 1693 bytes)
15/08/21 13:49:17 INFO Executor: Running task 5.0 in stage 6.0 (TID 452)
15/08/21 13:49:17 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 430) in 14234 ms on localhost (44/57)
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00129-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3497927 length: 3497927 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO Executor: Finished task 3.0 in stage 6.0 (TID 450). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 453, localhost, ANY, 1691 bytes)
15/08/21 13:49:17 INFO Executor: Running task 6.0 in stage 6.0 (TID 453)
15/08/21 13:49:17 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 450) in 445 ms on localhost (4/200)
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00132-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502651 length: 3502651 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 750000
15/08/21 13:49:17 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 13:49:17 INFO Executor: Finished task 50.0 in stage 4.0 (TID 440). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 454, localhost, ANY, 1693 bytes)
15/08/21 13:49:17 INFO Executor: Running task 7.0 in stage 6.0 (TID 454)
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00061-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506496 length: 3506496 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 440) in 6936 ms on localhost (45/57)
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO Executor: Finished task 4.0 in stage 6.0 (TID 451). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 455, localhost, ANY, 1691 bytes)
15/08/21 13:49:17 INFO Executor: Running task 8.0 in stage 6.0 (TID 455)
15/08/21 13:49:17 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 451) in 389 ms on localhost (5/200)
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00199-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505199 length: 3505199 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 13:49:17 INFO Executor: Finished task 5.0 in stage 6.0 (TID 452). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 456, localhost, ANY, 1693 bytes)
15/08/21 13:49:17 INFO Executor: Running task 9.0 in stage 6.0 (TID 456)
15/08/21 13:49:17 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 452) in 303 ms on localhost (6/200)
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00125-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505822 length: 3505822 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 13:49:17 INFO Executor: Finished task 6.0 in stage 6.0 (TID 453). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 457, localhost, ANY, 1691 bytes)
15/08/21 13:49:17 INFO Executor: Running task 10.0 in stage 6.0 (TID 457)
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00029-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505200 length: 3505200 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 453) in 459 ms on localhost (7/200)
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 13:49:17 INFO Executor: Finished task 7.0 in stage 6.0 (TID 454). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 458, localhost, ANY, 1691 bytes)
15/08/21 13:49:17 INFO Executor: Running task 11.0 in stage 6.0 (TID 458)
15/08/21 13:49:17 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 454) in 449 ms on localhost (8/200)
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00034-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498833 length: 3498833 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO Executor: Finished task 8.0 in stage 6.0 (TID 455). 2125 bytes result sent to driver
15/08/21 13:49:17 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 459, localhost, ANY, 1692 bytes)
15/08/21 13:49:17 INFO Executor: Running task 12.0 in stage 6.0 (TID 459)
15/08/21 13:49:17 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 455) in 452 ms on localhost (9/200)
15/08/21 13:49:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00010-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499442 length: 3499442 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 750000
15/08/21 13:49:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:17 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 13:49:18 INFO Executor: Finished task 9.0 in stage 6.0 (TID 456). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 460, localhost, ANY, 1692 bytes)
15/08/21 13:49:18 INFO Executor: Running task 13.0 in stage 6.0 (TID 460)
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00122-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500106 length: 3500106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 456) in 468 ms on localhost (10/200)
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 750000
15/08/21 13:49:18 INFO Executor: Finished task 10.0 in stage 6.0 (TID 457). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 461, localhost, ANY, 1691 bytes)
15/08/21 13:49:18 INFO Executor: Running task 14.0 in stage 6.0 (TID 461)
15/08/21 13:49:18 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 457) in 456 ms on localhost (11/200)
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00166-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3503996 length: 3503996 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 750000
15/08/21 13:49:18 INFO Executor: Finished task 11.0 in stage 6.0 (TID 458). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 462, localhost, ANY, 1693 bytes)
15/08/21 13:49:18 INFO Executor: Running task 15.0 in stage 6.0 (TID 462)
15/08/21 13:49:18 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 458) in 452 ms on localhost (12/200)
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00107-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500234 length: 3500234 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 13:49:18 INFO Executor: Finished task 12.0 in stage 6.0 (TID 459). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 463, localhost, ANY, 1692 bytes)
15/08/21 13:49:18 INFO Executor: Running task 16.0 in stage 6.0 (TID 463)
15/08/21 13:49:18 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 459) in 465 ms on localhost (13/200)
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00024-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502988 length: 3502988 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 13:49:18 INFO Executor: Finished task 13.0 in stage 6.0 (TID 460). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 464, localhost, ANY, 1691 bytes)
15/08/21 13:49:18 INFO Executor: Running task 17.0 in stage 6.0 (TID 464)
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00002-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499762 length: 3499762 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 460) in 528 ms on localhost (14/200)
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 750000
15/08/21 13:49:18 INFO Executor: Finished task 42.0 in stage 4.0 (TID 432). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 465, localhost, ANY, 1691 bytes)
15/08/21 13:49:18 INFO Executor: Running task 18.0 in stage 6.0 (TID 465)
15/08/21 13:49:18 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 432) in 15404 ms on localhost (46/57)
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00123-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501098 length: 3501098 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO Executor: Finished task 14.0 in stage 6.0 (TID 461). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 466, localhost, ANY, 1692 bytes)
15/08/21 13:49:18 INFO Executor: Running task 19.0 in stage 6.0 (TID 466)
15/08/21 13:49:18 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 461) in 569 ms on localhost (15/200)
15/08/21 13:49:18 INFO Executor: Finished task 46.0 in stage 4.0 (TID 436). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00172-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502333 length: 3502333 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 467, localhost, ANY, 1691 bytes)
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 436) in 14329 ms on localhost (47/57)
15/08/21 13:49:18 INFO Executor: Running task 20.0 in stage 6.0 (TID 467)
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00090-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3496809 length: 3496809 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO Executor: Finished task 15.0 in stage 6.0 (TID 462). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 468, localhost, ANY, 1691 bytes)
15/08/21 13:49:18 INFO Executor: Running task 21.0 in stage 6.0 (TID 468)
15/08/21 13:49:18 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 462) in 507 ms on localhost (16/200)
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00189-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3503657 length: 3503657 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO Executor: Finished task 16.0 in stage 6.0 (TID 463). 2125 bytes result sent to driver
15/08/21 13:49:18 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 469, localhost, ANY, 1693 bytes)
15/08/21 13:49:18 INFO Executor: Running task 22.0 in stage 6.0 (TID 469)
15/08/21 13:49:18 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 463) in 488 ms on localhost (17/200)
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 13:49:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00067-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501014 length: 3501014 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 750000
15/08/21 13:49:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:18 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 13:49:19 INFO Executor: Finished task 17.0 in stage 6.0 (TID 464). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 470, localhost, ANY, 1694 bytes)
15/08/21 13:49:19 INFO Executor: Running task 23.0 in stage 6.0 (TID 470)
15/08/21 13:49:19 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 464) in 540 ms on localhost (18/200)
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00102-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506891 length: 3506891 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 750000
15/08/21 13:49:19 INFO Executor: Finished task 45.0 in stage 4.0 (TID 435). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 24.0 in stage 6.0 (TID 471, localhost, ANY, 1693 bytes)
15/08/21 13:49:19 INFO Executor: Running task 24.0 in stage 6.0 (TID 471)
15/08/21 13:49:19 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 435) in 15145 ms on localhost (48/57)
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00059-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499497 length: 3499497 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 750000
15/08/21 13:49:19 INFO Executor: Finished task 19.0 in stage 6.0 (TID 466). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 25.0 in stage 6.0 (TID 472, localhost, ANY, 1693 bytes)
15/08/21 13:49:19 INFO Executor: Running task 25.0 in stage 6.0 (TID 472)
15/08/21 13:49:19 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 466) in 576 ms on localhost (19/200)
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00064-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504621 length: 3504621 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO Executor: Finished task 18.0 in stage 6.0 (TID 465). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 26.0 in stage 6.0 (TID 473, localhost, ANY, 1693 bytes)
15/08/21 13:49:19 INFO Executor: Running task 26.0 in stage 6.0 (TID 473)
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 465) in 649 ms on localhost (20/200)
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00149-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505750 length: 3505750 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO Executor: Finished task 21.0 in stage 6.0 (TID 468). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 13:49:19 INFO TaskSetManager: Starting task 27.0 in stage 6.0 (TID 474, localhost, ANY, 1693 bytes)
15/08/21 13:49:19 INFO Executor: Running task 27.0 in stage 6.0 (TID 474)
15/08/21 13:49:19 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 468) in 581 ms on localhost (21/200)
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00196-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500486 length: 3500486 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 INFO Executor: Finished task 20.0 in stage 6.0 (TID 467). 2125 bytes result sent to driver
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO TaskSetManager: Starting task 28.0 in stage 6.0 (TID 475, localhost, ANY, 1694 bytes)
15/08/21 13:49:19 INFO Executor: Running task 28.0 in stage 6.0 (TID 475)
15/08/21 13:49:19 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 467) in 653 ms on localhost (22/200)
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00032-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506885 length: 3506885 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO Executor: Finished task 22.0 in stage 6.0 (TID 469). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 29.0 in stage 6.0 (TID 476, localhost, ANY, 1692 bytes)
15/08/21 13:49:19 INFO Executor: Running task 29.0 in stage 6.0 (TID 476)
15/08/21 13:49:19 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 469) in 628 ms on localhost (23/200)
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00177-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502215 length: 3502215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 750000
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 166 ms. row count = 750000
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 76 ms. row count = 750000
15/08/21 13:49:19 INFO Executor: Finished task 23.0 in stage 6.0 (TID 470). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 30.0 in stage 6.0 (TID 477, localhost, ANY, 1693 bytes)
15/08/21 13:49:19 INFO Executor: Running task 30.0 in stage 6.0 (TID 477)
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00136-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506182 length: 3506182 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 470) in 589 ms on localhost (24/200)
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO Executor: Finished task 24.0 in stage 6.0 (TID 471). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 31.0 in stage 6.0 (TID 478, localhost, ANY, 1692 bytes)
15/08/21 13:49:19 INFO Executor: Running task 31.0 in stage 6.0 (TID 478)
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO TaskSetManager: Finished task 24.0 in stage 6.0 (TID 471) in 551 ms on localhost (25/200)
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00094-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504770 length: 3504770 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 750000
15/08/21 13:49:19 INFO Executor: Finished task 25.0 in stage 6.0 (TID 472). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 32.0 in stage 6.0 (TID 479, localhost, ANY, 1692 bytes)
15/08/21 13:49:19 INFO Executor: Running task 32.0 in stage 6.0 (TID 479)
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00056-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3503531 length: 3503531 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 INFO TaskSetManager: Finished task 25.0 in stage 6.0 (TID 472) in 483 ms on localhost (26/200)
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO Executor: Finished task 28.0 in stage 6.0 (TID 475). 2125 bytes result sent to driver
15/08/21 13:49:19 INFO TaskSetManager: Starting task 33.0 in stage 6.0 (TID 480, localhost, ANY, 1693 bytes)
15/08/21 13:49:19 INFO Executor: Running task 33.0 in stage 6.0 (TID 480)
15/08/21 13:49:19 INFO TaskSetManager: Finished task 28.0 in stage 6.0 (TID 475) in 492 ms on localhost (27/200)
15/08/21 13:49:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00096-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507797 length: 3507797 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:19 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 750000
15/08/21 13:49:20 INFO InternalParquetRecordReader: block read in memory in 263 ms. row count = 750000
15/08/21 13:49:20 INFO InternalParquetRecordReader: block read in memory in 370 ms. row count = 750000
15/08/21 13:49:20 INFO Executor: Finished task 29.0 in stage 6.0 (TID 476). 2125 bytes result sent to driver
15/08/21 13:49:20 INFO TaskSetManager: Starting task 34.0 in stage 6.0 (TID 481, localhost, ANY, 1691 bytes)
15/08/21 13:49:20 INFO Executor: Running task 34.0 in stage 6.0 (TID 481)
15/08/21 13:49:20 INFO TaskSetManager: Finished task 29.0 in stage 6.0 (TID 476) in 691 ms on localhost (28/200)
15/08/21 13:49:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00047-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504640 length: 3504640 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:20 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 750000
15/08/21 13:49:20 INFO Executor: Finished task 27.0 in stage 6.0 (TID 474). 2125 bytes result sent to driver
15/08/21 13:49:20 INFO TaskSetManager: Starting task 35.0 in stage 6.0 (TID 482, localhost, ANY, 1691 bytes)
15/08/21 13:49:20 INFO Executor: Running task 35.0 in stage 6.0 (TID 482)
15/08/21 13:49:20 INFO TaskSetManager: Finished task 27.0 in stage 6.0 (TID 474) in 1233 ms on localhost (29/200)
15/08/21 13:49:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00103-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504162 length: 3504162 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:20 INFO Executor: Finished task 26.0 in stage 6.0 (TID 473). 2125 bytes result sent to driver
15/08/21 13:49:20 INFO TaskSetManager: Starting task 36.0 in stage 6.0 (TID 483, localhost, ANY, 1691 bytes)
15/08/21 13:49:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:20 INFO Executor: Running task 36.0 in stage 6.0 (TID 483)
15/08/21 13:49:20 INFO TaskSetManager: Finished task 26.0 in stage 6.0 (TID 473) in 1323 ms on localhost (30/200)
15/08/21 13:49:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00164-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500845 length: 3500845 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:20 INFO Executor: Finished task 30.0 in stage 6.0 (TID 477). 2125 bytes result sent to driver
15/08/21 13:49:20 INFO TaskSetManager: Starting task 37.0 in stage 6.0 (TID 484, localhost, ANY, 1691 bytes)
15/08/21 13:49:20 INFO Executor: Running task 37.0 in stage 6.0 (TID 484)
15/08/21 13:49:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00007-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506054 length: 3506054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:20 INFO TaskSetManager: Finished task 30.0 in stage 6.0 (TID 477) in 1048 ms on localhost (31/200)
15/08/21 13:49:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:20 INFO InternalParquetRecordReader: block read in memory in 55 ms. row count = 750000
15/08/21 13:49:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:20 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 13:49:20 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 750000
15/08/21 13:49:20 INFO Executor: Finished task 33.0 in stage 6.0 (TID 480). 2125 bytes result sent to driver
15/08/21 13:49:20 INFO TaskSetManager: Starting task 38.0 in stage 6.0 (TID 485, localhost, ANY, 1693 bytes)
15/08/21 13:49:20 INFO Executor: Running task 38.0 in stage 6.0 (TID 485)
15/08/21 13:49:20 INFO TaskSetManager: Finished task 33.0 in stage 6.0 (TID 480) in 908 ms on localhost (32/200)
15/08/21 13:49:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00068-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500774 length: 3500774 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:20 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 13:49:20 INFO Executor: Finished task 35.0 in stage 6.0 (TID 482). 2125 bytes result sent to driver
15/08/21 13:49:20 INFO TaskSetManager: Starting task 39.0 in stage 6.0 (TID 486, localhost, ANY, 1690 bytes)
15/08/21 13:49:20 INFO Executor: Running task 39.0 in stage 6.0 (TID 486)
15/08/21 13:49:20 INFO TaskSetManager: Finished task 35.0 in stage 6.0 (TID 482) in 356 ms on localhost (33/200)
15/08/21 13:49:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00099-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500145 length: 3500145 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:21 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 13:49:21 INFO Executor: Finished task 32.0 in stage 6.0 (TID 479). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO TaskSetManager: Starting task 40.0 in stage 6.0 (TID 487, localhost, ANY, 1693 bytes)
15/08/21 13:49:21 INFO Executor: Running task 40.0 in stage 6.0 (TID 487)
15/08/21 13:49:21 INFO TaskSetManager: Finished task 32.0 in stage 6.0 (TID 479) in 1280 ms on localhost (34/200)
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00022-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506607 length: 3506607 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:21 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 750000
15/08/21 13:49:21 INFO Executor: Finished task 43.0 in stage 4.0 (TID 433). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO TaskSetManager: Starting task 41.0 in stage 6.0 (TID 488, localhost, ANY, 1692 bytes)
15/08/21 13:49:21 INFO Executor: Running task 41.0 in stage 6.0 (TID 488)
15/08/21 13:49:21 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 433) in 17694 ms on localhost (49/57)
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00187-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501231 length: 3501231 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:21 INFO Executor: Finished task 34.0 in stage 6.0 (TID 481). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO TaskSetManager: Starting task 42.0 in stage 6.0 (TID 489, localhost, ANY, 1691 bytes)
15/08/21 13:49:21 INFO Executor: Running task 42.0 in stage 6.0 (TID 489)
15/08/21 13:49:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:21 INFO TaskSetManager: Finished task 34.0 in stage 6.0 (TID 481) in 1062 ms on localhost (35/200)
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00137-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498602 length: 3498602 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO Executor: Finished task 31.0 in stage 6.0 (TID 478). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO TaskSetManager: Starting task 43.0 in stage 6.0 (TID 490, localhost, ANY, 1692 bytes)
15/08/21 13:49:21 INFO Executor: Running task 43.0 in stage 6.0 (TID 490)
15/08/21 13:49:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:21 INFO TaskSetManager: Finished task 31.0 in stage 6.0 (TID 478) in 1543 ms on localhost (36/200)
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00081-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500815 length: 3500815 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:21 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 750000
15/08/21 13:49:21 INFO InternalParquetRecordReader: block read in memory in 76 ms. row count = 750000
15/08/21 13:49:21 INFO Executor: Finished task 37.0 in stage 6.0 (TID 484). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO InternalParquetRecordReader: block read in memory in 132 ms. row count = 750000
15/08/21 13:49:21 INFO TaskSetManager: Starting task 44.0 in stage 6.0 (TID 491, localhost, ANY, 1691 bytes)
15/08/21 13:49:21 INFO Executor: Running task 44.0 in stage 6.0 (TID 491)
15/08/21 13:49:21 INFO TaskSetManager: Finished task 37.0 in stage 6.0 (TID 484) in 675 ms on localhost (37/200)
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00039-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506046 length: 3506046 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:21 INFO Executor: Finished task 38.0 in stage 6.0 (TID 485). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO TaskSetManager: Starting task 45.0 in stage 6.0 (TID 492, localhost, ANY, 1690 bytes)
15/08/21 13:49:21 INFO Executor: Running task 45.0 in stage 6.0 (TID 492)
15/08/21 13:49:21 INFO TaskSetManager: Finished task 38.0 in stage 6.0 (TID 485) in 657 ms on localhost (38/200)
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00009-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499561 length: 3499561 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:21 INFO Executor: Finished task 36.0 in stage 6.0 (TID 483). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO TaskSetManager: Starting task 46.0 in stage 6.0 (TID 493, localhost, ANY, 1691 bytes)
15/08/21 13:49:21 INFO TaskSetManager: Finished task 36.0 in stage 6.0 (TID 483) in 811 ms on localhost (39/200)
15/08/21 13:49:21 INFO Executor: Running task 46.0 in stage 6.0 (TID 493)
15/08/21 13:49:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:21 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 750000
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00195-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500801 length: 3500801 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:21 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 13:49:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:21 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:21 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 750000
15/08/21 13:49:21 INFO Executor: Finished task 39.0 in stage 6.0 (TID 486). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO TaskSetManager: Starting task 47.0 in stage 6.0 (TID 494, localhost, ANY, 1691 bytes)
15/08/21 13:49:21 INFO Executor: Running task 47.0 in stage 6.0 (TID 494)
15/08/21 13:49:21 INFO Executor: Finished task 40.0 in stage 6.0 (TID 487). 2125 bytes result sent to driver
15/08/21 13:49:21 INFO TaskSetManager: Starting task 48.0 in stage 6.0 (TID 495, localhost, ANY, 1692 bytes)
15/08/21 13:49:21 INFO TaskSetManager: Finished task 39.0 in stage 6.0 (TID 486) in 762 ms on localhost (40/200)
15/08/21 13:49:21 INFO Executor: Running task 48.0 in stage 6.0 (TID 495)
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00116-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504528 length: 3504528 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:21 INFO TaskSetManager: Finished task 40.0 in stage 6.0 (TID 487) in 675 ms on localhost (41/200)
15/08/21 13:49:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00044-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500433 length: 3500433 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:22 INFO InternalParquetRecordReader: block read in memory in 67 ms. row count = 750000
15/08/21 13:49:22 INFO InternalParquetRecordReader: block read in memory in 78 ms. row count = 750000
15/08/21 13:49:22 INFO Executor: Finished task 43.0 in stage 6.0 (TID 490). 2125 bytes result sent to driver
15/08/21 13:49:22 INFO TaskSetManager: Starting task 49.0 in stage 6.0 (TID 496, localhost, ANY, 1691 bytes)
15/08/21 13:49:22 INFO Executor: Running task 49.0 in stage 6.0 (TID 496)
15/08/21 13:49:22 INFO TaskSetManager: Finished task 43.0 in stage 6.0 (TID 490) in 1398 ms on localhost (42/200)
15/08/21 13:49:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00179-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501177 length: 3501177 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:22 INFO Executor: Finished task 42.0 in stage 6.0 (TID 489). 2125 bytes result sent to driver
15/08/21 13:49:22 INFO TaskSetManager: Starting task 50.0 in stage 6.0 (TID 497, localhost, ANY, 1692 bytes)
15/08/21 13:49:22 INFO Executor: Running task 50.0 in stage 6.0 (TID 497)
15/08/21 13:49:22 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 13:49:22 INFO TaskSetManager: Finished task 42.0 in stage 6.0 (TID 489) in 1510 ms on localhost (43/200)
15/08/21 13:49:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00038-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506215 length: 3506215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:22 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 13:49:22 INFO Executor: Finished task 41.0 in stage 6.0 (TID 488). 2125 bytes result sent to driver
15/08/21 13:49:22 INFO TaskSetManager: Starting task 51.0 in stage 6.0 (TID 498, localhost, ANY, 1693 bytes)
15/08/21 13:49:22 INFO Executor: Running task 51.0 in stage 6.0 (TID 498)
15/08/21 13:49:22 INFO Executor: Finished task 44.0 in stage 6.0 (TID 491). 2125 bytes result sent to driver
15/08/21 13:49:22 INFO TaskSetManager: Starting task 52.0 in stage 6.0 (TID 499, localhost, ANY, 1692 bytes)
15/08/21 13:49:22 INFO Executor: Running task 52.0 in stage 6.0 (TID 499)
15/08/21 13:49:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00104-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507353 length: 3507353 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:22 INFO TaskSetManager: Finished task 41.0 in stage 6.0 (TID 488) in 1713 ms on localhost (44/200)
15/08/21 13:49:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00181-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3503343 length: 3503343 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:22 INFO TaskSetManager: Finished task 44.0 in stage 6.0 (TID 491) in 1524 ms on localhost (45/200)
15/08/21 13:49:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:22 INFO InternalParquetRecordReader: block read in memory in 78 ms. row count = 750000
15/08/21 13:49:23 INFO Executor: Finished task 56.0 in stage 4.0 (TID 446). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO Executor: Finished task 46.0 in stage 6.0 (TID 493). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 118 ms. row count = 750000
15/08/21 13:49:23 INFO Executor: Finished task 45.0 in stage 6.0 (TID 492). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO TaskSetManager: Starting task 53.0 in stage 6.0 (TID 500, localhost, ANY, 1694 bytes)
15/08/21 13:49:23 INFO Executor: Running task 53.0 in stage 6.0 (TID 500)
15/08/21 13:49:23 INFO TaskSetManager: Starting task 54.0 in stage 6.0 (TID 501, localhost, ANY, 1692 bytes)
15/08/21 13:49:23 INFO TaskSetManager: Starting task 55.0 in stage 6.0 (TID 502, localhost, ANY, 1692 bytes)
15/08/21 13:49:23 INFO Executor: Running task 55.0 in stage 6.0 (TID 502)
15/08/21 13:49:23 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 446) in 9064 ms on localhost (50/57)
15/08/21 13:49:23 INFO Executor: Running task 54.0 in stage 6.0 (TID 501)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00046-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507400 length: 3507400 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 INFO TaskSetManager: Finished task 46.0 in stage 6.0 (TID 493) in 1561 ms on localhost (46/200)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00048-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506806 length: 3506806 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 INFO TaskSetManager: Finished task 45.0 in stage 6.0 (TID 492) in 1601 ms on localhost (47/200)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00140-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499669 length: 3499669 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 13:49:23 INFO Executor: Finished task 48.0 in stage 6.0 (TID 495). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO Executor: Finished task 53.0 in stage 4.0 (TID 443). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO Executor: Finished task 47.0 in stage 6.0 (TID 494). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 750000
15/08/21 13:49:23 INFO TaskSetManager: Starting task 56.0 in stage 6.0 (TID 503, localhost, ANY, 1694 bytes)
15/08/21 13:49:23 INFO Executor: Running task 56.0 in stage 6.0 (TID 503)
15/08/21 13:49:23 INFO TaskSetManager: Starting task 57.0 in stage 6.0 (TID 504, localhost, ANY, 1692 bytes)
15/08/21 13:49:23 INFO Executor: Running task 57.0 in stage 6.0 (TID 504)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00023-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507108 length: 3507108 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00031-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507536 length: 3507536 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO TaskSetManager: Starting task 58.0 in stage 6.0 (TID 505, localhost, ANY, 1692 bytes)
15/08/21 13:49:23 INFO Executor: Running task 58.0 in stage 6.0 (TID 505)
15/08/21 13:49:23 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 443) in 12096 ms on localhost (51/57)
15/08/21 13:49:23 INFO TaskSetManager: Finished task 48.0 in stage 6.0 (TID 495) in 1420 ms on localhost (48/200)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00082-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501309 length: 3501309 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 INFO TaskSetManager: Finished task 47.0 in stage 6.0 (TID 494) in 1446 ms on localhost (49/200)
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 750000
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 13:49:23 INFO Executor: Finished task 49.0 in stage 4.0 (TID 439). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO TaskSetManager: Starting task 59.0 in stage 6.0 (TID 506, localhost, ANY, 1691 bytes)
15/08/21 13:49:23 INFO Executor: Running task 59.0 in stage 6.0 (TID 506)
15/08/21 13:49:23 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 439) in 16208 ms on localhost (52/57)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00184-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507565 length: 3507565 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 89 ms. row count = 750000
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 750000
15/08/21 13:49:23 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 750000
15/08/21 13:49:23 INFO Executor: Finished task 50.0 in stage 6.0 (TID 497). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO TaskSetManager: Starting task 60.0 in stage 6.0 (TID 507, localhost, ANY, 1691 bytes)
15/08/21 13:49:23 INFO Executor: Running task 60.0 in stage 6.0 (TID 507)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00154-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500858 length: 3500858 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 INFO TaskSetManager: Finished task 50.0 in stage 6.0 (TID 497) in 651 ms on localhost (50/200)
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 77 ms. row count = 750000
15/08/21 13:49:23 INFO Executor: Finished task 51.0 in stage 6.0 (TID 498). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO TaskSetManager: Starting task 61.0 in stage 6.0 (TID 508, localhost, ANY, 1692 bytes)
15/08/21 13:49:23 INFO Executor: Running task 61.0 in stage 6.0 (TID 508)
15/08/21 13:49:23 INFO TaskSetManager: Finished task 51.0 in stage 6.0 (TID 498) in 830 ms on localhost (51/200)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00118-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507229 length: 3507229 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 INFO Executor: Finished task 49.0 in stage 6.0 (TID 496). 2125 bytes result sent to driver
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO TaskSetManager: Starting task 62.0 in stage 6.0 (TID 509, localhost, ANY, 1692 bytes)
15/08/21 13:49:23 INFO Executor: Running task 62.0 in stage 6.0 (TID 509)
15/08/21 13:49:23 INFO TaskSetManager: Finished task 49.0 in stage 6.0 (TID 496) in 1108 ms on localhost (52/200)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00130-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500816 length: 3500816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 13:49:23 INFO Executor: Finished task 52.0 in stage 6.0 (TID 499). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 750000
15/08/21 13:49:23 INFO TaskSetManager: Starting task 63.0 in stage 6.0 (TID 510, localhost, ANY, 1692 bytes)
15/08/21 13:49:23 INFO Executor: Running task 63.0 in stage 6.0 (TID 510)
15/08/21 13:49:23 INFO TaskSetManager: Finished task 52.0 in stage 6.0 (TID 499) in 921 ms on localhost (53/200)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00182-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506307 length: 3506307 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:23 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 13:49:23 INFO Executor: Finished task 55.0 in stage 6.0 (TID 502). 2125 bytes result sent to driver
15/08/21 13:49:23 INFO TaskSetManager: Starting task 64.0 in stage 6.0 (TID 511, localhost, ANY, 1690 bytes)
15/08/21 13:49:23 INFO Executor: Running task 64.0 in stage 6.0 (TID 511)
15/08/21 13:49:23 INFO TaskSetManager: Finished task 55.0 in stage 6.0 (TID 502) in 888 ms on localhost (54/200)
15/08/21 13:49:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00083-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499387 length: 3499387 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 13:49:24 INFO Executor: Finished task 54.0 in stage 6.0 (TID 501). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 65.0 in stage 6.0 (TID 512, localhost, ANY, 1691 bytes)
15/08/21 13:49:24 INFO Executor: Running task 65.0 in stage 6.0 (TID 512)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 54.0 in stage 6.0 (TID 501) in 1052 ms on localhost (55/200)
15/08/21 13:49:24 INFO Executor: Finished task 53.0 in stage 6.0 (TID 500). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00155-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499054 length: 3499054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO TaskSetManager: Starting task 66.0 in stage 6.0 (TID 513, localhost, ANY, 1693 bytes)
15/08/21 13:49:24 INFO Executor: Running task 66.0 in stage 6.0 (TID 513)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00171-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3497374 length: 3497374 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO TaskSetManager: Finished task 53.0 in stage 6.0 (TID 500) in 1069 ms on localhost (56/200)
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO Executor: Finished task 58.0 in stage 6.0 (TID 505). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 67.0 in stage 6.0 (TID 514, localhost, ANY, 1693 bytes)
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO Executor: Running task 67.0 in stage 6.0 (TID 514)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 58.0 in stage 6.0 (TID 505) in 982 ms on localhost (57/200)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00127-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504800 length: 3504800 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 750000
15/08/21 13:49:24 INFO Executor: Finished task 56.0 in stage 6.0 (TID 503). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 68.0 in stage 6.0 (TID 515, localhost, ANY, 1692 bytes)
15/08/21 13:49:24 INFO Executor: Running task 68.0 in stage 6.0 (TID 515)
15/08/21 13:49:24 INFO Executor: Finished task 57.0 in stage 6.0 (TID 504). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO Executor: Finished task 59.0 in stage 6.0 (TID 506). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 69.0 in stage 6.0 (TID 516, localhost, ANY, 1692 bytes)
15/08/21 13:49:24 INFO Executor: Running task 69.0 in stage 6.0 (TID 516)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 56.0 in stage 6.0 (TID 503) in 1061 ms on localhost (58/200)
15/08/21 13:49:24 INFO TaskSetManager: Starting task 70.0 in stage 6.0 (TID 517, localhost, ANY, 1693 bytes)
15/08/21 13:49:24 INFO Executor: Running task 70.0 in stage 6.0 (TID 517)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00019-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501762 length: 3501762 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 INFO TaskSetManager: Finished task 57.0 in stage 6.0 (TID 504) in 1065 ms on localhost (59/200)
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00100-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500864 length: 3500864 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00183-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506119 length: 3506119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 INFO TaskSetManager: Finished task 59.0 in stage 6.0 (TID 506) in 953 ms on localhost (60/200)
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 99 ms. row count = 750000
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 750000
15/08/21 13:49:24 INFO Executor: Finished task 60.0 in stage 6.0 (TID 507). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 71.0 in stage 6.0 (TID 518, localhost, ANY, 1689 bytes)
15/08/21 13:49:24 INFO Executor: Running task 71.0 in stage 6.0 (TID 518)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00001-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500034 length: 3500034 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 INFO TaskSetManager: Finished task 60.0 in stage 6.0 (TID 507) in 907 ms on localhost (61/200)
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 750000
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO Executor: Finished task 64.0 in stage 6.0 (TID 511). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 72.0 in stage 6.0 (TID 519, localhost, ANY, 1691 bytes)
15/08/21 13:49:24 INFO Executor: Running task 72.0 in stage 6.0 (TID 519)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 64.0 in stage 6.0 (TID 511) in 398 ms on localhost (62/200)
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00091-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498759 length: 3498759 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 92 ms. row count = 750000
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 750000
15/08/21 13:49:24 INFO Executor: Finished task 62.0 in stage 6.0 (TID 509). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO Executor: Finished task 61.0 in stage 6.0 (TID 508). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 73.0 in stage 6.0 (TID 520, localhost, ANY, 1691 bytes)
15/08/21 13:49:24 INFO Executor: Running task 73.0 in stage 6.0 (TID 520)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00178-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499383 length: 3499383 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 INFO TaskSetManager: Starting task 74.0 in stage 6.0 (TID 521, localhost, ANY, 1691 bytes)
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO Executor: Running task 74.0 in stage 6.0 (TID 521)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 62.0 in stage 6.0 (TID 509) in 718 ms on localhost (63/200)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 61.0 in stage 6.0 (TID 508) in 734 ms on localhost (64/200)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00084-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498871 length: 3498871 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 212 ms. row count = 750000
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 135 ms. row count = 750000
15/08/21 13:49:24 INFO Executor: Finished task 63.0 in stage 6.0 (TID 510). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 75.0 in stage 6.0 (TID 522, localhost, ANY, 1694 bytes)
15/08/21 13:49:24 INFO Executor: Running task 75.0 in stage 6.0 (TID 522)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 63.0 in stage 6.0 (TID 510) in 717 ms on localhost (65/200)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00173-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3508672 length: 3508672 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 750000
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO Executor: Finished task 48.0 in stage 4.0 (TID 438). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 76.0 in stage 6.0 (TID 523, localhost, ANY, 1691 bytes)
15/08/21 13:49:24 INFO Executor: Running task 76.0 in stage 6.0 (TID 523)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 438) in 18071 ms on localhost (53/57)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00058-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3497871 length: 3497871 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 750000
15/08/21 13:49:24 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:24 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 750000
15/08/21 13:49:24 INFO Executor: Finished task 65.0 in stage 6.0 (TID 512). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO TaskSetManager: Starting task 77.0 in stage 6.0 (TID 524, localhost, ANY, 1691 bytes)
15/08/21 13:49:24 INFO Executor: Running task 77.0 in stage 6.0 (TID 524)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 65.0 in stage 6.0 (TID 512) in 821 ms on localhost (66/200)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00017-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500155 length: 3500155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:24 INFO Executor: Finished task 66.0 in stage 6.0 (TID 513). 2125 bytes result sent to driver
15/08/21 13:49:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:24 INFO TaskSetManager: Starting task 78.0 in stage 6.0 (TID 525, localhost, ANY, 1693 bytes)
15/08/21 13:49:24 INFO Executor: Running task 78.0 in stage 6.0 (TID 525)
15/08/21 13:49:24 INFO TaskSetManager: Finished task 66.0 in stage 6.0 (TID 513) in 911 ms on localhost (67/200)
15/08/21 13:49:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00030-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506260 length: 3506260 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 750000
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 750000
15/08/21 13:49:25 INFO Executor: Finished task 67.0 in stage 6.0 (TID 514). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Starting task 79.0 in stage 6.0 (TID 526, localhost, ANY, 1693 bytes)
15/08/21 13:49:25 INFO Executor: Running task 79.0 in stage 6.0 (TID 526)
15/08/21 13:49:25 INFO TaskSetManager: Finished task 67.0 in stage 6.0 (TID 514) in 1123 ms on localhost (68/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00124-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502312 length: 3502312 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO Executor: Finished task 68.0 in stage 6.0 (TID 515). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Starting task 80.0 in stage 6.0 (TID 527, localhost, ANY, 1694 bytes)
15/08/21 13:49:25 INFO Executor: Running task 80.0 in stage 6.0 (TID 527)
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO TaskSetManager: Finished task 68.0 in stage 6.0 (TID 515) in 1058 ms on localhost (69/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00016-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507180 length: 3507180 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO Executor: Finished task 70.0 in stage 6.0 (TID 517). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 13:49:25 INFO TaskSetManager: Starting task 81.0 in stage 6.0 (TID 528, localhost, ANY, 1694 bytes)
15/08/21 13:49:25 INFO Executor: Running task 81.0 in stage 6.0 (TID 528)
15/08/21 13:49:25 INFO TaskSetManager: Finished task 70.0 in stage 6.0 (TID 517) in 1161 ms on localhost (70/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00086-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507402 length: 3507402 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 118 ms. row count = 750000
15/08/21 13:49:25 INFO Executor: Finished task 77.0 in stage 6.0 (TID 524). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Starting task 82.0 in stage 6.0 (TID 529, localhost, ANY, 1692 bytes)
15/08/21 13:49:25 INFO Executor: Running task 82.0 in stage 6.0 (TID 529)
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00062-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502754 length: 3502754 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO TaskSetManager: Finished task 77.0 in stage 6.0 (TID 524) in 535 ms on localhost (71/200)
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO Executor: Finished task 71.0 in stage 6.0 (TID 518). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 750000
15/08/21 13:49:25 INFO TaskSetManager: Starting task 83.0 in stage 6.0 (TID 530, localhost, ANY, 1693 bytes)
15/08/21 13:49:25 INFO Executor: Running task 83.0 in stage 6.0 (TID 530)
15/08/21 13:49:25 INFO TaskSetManager: Finished task 71.0 in stage 6.0 (TID 518) in 1222 ms on localhost (72/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00050-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501241 length: 3501241 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 INFO Executor: Finished task 69.0 in stage 6.0 (TID 516). 2125 bytes result sent to driver
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO TaskSetManager: Starting task 84.0 in stage 6.0 (TID 531, localhost, ANY, 1690 bytes)
15/08/21 13:49:25 INFO Executor: Running task 84.0 in stage 6.0 (TID 531)
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00186-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501858 length: 3501858 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 INFO TaskSetManager: Finished task 69.0 in stage 6.0 (TID 516) in 1301 ms on localhost (73/200)
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 52 ms. row count = 750000
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO Executor: Finished task 72.0 in stage 6.0 (TID 519). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Starting task 85.0 in stage 6.0 (TID 532, localhost, ANY, 1692 bytes)
15/08/21 13:49:25 INFO Executor: Running task 85.0 in stage 6.0 (TID 532)
15/08/21 13:49:25 INFO TaskSetManager: Finished task 72.0 in stage 6.0 (TID 519) in 1280 ms on localhost (74/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00194-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498074 length: 3498074 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 INFO Executor: Finished task 73.0 in stage 6.0 (TID 520). 2125 bytes result sent to driver
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO TaskSetManager: Starting task 86.0 in stage 6.0 (TID 533, localhost, ANY, 1690 bytes)
15/08/21 13:49:25 INFO Executor: Running task 86.0 in stage 6.0 (TID 533)
15/08/21 13:49:25 INFO Executor: Finished task 74.0 in stage 6.0 (TID 521). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Finished task 73.0 in stage 6.0 (TID 520) in 1195 ms on localhost (75/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00006-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504043 length: 3504043 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 INFO TaskSetManager: Starting task 87.0 in stage 6.0 (TID 534, localhost, ANY, 1691 bytes)
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO Executor: Running task 87.0 in stage 6.0 (TID 534)
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO TaskSetManager: Finished task 74.0 in stage 6.0 (TID 521) in 1193 ms on localhost (76/200)
15/08/21 13:49:25 INFO Executor: Finished task 75.0 in stage 6.0 (TID 522). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00170-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498753 length: 3498753 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 INFO TaskSetManager: Starting task 88.0 in stage 6.0 (TID 535, localhost, ANY, 1694 bytes)
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO Executor: Running task 88.0 in stage 6.0 (TID 535)
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 750000
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00167-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507097 length: 3507097 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 INFO TaskSetManager: Finished task 75.0 in stage 6.0 (TID 522) in 1133 ms on localhost (77/200)
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 149 ms. row count = 750000
15/08/21 13:49:25 INFO Executor: Finished task 76.0 in stage 6.0 (TID 523). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Starting task 89.0 in stage 6.0 (TID 536, localhost, ANY, 1691 bytes)
15/08/21 13:49:25 INFO Executor: Running task 89.0 in stage 6.0 (TID 536)
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO TaskSetManager: Finished task 76.0 in stage 6.0 (TID 523) in 1088 ms on localhost (78/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00161-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499824 length: 3499824 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 750000
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 750000
15/08/21 13:49:25 INFO Executor: Finished task 78.0 in stage 6.0 (TID 525). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Starting task 90.0 in stage 6.0 (TID 537, localhost, ANY, 1694 bytes)
15/08/21 13:49:25 INFO Executor: Running task 90.0 in stage 6.0 (TID 537)
15/08/21 13:49:25 INFO TaskSetManager: Finished task 78.0 in stage 6.0 (TID 525) in 834 ms on localhost (79/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00069-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506964 length: 3506964 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO Executor: Finished task 79.0 in stage 6.0 (TID 526). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO TaskSetManager: Starting task 91.0 in stage 6.0 (TID 538, localhost, ANY, 1693 bytes)
15/08/21 13:49:25 INFO Executor: Running task 91.0 in stage 6.0 (TID 538)
15/08/21 13:49:25 INFO TaskSetManager: Finished task 79.0 in stage 6.0 (TID 526) in 614 ms on localhost (80/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00198-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506610 length: 3506610 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO Executor: Finished task 80.0 in stage 6.0 (TID 527). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Starting task 92.0 in stage 6.0 (TID 539, localhost, ANY, 1693 bytes)
15/08/21 13:49:25 INFO Executor: Running task 92.0 in stage 6.0 (TID 539)
15/08/21 13:49:25 INFO TaskSetManager: Finished task 80.0 in stage 6.0 (TID 527) in 665 ms on localhost (81/200)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00111-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507275 length: 3507275 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO Executor: Finished task 81.0 in stage 6.0 (TID 528). 2125 bytes result sent to driver
15/08/21 13:49:25 INFO TaskSetManager: Starting task 93.0 in stage 6.0 (TID 540, localhost, ANY, 1690 bytes)
15/08/21 13:49:25 INFO TaskSetManager: Finished task 81.0 in stage 6.0 (TID 528) in 588 ms on localhost (82/200)
15/08/21 13:49:25 INFO Executor: Running task 93.0 in stage 6.0 (TID 540)
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00191-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506064 length: 3506064 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 INFO Executor: Finished task 82.0 in stage 6.0 (TID 529). 2125 bytes result sent to driver
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO TaskSetManager: Starting task 94.0 in stage 6.0 (TID 541, localhost, ANY, 1692 bytes)
15/08/21 13:49:25 INFO Executor: Running task 94.0 in stage 6.0 (TID 541)
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 79 ms. row count = 750000
15/08/21 13:49:25 INFO TaskSetManager: Finished task 82.0 in stage 6.0 (TID 529) in 564 ms on localhost (83/200)
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00045-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506318 length: 3506318 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:25 INFO InternalParquetRecordReader: block read in memory in 133 ms. row count = 750000
15/08/21 13:49:26 INFO InternalParquetRecordReader: block read in memory in 80 ms. row count = 750000
15/08/21 13:49:26 INFO Executor: Finished task 83.0 in stage 6.0 (TID 530). 2125 bytes result sent to driver
15/08/21 13:49:26 INFO TaskSetManager: Starting task 95.0 in stage 6.0 (TID 542, localhost, ANY, 1690 bytes)
15/08/21 13:49:26 INFO Executor: Running task 95.0 in stage 6.0 (TID 542)
15/08/21 13:49:26 INFO TaskSetManager: Finished task 83.0 in stage 6.0 (TID 530) in 629 ms on localhost (84/200)
15/08/21 13:49:26 INFO Executor: Finished task 86.0 in stage 6.0 (TID 533). 2125 bytes result sent to driver
15/08/21 13:49:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00078-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505418 length: 3505418 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:26 INFO TaskSetManager: Starting task 96.0 in stage 6.0 (TID 543, localhost, ANY, 1691 bytes)
15/08/21 13:49:26 INFO Executor: Running task 96.0 in stage 6.0 (TID 543)
15/08/21 13:49:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00042-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499078 length: 3499078 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:26 INFO TaskSetManager: Finished task 86.0 in stage 6.0 (TID 533) in 520 ms on localhost (85/200)
15/08/21 13:49:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:26 INFO InternalParquetRecordReader: block read in memory in 413 ms. row count = 750000
15/08/21 13:49:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:26 INFO InternalParquetRecordReader: block read in memory in 488 ms. row count = 750000
15/08/21 13:49:26 INFO InternalParquetRecordReader: block read in memory in 542 ms. row count = 750000
15/08/21 13:49:26 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 750000
15/08/21 13:49:26 INFO Executor: Finished task 87.0 in stage 6.0 (TID 534). 2125 bytes result sent to driver
15/08/21 13:49:26 INFO TaskSetManager: Starting task 97.0 in stage 6.0 (TID 544, localhost, ANY, 1691 bytes)
15/08/21 13:49:26 INFO Executor: Running task 97.0 in stage 6.0 (TID 544)
15/08/21 13:49:26 INFO InternalParquetRecordReader: block read in memory in 118 ms. row count = 750000
15/08/21 13:49:26 INFO TaskSetManager: Finished task 87.0 in stage 6.0 (TID 534) in 648 ms on localhost (86/200)
15/08/21 13:49:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00147-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499919 length: 3499919 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:26 INFO Executor: Finished task 90.0 in stage 6.0 (TID 537). 2125 bytes result sent to driver
15/08/21 13:49:26 INFO TaskSetManager: Starting task 98.0 in stage 6.0 (TID 545, localhost, ANY, 1693 bytes)
15/08/21 13:49:26 INFO Executor: Running task 98.0 in stage 6.0 (TID 545)
15/08/21 13:49:26 INFO TaskSetManager: Finished task 90.0 in stage 6.0 (TID 537) in 1033 ms on localhost (87/200)
15/08/21 13:49:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00074-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499991 length: 3499991 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:26 INFO Executor: Finished task 51.0 in stage 4.0 (TID 441). 2125 bytes result sent to driver
15/08/21 13:49:26 INFO TaskSetManager: Starting task 99.0 in stage 6.0 (TID 546, localhost, ANY, 1691 bytes)
15/08/21 13:49:26 INFO Executor: Running task 99.0 in stage 6.0 (TID 546)
15/08/21 13:49:26 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 441) in 16350 ms on localhost (54/57)
15/08/21 13:49:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00026-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498365 length: 3498365 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:26 INFO Executor: Finished task 84.0 in stage 6.0 (TID 531). 2125 bytes result sent to driver
15/08/21 13:49:26 INFO TaskSetManager: Starting task 100.0 in stage 6.0 (TID 547, localhost, ANY, 1693 bytes)
15/08/21 13:49:26 INFO Executor: Running task 100.0 in stage 6.0 (TID 547)
15/08/21 13:49:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00051-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500251 length: 3500251 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:26 INFO Executor: Finished task 85.0 in stage 6.0 (TID 532). 2125 bytes result sent to driver
15/08/21 13:49:26 INFO Executor: Finished task 91.0 in stage 6.0 (TID 538). 2125 bytes result sent to driver
15/08/21 13:49:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:26 INFO TaskSetManager: Starting task 101.0 in stage 6.0 (TID 548, localhost, ANY, 1693 bytes)
15/08/21 13:49:26 INFO Executor: Running task 101.0 in stage 6.0 (TID 548)
15/08/21 13:49:26 INFO TaskSetManager: Starting task 102.0 in stage 6.0 (TID 549, localhost, ANY, 1692 bytes)
15/08/21 13:49:26 INFO Executor: Running task 102.0 in stage 6.0 (TID 549)
15/08/21 13:49:26 INFO TaskSetManager: Finished task 84.0 in stage 6.0 (TID 531) in 1444 ms on localhost (88/200)
15/08/21 13:49:26 INFO TaskSetManager: Finished task 85.0 in stage 6.0 (TID 532) in 1354 ms on localhost (89/200)
15/08/21 13:49:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00138-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3497916 length: 3497916 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00146-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501235 length: 3501235 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:26 INFO TaskSetManager: Finished task 91.0 in stage 6.0 (TID 538) in 1096 ms on localhost (90/200)
15/08/21 13:49:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO Executor: Finished task 93.0 in stage 6.0 (TID 540). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO TaskSetManager: Starting task 103.0 in stage 6.0 (TID 550, localhost, ANY, 1692 bytes)
15/08/21 13:49:27 INFO Executor: Running task 103.0 in stage 6.0 (TID 550)
15/08/21 13:49:27 INFO TaskSetManager: Finished task 93.0 in stage 6.0 (TID 540) in 1093 ms on localhost (91/200)
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00049-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501976 length: 3501976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:27 INFO Executor: Finished task 89.0 in stage 6.0 (TID 536). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO TaskSetManager: Starting task 104.0 in stage 6.0 (TID 551, localhost, ANY, 1693 bytes)
15/08/21 13:49:27 INFO Executor: Running task 104.0 in stage 6.0 (TID 551)
15/08/21 13:49:27 INFO TaskSetManager: Finished task 89.0 in stage 6.0 (TID 536) in 1388 ms on localhost (92/200)
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00057-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501590 length: 3501590 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 1122 ms. row count = 750000
15/08/21 13:49:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 1131 ms. row count = 750000
15/08/21 13:49:27 INFO Executor: Finished task 88.0 in stage 6.0 (TID 535). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO TaskSetManager: Starting task 105.0 in stage 6.0 (TID 552, localhost, ANY, 1692 bytes)
15/08/21 13:49:27 INFO Executor: Running task 105.0 in stage 6.0 (TID 552)
15/08/21 13:49:27 INFO TaskSetManager: Finished task 88.0 in stage 6.0 (TID 535) in 1526 ms on localhost (93/200)
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00054-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3508574 length: 3508574 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 359 ms. row count = 750000
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 300 ms. row count = 750000
15/08/21 13:49:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO Executor: Finished task 96.0 in stage 6.0 (TID 543). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 274 ms. row count = 750000
15/08/21 13:49:27 INFO TaskSetManager: Starting task 106.0 in stage 6.0 (TID 553, localhost, ANY, 1693 bytes)
15/08/21 13:49:27 INFO Executor: Running task 106.0 in stage 6.0 (TID 553)
15/08/21 13:49:27 INFO TaskSetManager: Finished task 96.0 in stage 6.0 (TID 543) in 1125 ms on localhost (94/200)
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00095-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505119 length: 3505119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:27 INFO Executor: Finished task 95.0 in stage 6.0 (TID 542). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO TaskSetManager: Starting task 107.0 in stage 6.0 (TID 554, localhost, ANY, 1693 bytes)
15/08/21 13:49:27 INFO Executor: Running task 107.0 in stage 6.0 (TID 554)
15/08/21 13:49:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO TaskSetManager: Finished task 95.0 in stage 6.0 (TID 542) in 1174 ms on localhost (95/200)
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00134-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506718 length: 3506718 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 329 ms. row count = 750000
15/08/21 13:49:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 372 ms. row count = 750000
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 253 ms. row count = 750000
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 404 ms. row count = 750000
15/08/21 13:49:27 INFO Executor: Finished task 52.0 in stage 4.0 (TID 442). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO TaskSetManager: Starting task 108.0 in stage 6.0 (TID 555, localhost, ANY, 1691 bytes)
15/08/21 13:49:27 INFO Executor: Running task 108.0 in stage 6.0 (TID 555)
15/08/21 13:49:27 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 442) in 16403 ms on localhost (55/57)
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 750000
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00139-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502079 length: 3502079 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 342 ms. row count = 750000
15/08/21 13:49:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 290 ms. row count = 750000
15/08/21 13:49:27 INFO Executor: Finished task 54.0 in stage 4.0 (TID 444). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO TaskSetManager: Starting task 109.0 in stage 6.0 (TID 556, localhost, ANY, 1693 bytes)
15/08/21 13:49:27 INFO Executor: Running task 109.0 in stage 6.0 (TID 556)
15/08/21 13:49:27 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 240 ms. row count = 750000
15/08/21 13:49:27 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 444) in 15227 ms on localhost (56/57)
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00133-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506051 length: 3506051 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:27 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 750000
15/08/21 13:49:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 83 ms. row count = 750000
15/08/21 13:49:27 INFO Executor: Finished task 94.0 in stage 6.0 (TID 541). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO TaskSetManager: Starting task 110.0 in stage 6.0 (TID 557, localhost, ANY, 1691 bytes)
15/08/21 13:49:27 INFO Executor: Running task 110.0 in stage 6.0 (TID 557)
15/08/21 13:49:27 INFO TaskSetManager: Finished task 94.0 in stage 6.0 (TID 541) in 1804 ms on localhost (96/200)
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00092-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500083 length: 3500083 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:27 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 13:49:27 INFO Executor: Finished task 92.0 in stage 6.0 (TID 539). 2125 bytes result sent to driver
15/08/21 13:49:27 INFO TaskSetManager: Starting task 111.0 in stage 6.0 (TID 558, localhost, ANY, 1693 bytes)
15/08/21 13:49:27 INFO Executor: Running task 111.0 in stage 6.0 (TID 558)
15/08/21 13:49:27 INFO TaskSetManager: Finished task 92.0 in stage 6.0 (TID 539) in 2091 ms on localhost (97/200)
15/08/21 13:49:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00013-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505997 length: 3505997 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:28 INFO Executor: Finished task 97.0 in stage 6.0 (TID 544). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:28 INFO TaskSetManager: Starting task 112.0 in stage 6.0 (TID 559, localhost, ANY, 1690 bytes)
15/08/21 13:49:28 INFO Executor: Running task 112.0 in stage 6.0 (TID 559)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 97.0 in stage 6.0 (TID 544) in 1787 ms on localhost (98/200)
15/08/21 13:49:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00114-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501155 length: 3501155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:28 INFO Executor: Finished task 98.0 in stage 6.0 (TID 545). 2125 bytes result sent to driver
15/08/21 13:49:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:28 INFO TaskSetManager: Starting task 113.0 in stage 6.0 (TID 560, localhost, ANY, 1692 bytes)
15/08/21 13:49:28 INFO Executor: Running task 113.0 in stage 6.0 (TID 560)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 98.0 in stage 6.0 (TID 545) in 1234 ms on localhost (99/200)
15/08/21 13:49:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:28 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 13:49:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00148-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499209 length: 3499209 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:28 INFO Executor: Finished task 99.0 in stage 6.0 (TID 546). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Starting task 114.0 in stage 6.0 (TID 561, localhost, ANY, 1691 bytes)
15/08/21 13:49:28 INFO Executor: Running task 114.0 in stage 6.0 (TID 561)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 99.0 in stage 6.0 (TID 546) in 1208 ms on localhost (100/200)
15/08/21 13:49:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00106-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498575 length: 3498575 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:28 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 13:49:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:28 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 750000
15/08/21 13:49:28 INFO Executor: Finished task 55.0 in stage 4.0 (TID 445). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Starting task 115.0 in stage 6.0 (TID 562, localhost, ANY, 1691 bytes)
15/08/21 13:49:28 INFO Executor: Running task 115.0 in stage 6.0 (TID 562)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 445) in 15465 ms on localhost (57/57)
15/08/21 13:49:28 INFO DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:423) finished in 51.528 s
15/08/21 13:49:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/21 13:49:28 INFO DAGScheduler: looking for newly runnable stages
15/08/21 13:49:28 INFO DAGScheduler: running: Set(ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 13:49:28 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/21 13:49:28 INFO DAGScheduler: failed: Set()
15/08/21 13:49:28 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3b523e39
15/08/21 13:49:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00036-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500319 length: 3500319 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:49:28 INFO StatsReportListener: task runtime:(count: 157, mean: 4756.006369, stdev: 5474.981533, max: 18071.000000, min: 303.000000)
15/08/21 13:49:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:49:28 INFO StatsReportListener: 	303.0 ms	452.0 ms	507.0 ms	675.0 ms	1.2 s	10.0 s	14.2 s	15.2 s	18.1 s
15/08/21 13:49:28 INFO StatsReportListener: shuffle bytes written:(count: 157, mean: 23803601.802548, stdev: 32830118.086547, max: 76375224.000000, min: 0.000000)
15/08/21 13:49:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:49:28 INFO StatsReportListener: 	0.0 B	3.1 KB	3.1 KB	3.2 KB	3.2 KB	44.0 MB	72.8 MB	72.8 MB	72.8 MB
15/08/21 13:49:28 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List()
15/08/21 13:49:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:49:28 INFO StatsReportListener: task result size:(count: 157, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 13:49:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:49:28 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 13:49:28 INFO StatsReportListener: executor (non-fetch) time pct: (count: 157, mean: 97.571049, stdev: 2.129176, max: 99.875664, min: 90.654206)
15/08/21 13:49:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:49:28 INFO StatsReportListener: 	91 %	94 %	95 %	96 %	98 %	100 %	100 %	100 %	100 %
15/08/21 13:49:28 INFO StatsReportListener: other time pct: (count: 157, mean: 2.428951, stdev: 2.129176, max: 9.345794, min: 0.124336)
15/08/21 13:49:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:49:28 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 2 %	 4 %	 5 %	 6 %	 9 %
15/08/21 13:49:28 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 13:49:28 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[33] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 13:49:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:49:28 INFO MemoryStore: ensureFreeSpace(9704) called with curMem=1898396, maxMem=22226833244
15/08/21 13:49:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.5 KB, free 20.7 GB)
15/08/21 13:49:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:49:28 INFO MemoryStore: ensureFreeSpace(5005) called with curMem=1908100, maxMem=22226833244
15/08/21 13:49:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.9 KB, free 20.7 GB)
15/08/21 13:49:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:45550 (size: 4.9 KB, free: 20.7 GB)
15/08/21 13:49:28 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874
15/08/21 13:49:28 INFO InternalParquetRecordReader: block read in memory in 114 ms. row count = 750000
15/08/21 13:49:28 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[33] at processCmd at CliDriver.java:423)
15/08/21 13:49:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 200 tasks
15/08/21 13:49:28 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 13:49:28 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 13:49:28 INFO Executor: Finished task 101.0 in stage 6.0 (TID 548). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO Executor: Finished task 103.0 in stage 6.0 (TID 550). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO Executor: Finished task 102.0 in stage 6.0 (TID 549). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 563, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 0.0 in stage 5.0 (TID 563)
15/08/21 13:49:28 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 564, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 1.0 in stage 5.0 (TID 564)
15/08/21 13:49:28 INFO Executor: Finished task 100.0 in stage 6.0 (TID 547). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Finished task 101.0 in stage 6.0 (TID 548) in 1466 ms on localhost (101/200)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 103.0 in stage 6.0 (TID 550) in 1360 ms on localhost (102/200)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 102.0 in stage 6.0 (TID 549) in 1460 ms on localhost (103/200)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 565, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 2.0 in stage 5.0 (TID 565)
15/08/21 13:49:28 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 566, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 3.0 in stage 5.0 (TID 566)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:49:28 INFO TaskSetManager: Finished task 100.0 in stage 6.0 (TID 547) in 1906 ms on localhost (104/200)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:49:28 INFO Executor: Finished task 107.0 in stage 6.0 (TID 554). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 567, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 4.0 in stage 5.0 (TID 567)
15/08/21 13:49:28 INFO Executor: Finished task 105.0 in stage 6.0 (TID 552). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Finished task 107.0 in stage 6.0 (TID 554) in 1612 ms on localhost (105/200)
15/08/21 13:49:28 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 568, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 5.0 in stage 5.0 (TID 568)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO TaskSetManager: Finished task 105.0 in stage 6.0 (TID 552) in 1744 ms on localhost (106/200)
15/08/21 13:49:28 INFO Executor: Finished task 106.0 in stage 6.0 (TID 553). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 569, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Finished task 108.0 in stage 6.0 (TID 555). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO Executor: Running task 6.0 in stage 5.0 (TID 569)
15/08/21 13:49:28 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 570, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Finished task 109.0 in stage 6.0 (TID 556). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO Executor: Running task 7.0 in stage 5.0 (TID 570)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 571, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 8.0 in stage 5.0 (TID 571)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO TaskSetManager: Finished task 106.0 in stage 6.0 (TID 553) in 1669 ms on localhost (107/200)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 109.0 in stage 6.0 (TID 556) in 1386 ms on localhost (108/200)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO Executor: Finished task 104.0 in stage 6.0 (TID 551). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Finished task 108.0 in stage 6.0 (TID 555) in 1507 ms on localhost (109/200)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 572, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 9.0 in stage 5.0 (TID 572)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 104.0 in stage 6.0 (TID 551) in 1840 ms on localhost (110/200)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO Executor: Finished task 110.0 in stage 6.0 (TID 557). 2125 bytes result sent to driver
15/08/21 13:49:28 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 573, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:28 INFO Executor: Running task 10.0 in stage 5.0 (TID 573)
15/08/21 13:49:28 INFO TaskSetManager: Finished task 110.0 in stage 6.0 (TID 557) in 1158 ms on localhost (111/200)
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:29 INFO Executor: Finished task 113.0 in stage 6.0 (TID 560). 2125 bytes result sent to driver
15/08/21 13:49:29 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 574, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:29 INFO Executor: Running task 11.0 in stage 5.0 (TID 574)
15/08/21 13:49:29 INFO TaskSetManager: Finished task 113.0 in stage 6.0 (TID 560) in 986 ms on localhost (112/200)
15/08/21 13:49:29 INFO Executor: Finished task 111.0 in stage 6.0 (TID 558). 2125 bytes result sent to driver
15/08/21 13:49:29 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 575, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:29 INFO Executor: Running task 12.0 in stage 5.0 (TID 575)
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:29 INFO TaskSetManager: Finished task 111.0 in stage 6.0 (TID 558) in 1071 ms on localhost (113/200)
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:29 INFO Executor: Finished task 112.0 in stage 6.0 (TID 559). 2125 bytes result sent to driver
15/08/21 13:49:29 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 576, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:29 INFO Executor: Running task 13.0 in stage 5.0 (TID 576)
15/08/21 13:49:29 INFO TaskSetManager: Finished task 112.0 in stage 6.0 (TID 559) in 1063 ms on localhost (114/200)
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:29 INFO Executor: Finished task 114.0 in stage 6.0 (TID 561). 2125 bytes result sent to driver
15/08/21 13:49:29 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 577, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:29 INFO Executor: Running task 14.0 in stage 5.0 (TID 577)
15/08/21 13:49:29 INFO TaskSetManager: Finished task 114.0 in stage 6.0 (TID 561) in 1050 ms on localhost (115/200)
15/08/21 13:49:29 INFO Executor: Finished task 115.0 in stage 6.0 (TID 562). 2125 bytes result sent to driver
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:29 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 578, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:29 INFO Executor: Running task 15.0 in stage 5.0 (TID 578)
15/08/21 13:49:29 INFO TaskSetManager: Finished task 115.0 in stage 6.0 (TID 562) in 962 ms on localhost (116/200)
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:37 INFO Executor: Finished task 9.0 in stage 5.0 (TID 572). 1219 bytes result sent to driver
15/08/21 13:49:37 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 579, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:37 INFO Executor: Running task 16.0 in stage 5.0 (TID 579)
15/08/21 13:49:37 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 572) in 8235 ms on localhost (1/200)
15/08/21 13:49:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:38 INFO Executor: Finished task 15.0 in stage 5.0 (TID 578). 1219 bytes result sent to driver
15/08/21 13:49:38 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 580, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:38 INFO Executor: Running task 17.0 in stage 5.0 (TID 580)
15/08/21 13:49:38 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 578) in 9235 ms on localhost (2/200)
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:38 INFO Executor: Finished task 1.0 in stage 5.0 (TID 564). 1219 bytes result sent to driver
15/08/21 13:49:38 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 581, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:38 INFO Executor: Running task 18.0 in stage 5.0 (TID 581)
15/08/21 13:49:38 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 564) in 10261 ms on localhost (3/200)
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:38 INFO Executor: Finished task 5.0 in stage 5.0 (TID 568). 1219 bytes result sent to driver
15/08/21 13:49:38 INFO TaskSetManager: Starting task 19.0 in stage 5.0 (TID 582, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:38 INFO Executor: Running task 19.0 in stage 5.0 (TID 582)
15/08/21 13:49:38 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 568) in 10051 ms on localhost (4/200)
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:39 INFO Executor: Finished task 6.0 in stage 5.0 (TID 569). 1219 bytes result sent to driver
15/08/21 13:49:39 INFO Executor: Finished task 13.0 in stage 5.0 (TID 576). 1219 bytes result sent to driver
15/08/21 13:49:39 INFO TaskSetManager: Starting task 20.0 in stage 5.0 (TID 583, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:39 INFO Executor: Running task 20.0 in stage 5.0 (TID 583)
15/08/21 13:49:39 INFO TaskSetManager: Starting task 21.0 in stage 5.0 (TID 584, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:39 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 569) in 10274 ms on localhost (5/200)
15/08/21 13:49:39 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 576) in 10072 ms on localhost (6/200)
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:39 INFO Executor: Running task 21.0 in stage 5.0 (TID 584)
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:39 INFO Executor: Finished task 7.0 in stage 5.0 (TID 570). 1219 bytes result sent to driver
15/08/21 13:49:39 INFO TaskSetManager: Starting task 22.0 in stage 5.0 (TID 585, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:39 INFO Executor: Running task 22.0 in stage 5.0 (TID 585)
15/08/21 13:49:39 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 570) in 10541 ms on localhost (7/200)
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:39 INFO Executor: Finished task 3.0 in stage 5.0 (TID 566). 1219 bytes result sent to driver
15/08/21 13:49:39 INFO TaskSetManager: Starting task 23.0 in stage 5.0 (TID 586, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:39 INFO Executor: Running task 23.0 in stage 5.0 (TID 586)
15/08/21 13:49:39 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 566) in 11396 ms on localhost (8/200)
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:39 INFO Executor: Finished task 11.0 in stage 5.0 (TID 574). 1219 bytes result sent to driver
15/08/21 13:49:39 INFO TaskSetManager: Starting task 24.0 in stage 5.0 (TID 587, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:39 INFO Executor: Running task 24.0 in stage 5.0 (TID 587)
15/08/21 13:49:39 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 574) in 10791 ms on localhost (9/200)
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:39 INFO Executor: Finished task 14.0 in stage 5.0 (TID 577). 1219 bytes result sent to driver
15/08/21 13:49:39 INFO TaskSetManager: Starting task 25.0 in stage 5.0 (TID 588, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:39 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 577) in 10829 ms on localhost (10/200)
15/08/21 13:49:39 INFO Executor: Running task 25.0 in stage 5.0 (TID 588)
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO Executor: Finished task 8.0 in stage 5.0 (TID 571). 1219 bytes result sent to driver
15/08/21 13:49:40 INFO TaskSetManager: Starting task 26.0 in stage 5.0 (TID 589, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:40 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 571) in 11139 ms on localhost (11/200)
15/08/21 13:49:40 INFO Executor: Running task 26.0 in stage 5.0 (TID 589)
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO Executor: Finished task 2.0 in stage 5.0 (TID 565). 1219 bytes result sent to driver
15/08/21 13:49:40 INFO TaskSetManager: Starting task 27.0 in stage 5.0 (TID 590, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:40 INFO Executor: Running task 27.0 in stage 5.0 (TID 590)
15/08/21 13:49:40 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 565) in 11813 ms on localhost (12/200)
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO Executor: Finished task 0.0 in stage 5.0 (TID 563). 1219 bytes result sent to driver
15/08/21 13:49:40 INFO TaskSetManager: Starting task 28.0 in stage 5.0 (TID 591, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:40 INFO Executor: Running task 28.0 in stage 5.0 (TID 591)
15/08/21 13:49:40 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 563) in 11894 ms on localhost (13/200)
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO Executor: Finished task 4.0 in stage 5.0 (TID 567). 1219 bytes result sent to driver
15/08/21 13:49:40 INFO TaskSetManager: Starting task 29.0 in stage 5.0 (TID 592, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:40 INFO Executor: Running task 29.0 in stage 5.0 (TID 592)
15/08/21 13:49:40 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 567) in 11379 ms on localhost (14/200)
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:40 INFO Executor: Finished task 12.0 in stage 5.0 (TID 575). 1219 bytes result sent to driver
15/08/21 13:49:40 INFO TaskSetManager: Starting task 30.0 in stage 5.0 (TID 593, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:40 INFO Executor: Running task 30.0 in stage 5.0 (TID 593)
15/08/21 13:49:40 INFO Executor: Finished task 10.0 in stage 5.0 (TID 573). 1219 bytes result sent to driver
15/08/21 13:49:40 INFO TaskSetManager: Starting task 31.0 in stage 5.0 (TID 594, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:40 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 575) in 11246 ms on localhost (15/200)
15/08/21 13:49:40 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 573) in 11389 ms on localhost (16/200)
15/08/21 13:49:40 INFO Executor: Running task 31.0 in stage 5.0 (TID 594)
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:43 INFO Executor: Finished task 16.0 in stage 5.0 (TID 579). 1219 bytes result sent to driver
15/08/21 13:49:43 INFO TaskSetManager: Starting task 32.0 in stage 5.0 (TID 595, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:43 INFO Executor: Running task 32.0 in stage 5.0 (TID 595)
15/08/21 13:49:43 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 579) in 6711 ms on localhost (17/200)
15/08/21 13:49:43 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:43 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:49:44 INFO Executor: Finished task 19.0 in stage 5.0 (TID 582). 1219 bytes result sent to driver
15/08/21 13:49:44 INFO TaskSetManager: Starting task 33.0 in stage 5.0 (TID 596, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:44 INFO Executor: Running task 33.0 in stage 5.0 (TID 596)
15/08/21 13:49:44 INFO TaskSetManager: Finished task 19.0 in stage 5.0 (TID 582) in 5092 ms on localhost (18/200)
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:44 INFO Executor: Finished task 18.0 in stage 5.0 (TID 581). 1219 bytes result sent to driver
15/08/21 13:49:44 INFO TaskSetManager: Starting task 34.0 in stage 5.0 (TID 597, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:44 INFO Executor: Running task 34.0 in stage 5.0 (TID 597)
15/08/21 13:49:44 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 581) in 5536 ms on localhost (19/200)
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:44 INFO Executor: Finished task 17.0 in stage 5.0 (TID 580). 1219 bytes result sent to driver
15/08/21 13:49:44 INFO TaskSetManager: Starting task 35.0 in stage 5.0 (TID 598, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:44 INFO Executor: Running task 35.0 in stage 5.0 (TID 598)
15/08/21 13:49:44 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 580) in 6258 ms on localhost (20/200)
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:45 INFO Executor: Finished task 22.0 in stage 5.0 (TID 585). 1219 bytes result sent to driver
15/08/21 13:49:45 INFO TaskSetManager: Starting task 36.0 in stage 5.0 (TID 599, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:45 INFO TaskSetManager: Finished task 22.0 in stage 5.0 (TID 585) in 5812 ms on localhost (21/200)
15/08/21 13:49:45 INFO Executor: Running task 36.0 in stage 5.0 (TID 599)
15/08/21 13:49:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:46 INFO Executor: Finished task 20.0 in stage 5.0 (TID 583). 1219 bytes result sent to driver
15/08/21 13:49:46 INFO TaskSetManager: Starting task 37.0 in stage 5.0 (TID 600, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:46 INFO TaskSetManager: Finished task 20.0 in stage 5.0 (TID 583) in 7583 ms on localhost (22/200)
15/08/21 13:49:46 INFO Executor: Running task 37.0 in stage 5.0 (TID 600)
15/08/21 13:49:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:46 INFO Executor: Finished task 21.0 in stage 5.0 (TID 584). 1219 bytes result sent to driver
15/08/21 13:49:47 INFO TaskSetManager: Starting task 38.0 in stage 5.0 (TID 601, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:47 INFO TaskSetManager: Finished task 21.0 in stage 5.0 (TID 584) in 7851 ms on localhost (23/200)
15/08/21 13:49:47 INFO Executor: Running task 38.0 in stage 5.0 (TID 601)
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:49:47 INFO Executor: Finished task 23.0 in stage 5.0 (TID 586). 1219 bytes result sent to driver
15/08/21 13:49:47 INFO TaskSetManager: Starting task 39.0 in stage 5.0 (TID 602, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:47 INFO Executor: Running task 39.0 in stage 5.0 (TID 602)
15/08/21 13:49:47 INFO TaskSetManager: Finished task 23.0 in stage 5.0 (TID 586) in 7270 ms on localhost (24/200)
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:47 INFO Executor: Finished task 25.0 in stage 5.0 (TID 588). 1219 bytes result sent to driver
15/08/21 13:49:47 INFO TaskSetManager: Starting task 40.0 in stage 5.0 (TID 603, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:47 INFO Executor: Running task 40.0 in stage 5.0 (TID 603)
15/08/21 13:49:47 INFO TaskSetManager: Finished task 25.0 in stage 5.0 (TID 588) in 7424 ms on localhost (25/200)
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:47 INFO Executor: Finished task 27.0 in stage 5.0 (TID 590). 1219 bytes result sent to driver
15/08/21 13:49:47 INFO TaskSetManager: Starting task 41.0 in stage 5.0 (TID 604, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:47 INFO Executor: Running task 41.0 in stage 5.0 (TID 604)
15/08/21 13:49:47 INFO TaskSetManager: Finished task 27.0 in stage 5.0 (TID 590) in 7344 ms on localhost (26/200)
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:47 INFO Executor: Finished task 24.0 in stage 5.0 (TID 587). 1219 bytes result sent to driver
15/08/21 13:49:47 INFO TaskSetManager: Starting task 42.0 in stage 5.0 (TID 605, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:47 INFO Executor: Running task 42.0 in stage 5.0 (TID 605)
15/08/21 13:49:47 INFO TaskSetManager: Finished task 24.0 in stage 5.0 (TID 587) in 8073 ms on localhost (27/200)
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:48 INFO Executor: Finished task 31.0 in stage 5.0 (TID 594). 1219 bytes result sent to driver
15/08/21 13:49:48 INFO TaskSetManager: Starting task 43.0 in stage 5.0 (TID 606, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:48 INFO TaskSetManager: Finished task 31.0 in stage 5.0 (TID 594) in 7948 ms on localhost (28/200)
15/08/21 13:49:48 INFO Executor: Running task 43.0 in stage 5.0 (TID 606)
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:48 INFO Executor: Finished task 28.0 in stage 5.0 (TID 591). 1219 bytes result sent to driver
15/08/21 13:49:48 INFO TaskSetManager: Starting task 44.0 in stage 5.0 (TID 607, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:48 INFO Executor: Running task 44.0 in stage 5.0 (TID 607)
15/08/21 13:49:48 INFO TaskSetManager: Finished task 28.0 in stage 5.0 (TID 591) in 8428 ms on localhost (29/200)
15/08/21 13:49:48 INFO Executor: Finished task 26.0 in stage 5.0 (TID 589). 1219 bytes result sent to driver
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:48 INFO TaskSetManager: Starting task 45.0 in stage 5.0 (TID 608, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:48 INFO Executor: Running task 45.0 in stage 5.0 (TID 608)
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:48 INFO TaskSetManager: Finished task 26.0 in stage 5.0 (TID 589) in 8652 ms on localhost (30/200)
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:48 INFO Executor: Finished task 30.0 in stage 5.0 (TID 593). 1219 bytes result sent to driver
15/08/21 13:49:48 INFO TaskSetManager: Starting task 46.0 in stage 5.0 (TID 609, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:48 INFO Executor: Running task 46.0 in stage 5.0 (TID 609)
15/08/21 13:49:48 INFO TaskSetManager: Finished task 30.0 in stage 5.0 (TID 593) in 8427 ms on localhost (31/200)
15/08/21 13:49:48 INFO Executor: Finished task 29.0 in stage 5.0 (TID 592). 1219 bytes result sent to driver
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:48 INFO TaskSetManager: Starting task 47.0 in stage 5.0 (TID 610, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:48 INFO Executor: Running task 47.0 in stage 5.0 (TID 610)
15/08/21 13:49:48 INFO TaskSetManager: Finished task 29.0 in stage 5.0 (TID 592) in 8472 ms on localhost (32/200)
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 13:49:51 INFO Executor: Finished task 32.0 in stage 5.0 (TID 595). 1219 bytes result sent to driver
15/08/21 13:49:51 INFO TaskSetManager: Starting task 48.0 in stage 5.0 (TID 611, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:51 INFO TaskSetManager: Finished task 32.0 in stage 5.0 (TID 595) in 7250 ms on localhost (33/200)
15/08/21 13:49:51 INFO Executor: Running task 48.0 in stage 5.0 (TID 611)
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:51 INFO Executor: Finished task 33.0 in stage 5.0 (TID 596). 1219 bytes result sent to driver
15/08/21 13:49:51 INFO TaskSetManager: Starting task 49.0 in stage 5.0 (TID 612, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:51 INFO Executor: Running task 49.0 in stage 5.0 (TID 612)
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:51 INFO TaskSetManager: Finished task 33.0 in stage 5.0 (TID 596) in 7121 ms on localhost (34/200)
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:51 INFO Executor: Finished task 34.0 in stage 5.0 (TID 597). 1219 bytes result sent to driver
15/08/21 13:49:51 INFO TaskSetManager: Starting task 50.0 in stage 5.0 (TID 613, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:51 INFO Executor: Running task 50.0 in stage 5.0 (TID 613)
15/08/21 13:49:51 INFO TaskSetManager: Finished task 34.0 in stage 5.0 (TID 597) in 7413 ms on localhost (35/200)
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:52 INFO Executor: Finished task 36.0 in stage 5.0 (TID 599). 1219 bytes result sent to driver
15/08/21 13:49:52 INFO TaskSetManager: Starting task 51.0 in stage 5.0 (TID 614, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:52 INFO TaskSetManager: Finished task 36.0 in stage 5.0 (TID 599) in 7066 ms on localhost (36/200)
15/08/21 13:49:52 INFO Executor: Running task 51.0 in stage 5.0 (TID 614)
15/08/21 13:49:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:52 INFO Executor: Finished task 35.0 in stage 5.0 (TID 598). 1219 bytes result sent to driver
15/08/21 13:49:52 INFO TaskSetManager: Starting task 52.0 in stage 5.0 (TID 615, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:52 INFO Executor: Running task 52.0 in stage 5.0 (TID 615)
15/08/21 13:49:52 INFO TaskSetManager: Finished task 35.0 in stage 5.0 (TID 598) in 7727 ms on localhost (37/200)
15/08/21 13:49:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:53 INFO Executor: Finished task 39.0 in stage 5.0 (TID 602). 1219 bytes result sent to driver
15/08/21 13:49:53 INFO TaskSetManager: Starting task 53.0 in stage 5.0 (TID 616, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:53 INFO Executor: Running task 53.0 in stage 5.0 (TID 616)
15/08/21 13:49:53 INFO TaskSetManager: Finished task 39.0 in stage 5.0 (TID 602) in 6933 ms on localhost (38/200)
15/08/21 13:49:53 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:53 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:54 INFO Executor: Finished task 37.0 in stage 5.0 (TID 600). 1219 bytes result sent to driver
15/08/21 13:49:54 INFO TaskSetManager: Starting task 54.0 in stage 5.0 (TID 617, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:54 INFO Executor: Running task 54.0 in stage 5.0 (TID 617)
15/08/21 13:49:54 INFO TaskSetManager: Finished task 37.0 in stage 5.0 (TID 600) in 8207 ms on localhost (39/200)
15/08/21 13:49:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:49:55 INFO Executor: Finished task 40.0 in stage 5.0 (TID 603). 1219 bytes result sent to driver
15/08/21 13:49:55 INFO TaskSetManager: Starting task 55.0 in stage 5.0 (TID 618, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:55 INFO Executor: Running task 55.0 in stage 5.0 (TID 618)
15/08/21 13:49:55 INFO TaskSetManager: Finished task 40.0 in stage 5.0 (TID 603) in 7660 ms on localhost (40/200)
15/08/21 13:49:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:55 INFO Executor: Finished task 38.0 in stage 5.0 (TID 601). 1219 bytes result sent to driver
15/08/21 13:49:55 INFO TaskSetManager: Starting task 56.0 in stage 5.0 (TID 619, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:55 INFO Executor: Running task 56.0 in stage 5.0 (TID 619)
15/08/21 13:49:55 INFO TaskSetManager: Finished task 38.0 in stage 5.0 (TID 601) in 8298 ms on localhost (41/200)
15/08/21 13:49:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:56 INFO Executor: Finished task 42.0 in stage 5.0 (TID 605). 1219 bytes result sent to driver
15/08/21 13:49:56 INFO TaskSetManager: Starting task 57.0 in stage 5.0 (TID 620, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:56 INFO Executor: Running task 57.0 in stage 5.0 (TID 620)
15/08/21 13:49:56 INFO TaskSetManager: Finished task 42.0 in stage 5.0 (TID 605) in 8121 ms on localhost (42/200)
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:56 INFO Executor: Finished task 41.0 in stage 5.0 (TID 604). 1219 bytes result sent to driver
15/08/21 13:49:56 INFO TaskSetManager: Starting task 58.0 in stage 5.0 (TID 621, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:56 INFO Executor: Running task 58.0 in stage 5.0 (TID 621)
15/08/21 13:49:56 INFO TaskSetManager: Finished task 41.0 in stage 5.0 (TID 604) in 8521 ms on localhost (43/200)
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:56 INFO Executor: Finished task 44.0 in stage 5.0 (TID 607). 1219 bytes result sent to driver
15/08/21 13:49:56 INFO TaskSetManager: Starting task 59.0 in stage 5.0 (TID 622, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:56 INFO Executor: Running task 59.0 in stage 5.0 (TID 622)
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:56 INFO TaskSetManager: Finished task 44.0 in stage 5.0 (TID 607) in 7524 ms on localhost (44/200)
15/08/21 13:49:56 INFO Executor: Finished task 43.0 in stage 5.0 (TID 606). 1219 bytes result sent to driver
15/08/21 13:49:56 INFO TaskSetManager: Starting task 60.0 in stage 5.0 (TID 623, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:56 INFO Executor: Running task 60.0 in stage 5.0 (TID 623)
15/08/21 13:49:56 INFO TaskSetManager: Finished task 43.0 in stage 5.0 (TID 606) in 7990 ms on localhost (45/200)
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:56 INFO Executor: Finished task 45.0 in stage 5.0 (TID 608). 1219 bytes result sent to driver
15/08/21 13:49:56 INFO TaskSetManager: Starting task 61.0 in stage 5.0 (TID 624, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:56 INFO Executor: Running task 61.0 in stage 5.0 (TID 624)
15/08/21 13:49:56 INFO TaskSetManager: Finished task 45.0 in stage 5.0 (TID 608) in 7759 ms on localhost (46/200)
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:56 INFO Executor: Finished task 46.0 in stage 5.0 (TID 609). 1219 bytes result sent to driver
15/08/21 13:49:56 INFO TaskSetManager: Starting task 62.0 in stage 5.0 (TID 625, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:56 INFO TaskSetManager: Finished task 46.0 in stage 5.0 (TID 609) in 7733 ms on localhost (47/200)
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:56 INFO Executor: Running task 62.0 in stage 5.0 (TID 625)
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:56 INFO Executor: Finished task 47.0 in stage 5.0 (TID 610). 1219 bytes result sent to driver
15/08/21 13:49:56 INFO TaskSetManager: Starting task 63.0 in stage 5.0 (TID 626, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:56 INFO Executor: Running task 63.0 in stage 5.0 (TID 626)
15/08/21 13:49:56 INFO TaskSetManager: Finished task 47.0 in stage 5.0 (TID 610) in 8265 ms on localhost (48/200)
15/08/21 13:49:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:49:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:57 INFO Executor: Finished task 49.0 in stage 5.0 (TID 612). 1219 bytes result sent to driver
15/08/21 13:49:57 INFO TaskSetManager: Starting task 64.0 in stage 5.0 (TID 627, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:57 INFO Executor: Running task 64.0 in stage 5.0 (TID 627)
15/08/21 13:49:57 INFO TaskSetManager: Finished task 49.0 in stage 5.0 (TID 612) in 6868 ms on localhost (49/200)
15/08/21 13:49:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:58 INFO Executor: Finished task 50.0 in stage 5.0 (TID 613). 1219 bytes result sent to driver
15/08/21 13:49:58 INFO TaskSetManager: Starting task 65.0 in stage 5.0 (TID 628, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:58 INFO Executor: Running task 65.0 in stage 5.0 (TID 628)
15/08/21 13:49:58 INFO TaskSetManager: Finished task 50.0 in stage 5.0 (TID 613) in 6609 ms on localhost (50/200)
15/08/21 13:49:58 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:58 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:58 INFO Executor: Finished task 48.0 in stage 5.0 (TID 611). 1219 bytes result sent to driver
15/08/21 13:49:58 INFO TaskSetManager: Starting task 66.0 in stage 5.0 (TID 629, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:58 INFO Executor: Running task 66.0 in stage 5.0 (TID 629)
15/08/21 13:49:58 INFO TaskSetManager: Finished task 48.0 in stage 5.0 (TID 611) in 7791 ms on localhost (51/200)
15/08/21 13:49:58 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:58 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:58 INFO Executor: Finished task 52.0 in stage 5.0 (TID 615). 1219 bytes result sent to driver
15/08/21 13:49:58 INFO TaskSetManager: Starting task 67.0 in stage 5.0 (TID 630, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:58 INFO Executor: Running task 67.0 in stage 5.0 (TID 630)
15/08/21 13:49:58 INFO TaskSetManager: Finished task 52.0 in stage 5.0 (TID 615) in 6645 ms on localhost (52/200)
15/08/21 13:49:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:49:59 INFO Executor: Finished task 51.0 in stage 5.0 (TID 614). 1219 bytes result sent to driver
15/08/21 13:49:59 INFO TaskSetManager: Starting task 68.0 in stage 5.0 (TID 631, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:49:59 INFO Executor: Running task 68.0 in stage 5.0 (TID 631)
15/08/21 13:49:59 INFO TaskSetManager: Finished task 51.0 in stage 5.0 (TID 614) in 6801 ms on localhost (53/200)
15/08/21 13:49:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:49:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:49:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:49:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:00 INFO Executor: Finished task 53.0 in stage 5.0 (TID 616). 1219 bytes result sent to driver
15/08/21 13:50:00 INFO TaskSetManager: Starting task 69.0 in stage 5.0 (TID 632, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:00 INFO Executor: Running task 69.0 in stage 5.0 (TID 632)
15/08/21 13:50:00 INFO TaskSetManager: Finished task 53.0 in stage 5.0 (TID 616) in 7098 ms on localhost (54/200)
15/08/21 13:50:00 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:00 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:00 INFO Executor: Finished task 54.0 in stage 5.0 (TID 617). 1219 bytes result sent to driver
15/08/21 13:50:00 INFO TaskSetManager: Starting task 70.0 in stage 5.0 (TID 633, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:00 INFO Executor: Running task 70.0 in stage 5.0 (TID 633)
15/08/21 13:50:00 INFO TaskSetManager: Finished task 54.0 in stage 5.0 (TID 617) in 5969 ms on localhost (55/200)
15/08/21 13:50:00 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:00 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO Executor: Finished task 57.0 in stage 5.0 (TID 620). 1219 bytes result sent to driver
15/08/21 13:50:02 INFO TaskSetManager: Starting task 71.0 in stage 5.0 (TID 634, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:02 INFO Executor: Running task 71.0 in stage 5.0 (TID 634)
15/08/21 13:50:02 INFO TaskSetManager: Finished task 57.0 in stage 5.0 (TID 620) in 6287 ms on localhost (56/200)
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO Executor: Finished task 60.0 in stage 5.0 (TID 623). 1219 bytes result sent to driver
15/08/21 13:50:02 INFO TaskSetManager: Starting task 72.0 in stage 5.0 (TID 635, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:02 INFO Executor: Running task 72.0 in stage 5.0 (TID 635)
15/08/21 13:50:02 INFO TaskSetManager: Finished task 60.0 in stage 5.0 (TID 623) in 6324 ms on localhost (57/200)
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO Executor: Finished task 56.0 in stage 5.0 (TID 619). 1219 bytes result sent to driver
15/08/21 13:50:02 INFO TaskSetManager: Starting task 73.0 in stage 5.0 (TID 636, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:02 INFO Executor: Running task 73.0 in stage 5.0 (TID 636)
15/08/21 13:50:02 INFO TaskSetManager: Finished task 56.0 in stage 5.0 (TID 619) in 7464 ms on localhost (58/200)
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO Executor: Finished task 61.0 in stage 5.0 (TID 624). 1219 bytes result sent to driver
15/08/21 13:50:02 INFO TaskSetManager: Starting task 74.0 in stage 5.0 (TID 637, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:02 INFO Executor: Running task 74.0 in stage 5.0 (TID 637)
15/08/21 13:50:02 INFO TaskSetManager: Finished task 61.0 in stage 5.0 (TID 624) in 6419 ms on localhost (59/200)
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO Executor: Finished task 59.0 in stage 5.0 (TID 622). 1219 bytes result sent to driver
15/08/21 13:50:02 INFO TaskSetManager: Starting task 75.0 in stage 5.0 (TID 638, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:02 INFO Executor: Running task 75.0 in stage 5.0 (TID 638)
15/08/21 13:50:02 INFO TaskSetManager: Finished task 59.0 in stage 5.0 (TID 622) in 6737 ms on localhost (60/200)
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO Executor: Finished task 58.0 in stage 5.0 (TID 621). 1219 bytes result sent to driver
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO TaskSetManager: Starting task 76.0 in stage 5.0 (TID 639, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:02 INFO Executor: Running task 76.0 in stage 5.0 (TID 639)
15/08/21 13:50:02 INFO TaskSetManager: Finished task 58.0 in stage 5.0 (TID 621) in 6857 ms on localhost (61/200)
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:50:02 INFO Executor: Finished task 63.0 in stage 5.0 (TID 626). 1219 bytes result sent to driver
15/08/21 13:50:02 INFO TaskSetManager: Starting task 77.0 in stage 5.0 (TID 640, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:02 INFO Executor: Running task 77.0 in stage 5.0 (TID 640)
15/08/21 13:50:02 INFO TaskSetManager: Finished task 63.0 in stage 5.0 (TID 626) in 6021 ms on localhost (62/200)
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:03 INFO Executor: Finished task 62.0 in stage 5.0 (TID 625). 1219 bytes result sent to driver
15/08/21 13:50:03 INFO TaskSetManager: Starting task 78.0 in stage 5.0 (TID 641, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:03 INFO Executor: Running task 78.0 in stage 5.0 (TID 641)
15/08/21 13:50:03 INFO Executor: Finished task 55.0 in stage 5.0 (TID 618). 1219 bytes result sent to driver
15/08/21 13:50:03 INFO TaskSetManager: Finished task 62.0 in stage 5.0 (TID 625) in 6603 ms on localhost (63/200)
15/08/21 13:50:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:03 INFO TaskSetManager: Starting task 79.0 in stage 5.0 (TID 642, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:03 INFO Executor: Running task 79.0 in stage 5.0 (TID 642)
15/08/21 13:50:03 INFO TaskSetManager: Finished task 55.0 in stage 5.0 (TID 618) in 8006 ms on localhost (64/200)
15/08/21 13:50:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:04 INFO Executor: Finished task 65.0 in stage 5.0 (TID 628). 1219 bytes result sent to driver
15/08/21 13:50:04 INFO TaskSetManager: Starting task 80.0 in stage 5.0 (TID 643, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:04 INFO Executor: Running task 80.0 in stage 5.0 (TID 643)
15/08/21 13:50:04 INFO Executor: Finished task 64.0 in stage 5.0 (TID 627). 1219 bytes result sent to driver
15/08/21 13:50:04 INFO TaskSetManager: Finished task 65.0 in stage 5.0 (TID 628) in 5930 ms on localhost (65/200)
15/08/21 13:50:04 INFO TaskSetManager: Starting task 81.0 in stage 5.0 (TID 644, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:04 INFO Executor: Running task 81.0 in stage 5.0 (TID 644)
15/08/21 13:50:04 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:04 INFO TaskSetManager: Finished task 64.0 in stage 5.0 (TID 627) in 6136 ms on localhost (66/200)
15/08/21 13:50:04 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:04 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:04 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:05 INFO Executor: Finished task 68.0 in stage 5.0 (TID 631). 1219 bytes result sent to driver
15/08/21 13:50:05 INFO TaskSetManager: Starting task 82.0 in stage 5.0 (TID 645, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:05 INFO Executor: Running task 82.0 in stage 5.0 (TID 645)
15/08/21 13:50:05 INFO TaskSetManager: Finished task 68.0 in stage 5.0 (TID 631) in 5964 ms on localhost (67/200)
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:05 INFO Executor: Finished task 66.0 in stage 5.0 (TID 629). 1219 bytes result sent to driver
15/08/21 13:50:05 INFO TaskSetManager: Starting task 83.0 in stage 5.0 (TID 646, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:05 INFO Executor: Running task 83.0 in stage 5.0 (TID 646)
15/08/21 13:50:05 INFO TaskSetManager: Finished task 66.0 in stage 5.0 (TID 629) in 6291 ms on localhost (68/200)
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:05 INFO Executor: Finished task 67.0 in stage 5.0 (TID 630). 1219 bytes result sent to driver
15/08/21 13:50:05 INFO TaskSetManager: Starting task 84.0 in stage 5.0 (TID 647, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:05 INFO Executor: Running task 84.0 in stage 5.0 (TID 647)
15/08/21 13:50:05 INFO TaskSetManager: Finished task 67.0 in stage 5.0 (TID 630) in 6274 ms on localhost (69/200)
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:06 INFO Executor: Finished task 69.0 in stage 5.0 (TID 632). 1219 bytes result sent to driver
15/08/21 13:50:06 INFO TaskSetManager: Starting task 85.0 in stage 5.0 (TID 648, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:06 INFO Executor: Running task 85.0 in stage 5.0 (TID 648)
15/08/21 13:50:06 INFO TaskSetManager: Finished task 69.0 in stage 5.0 (TID 632) in 5536 ms on localhost (70/200)
15/08/21 13:50:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:06 INFO Executor: Finished task 70.0 in stage 5.0 (TID 633). 1219 bytes result sent to driver
15/08/21 13:50:06 INFO TaskSetManager: Starting task 86.0 in stage 5.0 (TID 649, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:06 INFO Executor: Running task 86.0 in stage 5.0 (TID 649)
15/08/21 13:50:06 INFO TaskSetManager: Finished task 70.0 in stage 5.0 (TID 633) in 5885 ms on localhost (71/200)
15/08/21 13:50:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:07 INFO Executor: Finished task 71.0 in stage 5.0 (TID 634). 1219 bytes result sent to driver
15/08/21 13:50:07 INFO TaskSetManager: Starting task 87.0 in stage 5.0 (TID 650, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:07 INFO Executor: Running task 87.0 in stage 5.0 (TID 650)
15/08/21 13:50:07 INFO TaskSetManager: Finished task 71.0 in stage 5.0 (TID 634) in 5177 ms on localhost (72/200)
15/08/21 13:50:07 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:07 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:08 INFO Executor: Finished task 73.0 in stage 5.0 (TID 636). 1219 bytes result sent to driver
15/08/21 13:50:08 INFO TaskSetManager: Starting task 88.0 in stage 5.0 (TID 651, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:08 INFO TaskSetManager: Finished task 73.0 in stage 5.0 (TID 636) in 5318 ms on localhost (73/200)
15/08/21 13:50:08 INFO Executor: Running task 88.0 in stage 5.0 (TID 651)
15/08/21 13:50:08 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:08 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO Executor: Finished task 72.0 in stage 5.0 (TID 635). 1219 bytes result sent to driver
15/08/21 13:50:09 INFO TaskSetManager: Starting task 89.0 in stage 5.0 (TID 652, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:09 INFO TaskSetManager: Finished task 72.0 in stage 5.0 (TID 635) in 6635 ms on localhost (74/200)
15/08/21 13:50:09 INFO Executor: Running task 89.0 in stage 5.0 (TID 652)
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:09 INFO Executor: Finished task 75.0 in stage 5.0 (TID 638). 1219 bytes result sent to driver
15/08/21 13:50:09 INFO TaskSetManager: Starting task 90.0 in stage 5.0 (TID 653, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:09 INFO Executor: Running task 90.0 in stage 5.0 (TID 653)
15/08/21 13:50:09 INFO TaskSetManager: Finished task 75.0 in stage 5.0 (TID 638) in 6590 ms on localhost (75/200)
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:09 INFO Executor: Finished task 77.0 in stage 5.0 (TID 640). 1219 bytes result sent to driver
15/08/21 13:50:09 INFO TaskSetManager: Starting task 91.0 in stage 5.0 (TID 654, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:09 INFO Executor: Running task 91.0 in stage 5.0 (TID 654)
15/08/21 13:50:09 INFO TaskSetManager: Finished task 77.0 in stage 5.0 (TID 640) in 6616 ms on localhost (76/200)
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:50:09 INFO Executor: Finished task 76.0 in stage 5.0 (TID 639). 1219 bytes result sent to driver
15/08/21 13:50:09 INFO TaskSetManager: Starting task 92.0 in stage 5.0 (TID 655, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:09 INFO Executor: Running task 92.0 in stage 5.0 (TID 655)
15/08/21 13:50:09 INFO TaskSetManager: Finished task 76.0 in stage 5.0 (TID 639) in 6806 ms on localhost (77/200)
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:09 INFO Executor: Finished task 78.0 in stage 5.0 (TID 641). 1219 bytes result sent to driver
15/08/21 13:50:09 INFO TaskSetManager: Starting task 93.0 in stage 5.0 (TID 656, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:09 INFO Executor: Running task 93.0 in stage 5.0 (TID 656)
15/08/21 13:50:09 INFO TaskSetManager: Finished task 78.0 in stage 5.0 (TID 641) in 6802 ms on localhost (78/200)
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO Executor: Finished task 79.0 in stage 5.0 (TID 642). 1219 bytes result sent to driver
15/08/21 13:50:09 INFO TaskSetManager: Starting task 94.0 in stage 5.0 (TID 657, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:09 INFO Executor: Running task 94.0 in stage 5.0 (TID 657)
15/08/21 13:50:09 INFO TaskSetManager: Finished task 79.0 in stage 5.0 (TID 642) in 6861 ms on localhost (79/200)
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO Executor: Finished task 80.0 in stage 5.0 (TID 643). 1219 bytes result sent to driver
15/08/21 13:50:09 INFO TaskSetManager: Starting task 95.0 in stage 5.0 (TID 658, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:09 INFO Executor: Running task 95.0 in stage 5.0 (TID 658)
15/08/21 13:50:09 INFO TaskSetManager: Finished task 80.0 in stage 5.0 (TID 643) in 5872 ms on localhost (80/200)
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:09 INFO Executor: Finished task 74.0 in stage 5.0 (TID 637). 1219 bytes result sent to driver
15/08/21 13:50:09 INFO TaskSetManager: Starting task 96.0 in stage 5.0 (TID 659, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:09 INFO Executor: Running task 96.0 in stage 5.0 (TID 659)
15/08/21 13:50:10 INFO TaskSetManager: Finished task 74.0 in stage 5.0 (TID 637) in 7174 ms on localhost (81/200)
15/08/21 13:50:10 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:10 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:11 INFO Executor: Finished task 81.0 in stage 5.0 (TID 644). 1219 bytes result sent to driver
15/08/21 13:50:11 INFO TaskSetManager: Starting task 97.0 in stage 5.0 (TID 660, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:11 INFO Executor: Running task 97.0 in stage 5.0 (TID 660)
15/08/21 13:50:11 INFO TaskSetManager: Finished task 81.0 in stage 5.0 (TID 644) in 7184 ms on localhost (82/200)
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:11 INFO Executor: Finished task 82.0 in stage 5.0 (TID 645). 1219 bytes result sent to driver
15/08/21 13:50:11 INFO TaskSetManager: Starting task 98.0 in stage 5.0 (TID 661, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:11 INFO Executor: Running task 98.0 in stage 5.0 (TID 661)
15/08/21 13:50:11 INFO TaskSetManager: Finished task 82.0 in stage 5.0 (TID 645) in 6283 ms on localhost (83/200)
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:11 INFO Executor: Finished task 84.0 in stage 5.0 (TID 647). 1219 bytes result sent to driver
15/08/21 13:50:11 INFO Executor: Finished task 83.0 in stage 5.0 (TID 646). 1219 bytes result sent to driver
15/08/21 13:50:11 INFO TaskSetManager: Starting task 99.0 in stage 5.0 (TID 662, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:11 INFO TaskSetManager: Starting task 100.0 in stage 5.0 (TID 663, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:11 INFO Executor: Running task 100.0 in stage 5.0 (TID 663)
15/08/21 13:50:11 INFO TaskSetManager: Finished task 84.0 in stage 5.0 (TID 647) in 6209 ms on localhost (84/200)
15/08/21 13:50:11 INFO TaskSetManager: Finished task 83.0 in stage 5.0 (TID 646) in 6322 ms on localhost (85/200)
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:11 INFO Executor: Running task 99.0 in stage 5.0 (TID 662)
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:11 INFO Executor: Finished task 85.0 in stage 5.0 (TID 648). 1219 bytes result sent to driver
15/08/21 13:50:11 INFO TaskSetManager: Starting task 101.0 in stage 5.0 (TID 664, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:11 INFO Executor: Running task 101.0 in stage 5.0 (TID 664)
15/08/21 13:50:11 INFO TaskSetManager: Finished task 85.0 in stage 5.0 (TID 648) in 5508 ms on localhost (86/200)
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:12 INFO Executor: Finished task 86.0 in stage 5.0 (TID 649). 1219 bytes result sent to driver
15/08/21 13:50:12 INFO TaskSetManager: Starting task 102.0 in stage 5.0 (TID 665, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:12 INFO Executor: Running task 102.0 in stage 5.0 (TID 665)
15/08/21 13:50:12 INFO TaskSetManager: Finished task 86.0 in stage 5.0 (TID 649) in 5265 ms on localhost (87/200)
15/08/21 13:50:12 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:12 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:12 INFO Executor: Finished task 87.0 in stage 5.0 (TID 650). 1219 bytes result sent to driver
15/08/21 13:50:12 INFO TaskSetManager: Starting task 103.0 in stage 5.0 (TID 666, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:12 INFO Executor: Running task 103.0 in stage 5.0 (TID 666)
15/08/21 13:50:12 INFO TaskSetManager: Finished task 87.0 in stage 5.0 (TID 650) in 5077 ms on localhost (88/200)
15/08/21 13:50:12 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:12 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:13 INFO Executor: Finished task 88.0 in stage 5.0 (TID 651). 1219 bytes result sent to driver
15/08/21 13:50:13 INFO TaskSetManager: Starting task 104.0 in stage 5.0 (TID 667, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:13 INFO TaskSetManager: Finished task 88.0 in stage 5.0 (TID 651) in 5394 ms on localhost (89/200)
15/08/21 13:50:13 INFO Executor: Running task 104.0 in stage 5.0 (TID 667)
15/08/21 13:50:13 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:13 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:14 INFO Executor: Finished task 92.0 in stage 5.0 (TID 655). 1219 bytes result sent to driver
15/08/21 13:50:14 INFO TaskSetManager: Starting task 105.0 in stage 5.0 (TID 668, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:14 INFO Executor: Running task 105.0 in stage 5.0 (TID 668)
15/08/21 13:50:14 INFO TaskSetManager: Finished task 92.0 in stage 5.0 (TID 655) in 5241 ms on localhost (90/200)
15/08/21 13:50:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO Executor: Finished task 93.0 in stage 5.0 (TID 656). 1219 bytes result sent to driver
15/08/21 13:50:15 INFO TaskSetManager: Starting task 106.0 in stage 5.0 (TID 669, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:15 INFO Executor: Running task 106.0 in stage 5.0 (TID 669)
15/08/21 13:50:15 INFO TaskSetManager: Finished task 93.0 in stage 5.0 (TID 656) in 5192 ms on localhost (91/200)
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO Executor: Finished task 89.0 in stage 5.0 (TID 652). 1219 bytes result sent to driver
15/08/21 13:50:15 INFO TaskSetManager: Starting task 107.0 in stage 5.0 (TID 670, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:15 INFO Executor: Running task 107.0 in stage 5.0 (TID 670)
15/08/21 13:50:15 INFO TaskSetManager: Finished task 89.0 in stage 5.0 (TID 652) in 5876 ms on localhost (92/200)
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:15 INFO Executor: Finished task 90.0 in stage 5.0 (TID 653). 1219 bytes result sent to driver
15/08/21 13:50:15 INFO TaskSetManager: Starting task 108.0 in stage 5.0 (TID 671, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:15 INFO Executor: Running task 108.0 in stage 5.0 (TID 671)
15/08/21 13:50:15 INFO TaskSetManager: Finished task 90.0 in stage 5.0 (TID 653) in 5689 ms on localhost (93/200)
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:15 INFO Executor: Finished task 91.0 in stage 5.0 (TID 654). 1219 bytes result sent to driver
15/08/21 13:50:15 INFO TaskSetManager: Starting task 109.0 in stage 5.0 (TID 672, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:15 INFO Executor: Running task 109.0 in stage 5.0 (TID 672)
15/08/21 13:50:15 INFO TaskSetManager: Finished task 91.0 in stage 5.0 (TID 654) in 5619 ms on localhost (94/200)
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO Executor: Finished task 94.0 in stage 5.0 (TID 657). 1219 bytes result sent to driver
15/08/21 13:50:15 INFO TaskSetManager: Starting task 110.0 in stage 5.0 (TID 673, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:15 INFO Executor: Running task 110.0 in stage 5.0 (TID 673)
15/08/21 13:50:15 INFO TaskSetManager: Finished task 94.0 in stage 5.0 (TID 657) in 5430 ms on localhost (95/200)
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO Executor: Finished task 95.0 in stage 5.0 (TID 658). 1219 bytes result sent to driver
15/08/21 13:50:15 INFO TaskSetManager: Starting task 111.0 in stage 5.0 (TID 674, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:15 INFO Executor: Running task 111.0 in stage 5.0 (TID 674)
15/08/21 13:50:15 INFO TaskSetManager: Finished task 95.0 in stage 5.0 (TID 658) in 5444 ms on localhost (96/200)
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO Executor: Finished task 96.0 in stage 5.0 (TID 659). 1219 bytes result sent to driver
15/08/21 13:50:15 INFO TaskSetManager: Starting task 112.0 in stage 5.0 (TID 675, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:15 INFO Executor: Running task 112.0 in stage 5.0 (TID 675)
15/08/21 13:50:15 INFO TaskSetManager: Finished task 96.0 in stage 5.0 (TID 659) in 5926 ms on localhost (97/200)
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:16 INFO Executor: Finished task 98.0 in stage 5.0 (TID 661). 1219 bytes result sent to driver
15/08/21 13:50:16 INFO TaskSetManager: Starting task 113.0 in stage 5.0 (TID 676, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:16 INFO Executor: Running task 113.0 in stage 5.0 (TID 676)
15/08/21 13:50:16 INFO TaskSetManager: Finished task 98.0 in stage 5.0 (TID 661) in 5520 ms on localhost (98/200)
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:16 INFO Executor: Finished task 97.0 in stage 5.0 (TID 660). 1219 bytes result sent to driver
15/08/21 13:50:16 INFO Executor: Finished task 101.0 in stage 5.0 (TID 664). 1219 bytes result sent to driver
15/08/21 13:50:16 INFO TaskSetManager: Starting task 114.0 in stage 5.0 (TID 677, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:16 INFO Executor: Running task 114.0 in stage 5.0 (TID 677)
15/08/21 13:50:16 INFO TaskSetManager: Starting task 115.0 in stage 5.0 (TID 678, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:16 INFO Executor: Running task 115.0 in stage 5.0 (TID 678)
15/08/21 13:50:16 INFO TaskSetManager: Finished task 97.0 in stage 5.0 (TID 660) in 5655 ms on localhost (99/200)
15/08/21 13:50:16 INFO TaskSetManager: Finished task 101.0 in stage 5.0 (TID 664) in 5394 ms on localhost (100/200)
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:17 INFO Executor: Finished task 99.0 in stage 5.0 (TID 662). 1219 bytes result sent to driver
15/08/21 13:50:17 INFO TaskSetManager: Starting task 116.0 in stage 5.0 (TID 679, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:17 INFO Executor: Running task 116.0 in stage 5.0 (TID 679)
15/08/21 13:50:17 INFO TaskSetManager: Finished task 99.0 in stage 5.0 (TID 662) in 5956 ms on localhost (101/200)
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:17 INFO Executor: Finished task 102.0 in stage 5.0 (TID 665). 1219 bytes result sent to driver
15/08/21 13:50:17 INFO TaskSetManager: Starting task 117.0 in stage 5.0 (TID 680, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:17 INFO Executor: Running task 117.0 in stage 5.0 (TID 680)
15/08/21 13:50:17 INFO TaskSetManager: Finished task 102.0 in stage 5.0 (TID 665) in 5428 ms on localhost (102/200)
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:17 INFO Executor: Finished task 100.0 in stage 5.0 (TID 663). 1219 bytes result sent to driver
15/08/21 13:50:17 INFO TaskSetManager: Starting task 118.0 in stage 5.0 (TID 681, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:17 INFO Executor: Running task 118.0 in stage 5.0 (TID 681)
15/08/21 13:50:17 INFO TaskSetManager: Finished task 100.0 in stage 5.0 (TID 663) in 6279 ms on localhost (103/200)
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:19 INFO Executor: Finished task 104.0 in stage 5.0 (TID 667). 1219 bytes result sent to driver
15/08/21 13:50:19 INFO TaskSetManager: Starting task 119.0 in stage 5.0 (TID 682, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:19 INFO Executor: Running task 119.0 in stage 5.0 (TID 682)
15/08/21 13:50:19 INFO TaskSetManager: Finished task 104.0 in stage 5.0 (TID 667) in 5693 ms on localhost (104/200)
15/08/21 13:50:19 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:19 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:19 INFO Executor: Finished task 103.0 in stage 5.0 (TID 666). 1219 bytes result sent to driver
15/08/21 13:50:19 INFO TaskSetManager: Starting task 120.0 in stage 5.0 (TID 683, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:19 INFO TaskSetManager: Finished task 103.0 in stage 5.0 (TID 666) in 6670 ms on localhost (105/200)
15/08/21 13:50:19 INFO Executor: Running task 120.0 in stage 5.0 (TID 683)
15/08/21 13:50:19 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:19 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:20 INFO Executor: Finished task 105.0 in stage 5.0 (TID 668). 1219 bytes result sent to driver
15/08/21 13:50:20 INFO TaskSetManager: Starting task 121.0 in stage 5.0 (TID 684, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:20 INFO Executor: Running task 121.0 in stage 5.0 (TID 684)
15/08/21 13:50:20 INFO TaskSetManager: Finished task 105.0 in stage 5.0 (TID 668) in 5541 ms on localhost (106/200)
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:20 INFO Executor: Finished task 106.0 in stage 5.0 (TID 669). 1219 bytes result sent to driver
15/08/21 13:50:20 INFO TaskSetManager: Starting task 122.0 in stage 5.0 (TID 685, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:20 INFO TaskSetManager: Finished task 106.0 in stage 5.0 (TID 669) in 5605 ms on localhost (107/200)
15/08/21 13:50:20 INFO Executor: Running task 122.0 in stage 5.0 (TID 685)
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:20 INFO Executor: Finished task 107.0 in stage 5.0 (TID 670). 1219 bytes result sent to driver
15/08/21 13:50:20 INFO TaskSetManager: Starting task 123.0 in stage 5.0 (TID 686, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:20 INFO Executor: Running task 123.0 in stage 5.0 (TID 686)
15/08/21 13:50:20 INFO TaskSetManager: Finished task 107.0 in stage 5.0 (TID 670) in 5679 ms on localhost (108/200)
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:20 INFO Executor: Finished task 109.0 in stage 5.0 (TID 672). 1219 bytes result sent to driver
15/08/21 13:50:20 INFO TaskSetManager: Starting task 124.0 in stage 5.0 (TID 687, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:20 INFO Executor: Running task 124.0 in stage 5.0 (TID 687)
15/08/21 13:50:20 INFO TaskSetManager: Finished task 109.0 in stage 5.0 (TID 672) in 5555 ms on localhost (109/200)
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:21 INFO Executor: Finished task 108.0 in stage 5.0 (TID 671). 1219 bytes result sent to driver
15/08/21 13:50:21 INFO TaskSetManager: Starting task 125.0 in stage 5.0 (TID 688, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:21 INFO Executor: Running task 125.0 in stage 5.0 (TID 688)
15/08/21 13:50:21 INFO TaskSetManager: Finished task 108.0 in stage 5.0 (TID 671) in 5860 ms on localhost (110/200)
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:21 INFO Executor: Finished task 110.0 in stage 5.0 (TID 673). 1219 bytes result sent to driver
15/08/21 13:50:21 INFO TaskSetManager: Starting task 126.0 in stage 5.0 (TID 689, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:21 INFO Executor: Running task 126.0 in stage 5.0 (TID 689)
15/08/21 13:50:21 INFO TaskSetManager: Finished task 110.0 in stage 5.0 (TID 673) in 5832 ms on localhost (111/200)
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:21 INFO Executor: Finished task 111.0 in stage 5.0 (TID 674). 1219 bytes result sent to driver
15/08/21 13:50:21 INFO TaskSetManager: Starting task 127.0 in stage 5.0 (TID 690, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:21 INFO TaskSetManager: Finished task 111.0 in stage 5.0 (TID 674) in 6369 ms on localhost (112/200)
15/08/21 13:50:21 INFO Executor: Running task 127.0 in stage 5.0 (TID 690)
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:50:22 INFO Executor: Finished task 112.0 in stage 5.0 (TID 675). 1219 bytes result sent to driver
15/08/21 13:50:22 INFO TaskSetManager: Starting task 128.0 in stage 5.0 (TID 691, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:22 INFO TaskSetManager: Finished task 112.0 in stage 5.0 (TID 675) in 6122 ms on localhost (113/200)
15/08/21 13:50:22 INFO Executor: Running task 128.0 in stage 5.0 (TID 691)
15/08/21 13:50:22 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:22 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:22 INFO Executor: Finished task 113.0 in stage 5.0 (TID 676). 1219 bytes result sent to driver
15/08/21 13:50:22 INFO TaskSetManager: Starting task 129.0 in stage 5.0 (TID 692, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:22 INFO Executor: Running task 129.0 in stage 5.0 (TID 692)
15/08/21 13:50:22 INFO TaskSetManager: Finished task 113.0 in stage 5.0 (TID 676) in 5287 ms on localhost (114/200)
15/08/21 13:50:22 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:22 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:23 INFO Executor: Finished task 114.0 in stage 5.0 (TID 677). 1219 bytes result sent to driver
15/08/21 13:50:23 INFO TaskSetManager: Starting task 130.0 in stage 5.0 (TID 693, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:23 INFO Executor: Running task 130.0 in stage 5.0 (TID 693)
15/08/21 13:50:23 INFO TaskSetManager: Finished task 114.0 in stage 5.0 (TID 677) in 6871 ms on localhost (115/200)
15/08/21 13:50:23 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:23 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:24 INFO Executor: Finished task 116.0 in stage 5.0 (TID 679). 1219 bytes result sent to driver
15/08/21 13:50:24 INFO TaskSetManager: Starting task 131.0 in stage 5.0 (TID 694, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:24 INFO Executor: Running task 131.0 in stage 5.0 (TID 694)
15/08/21 13:50:24 INFO TaskSetManager: Finished task 116.0 in stage 5.0 (TID 679) in 6624 ms on localhost (116/200)
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:24 INFO Executor: Finished task 115.0 in stage 5.0 (TID 678). 1219 bytes result sent to driver
15/08/21 13:50:24 INFO TaskSetManager: Starting task 132.0 in stage 5.0 (TID 695, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:24 INFO TaskSetManager: Finished task 115.0 in stage 5.0 (TID 678) in 7116 ms on localhost (117/200)
15/08/21 13:50:24 INFO Executor: Running task 132.0 in stage 5.0 (TID 695)
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:24 INFO Executor: Finished task 117.0 in stage 5.0 (TID 680). 1219 bytes result sent to driver
15/08/21 13:50:24 INFO TaskSetManager: Starting task 133.0 in stage 5.0 (TID 696, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:24 INFO Executor: Running task 133.0 in stage 5.0 (TID 696)
15/08/21 13:50:24 INFO TaskSetManager: Finished task 117.0 in stage 5.0 (TID 680) in 7291 ms on localhost (118/200)
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:24 INFO Executor: Finished task 118.0 in stage 5.0 (TID 681). 1219 bytes result sent to driver
15/08/21 13:50:24 INFO TaskSetManager: Starting task 134.0 in stage 5.0 (TID 697, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:24 INFO Executor: Running task 134.0 in stage 5.0 (TID 697)
15/08/21 13:50:24 INFO TaskSetManager: Finished task 118.0 in stage 5.0 (TID 681) in 7096 ms on localhost (119/200)
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:27 INFO Executor: Finished task 119.0 in stage 5.0 (TID 682). 1219 bytes result sent to driver
15/08/21 13:50:27 INFO TaskSetManager: Starting task 135.0 in stage 5.0 (TID 698, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:27 INFO TaskSetManager: Finished task 119.0 in stage 5.0 (TID 682) in 8210 ms on localhost (120/200)
15/08/21 13:50:27 INFO Executor: Running task 135.0 in stage 5.0 (TID 698)
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:27 INFO Executor: Finished task 120.0 in stage 5.0 (TID 683). 1219 bytes result sent to driver
15/08/21 13:50:27 INFO TaskSetManager: Starting task 136.0 in stage 5.0 (TID 699, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:27 INFO Executor: Running task 136.0 in stage 5.0 (TID 699)
15/08/21 13:50:27 INFO TaskSetManager: Finished task 120.0 in stage 5.0 (TID 683) in 8222 ms on localhost (121/200)
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:27 INFO Executor: Finished task 122.0 in stage 5.0 (TID 685). 1219 bytes result sent to driver
15/08/21 13:50:27 INFO TaskSetManager: Starting task 137.0 in stage 5.0 (TID 700, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:27 INFO TaskSetManager: Finished task 122.0 in stage 5.0 (TID 685) in 7227 ms on localhost (122/200)
15/08/21 13:50:27 INFO Executor: Running task 137.0 in stage 5.0 (TID 700)
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:27 INFO Executor: Finished task 121.0 in stage 5.0 (TID 684). 1219 bytes result sent to driver
15/08/21 13:50:27 INFO TaskSetManager: Starting task 138.0 in stage 5.0 (TID 701, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:27 INFO Executor: Running task 138.0 in stage 5.0 (TID 701)
15/08/21 13:50:27 INFO TaskSetManager: Finished task 121.0 in stage 5.0 (TID 684) in 7408 ms on localhost (123/200)
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 13:50:28 INFO Executor: Finished task 124.0 in stage 5.0 (TID 687). 1219 bytes result sent to driver
15/08/21 13:50:28 INFO TaskSetManager: Starting task 139.0 in stage 5.0 (TID 702, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:28 INFO TaskSetManager: Finished task 124.0 in stage 5.0 (TID 687) in 7877 ms on localhost (124/200)
15/08/21 13:50:28 INFO Executor: Running task 139.0 in stage 5.0 (TID 702)
15/08/21 13:50:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:28 INFO Executor: Finished task 123.0 in stage 5.0 (TID 686). 1219 bytes result sent to driver
15/08/21 13:50:28 INFO TaskSetManager: Starting task 140.0 in stage 5.0 (TID 703, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:28 INFO Executor: Running task 140.0 in stage 5.0 (TID 703)
15/08/21 13:50:28 INFO TaskSetManager: Finished task 123.0 in stage 5.0 (TID 686) in 8146 ms on localhost (125/200)
15/08/21 13:50:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:29 INFO Executor: Finished task 125.0 in stage 5.0 (TID 688). 1219 bytes result sent to driver
15/08/21 13:50:29 INFO TaskSetManager: Starting task 141.0 in stage 5.0 (TID 704, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:29 INFO Executor: Running task 141.0 in stage 5.0 (TID 704)
15/08/21 13:50:29 INFO TaskSetManager: Finished task 125.0 in stage 5.0 (TID 688) in 8081 ms on localhost (126/200)
15/08/21 13:50:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:30 INFO Executor: Finished task 127.0 in stage 5.0 (TID 690). 1219 bytes result sent to driver
15/08/21 13:50:30 INFO Executor: Finished task 126.0 in stage 5.0 (TID 689). 1219 bytes result sent to driver
15/08/21 13:50:30 INFO TaskSetManager: Starting task 142.0 in stage 5.0 (TID 705, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:30 INFO TaskSetManager: Starting task 143.0 in stage 5.0 (TID 706, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:30 INFO Executor: Running task 143.0 in stage 5.0 (TID 706)
15/08/21 13:50:30 INFO Executor: Running task 142.0 in stage 5.0 (TID 705)
15/08/21 13:50:30 INFO TaskSetManager: Finished task 127.0 in stage 5.0 (TID 690) in 8487 ms on localhost (127/200)
15/08/21 13:50:30 INFO TaskSetManager: Finished task 126.0 in stage 5.0 (TID 689) in 9128 ms on localhost (128/200)
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 13:50:30 INFO Executor: Finished task 128.0 in stage 5.0 (TID 691). 1219 bytes result sent to driver
15/08/21 13:50:30 INFO TaskSetManager: Starting task 144.0 in stage 5.0 (TID 707, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:30 INFO Executor: Running task 144.0 in stage 5.0 (TID 707)
15/08/21 13:50:30 INFO TaskSetManager: Finished task 128.0 in stage 5.0 (TID 691) in 8346 ms on localhost (129/200)
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:30 INFO Executor: Finished task 129.0 in stage 5.0 (TID 692). 1219 bytes result sent to driver
15/08/21 13:50:30 INFO TaskSetManager: Starting task 145.0 in stage 5.0 (TID 708, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:30 INFO Executor: Running task 145.0 in stage 5.0 (TID 708)
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:30 INFO TaskSetManager: Finished task 129.0 in stage 5.0 (TID 692) in 8579 ms on localhost (130/200)
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:30 INFO Executor: Finished task 130.0 in stage 5.0 (TID 693). 1219 bytes result sent to driver
15/08/21 13:50:30 INFO TaskSetManager: Starting task 146.0 in stage 5.0 (TID 709, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:30 INFO Executor: Running task 146.0 in stage 5.0 (TID 709)
15/08/21 13:50:30 INFO TaskSetManager: Finished task 130.0 in stage 5.0 (TID 693) in 6998 ms on localhost (131/200)
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:31 INFO Executor: Finished task 131.0 in stage 5.0 (TID 694). 1219 bytes result sent to driver
15/08/21 13:50:31 INFO TaskSetManager: Starting task 147.0 in stage 5.0 (TID 710, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:31 INFO Executor: Running task 147.0 in stage 5.0 (TID 710)
15/08/21 13:50:31 INFO TaskSetManager: Finished task 131.0 in stage 5.0 (TID 694) in 7289 ms on localhost (132/200)
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:31 INFO Executor: Finished task 132.0 in stage 5.0 (TID 695). 1219 bytes result sent to driver
15/08/21 13:50:31 INFO TaskSetManager: Starting task 148.0 in stage 5.0 (TID 711, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:31 INFO Executor: Running task 148.0 in stage 5.0 (TID 711)
15/08/21 13:50:31 INFO TaskSetManager: Finished task 132.0 in stage 5.0 (TID 695) in 7633 ms on localhost (133/200)
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:31 INFO Executor: Finished task 133.0 in stage 5.0 (TID 696). 1219 bytes result sent to driver
15/08/21 13:50:31 INFO TaskSetManager: Starting task 149.0 in stage 5.0 (TID 712, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:31 INFO Executor: Running task 149.0 in stage 5.0 (TID 712)
15/08/21 13:50:31 INFO TaskSetManager: Finished task 133.0 in stage 5.0 (TID 696) in 7134 ms on localhost (134/200)
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 13:50:32 INFO Executor: Finished task 134.0 in stage 5.0 (TID 697). 1219 bytes result sent to driver
15/08/21 13:50:32 INFO TaskSetManager: Starting task 150.0 in stage 5.0 (TID 713, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:32 INFO Executor: Running task 150.0 in stage 5.0 (TID 713)
15/08/21 13:50:32 INFO TaskSetManager: Finished task 134.0 in stage 5.0 (TID 697) in 7673 ms on localhost (135/200)
15/08/21 13:50:32 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:32 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:38 INFO Executor: Finished task 136.0 in stage 5.0 (TID 699). 1219 bytes result sent to driver
15/08/21 13:50:38 INFO TaskSetManager: Starting task 151.0 in stage 5.0 (TID 714, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:38 INFO Executor: Running task 151.0 in stage 5.0 (TID 714)
15/08/21 13:50:38 INFO TaskSetManager: Finished task 136.0 in stage 5.0 (TID 699) in 11232 ms on localhost (136/200)
15/08/21 13:50:38 INFO Executor: Finished task 135.0 in stage 5.0 (TID 698). 1219 bytes result sent to driver
15/08/21 13:50:38 INFO TaskSetManager: Starting task 152.0 in stage 5.0 (TID 715, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:38 INFO Executor: Running task 152.0 in stage 5.0 (TID 715)
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:38 INFO TaskSetManager: Finished task 135.0 in stage 5.0 (TID 698) in 11302 ms on localhost (137/200)
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:38 INFO Executor: Finished task 138.0 in stage 5.0 (TID 701). 1219 bytes result sent to driver
15/08/21 13:50:38 INFO TaskSetManager: Starting task 153.0 in stage 5.0 (TID 716, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:38 INFO Executor: Running task 153.0 in stage 5.0 (TID 716)
15/08/21 13:50:38 INFO TaskSetManager: Finished task 138.0 in stage 5.0 (TID 701) in 10899 ms on localhost (138/200)
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:39 INFO Executor: Finished task 139.0 in stage 5.0 (TID 702). 1219 bytes result sent to driver
15/08/21 13:50:39 INFO TaskSetManager: Starting task 154.0 in stage 5.0 (TID 717, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:39 INFO Executor: Running task 154.0 in stage 5.0 (TID 717)
15/08/21 13:50:39 INFO TaskSetManager: Finished task 139.0 in stage 5.0 (TID 702) in 10725 ms on localhost (139/200)
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:39 INFO Executor: Finished task 137.0 in stage 5.0 (TID 700). 1219 bytes result sent to driver
15/08/21 13:50:39 INFO TaskSetManager: Starting task 155.0 in stage 5.0 (TID 718, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:39 INFO Executor: Running task 155.0 in stage 5.0 (TID 718)
15/08/21 13:50:39 INFO TaskSetManager: Finished task 137.0 in stage 5.0 (TID 700) in 11681 ms on localhost (140/200)
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:39 INFO Executor: Finished task 140.0 in stage 5.0 (TID 703). 1219 bytes result sent to driver
15/08/21 13:50:39 INFO TaskSetManager: Starting task 156.0 in stage 5.0 (TID 719, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:39 INFO Executor: Running task 156.0 in stage 5.0 (TID 719)
15/08/21 13:50:39 INFO TaskSetManager: Finished task 140.0 in stage 5.0 (TID 703) in 10757 ms on localhost (141/200)
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:39 INFO Executor: Finished task 141.0 in stage 5.0 (TID 704). 1219 bytes result sent to driver
15/08/21 13:50:39 INFO TaskSetManager: Starting task 157.0 in stage 5.0 (TID 720, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:39 INFO TaskSetManager: Finished task 141.0 in stage 5.0 (TID 704) in 10632 ms on localhost (142/200)
15/08/21 13:50:39 INFO Executor: Running task 157.0 in stage 5.0 (TID 720)
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:40 INFO Executor: Finished task 142.0 in stage 5.0 (TID 705). 1219 bytes result sent to driver
15/08/21 13:50:40 INFO TaskSetManager: Starting task 158.0 in stage 5.0 (TID 721, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:40 INFO Executor: Running task 158.0 in stage 5.0 (TID 721)
15/08/21 13:50:40 INFO TaskSetManager: Finished task 142.0 in stage 5.0 (TID 705) in 9826 ms on localhost (143/200)
15/08/21 13:50:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:40 INFO Executor: Finished task 143.0 in stage 5.0 (TID 706). 1219 bytes result sent to driver
15/08/21 13:50:40 INFO TaskSetManager: Starting task 159.0 in stage 5.0 (TID 722, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:40 INFO Executor: Running task 159.0 in stage 5.0 (TID 722)
15/08/21 13:50:40 INFO TaskSetManager: Finished task 143.0 in stage 5.0 (TID 706) in 10592 ms on localhost (144/200)
15/08/21 13:50:40 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:40 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 13:50:41 INFO Executor: Finished task 146.0 in stage 5.0 (TID 709). 1219 bytes result sent to driver
15/08/21 13:50:41 INFO TaskSetManager: Starting task 160.0 in stage 5.0 (TID 723, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:41 INFO Executor: Running task 160.0 in stage 5.0 (TID 723)
15/08/21 13:50:41 INFO TaskSetManager: Finished task 146.0 in stage 5.0 (TID 709) in 10407 ms on localhost (145/200)
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:41 INFO Executor: Finished task 145.0 in stage 5.0 (TID 708). 1219 bytes result sent to driver
15/08/21 13:50:41 INFO TaskSetManager: Starting task 161.0 in stage 5.0 (TID 724, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:41 INFO TaskSetManager: Finished task 145.0 in stage 5.0 (TID 708) in 11034 ms on localhost (146/200)
15/08/21 13:50:41 INFO Executor: Running task 161.0 in stage 5.0 (TID 724)
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:41 INFO Executor: Finished task 147.0 in stage 5.0 (TID 710). 1219 bytes result sent to driver
15/08/21 13:50:41 INFO TaskSetManager: Starting task 162.0 in stage 5.0 (TID 725, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:41 INFO Executor: Running task 162.0 in stage 5.0 (TID 725)
15/08/21 13:50:41 INFO TaskSetManager: Finished task 147.0 in stage 5.0 (TID 710) in 10516 ms on localhost (147/200)
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 13:50:42 INFO Executor: Finished task 148.0 in stage 5.0 (TID 711). 1219 bytes result sent to driver
15/08/21 13:50:42 INFO TaskSetManager: Starting task 163.0 in stage 5.0 (TID 726, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:42 INFO TaskSetManager: Finished task 148.0 in stage 5.0 (TID 711) in 10451 ms on localhost (148/200)
15/08/21 13:50:42 INFO Executor: Running task 163.0 in stage 5.0 (TID 726)
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:42 INFO Executor: Finished task 150.0 in stage 5.0 (TID 713). 1219 bytes result sent to driver
15/08/21 13:50:42 INFO TaskSetManager: Starting task 164.0 in stage 5.0 (TID 727, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:42 INFO Executor: Running task 164.0 in stage 5.0 (TID 727)
15/08/21 13:50:42 INFO TaskSetManager: Finished task 150.0 in stage 5.0 (TID 713) in 10024 ms on localhost (149/200)
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:42 INFO Executor: Finished task 149.0 in stage 5.0 (TID 712). 1219 bytes result sent to driver
15/08/21 13:50:42 INFO TaskSetManager: Starting task 165.0 in stage 5.0 (TID 728, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:42 INFO Executor: Running task 165.0 in stage 5.0 (TID 728)
15/08/21 13:50:42 INFO TaskSetManager: Finished task 149.0 in stage 5.0 (TID 712) in 10811 ms on localhost (150/200)
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:42 INFO Executor: Finished task 144.0 in stage 5.0 (TID 707). 1219 bytes result sent to driver
15/08/21 13:50:42 INFO TaskSetManager: Starting task 166.0 in stage 5.0 (TID 729, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:42 INFO Executor: Running task 166.0 in stage 5.0 (TID 729)
15/08/21 13:50:42 INFO TaskSetManager: Finished task 144.0 in stage 5.0 (TID 707) in 12325 ms on localhost (151/200)
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:47 INFO Executor: Finished task 151.0 in stage 5.0 (TID 714). 1219 bytes result sent to driver
15/08/21 13:50:47 INFO TaskSetManager: Starting task 167.0 in stage 5.0 (TID 730, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:47 INFO Executor: Running task 167.0 in stage 5.0 (TID 730)
15/08/21 13:50:47 INFO TaskSetManager: Finished task 151.0 in stage 5.0 (TID 714) in 9263 ms on localhost (152/200)
15/08/21 13:50:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:48 INFO Executor: Finished task 152.0 in stage 5.0 (TID 715). 1219 bytes result sent to driver
15/08/21 13:50:48 INFO TaskSetManager: Starting task 168.0 in stage 5.0 (TID 731, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:48 INFO Executor: Running task 168.0 in stage 5.0 (TID 731)
15/08/21 13:50:48 INFO TaskSetManager: Finished task 152.0 in stage 5.0 (TID 715) in 9467 ms on localhost (153/200)
15/08/21 13:50:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:50 INFO Executor: Finished task 156.0 in stage 5.0 (TID 719). 1219 bytes result sent to driver
15/08/21 13:50:50 INFO TaskSetManager: Starting task 169.0 in stage 5.0 (TID 732, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:50 INFO Executor: Running task 169.0 in stage 5.0 (TID 732)
15/08/21 13:50:50 INFO TaskSetManager: Finished task 156.0 in stage 5.0 (TID 719) in 10431 ms on localhost (154/200)
15/08/21 13:50:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:50 INFO Executor: Finished task 154.0 in stage 5.0 (TID 717). 1219 bytes result sent to driver
15/08/21 13:50:50 INFO TaskSetManager: Starting task 170.0 in stage 5.0 (TID 733, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:50 INFO Executor: Running task 170.0 in stage 5.0 (TID 733)
15/08/21 13:50:50 INFO TaskSetManager: Finished task 154.0 in stage 5.0 (TID 717) in 10730 ms on localhost (155/200)
15/08/21 13:50:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:51 INFO Executor: Finished task 155.0 in stage 5.0 (TID 718). 1219 bytes result sent to driver
15/08/21 13:50:51 INFO TaskSetManager: Starting task 171.0 in stage 5.0 (TID 734, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:51 INFO Executor: Running task 171.0 in stage 5.0 (TID 734)
15/08/21 13:50:51 INFO TaskSetManager: Finished task 155.0 in stage 5.0 (TID 718) in 11830 ms on localhost (156/200)
15/08/21 13:50:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:50:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:52 INFO Executor: Finished task 157.0 in stage 5.0 (TID 720). 1219 bytes result sent to driver
15/08/21 13:50:52 INFO TaskSetManager: Starting task 172.0 in stage 5.0 (TID 735, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:52 INFO Executor: Running task 172.0 in stage 5.0 (TID 735)
15/08/21 13:50:52 INFO TaskSetManager: Finished task 157.0 in stage 5.0 (TID 720) in 12509 ms on localhost (157/200)
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:52 INFO Executor: Finished task 158.0 in stage 5.0 (TID 721). 1219 bytes result sent to driver
15/08/21 13:50:52 INFO TaskSetManager: Starting task 173.0 in stage 5.0 (TID 736, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:52 INFO Executor: Running task 173.0 in stage 5.0 (TID 736)
15/08/21 13:50:52 INFO TaskSetManager: Finished task 158.0 in stage 5.0 (TID 721) in 12352 ms on localhost (158/200)
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:52 INFO Executor: Finished task 153.0 in stage 5.0 (TID 716). 1219 bytes result sent to driver
15/08/21 13:50:52 INFO TaskSetManager: Starting task 174.0 in stage 5.0 (TID 737, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:52 INFO Executor: Running task 174.0 in stage 5.0 (TID 737)
15/08/21 13:50:52 INFO TaskSetManager: Finished task 153.0 in stage 5.0 (TID 716) in 13710 ms on localhost (159/200)
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:52 INFO Executor: Finished task 159.0 in stage 5.0 (TID 722). 1219 bytes result sent to driver
15/08/21 13:50:52 INFO TaskSetManager: Starting task 175.0 in stage 5.0 (TID 738, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:52 INFO Executor: Running task 175.0 in stage 5.0 (TID 738)
15/08/21 13:50:52 INFO TaskSetManager: Finished task 159.0 in stage 5.0 (TID 722) in 11858 ms on localhost (160/200)
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:52 INFO Executor: Finished task 162.0 in stage 5.0 (TID 725). 1219 bytes result sent to driver
15/08/21 13:50:52 INFO TaskSetManager: Starting task 176.0 in stage 5.0 (TID 739, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:52 INFO Executor: Running task 176.0 in stage 5.0 (TID 739)
15/08/21 13:50:52 INFO TaskSetManager: Finished task 162.0 in stage 5.0 (TID 725) in 11148 ms on localhost (161/200)
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:53 INFO Executor: Finished task 161.0 in stage 5.0 (TID 724). 1219 bytes result sent to driver
15/08/21 13:50:53 INFO TaskSetManager: Starting task 177.0 in stage 5.0 (TID 740, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:53 INFO Executor: Running task 177.0 in stage 5.0 (TID 740)
15/08/21 13:50:53 INFO TaskSetManager: Finished task 161.0 in stage 5.0 (TID 724) in 11303 ms on localhost (162/200)
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:53 INFO Executor: Finished task 160.0 in stage 5.0 (TID 723). 1219 bytes result sent to driver
15/08/21 13:50:53 INFO TaskSetManager: Starting task 178.0 in stage 5.0 (TID 741, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:53 INFO Executor: Running task 178.0 in stage 5.0 (TID 741)
15/08/21 13:50:53 INFO TaskSetManager: Finished task 160.0 in stage 5.0 (TID 723) in 12068 ms on localhost (163/200)
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:53 INFO Executor: Finished task 166.0 in stage 5.0 (TID 729). 1219 bytes result sent to driver
15/08/21 13:50:53 INFO TaskSetManager: Starting task 179.0 in stage 5.0 (TID 742, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:53 INFO TaskSetManager: Finished task 166.0 in stage 5.0 (TID 729) in 11047 ms on localhost (164/200)
15/08/21 13:50:53 INFO Executor: Running task 179.0 in stage 5.0 (TID 742)
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:54 INFO Executor: Finished task 164.0 in stage 5.0 (TID 727). 1219 bytes result sent to driver
15/08/21 13:50:54 INFO TaskSetManager: Starting task 180.0 in stage 5.0 (TID 743, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:54 INFO Executor: Running task 180.0 in stage 5.0 (TID 743)
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:54 INFO TaskSetManager: Finished task 164.0 in stage 5.0 (TID 727) in 11529 ms on localhost (165/200)
15/08/21 13:50:54 INFO Executor: Finished task 163.0 in stage 5.0 (TID 726). 1219 bytes result sent to driver
15/08/21 13:50:54 INFO TaskSetManager: Starting task 181.0 in stage 5.0 (TID 744, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:54 INFO Executor: Running task 181.0 in stage 5.0 (TID 744)
15/08/21 13:50:54 INFO Executor: Finished task 165.0 in stage 5.0 (TID 728). 1219 bytes result sent to driver
15/08/21 13:50:54 INFO TaskSetManager: Finished task 163.0 in stage 5.0 (TID 726) in 11956 ms on localhost (166/200)
15/08/21 13:50:54 INFO TaskSetManager: Starting task 182.0 in stage 5.0 (TID 745, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:54 INFO Executor: Running task 182.0 in stage 5.0 (TID 745)
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:54 INFO TaskSetManager: Finished task 165.0 in stage 5.0 (TID 728) in 11415 ms on localhost (167/200)
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:50:56 INFO Executor: Finished task 167.0 in stage 5.0 (TID 730). 1219 bytes result sent to driver
15/08/21 13:50:56 INFO TaskSetManager: Starting task 183.0 in stage 5.0 (TID 746, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:56 INFO Executor: Running task 183.0 in stage 5.0 (TID 746)
15/08/21 13:50:56 INFO TaskSetManager: Finished task 167.0 in stage 5.0 (TID 730) in 8925 ms on localhost (168/200)
15/08/21 13:50:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:56 INFO Executor: Finished task 168.0 in stage 5.0 (TID 731). 1219 bytes result sent to driver
15/08/21 13:50:56 INFO TaskSetManager: Starting task 184.0 in stage 5.0 (TID 747, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:56 INFO Executor: Running task 184.0 in stage 5.0 (TID 747)
15/08/21 13:50:56 INFO TaskSetManager: Finished task 168.0 in stage 5.0 (TID 731) in 8740 ms on localhost (169/200)
15/08/21 13:50:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:59 INFO Executor: Finished task 169.0 in stage 5.0 (TID 732). 1219 bytes result sent to driver
15/08/21 13:50:59 INFO TaskSetManager: Starting task 185.0 in stage 5.0 (TID 748, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:59 INFO Executor: Running task 185.0 in stage 5.0 (TID 748)
15/08/21 13:50:59 INFO TaskSetManager: Finished task 169.0 in stage 5.0 (TID 732) in 9117 ms on localhost (170/200)
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:59 INFO Executor: Finished task 170.0 in stage 5.0 (TID 733). 1219 bytes result sent to driver
15/08/21 13:50:59 INFO TaskSetManager: Starting task 186.0 in stage 5.0 (TID 749, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:59 INFO Executor: Running task 186.0 in stage 5.0 (TID 749)
15/08/21 13:50:59 INFO TaskSetManager: Finished task 170.0 in stage 5.0 (TID 733) in 9307 ms on localhost (171/200)
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:50:59 INFO Executor: Finished task 171.0 in stage 5.0 (TID 734). 1219 bytes result sent to driver
15/08/21 13:50:59 INFO TaskSetManager: Starting task 187.0 in stage 5.0 (TID 750, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:50:59 INFO Executor: Running task 187.0 in stage 5.0 (TID 750)
15/08/21 13:50:59 INFO TaskSetManager: Finished task 171.0 in stage 5.0 (TID 734) in 8252 ms on localhost (172/200)
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:00 INFO Executor: Finished task 172.0 in stage 5.0 (TID 735). 1219 bytes result sent to driver
15/08/21 13:51:00 INFO TaskSetManager: Starting task 188.0 in stage 5.0 (TID 751, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:00 INFO Executor: Running task 188.0 in stage 5.0 (TID 751)
15/08/21 13:51:00 INFO TaskSetManager: Finished task 172.0 in stage 5.0 (TID 735) in 8397 ms on localhost (173/200)
15/08/21 13:51:00 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:00 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:00 INFO Executor: Finished task 173.0 in stage 5.0 (TID 736). 1219 bytes result sent to driver
15/08/21 13:51:00 INFO TaskSetManager: Starting task 189.0 in stage 5.0 (TID 752, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:00 INFO TaskSetManager: Finished task 173.0 in stage 5.0 (TID 736) in 8487 ms on localhost (174/200)
15/08/21 13:51:00 INFO Executor: Running task 189.0 in stage 5.0 (TID 752)
15/08/21 13:51:00 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/21 13:51:00 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:02 INFO Executor: Finished task 174.0 in stage 5.0 (TID 737). 1219 bytes result sent to driver
15/08/21 13:51:02 INFO TaskSetManager: Starting task 190.0 in stage 5.0 (TID 753, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:02 INFO Executor: Running task 190.0 in stage 5.0 (TID 753)
15/08/21 13:51:02 INFO TaskSetManager: Finished task 174.0 in stage 5.0 (TID 737) in 10474 ms on localhost (175/200)
15/08/21 13:51:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:51:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:03 INFO Executor: Finished task 178.0 in stage 5.0 (TID 741). 1219 bytes result sent to driver
15/08/21 13:51:03 INFO TaskSetManager: Starting task 191.0 in stage 5.0 (TID 754, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:03 INFO Executor: Running task 191.0 in stage 5.0 (TID 754)
15/08/21 13:51:03 INFO TaskSetManager: Finished task 178.0 in stage 5.0 (TID 741) in 9835 ms on localhost (176/200)
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:03 INFO Executor: Finished task 179.0 in stage 5.0 (TID 742). 1219 bytes result sent to driver
15/08/21 13:51:03 INFO TaskSetManager: Starting task 192.0 in stage 5.0 (TID 755, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:03 INFO Executor: Running task 192.0 in stage 5.0 (TID 755)
15/08/21 13:51:03 INFO TaskSetManager: Finished task 179.0 in stage 5.0 (TID 742) in 9407 ms on localhost (177/200)
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:03 INFO Executor: Finished task 176.0 in stage 5.0 (TID 739). 1219 bytes result sent to driver
15/08/21 13:51:03 INFO TaskSetManager: Starting task 193.0 in stage 5.0 (TID 756, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:03 INFO Executor: Running task 193.0 in stage 5.0 (TID 756)
15/08/21 13:51:03 INFO TaskSetManager: Finished task 176.0 in stage 5.0 (TID 739) in 10347 ms on localhost (178/200)
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:51:03 INFO Executor: Finished task 175.0 in stage 5.0 (TID 738). 1219 bytes result sent to driver
15/08/21 13:51:03 INFO TaskSetManager: Starting task 194.0 in stage 5.0 (TID 757, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:03 INFO Executor: Running task 194.0 in stage 5.0 (TID 757)
15/08/21 13:51:03 INFO TaskSetManager: Finished task 175.0 in stage 5.0 (TID 738) in 11177 ms on localhost (179/200)
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:03 INFO Executor: Finished task 177.0 in stage 5.0 (TID 740). 1219 bytes result sent to driver
15/08/21 13:51:03 INFO TaskSetManager: Starting task 195.0 in stage 5.0 (TID 758, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:03 INFO Executor: Running task 195.0 in stage 5.0 (TID 758)
15/08/21 13:51:03 INFO TaskSetManager: Finished task 177.0 in stage 5.0 (TID 740) in 10904 ms on localhost (180/200)
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/21 13:51:04 INFO Executor: Finished task 181.0 in stage 5.0 (TID 744). 1219 bytes result sent to driver
15/08/21 13:51:04 INFO TaskSetManager: Starting task 196.0 in stage 5.0 (TID 759, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:04 INFO Executor: Running task 196.0 in stage 5.0 (TID 759)
15/08/21 13:51:04 INFO TaskSetManager: Finished task 181.0 in stage 5.0 (TID 744) in 10133 ms on localhost (181/200)
15/08/21 13:51:04 INFO Executor: Finished task 182.0 in stage 5.0 (TID 745). 1219 bytes result sent to driver
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 13:51:04 INFO Executor: Finished task 180.0 in stage 5.0 (TID 743). 1219 bytes result sent to driver
15/08/21 13:51:04 INFO TaskSetManager: Starting task 197.0 in stage 5.0 (TID 760, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:04 INFO TaskSetManager: Starting task 198.0 in stage 5.0 (TID 761, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:04 INFO TaskSetManager: Finished task 182.0 in stage 5.0 (TID 745) in 10153 ms on localhost (182/200)
15/08/21 13:51:04 INFO TaskSetManager: Finished task 180.0 in stage 5.0 (TID 743) in 10221 ms on localhost (183/200)
15/08/21 13:51:04 INFO Executor: Running task 197.0 in stage 5.0 (TID 760)
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:04 INFO Executor: Running task 198.0 in stage 5.0 (TID 761)
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:51:07 INFO Executor: Finished task 183.0 in stage 5.0 (TID 746). 1219 bytes result sent to driver
15/08/21 13:51:07 INFO TaskSetManager: Starting task 199.0 in stage 5.0 (TID 762, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 13:51:07 INFO Executor: Running task 199.0 in stage 5.0 (TID 762)
15/08/21 13:51:07 INFO TaskSetManager: Finished task 183.0 in stage 5.0 (TID 746) in 10994 ms on localhost (184/200)
15/08/21 13:51:07 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 13:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:51:07 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 13:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:51:08 INFO Executor: Finished task 184.0 in stage 5.0 (TID 747). 1219 bytes result sent to driver
15/08/21 13:51:08 INFO TaskSetManager: Starting task 116.0 in stage 6.0 (TID 763, localhost, ANY, 1693 bytes)
15/08/21 13:51:08 INFO Executor: Running task 116.0 in stage 6.0 (TID 763)
15/08/21 13:51:08 INFO TaskSetManager: Finished task 184.0 in stage 5.0 (TID 747) in 11987 ms on localhost (185/200)
15/08/21 13:51:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00033-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498478 length: 3498478 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:09 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 750000
15/08/21 13:51:09 INFO Executor: Finished task 116.0 in stage 6.0 (TID 763). 2125 bytes result sent to driver
15/08/21 13:51:09 INFO TaskSetManager: Starting task 117.0 in stage 6.0 (TID 764, localhost, ANY, 1693 bytes)
15/08/21 13:51:09 INFO Executor: Running task 117.0 in stage 6.0 (TID 764)
15/08/21 13:51:09 INFO TaskSetManager: Finished task 116.0 in stage 6.0 (TID 763) in 637 ms on localhost (117/200)
15/08/21 13:51:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00110-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507283 length: 3507283 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:09 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 13:51:09 INFO Executor: Finished task 185.0 in stage 5.0 (TID 748). 1219 bytes result sent to driver
15/08/21 13:51:09 INFO Executor: Finished task 117.0 in stage 6.0 (TID 764). 2125 bytes result sent to driver
15/08/21 13:51:09 INFO TaskSetManager: Starting task 118.0 in stage 6.0 (TID 765, localhost, ANY, 1692 bytes)
15/08/21 13:51:09 INFO Executor: Running task 118.0 in stage 6.0 (TID 765)
15/08/21 13:51:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00162-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498776 length: 3498776 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:09 INFO TaskSetManager: Starting task 119.0 in stage 6.0 (TID 766, localhost, ANY, 1691 bytes)
15/08/21 13:51:09 INFO Executor: Running task 119.0 in stage 6.0 (TID 766)
15/08/21 13:51:09 INFO TaskSetManager: Finished task 185.0 in stage 5.0 (TID 748) in 10807 ms on localhost (186/200)
15/08/21 13:51:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:09 INFO TaskSetManager: Finished task 117.0 in stage 6.0 (TID 764) in 477 ms on localhost (118/200)
15/08/21 13:51:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00112-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506027 length: 3506027 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:10 INFO InternalParquetRecordReader: block read in memory in 262 ms. row count = 750000
15/08/21 13:51:10 INFO InternalParquetRecordReader: block read in memory in 275 ms. row count = 750000
15/08/21 13:51:10 INFO Executor: Finished task 187.0 in stage 5.0 (TID 750). 1219 bytes result sent to driver
15/08/21 13:51:10 INFO TaskSetManager: Starting task 120.0 in stage 6.0 (TID 767, localhost, ANY, 1691 bytes)
15/08/21 13:51:10 INFO TaskSetManager: Finished task 187.0 in stage 5.0 (TID 750) in 10705 ms on localhost (187/200)
15/08/21 13:51:10 INFO Executor: Finished task 186.0 in stage 5.0 (TID 749). 1219 bytes result sent to driver
15/08/21 13:51:10 INFO Executor: Running task 120.0 in stage 6.0 (TID 767)
15/08/21 13:51:10 INFO TaskSetManager: Starting task 121.0 in stage 6.0 (TID 768, localhost, ANY, 1692 bytes)
15/08/21 13:51:10 INFO Executor: Running task 121.0 in stage 6.0 (TID 768)
15/08/21 13:51:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00190-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505708 length: 3505708 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:10 INFO TaskSetManager: Finished task 186.0 in stage 5.0 (TID 749) in 10906 ms on localhost (188/200)
15/08/21 13:51:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00021-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506265 length: 3506265 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:10 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 750000
15/08/21 13:51:10 INFO InternalParquetRecordReader: block read in memory in 172 ms. row count = 750000
15/08/21 13:51:10 INFO Executor: Finished task 119.0 in stage 6.0 (TID 766). 2125 bytes result sent to driver
15/08/21 13:51:10 INFO TaskSetManager: Starting task 122.0 in stage 6.0 (TID 769, localhost, ANY, 1694 bytes)
15/08/21 13:51:10 INFO Executor: Running task 122.0 in stage 6.0 (TID 769)
15/08/21 13:51:10 INFO TaskSetManager: Finished task 119.0 in stage 6.0 (TID 766) in 961 ms on localhost (119/200)
15/08/21 13:51:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00157-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506076 length: 3506076 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:10 INFO Executor: Finished task 121.0 in stage 6.0 (TID 768). 2125 bytes result sent to driver
15/08/21 13:51:10 INFO TaskSetManager: Starting task 123.0 in stage 6.0 (TID 770, localhost, ANY, 1693 bytes)
15/08/21 13:51:10 INFO Executor: Running task 123.0 in stage 6.0 (TID 770)
15/08/21 13:51:10 INFO TaskSetManager: Finished task 121.0 in stage 6.0 (TID 768) in 652 ms on localhost (120/200)
15/08/21 13:51:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00093-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507082 length: 3507082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:10 INFO Executor: Finished task 118.0 in stage 6.0 (TID 765). 2125 bytes result sent to driver
15/08/21 13:51:10 INFO TaskSetManager: Starting task 124.0 in stage 6.0 (TID 771, localhost, ANY, 1691 bytes)
15/08/21 13:51:10 INFO Executor: Running task 124.0 in stage 6.0 (TID 771)
15/08/21 13:51:10 INFO TaskSetManager: Finished task 118.0 in stage 6.0 (TID 765) in 1078 ms on localhost (121/200)
15/08/21 13:51:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00158-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502636 length: 3502636 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:11 INFO InternalParquetRecordReader: block read in memory in 62 ms. row count = 750000
15/08/21 13:51:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:11 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 750000
15/08/21 13:51:11 INFO Executor: Finished task 120.0 in stage 6.0 (TID 767). 2125 bytes result sent to driver
15/08/21 13:51:11 INFO InternalParquetRecordReader: block read in memory in 148 ms. row count = 750000
15/08/21 13:51:11 INFO TaskSetManager: Starting task 125.0 in stage 6.0 (TID 772, localhost, ANY, 1693 bytes)
15/08/21 13:51:11 INFO Executor: Running task 125.0 in stage 6.0 (TID 772)
15/08/21 13:51:11 INFO TaskSetManager: Finished task 120.0 in stage 6.0 (TID 767) in 904 ms on localhost (122/200)
15/08/21 13:51:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00055-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505938 length: 3505938 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:11 INFO Executor: Finished task 188.0 in stage 5.0 (TID 751). 1219 bytes result sent to driver
15/08/21 13:51:11 INFO TaskSetManager: Starting task 126.0 in stage 6.0 (TID 773, localhost, ANY, 1693 bytes)
15/08/21 13:51:11 INFO Executor: Running task 126.0 in stage 6.0 (TID 773)
15/08/21 13:51:11 INFO TaskSetManager: Finished task 188.0 in stage 5.0 (TID 751) in 10688 ms on localhost (189/200)
15/08/21 13:51:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00197-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504816 length: 3504816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:11 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 13:51:11 INFO InternalParquetRecordReader: block read in memory in 454 ms. row count = 750000
15/08/21 13:51:12 INFO Executor: Finished task 124.0 in stage 6.0 (TID 771). 2125 bytes result sent to driver
15/08/21 13:51:12 INFO TaskSetManager: Starting task 127.0 in stage 6.0 (TID 774, localhost, ANY, 1689 bytes)
15/08/21 13:51:12 INFO Executor: Running task 127.0 in stage 6.0 (TID 774)
15/08/21 13:51:12 INFO TaskSetManager: Finished task 124.0 in stage 6.0 (TID 771) in 1118 ms on localhost (123/200)
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00020-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501937 length: 3501937 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:12 INFO Executor: Finished task 122.0 in stage 6.0 (TID 769). 2125 bytes result sent to driver
15/08/21 13:51:12 INFO Executor: Finished task 123.0 in stage 6.0 (TID 770). 2125 bytes result sent to driver
15/08/21 13:51:12 INFO TaskSetManager: Starting task 128.0 in stage 6.0 (TID 775, localhost, ANY, 1692 bytes)
15/08/21 13:51:12 INFO TaskSetManager: Starting task 129.0 in stage 6.0 (TID 776, localhost, ANY, 1694 bytes)
15/08/21 13:51:12 INFO Executor: Running task 128.0 in stage 6.0 (TID 775)
15/08/21 13:51:12 INFO Executor: Running task 129.0 in stage 6.0 (TID 776)
15/08/21 13:51:12 INFO TaskSetManager: Finished task 122.0 in stage 6.0 (TID 769) in 1263 ms on localhost (124/200)
15/08/21 13:51:12 INFO TaskSetManager: Finished task 123.0 in stage 6.0 (TID 770) in 1200 ms on localhost (125/200)
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00144-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507412 length: 3507412 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00005-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506506 length: 3506506 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:12 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 750000
15/08/21 13:51:12 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 750000
15/08/21 13:51:12 INFO InternalParquetRecordReader: block read in memory in 140 ms. row count = 750000
15/08/21 13:51:12 INFO Executor: Finished task 125.0 in stage 6.0 (TID 772). 2125 bytes result sent to driver
15/08/21 13:51:12 INFO TaskSetManager: Starting task 130.0 in stage 6.0 (TID 777, localhost, ANY, 1690 bytes)
15/08/21 13:51:12 INFO Executor: Running task 130.0 in stage 6.0 (TID 777)
15/08/21 13:51:12 INFO TaskSetManager: Finished task 125.0 in stage 6.0 (TID 772) in 1355 ms on localhost (126/200)
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00060-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3503479 length: 3503479 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:12 INFO Executor: Finished task 126.0 in stage 6.0 (TID 773). 2125 bytes result sent to driver
15/08/21 13:51:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:12 INFO TaskSetManager: Starting task 131.0 in stage 6.0 (TID 778, localhost, ANY, 1692 bytes)
15/08/21 13:51:12 INFO Executor: Running task 131.0 in stage 6.0 (TID 778)
15/08/21 13:51:12 INFO TaskSetManager: Finished task 126.0 in stage 6.0 (TID 773) in 1295 ms on localhost (127/200)
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00145-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498381 length: 3498381 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:12 INFO InternalParquetRecordReader: block read in memory in 177 ms. row count = 750000
15/08/21 13:51:12 INFO InternalParquetRecordReader: block read in memory in 123 ms. row count = 750000
15/08/21 13:51:12 INFO Executor: Finished task 189.0 in stage 5.0 (TID 752). 1219 bytes result sent to driver
15/08/21 13:51:12 INFO TaskSetManager: Starting task 132.0 in stage 6.0 (TID 779, localhost, ANY, 1691 bytes)
15/08/21 13:51:12 INFO Executor: Running task 132.0 in stage 6.0 (TID 779)
15/08/21 13:51:12 INFO TaskSetManager: Finished task 189.0 in stage 5.0 (TID 752) in 11958 ms on localhost (190/200)
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00076-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500080 length: 3500080 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 INFO Executor: Finished task 127.0 in stage 6.0 (TID 774). 2125 bytes result sent to driver
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:12 INFO TaskSetManager: Starting task 133.0 in stage 6.0 (TID 780, localhost, ANY, 1692 bytes)
15/08/21 13:51:12 INFO Executor: Running task 133.0 in stage 6.0 (TID 780)
15/08/21 13:51:12 INFO TaskSetManager: Finished task 127.0 in stage 6.0 (TID 774) in 809 ms on localhost (128/200)
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00052-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501586 length: 3501586 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:12 INFO Executor: Finished task 129.0 in stage 6.0 (TID 776). 2125 bytes result sent to driver
15/08/21 13:51:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:12 INFO TaskSetManager: Starting task 134.0 in stage 6.0 (TID 781, localhost, ANY, 1694 bytes)
15/08/21 13:51:12 INFO Executor: Running task 134.0 in stage 6.0 (TID 781)
15/08/21 13:51:12 INFO TaskSetManager: Finished task 129.0 in stage 6.0 (TID 776) in 793 ms on localhost (129/200)
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00192-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507651 length: 3507651 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:12 INFO Executor: Finished task 128.0 in stage 6.0 (TID 775). 2125 bytes result sent to driver
15/08/21 13:51:12 INFO TaskSetManager: Starting task 135.0 in stage 6.0 (TID 782, localhost, ANY, 1692 bytes)
15/08/21 13:51:12 INFO Executor: Running task 135.0 in stage 6.0 (TID 782)
15/08/21 13:51:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00087-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506179 length: 3506179 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:12 INFO TaskSetManager: Finished task 128.0 in stage 6.0 (TID 775) in 855 ms on localhost (130/200)
15/08/21 13:51:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 750000
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 107 ms. row count = 750000
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 104 ms. row count = 750000
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 83 ms. row count = 750000
15/08/21 13:51:13 INFO Executor: Finished task 130.0 in stage 6.0 (TID 777). 2125 bytes result sent to driver
15/08/21 13:51:13 INFO Executor: Finished task 131.0 in stage 6.0 (TID 778). 2125 bytes result sent to driver
15/08/21 13:51:13 INFO TaskSetManager: Starting task 136.0 in stage 6.0 (TID 783, localhost, ANY, 1692 bytes)
15/08/21 13:51:13 INFO Executor: Running task 136.0 in stage 6.0 (TID 783)
15/08/21 13:51:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00105-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500366 length: 3500366 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:13 INFO TaskSetManager: Starting task 137.0 in stage 6.0 (TID 784, localhost, ANY, 1692 bytes)
15/08/21 13:51:13 INFO Executor: Running task 137.0 in stage 6.0 (TID 784)
15/08/21 13:51:13 INFO TaskSetManager: Finished task 130.0 in stage 6.0 (TID 777) in 830 ms on localhost (131/200)
15/08/21 13:51:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO TaskSetManager: Finished task 131.0 in stage 6.0 (TID 778) in 780 ms on localhost (132/200)
15/08/21 13:51:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00079-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505149 length: 3505149 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 750000
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 750000
15/08/21 13:51:13 INFO Executor: Finished task 190.0 in stage 5.0 (TID 753). 1219 bytes result sent to driver
15/08/21 13:51:13 INFO TaskSetManager: Starting task 138.0 in stage 6.0 (TID 785, localhost, ANY, 1691 bytes)
15/08/21 13:51:13 INFO Executor: Running task 138.0 in stage 6.0 (TID 785)
15/08/21 13:51:13 INFO TaskSetManager: Finished task 190.0 in stage 5.0 (TID 753) in 10578 ms on localhost (191/200)
15/08/21 13:51:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00163-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499870 length: 3499870 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 750000
15/08/21 13:51:13 INFO Executor: Finished task 132.0 in stage 6.0 (TID 779). 2125 bytes result sent to driver
15/08/21 13:51:13 INFO TaskSetManager: Starting task 139.0 in stage 6.0 (TID 786, localhost, ANY, 1690 bytes)
15/08/21 13:51:13 INFO Executor: Running task 139.0 in stage 6.0 (TID 786)
15/08/21 13:51:13 INFO TaskSetManager: Finished task 132.0 in stage 6.0 (TID 779) in 818 ms on localhost (133/200)
15/08/21 13:51:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00028-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501169 length: 3501169 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 13:51:13 INFO Executor: Finished task 136.0 in stage 6.0 (TID 783). 2125 bytes result sent to driver
15/08/21 13:51:13 INFO TaskSetManager: Starting task 140.0 in stage 6.0 (TID 787, localhost, ANY, 1692 bytes)
15/08/21 13:51:13 INFO Executor: Running task 140.0 in stage 6.0 (TID 787)
15/08/21 13:51:13 INFO TaskSetManager: Finished task 136.0 in stage 6.0 (TID 783) in 475 ms on localhost (134/200)
15/08/21 13:51:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00011-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3498475 length: 3498475 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:13 INFO Executor: Finished task 133.0 in stage 6.0 (TID 780). 2125 bytes result sent to driver
15/08/21 13:51:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO TaskSetManager: Starting task 141.0 in stage 6.0 (TID 788, localhost, ANY, 1691 bytes)
15/08/21 13:51:13 INFO Executor: Running task 141.0 in stage 6.0 (TID 788)
15/08/21 13:51:13 INFO TaskSetManager: Finished task 133.0 in stage 6.0 (TID 780) in 938 ms on localhost (135/200)
15/08/21 13:51:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00035-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499639 length: 3499639 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO Executor: Finished task 134.0 in stage 6.0 (TID 781). 2125 bytes result sent to driver
15/08/21 13:51:13 INFO TaskSetManager: Starting task 142.0 in stage 6.0 (TID 789, localhost, ANY, 1692 bytes)
15/08/21 13:51:13 INFO Executor: Running task 142.0 in stage 6.0 (TID 789)
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 INFO TaskSetManager: Finished task 134.0 in stage 6.0 (TID 781) in 930 ms on localhost (136/200)
15/08/21 13:51:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00115-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501230 length: 3501230 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO Executor: Finished task 135.0 in stage 6.0 (TID 782). 2125 bytes result sent to driver
15/08/21 13:51:13 INFO TaskSetManager: Starting task 143.0 in stage 6.0 (TID 790, localhost, ANY, 1693 bytes)
15/08/21 13:51:13 INFO TaskSetManager: Finished task 135.0 in stage 6.0 (TID 782) in 914 ms on localhost (137/200)
15/08/21 13:51:13 INFO Executor: Running task 143.0 in stage 6.0 (TID 790)
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00193-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500019 length: 3500019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 750000
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 117 ms. row count = 750000
15/08/21 13:51:13 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 750000
15/08/21 13:51:14 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 750000
15/08/21 13:51:14 INFO Executor: Finished task 137.0 in stage 6.0 (TID 784). 2125 bytes result sent to driver
15/08/21 13:51:14 INFO TaskSetManager: Starting task 144.0 in stage 6.0 (TID 791, localhost, ANY, 1692 bytes)
15/08/21 13:51:14 INFO Executor: Running task 144.0 in stage 6.0 (TID 791)
15/08/21 13:51:14 INFO TaskSetManager: Finished task 137.0 in stage 6.0 (TID 784) in 900 ms on localhost (138/200)
15/08/21 13:51:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00160-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3503635 length: 3503635 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:14 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 750000
15/08/21 13:51:14 INFO Executor: Finished task 138.0 in stage 6.0 (TID 785). 2125 bytes result sent to driver
15/08/21 13:51:14 INFO TaskSetManager: Starting task 145.0 in stage 6.0 (TID 792, localhost, ANY, 1692 bytes)
15/08/21 13:51:14 INFO Executor: Running task 145.0 in stage 6.0 (TID 792)
15/08/21 13:51:14 INFO TaskSetManager: Finished task 138.0 in stage 6.0 (TID 785) in 992 ms on localhost (139/200)
15/08/21 13:51:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00120-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505053 length: 3505053 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:14 INFO Executor: Finished task 192.0 in stage 5.0 (TID 755). 1219 bytes result sent to driver
15/08/21 13:51:14 INFO TaskSetManager: Starting task 146.0 in stage 6.0 (TID 793, localhost, ANY, 1693 bytes)
15/08/21 13:51:14 INFO Executor: Running task 146.0 in stage 6.0 (TID 793)
15/08/21 13:51:14 INFO TaskSetManager: Finished task 192.0 in stage 5.0 (TID 755) in 11463 ms on localhost (192/200)
15/08/21 13:51:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00037-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505909 length: 3505909 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:14 INFO Executor: Finished task 139.0 in stage 6.0 (TID 786). 2125 bytes result sent to driver
15/08/21 13:51:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:14 INFO InternalParquetRecordReader: block read in memory in 82 ms. row count = 750000
15/08/21 13:51:14 INFO TaskSetManager: Starting task 147.0 in stage 6.0 (TID 794, localhost, ANY, 1692 bytes)
15/08/21 13:51:14 INFO Executor: Running task 147.0 in stage 6.0 (TID 794)
15/08/21 13:51:14 INFO TaskSetManager: Finished task 139.0 in stage 6.0 (TID 786) in 963 ms on localhost (140/200)
15/08/21 13:51:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00014-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505682 length: 3505682 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:14 INFO InternalParquetRecordReader: block read in memory in 89 ms. row count = 750000
15/08/21 13:51:14 INFO Executor: Finished task 141.0 in stage 6.0 (TID 788). 2125 bytes result sent to driver
15/08/21 13:51:14 INFO Executor: Finished task 140.0 in stage 6.0 (TID 787). 2125 bytes result sent to driver
15/08/21 13:51:14 INFO TaskSetManager: Starting task 148.0 in stage 6.0 (TID 795, localhost, ANY, 1691 bytes)
15/08/21 13:51:14 INFO Executor: Running task 148.0 in stage 6.0 (TID 795)
15/08/21 13:51:14 INFO TaskSetManager: Starting task 149.0 in stage 6.0 (TID 796, localhost, ANY, 1691 bytes)
15/08/21 13:51:14 INFO TaskSetManager: Finished task 141.0 in stage 6.0 (TID 788) in 945 ms on localhost (141/200)
15/08/21 13:51:14 INFO TaskSetManager: Finished task 140.0 in stage 6.0 (TID 787) in 959 ms on localhost (142/200)
15/08/21 13:51:14 INFO Executor: Running task 149.0 in stage 6.0 (TID 796)
15/08/21 13:51:14 INFO InternalParquetRecordReader: block read in memory in 87 ms. row count = 750000
15/08/21 13:51:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00101-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3509794 length: 3509794 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00071-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505003 length: 3505003 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:14 INFO Executor: Finished task 143.0 in stage 6.0 (TID 790). 2125 bytes result sent to driver
15/08/21 13:51:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:14 INFO TaskSetManager: Starting task 150.0 in stage 6.0 (TID 797, localhost, ANY, 1692 bytes)
15/08/21 13:51:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:14 INFO Executor: Running task 150.0 in stage 6.0 (TID 797)
15/08/21 13:51:14 INFO TaskSetManager: Finished task 143.0 in stage 6.0 (TID 790) in 888 ms on localhost (143/200)
15/08/21 13:51:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00151-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3503903 length: 3503903 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:14 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 750000
15/08/21 13:51:14 INFO Executor: Finished task 142.0 in stage 6.0 (TID 789). 2125 bytes result sent to driver
15/08/21 13:51:14 INFO InternalParquetRecordReader: block read in memory in 93 ms. row count = 750000
15/08/21 13:51:14 INFO TaskSetManager: Starting task 151.0 in stage 6.0 (TID 798, localhost, ANY, 1692 bytes)
15/08/21 13:51:14 INFO Executor: Running task 151.0 in stage 6.0 (TID 798)
15/08/21 13:51:14 INFO TaskSetManager: Finished task 142.0 in stage 6.0 (TID 789) in 1046 ms on localhost (144/200)
15/08/21 13:51:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00077-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505733 length: 3505733 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:14 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 750000
15/08/21 13:51:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:14 INFO Executor: Finished task 144.0 in stage 6.0 (TID 791). 2125 bytes result sent to driver
15/08/21 13:51:14 INFO TaskSetManager: Starting task 152.0 in stage 6.0 (TID 799, localhost, ANY, 1691 bytes)
15/08/21 13:51:14 INFO Executor: Running task 152.0 in stage 6.0 (TID 799)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 144.0 in stage 6.0 (TID 791) in 801 ms on localhost (145/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00063-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3503402 length: 3503402 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 750000
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO Executor: Finished task 191.0 in stage 5.0 (TID 754). 1219 bytes result sent to driver
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO TaskSetManager: Starting task 153.0 in stage 6.0 (TID 800, localhost, ANY, 1693 bytes)
15/08/21 13:51:15 INFO Executor: Running task 153.0 in stage 6.0 (TID 800)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 191.0 in stage 5.0 (TID 754) in 11984 ms on localhost (193/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00142-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505692 length: 3505692 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 750000
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 750000
15/08/21 13:51:15 INFO Executor: Finished task 145.0 in stage 6.0 (TID 792). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO TaskSetManager: Starting task 154.0 in stage 6.0 (TID 801, localhost, ANY, 1693 bytes)
15/08/21 13:51:15 INFO Executor: Running task 154.0 in stage 6.0 (TID 801)
15/08/21 13:51:15 INFO Executor: Finished task 194.0 in stage 5.0 (TID 757). 1219 bytes result sent to driver
15/08/21 13:51:15 INFO TaskSetManager: Starting task 155.0 in stage 6.0 (TID 802, localhost, ANY, 1693 bytes)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 145.0 in stage 6.0 (TID 792) in 712 ms on localhost (146/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00168-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505102 length: 3505102 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 INFO Executor: Running task 155.0 in stage 6.0 (TID 802)
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO TaskSetManager: Finished task 194.0 in stage 5.0 (TID 757) in 11333 ms on localhost (194/200)
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00135-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505648 length: 3505648 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 750000
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 47 ms. row count = 750000
15/08/21 13:51:15 INFO Executor: Finished task 146.0 in stage 6.0 (TID 793). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO Executor: Finished task 147.0 in stage 6.0 (TID 794). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO TaskSetManager: Starting task 156.0 in stage 6.0 (TID 803, localhost, ANY, 1691 bytes)
15/08/21 13:51:15 INFO Executor: Running task 156.0 in stage 6.0 (TID 803)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00085-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504735 length: 3504735 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO TaskSetManager: Starting task 157.0 in stage 6.0 (TID 804, localhost, ANY, 1691 bytes)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 146.0 in stage 6.0 (TID 793) in 919 ms on localhost (147/200)
15/08/21 13:51:15 INFO Executor: Running task 157.0 in stage 6.0 (TID 804)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 147.0 in stage 6.0 (TID 794) in 894 ms on localhost (148/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00041-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500431 length: 3500431 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 750000
15/08/21 13:51:15 INFO Executor: Finished task 148.0 in stage 6.0 (TID 795). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO TaskSetManager: Starting task 158.0 in stage 6.0 (TID 805, localhost, ANY, 1692 bytes)
15/08/21 13:51:15 INFO Executor: Running task 158.0 in stage 6.0 (TID 805)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 148.0 in stage 6.0 (TID 795) in 947 ms on localhost (149/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00126-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506809 length: 3506809 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO Executor: Finished task 149.0 in stage 6.0 (TID 796). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO TaskSetManager: Starting task 159.0 in stage 6.0 (TID 806, localhost, ANY, 1692 bytes)
15/08/21 13:51:15 INFO Executor: Running task 159.0 in stage 6.0 (TID 806)
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO TaskSetManager: Finished task 149.0 in stage 6.0 (TID 796) in 966 ms on localhost (150/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00027-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501052 length: 3501052 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO Executor: Finished task 150.0 in stage 6.0 (TID 797). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO TaskSetManager: Starting task 160.0 in stage 6.0 (TID 807, localhost, ANY, 1692 bytes)
15/08/21 13:51:15 INFO Executor: Running task 160.0 in stage 6.0 (TID 807)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 150.0 in stage 6.0 (TID 797) in 1029 ms on localhost (151/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00131-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501547 length: 3501547 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 750000
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO Executor: Finished task 151.0 in stage 6.0 (TID 798). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 125 ms. row count = 750000
15/08/21 13:51:15 INFO TaskSetManager: Starting task 161.0 in stage 6.0 (TID 808, localhost, ANY, 1693 bytes)
15/08/21 13:51:15 INFO Executor: Running task 161.0 in stage 6.0 (TID 808)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 151.0 in stage 6.0 (TID 798) in 971 ms on localhost (152/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00015-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505595 length: 3505595 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO Executor: Finished task 153.0 in stage 6.0 (TID 800). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 750000
15/08/21 13:51:15 INFO TaskSetManager: Starting task 162.0 in stage 6.0 (TID 809, localhost, ANY, 1694 bytes)
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO TaskSetManager: Finished task 153.0 in stage 6.0 (TID 800) in 848 ms on localhost (153/200)
15/08/21 13:51:15 INFO Executor: Running task 162.0 in stage 6.0 (TID 809)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00175-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504840 length: 3504840 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO Executor: Finished task 152.0 in stage 6.0 (TID 799). 2125 bytes result sent to driver
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO TaskSetManager: Starting task 163.0 in stage 6.0 (TID 810, localhost, ANY, 1690 bytes)
15/08/21 13:51:15 INFO Executor: Running task 163.0 in stage 6.0 (TID 810)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 152.0 in stage 6.0 (TID 799) in 913 ms on localhost (154/200)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00018-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500127 length: 3500127 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO Executor: Finished task 197.0 in stage 5.0 (TID 760). 1219 bytes result sent to driver
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 750000
15/08/21 13:51:15 INFO TaskSetManager: Starting task 164.0 in stage 6.0 (TID 811, localhost, ANY, 1690 bytes)
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO Executor: Running task 164.0 in stage 6.0 (TID 811)
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00004-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500159 length: 3500159 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 INFO TaskSetManager: Finished task 197.0 in stage 5.0 (TID 760) in 11724 ms on localhost (195/200)
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 750000
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:15 INFO Executor: Finished task 198.0 in stage 5.0 (TID 761). 1219 bytes result sent to driver
15/08/21 13:51:15 INFO TaskSetManager: Starting task 165.0 in stage 6.0 (TID 812, localhost, ANY, 1692 bytes)
15/08/21 13:51:15 INFO Executor: Running task 165.0 in stage 6.0 (TID 812)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 198.0 in stage 5.0 (TID 761) in 11752 ms on localhost (196/200)
15/08/21 13:51:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00040-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505811 length: 3505811 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:15 INFO Executor: Finished task 193.0 in stage 5.0 (TID 756). 1219 bytes result sent to driver
15/08/21 13:51:15 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 750000
15/08/21 13:51:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:15 INFO TaskSetManager: Starting task 166.0 in stage 6.0 (TID 813, localhost, ANY, 1693 bytes)
15/08/21 13:51:15 INFO Executor: Running task 166.0 in stage 6.0 (TID 813)
15/08/21 13:51:15 INFO TaskSetManager: Finished task 193.0 in stage 5.0 (TID 756) in 12697 ms on localhost (197/200)
15/08/21 13:51:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00065-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500529 length: 3500529 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO Executor: Finished task 154.0 in stage 6.0 (TID 801). 2125 bytes result sent to driver
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 13:51:16 INFO TaskSetManager: Starting task 167.0 in stage 6.0 (TID 814, localhost, ANY, 1693 bytes)
15/08/21 13:51:16 INFO Executor: Running task 167.0 in stage 6.0 (TID 814)
15/08/21 13:51:16 INFO TaskSetManager: Finished task 154.0 in stage 6.0 (TID 801) in 839 ms on localhost (155/200)
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00097-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499721 length: 3499721 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 750000
15/08/21 13:51:16 INFO Executor: Finished task 155.0 in stage 6.0 (TID 802). 2125 bytes result sent to driver
15/08/21 13:51:16 INFO TaskSetManager: Starting task 168.0 in stage 6.0 (TID 815, localhost, ANY, 1693 bytes)
15/08/21 13:51:16 INFO Executor: Running task 168.0 in stage 6.0 (TID 815)
15/08/21 13:51:16 INFO TaskSetManager: Finished task 155.0 in stage 6.0 (TID 802) in 906 ms on localhost (156/200)
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00053-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506395 length: 3506395 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO Executor: Finished task 156.0 in stage 6.0 (TID 803). 2125 bytes result sent to driver
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 13:51:16 INFO TaskSetManager: Starting task 169.0 in stage 6.0 (TID 816, localhost, ANY, 1693 bytes)
15/08/21 13:51:16 INFO Executor: Running task 169.0 in stage 6.0 (TID 816)
15/08/21 13:51:16 INFO TaskSetManager: Finished task 156.0 in stage 6.0 (TID 803) in 736 ms on localhost (157/200)
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00108-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501015 length: 3501015 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO Executor: Finished task 196.0 in stage 5.0 (TID 759). 1219 bytes result sent to driver
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO TaskSetManager: Starting task 170.0 in stage 6.0 (TID 817, localhost, ANY, 1691 bytes)
15/08/21 13:51:16 INFO Executor: Running task 170.0 in stage 6.0 (TID 817)
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00159-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505777 length: 3505777 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 INFO TaskSetManager: Finished task 196.0 in stage 5.0 (TID 759) in 12044 ms on localhost (198/200)
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO Executor: Finished task 157.0 in stage 6.0 (TID 804). 2125 bytes result sent to driver
15/08/21 13:51:16 INFO TaskSetManager: Starting task 171.0 in stage 6.0 (TID 818, localhost, ANY, 1692 bytes)
15/08/21 13:51:16 INFO Executor: Running task 171.0 in stage 6.0 (TID 818)
15/08/21 13:51:16 INFO TaskSetManager: Finished task 157.0 in stage 6.0 (TID 804) in 773 ms on localhost (158/200)
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00113-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502019 length: 3502019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO Executor: Finished task 195.0 in stage 5.0 (TID 758). 1219 bytes result sent to driver
15/08/21 13:51:16 INFO TaskSetManager: Starting task 172.0 in stage 6.0 (TID 819, localhost, ANY, 1693 bytes)
15/08/21 13:51:16 INFO Executor: Running task 172.0 in stage 6.0 (TID 819)
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO TaskSetManager: Finished task 195.0 in stage 5.0 (TID 758) in 12394 ms on localhost (199/200)
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00080-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3508460 length: 3508460 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 750000
15/08/21 13:51:16 INFO Executor: Finished task 159.0 in stage 6.0 (TID 806). 2125 bytes result sent to driver
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO TaskSetManager: Starting task 173.0 in stage 6.0 (TID 820, localhost, ANY, 1692 bytes)
15/08/21 13:51:16 INFO Executor: Running task 173.0 in stage 6.0 (TID 820)
15/08/21 13:51:16 INFO TaskSetManager: Finished task 159.0 in stage 6.0 (TID 806) in 675 ms on localhost (159/200)
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 750000
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00174-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505785 length: 3505785 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 750000
15/08/21 13:51:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 83 ms. row count = 750000
15/08/21 13:51:16 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 13:51:16 INFO Executor: Finished task 158.0 in stage 6.0 (TID 805). 2125 bytes result sent to driver
15/08/21 13:51:16 INFO Executor: Finished task 161.0 in stage 6.0 (TID 808). 2125 bytes result sent to driver
15/08/21 13:51:16 INFO TaskSetManager: Starting task 174.0 in stage 6.0 (TID 821, localhost, ANY, 1693 bytes)
15/08/21 13:51:16 INFO Executor: Running task 174.0 in stage 6.0 (TID 821)
15/08/21 13:51:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00012-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500010 length: 3500010 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:16 INFO TaskSetManager: Starting task 175.0 in stage 6.0 (TID 822, localhost, ANY, 1694 bytes)
15/08/21 13:51:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:16 INFO TaskSetManager: Finished task 158.0 in stage 6.0 (TID 805) in 1333 ms on localhost (160/200)
15/08/21 13:51:16 INFO Executor: Running task 175.0 in stage 6.0 (TID 822)
15/08/21 13:51:16 INFO TaskSetManager: Finished task 161.0 in stage 6.0 (TID 808) in 1167 ms on localhost (161/200)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00128-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3507147 length: 3507147 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO Executor: Finished task 160.0 in stage 6.0 (TID 807). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO Executor: Finished task 162.0 in stage 6.0 (TID 809). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO TaskSetManager: Starting task 176.0 in stage 6.0 (TID 823, localhost, ANY, 1692 bytes)
15/08/21 13:51:17 INFO Executor: Running task 176.0 in stage 6.0 (TID 823)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00152-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505322 length: 3505322 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO TaskSetManager: Starting task 177.0 in stage 6.0 (TID 824, localhost, ANY, 1694 bytes)
15/08/21 13:51:17 INFO TaskSetManager: Finished task 160.0 in stage 6.0 (TID 807) in 1353 ms on localhost (162/200)
15/08/21 13:51:17 INFO Executor: Running task 177.0 in stage 6.0 (TID 824)
15/08/21 13:51:17 INFO TaskSetManager: Finished task 162.0 in stage 6.0 (TID 809) in 1266 ms on localhost (163/200)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00165-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3508457 length: 3508457 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 122 ms. row count = 750000
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO Executor: Finished task 164.0 in stage 6.0 (TID 811). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 135 ms. row count = 750000
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO TaskSetManager: Starting task 178.0 in stage 6.0 (TID 825, localhost, ANY, 1693 bytes)
15/08/21 13:51:17 INFO Executor: Running task 178.0 in stage 6.0 (TID 825)
15/08/21 13:51:17 INFO TaskSetManager: Finished task 164.0 in stage 6.0 (TID 811) in 1275 ms on localhost (164/200)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00169-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3496922 length: 3496922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 INFO Executor: Finished task 163.0 in stage 6.0 (TID 810). 2125 bytes result sent to driver
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 750000
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO TaskSetManager: Starting task 179.0 in stage 6.0 (TID 826, localhost, ANY, 1692 bytes)
15/08/21 13:51:17 INFO Executor: Running task 179.0 in stage 6.0 (TID 826)
15/08/21 13:51:17 INFO TaskSetManager: Finished task 163.0 in stage 6.0 (TID 810) in 1326 ms on localhost (165/200)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00088-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3508314 length: 3508314 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 750000
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO Executor: Finished task 166.0 in stage 6.0 (TID 813). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO Executor: Finished task 165.0 in stage 6.0 (TID 812). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 750000
15/08/21 13:51:17 INFO TaskSetManager: Starting task 180.0 in stage 6.0 (TID 827, localhost, ANY, 1691 bytes)
15/08/21 13:51:17 INFO Executor: Running task 180.0 in stage 6.0 (TID 827)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00043-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502639 length: 3502639 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO TaskSetManager: Starting task 181.0 in stage 6.0 (TID 828, localhost, ANY, 1693 bytes)
15/08/21 13:51:17 INFO TaskSetManager: Finished task 166.0 in stage 6.0 (TID 813) in 1309 ms on localhost (166/200)
15/08/21 13:51:17 INFO Executor: Running task 181.0 in stage 6.0 (TID 828)
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 750000
15/08/21 13:51:17 INFO TaskSetManager: Finished task 165.0 in stage 6.0 (TID 812) in 1332 ms on localhost (167/200)
15/08/21 13:51:17 INFO Executor: Finished task 167.0 in stage 6.0 (TID 814). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00119-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506360 length: 3506360 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO TaskSetManager: Starting task 182.0 in stage 6.0 (TID 829, localhost, ANY, 1692 bytes)
15/08/21 13:51:17 INFO Executor: Running task 182.0 in stage 6.0 (TID 829)
15/08/21 13:51:17 INFO TaskSetManager: Finished task 167.0 in stage 6.0 (TID 814) in 1330 ms on localhost (168/200)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00066-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499206 length: 3499206 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 750000
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO Executor: Finished task 168.0 in stage 6.0 (TID 815). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO TaskSetManager: Starting task 183.0 in stage 6.0 (TID 830, localhost, ANY, 1692 bytes)
15/08/21 13:51:17 INFO Executor: Finished task 171.0 in stage 6.0 (TID 818). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO Executor: Running task 183.0 in stage 6.0 (TID 830)
15/08/21 13:51:17 INFO TaskSetManager: Starting task 184.0 in stage 6.0 (TID 831, localhost, ANY, 1693 bytes)
15/08/21 13:51:17 INFO Executor: Running task 184.0 in stage 6.0 (TID 831)
15/08/21 13:51:17 INFO TaskSetManager: Finished task 168.0 in stage 6.0 (TID 815) in 1332 ms on localhost (169/200)
15/08/21 13:51:17 INFO Executor: Finished task 169.0 in stage 6.0 (TID 816). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO Executor: Finished task 172.0 in stage 6.0 (TID 819). 2125 bytes result sent to driver
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00098-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500259 length: 3500259 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00073-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501504 length: 3501504 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 INFO Executor: Finished task 170.0 in stage 6.0 (TID 817). 2125 bytes result sent to driver
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 116 ms. row count = 750000
15/08/21 13:51:17 INFO TaskSetManager: Starting task 185.0 in stage 6.0 (TID 832, localhost, ANY, 1692 bytes)
15/08/21 13:51:17 INFO TaskSetManager: Finished task 171.0 in stage 6.0 (TID 818) in 1190 ms on localhost (170/200)
15/08/21 13:51:17 INFO Executor: Running task 185.0 in stage 6.0 (TID 832)
15/08/21 13:51:17 INFO TaskSetManager: Starting task 186.0 in stage 6.0 (TID 833, localhost, ANY, 1693 bytes)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00156-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3501563 length: 3501563 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 INFO Executor: Running task 186.0 in stage 6.0 (TID 833)
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO TaskSetManager: Finished task 169.0 in stage 6.0 (TID 816) in 1271 ms on localhost (171/200)
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO TaskSetManager: Starting task 187.0 in stage 6.0 (TID 834, localhost, ANY, 1693 bytes)
15/08/21 13:51:17 INFO Executor: Running task 187.0 in stage 6.0 (TID 834)
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO TaskSetManager: Finished task 172.0 in stage 6.0 (TID 819) in 1153 ms on localhost (172/200)
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00141-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506341 length: 3506341 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 INFO TaskSetManager: Finished task 170.0 in stage 6.0 (TID 817) in 1233 ms on localhost (173/200)
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00072-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3506406 length: 3506406 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 750000
15/08/21 13:51:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 559 ms. row count = 750000
15/08/21 13:51:18 INFO Executor: Finished task 173.0 in stage 6.0 (TID 820). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 188.0 in stage 6.0 (TID 835, localhost, ANY, 1693 bytes)
15/08/21 13:51:18 INFO Executor: Finished task 199.0 in stage 5.0 (TID 762). 1219 bytes result sent to driver
15/08/21 13:51:18 INFO Executor: Running task 188.0 in stage 6.0 (TID 835)
15/08/21 13:51:18 INFO TaskSetManager: Starting task 189.0 in stage 6.0 (TID 836, localhost, ANY, 1692 bytes)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 173.0 in stage 6.0 (TID 820) in 1687 ms on localhost (174/200)
15/08/21 13:51:18 INFO Executor: Running task 189.0 in stage 6.0 (TID 836)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 199.0 in stage 5.0 (TID 762) in 10231 ms on localhost (200/200)
15/08/21 13:51:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/21 13:51:18 INFO DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:423) finished in 109.756 s
15/08/21 13:51:18 INFO DAGScheduler: looking for newly runnable stages
15/08/21 13:51:18 INFO DAGScheduler: running: Set(ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 13:51:18 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/21 13:51:18 INFO DAGScheduler: failed: Set()
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00143-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504817 length: 3504817 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@72d54874
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00109-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502812 length: 3502812 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO StatsReportListener: task runtime:(count: 274, mean: 6428.364964, stdev: 3770.304813, max: 13710.000000, min: 475.000000)
15/08/21 13:51:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:18 INFO StatsReportListener: 	475.0 ms	830.0 ms	945.0 ms	1.6 s	6.9 s	9.8 s	11.3 s	11.9 s	13.7 s
15/08/21 13:51:18 INFO StatsReportListener: shuffle bytes written:(count: 274, mean: 17361975.416058, stdev: 10559028.341038, max: 23872475.000000, min: 0.000000)
15/08/21 13:51:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:18 INFO StatsReportListener: 	0.0 B	3.2 KB	3.2 KB	3.2 KB	22.7 MB	22.7 MB	22.7 MB	22.7 MB	22.8 MB
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 750000
15/08/21 13:51:18 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.725000, stdev: 1.728981, max: 16.000000, min: 0.000000)
15/08/21 13:51:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:18 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	3.0 ms	16.0 ms
15/08/21 13:51:18 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 13:51:18 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 13:51:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:18 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 13:51:18 INFO StatsReportListener: task result size:(count: 274, mean: 1463.686131, stdev: 402.261522, max: 2125.000000, min: 1219.000000)
15/08/21 13:51:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:18 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO StatsReportListener: executor (non-fetch) time pct: (count: 274, mean: 98.524751, stdev: 2.338944, max: 99.912196, min: 76.967471)
15/08/21 13:51:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:18 INFO StatsReportListener: 	77 %	94 %	95 %	98 %	100 %	100 %	100 %	100 %	100 %
15/08/21 13:51:18 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.008546, stdev: 0.018844, max: 0.156540, min: 0.000000)
15/08/21 13:51:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:18 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 13:51:18 INFO StatsReportListener: other time pct: (count: 274, mean: 1.469011, stdev: 2.341742, max: 23.032529, min: 0.087804)
15/08/21 13:51:18 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:18 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 2 %	 5 %	 6 %	23 %
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 613 ms. row count = 750000
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO Executor: Finished task 174.0 in stage 6.0 (TID 821). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 125 ms. row count = 750000
15/08/21 13:51:18 INFO TaskSetManager: Starting task 190.0 in stage 6.0 (TID 837, localhost, ANY, 1692 bytes)
15/08/21 13:51:18 INFO Executor: Running task 190.0 in stage 6.0 (TID 837)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 174.0 in stage 6.0 (TID 821) in 1184 ms on localhost (175/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00075-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3497524 length: 3497524 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 750000
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 750000
15/08/21 13:51:18 INFO Executor: Finished task 175.0 in stage 6.0 (TID 822). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 119 ms. row count = 750000
15/08/21 13:51:18 INFO TaskSetManager: Starting task 191.0 in stage 6.0 (TID 838, localhost, ANY, 1692 bytes)
15/08/21 13:51:18 INFO Executor: Running task 191.0 in stage 6.0 (TID 838)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 175.0 in stage 6.0 (TID 822) in 1233 ms on localhost (176/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00153-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500279 length: 3500279 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO Executor: Finished task 176.0 in stage 6.0 (TID 823). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 192.0 in stage 6.0 (TID 839, localhost, ANY, 1691 bytes)
15/08/21 13:51:18 INFO Executor: Running task 192.0 in stage 6.0 (TID 839)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 176.0 in stage 6.0 (TID 823) in 1149 ms on localhost (177/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00025-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500335 length: 3500335 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 78 ms. row count = 750000
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 750000
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO Executor: Finished task 178.0 in stage 6.0 (TID 825). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 193.0 in stage 6.0 (TID 840, localhost, ANY, 1690 bytes)
15/08/21 13:51:18 INFO Executor: Running task 193.0 in stage 6.0 (TID 840)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 178.0 in stage 6.0 (TID 825) in 1196 ms on localhost (178/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00089-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3500320 length: 3500320 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO Executor: Finished task 179.0 in stage 6.0 (TID 826). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 194.0 in stage 6.0 (TID 841, localhost, ANY, 1689 bytes)
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO Executor: Running task 194.0 in stage 6.0 (TID 841)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 179.0 in stage 6.0 (TID 826) in 1214 ms on localhost (179/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00003-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499057 length: 3499057 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO Executor: Finished task 177.0 in stage 6.0 (TID 824). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 750000
15/08/21 13:51:18 INFO TaskSetManager: Starting task 195.0 in stage 6.0 (TID 842, localhost, ANY, 1692 bytes)
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO TaskSetManager: Finished task 177.0 in stage 6.0 (TID 824) in 1309 ms on localhost (180/200)
15/08/21 13:51:18 INFO Executor: Running task 195.0 in stage 6.0 (TID 842)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00185-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3499076 length: 3499076 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO Executor: Finished task 187.0 in stage 6.0 (TID 834). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO TaskSetManager: Starting task 196.0 in stage 6.0 (TID 843, localhost, ANY, 1692 bytes)
15/08/21 13:51:18 INFO Executor: Running task 196.0 in stage 6.0 (TID 843)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 187.0 in stage 6.0 (TID 834) in 1023 ms on localhost (181/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00117-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3505296 length: 3505296 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO Executor: Finished task 180.0 in stage 6.0 (TID 827). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO TaskSetManager: Starting task 197.0 in stage 6.0 (TID 844, localhost, ANY, 1692 bytes)
15/08/21 13:51:18 INFO Executor: Running task 197.0 in stage 6.0 (TID 844)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 180.0 in stage 6.0 (TID 827) in 1247 ms on localhost (182/200)
15/08/21 13:51:18 INFO Executor: Finished task 181.0 in stage 6.0 (TID 828). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00121-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3502280 length: 3502280 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 INFO TaskSetManager: Starting task 198.0 in stage 6.0 (TID 845, localhost, ANY, 1693 bytes)
15/08/21 13:51:18 INFO Executor: Running task 198.0 in stage 6.0 (TID 845)
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00176-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3504086 length: 3504086 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 INFO TaskSetManager: Finished task 181.0 in stage 6.0 (TID 828) in 1233 ms on localhost (183/200)
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 51 ms. row count = 750000
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 119 ms. row count = 750000
15/08/21 13:51:18 INFO Executor: Finished task 182.0 in stage 6.0 (TID 829). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 750000
15/08/21 13:51:18 INFO TaskSetManager: Starting task 199.0 in stage 6.0 (TID 846, localhost, ANY, 1692 bytes)
15/08/21 13:51:18 INFO Executor: Running task 199.0 in stage 6.0 (TID 846)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 182.0 in stage 6.0 (TID 829) in 1329 ms on localhost (184/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00000-e4d6dea2-9191-4818-8deb-e07bd86cb1a9.gz.parquet start: 0 end: 3509124 length: 3509124 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 750000
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 13:51:18 INFO Executor: Finished task 183.0 in stage 6.0 (TID 830). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 847, localhost, ANY, 1762 bytes)
15/08/21 13:51:18 INFO Executor: Running task 0.0 in stage 7.0 (TID 847)
15/08/21 13:51:18 INFO Executor: Finished task 184.0 in stage 6.0 (TID 831). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Finished task 183.0 in stage 6.0 (TID 830) in 1259 ms on localhost (185/200)
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 236 ms. row count = 750000
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 848, localhost, ANY, 1773 bytes)
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO Executor: Running task 1.0 in stage 7.0 (TID 848)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 134217728 end: 257568535 length: 123350807 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 INFO TaskSetManager: Finished task 184.0 in stage 6.0 (TID 831) in 1282 ms on localhost (186/200)
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 76 ms. row count = 750000
15/08/21 13:51:18 INFO Executor: Finished task 185.0 in stage 6.0 (TID 832). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 849, localhost, ANY, 1761 bytes)
15/08/21 13:51:18 INFO Executor: Running task 2.0 in stage 7.0 (TID 849)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 185.0 in stage 6.0 (TID 832) in 1353 ms on localhost (187/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO Executor: Finished task 192.0 in stage 6.0 (TID 839). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 850, localhost, ANY, 1772 bytes)
15/08/21 13:51:18 INFO Executor: Running task 3.0 in stage 7.0 (TID 850)
15/08/21 13:51:18 INFO InternalParquetRecordReader: block read in memory in 258 ms. row count = 750000
15/08/21 13:51:18 INFO TaskSetManager: Finished task 192.0 in stage 6.0 (TID 839) in 556 ms on localhost (188/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 134217728 end: 257171772 length: 122954044 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO Executor: Finished task 186.0 in stage 6.0 (TID 833). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 851, localhost, ANY, 1761 bytes)
15/08/21 13:51:18 INFO Executor: Running task 4.0 in stage 7.0 (TID 851)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 186.0 in stage 6.0 (TID 833) in 1401 ms on localhost (189/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572843 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501150 records.
15/08/21 13:51:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572933 records.
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO Executor: Finished task 188.0 in stage 6.0 (TID 835). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:18 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 852, localhost, ANY, 1775 bytes)
15/08/21 13:51:18 INFO Executor: Running task 5.0 in stage 7.0 (TID 852)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 188.0 in stage 6.0 (TID 835) in 868 ms on localhost (190/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 134217728 end: 257477064 length: 123259336 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:18 INFO Executor: Finished task 189.0 in stage 6.0 (TID 836). 2125 bytes result sent to driver
15/08/21 13:51:18 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 853, localhost, ANY, 1762 bytes)
15/08/21 13:51:18 INFO Executor: Running task 6.0 in stage 7.0 (TID 853)
15/08/21 13:51:18 INFO TaskSetManager: Finished task 189.0 in stage 6.0 (TID 836) in 935 ms on localhost (191/200)
15/08/21 13:51:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO Executor: Finished task 190.0 in stage 6.0 (TID 837). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 854, localhost, ANY, 1777 bytes)
15/08/21 13:51:19 INFO Executor: Running task 7.0 in stage 7.0 (TID 854)
15/08/21 13:51:19 INFO TaskSetManager: Finished task 190.0 in stage 6.0 (TID 837) in 898 ms on localhost (192/200)
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 134217728 end: 257329816 length: 123112088 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO Executor: Finished task 191.0 in stage 6.0 (TID 838). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573983 records.
15/08/21 13:51:19 INFO Executor: Finished task 195.0 in stage 6.0 (TID 842). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 855, localhost, ANY, 1762 bytes)
15/08/21 13:51:19 INFO Executor: Running task 8.0 in stage 7.0 (TID 855)
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 856, localhost, ANY, 1774 bytes)
15/08/21 13:51:19 INFO TaskSetManager: Finished task 191.0 in stage 6.0 (TID 838) in 908 ms on localhost (193/200)
15/08/21 13:51:19 INFO Executor: Running task 9.0 in stage 7.0 (TID 856)
15/08/21 13:51:19 INFO TaskSetManager: Finished task 195.0 in stage 6.0 (TID 842) in 692 ms on localhost (194/200)
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 134217728 end: 257425918 length: 123208190 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 INFO InternalParquetRecordReader: block read in memory in 244 ms. row count = 3502678
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO Executor: Finished task 196.0 in stage 6.0 (TID 843). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO InternalParquetRecordReader: block read in memory in 304 ms. row count = 3500939
15/08/21 13:51:19 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 857, localhost, ANY, 1760 bytes)
15/08/21 13:51:19 INFO Executor: Running task 10.0 in stage 7.0 (TID 857)
15/08/21 13:51:19 INFO TaskSetManager: Finished task 196.0 in stage 6.0 (TID 843) in 725 ms on localhost (195/200)
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO InternalParquetRecordReader: block read in memory in 346 ms. row count = 3501150
15/08/21 13:51:19 INFO InternalParquetRecordReader: block read in memory in 341 ms. row count = 3501191
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501187 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573045 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO Executor: Finished task 197.0 in stage 6.0 (TID 844). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 858, localhost, ANY, 1771 bytes)
15/08/21 13:51:19 INFO Executor: Running task 11.0 in stage 7.0 (TID 858)
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:19 INFO TaskSetManager: Finished task 197.0 in stage 6.0 (TID 844) in 789 ms on localhost (196/200)
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 134217728 end: 259746640 length: 125528912 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO Executor: Finished task 193.0 in stage 6.0 (TID 840). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 859, localhost, ANY, 1762 bytes)
15/08/21 13:51:19 INFO Executor: Running task 12.0 in stage 7.0 (TID 859)
15/08/21 13:51:19 INFO Executor: Finished task 194.0 in stage 6.0 (TID 841). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 860, localhost, ANY, 1775 bytes)
15/08/21 13:51:19 INFO TaskSetManager: Finished task 193.0 in stage 6.0 (TID 840) in 919 ms on localhost (197/200)
15/08/21 13:51:19 INFO Executor: Running task 13.0 in stage 7.0 (TID 860)
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627997 records.
15/08/21 13:51:19 INFO TaskSetManager: Finished task 194.0 in stage 6.0 (TID 841) in 883 ms on localhost (198/200)
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 134217728 end: 257420451 length: 123202723 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501583 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572784 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573892 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO InternalParquetRecordReader: block read in memory in 237 ms. row count = 3500100
15/08/21 13:51:19 INFO InternalParquetRecordReader: block read in memory in 307 ms. row count = 3501583
15/08/21 13:51:19 INFO Executor: Finished task 198.0 in stage 6.0 (TID 845). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 861, localhost, ANY, 1762 bytes)
15/08/21 13:51:19 INFO Executor: Running task 14.0 in stage 7.0 (TID 861)
15/08/21 13:51:19 INFO TaskSetManager: Finished task 198.0 in stage 6.0 (TID 845) in 1279 ms on localhost (199/200)
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO Executor: Finished task 199.0 in stage 6.0 (TID 846). 2125 bytes result sent to driver
15/08/21 13:51:19 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 862, localhost, ANY, 1773 bytes)
15/08/21 13:51:19 INFO Executor: Running task 15.0 in stage 7.0 (TID 862)
15/08/21 13:51:19 INFO TaskSetManager: Finished task 199.0 in stage 6.0 (TID 846) in 1200 ms on localhost (200/200)
15/08/21 13:51:19 INFO DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:423) finished in 162.920 s
15/08/21 13:51:19 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/21 13:51:19 INFO DAGScheduler: looking for newly runnable stages
15/08/21 13:51:19 INFO DAGScheduler: running: Set(ShuffleMapStage 7)
15/08/21 13:51:19 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/21 13:51:19 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@21892ff3
15/08/21 13:51:19 INFO DAGScheduler: failed: Set()
15/08/21 13:51:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 134217728 end: 257463677 length: 123245949 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:19 INFO StatsReportListener: task runtime:(count: 26, mean: 1079.384615, stdev: 229.801432, max: 1401.000000, min: 556.000000)
15/08/21 13:51:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:19 INFO StatsReportListener: 	556.0 ms	692.0 ms	725.0 ms	898.0 ms	1.2 s	1.3 s	1.3 s	1.4 s	1.4 s
15/08/21 13:51:19 INFO StatsReportListener: shuffle bytes written:(count: 26, mean: 2985.500000, stdev: 861.908716, max: 3261.000000, min: 0.000000)
15/08/21 13:51:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:19 INFO StatsReportListener: 	0.0 B	0.0 B	3.1 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB	3.2 KB
15/08/21 13:51:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:19 INFO StatsReportListener: task result size:(count: 26, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 13:51:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:19 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 13:51:19 INFO StatsReportListener: executor (non-fetch) time pct: (count: 26, mean: 96.370322, stdev: 1.651066, max: 98.476605, min: 92.625899)
15/08/21 13:51:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:19 INFO StatsReportListener: 	93 %	93 %	94 %	95 %	97 %	98 %	98 %	98 %	98 %
15/08/21 13:51:19 INFO StatsReportListener: other time pct: (count: 26, mean: 3.629678, stdev: 1.651066, max: 7.374101, min: 1.523395)
15/08/21 13:51:19 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:51:19 INFO StatsReportListener: 	 2 %	 2 %	 2 %	 2 %	 3 %	 5 %	 6 %	 7 %	 7 %
15/08/21 13:51:19 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 7)
15/08/21 13:51:19 INFO InternalParquetRecordReader: block read in memory in 762 ms. row count = 3500100
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574066 records.
15/08/21 13:51:19 INFO InternalParquetRecordReader: block read in memory in 664 ms. row count = 3500100
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:20 INFO InternalParquetRecordReader: block read in memory in 871 ms. row count = 3500100
15/08/21 13:51:20 INFO InternalParquetRecordReader: block read in memory in 933 ms. row count = 3500100
15/08/21 13:51:20 INFO InternalParquetRecordReader: block read in memory in 1089 ms. row count = 3501187
15/08/21 13:51:20 INFO InternalParquetRecordReader: block read in memory in 522 ms. row count = 3500100
15/08/21 13:51:20 INFO InternalParquetRecordReader: block read in memory in 676 ms. row count = 3500780
15/08/21 13:51:20 INFO InternalParquetRecordReader: block read in memory in 1550 ms. row count = 3500100
15/08/21 13:51:20 INFO InternalParquetRecordReader: block read in memory in 1374 ms. row count = 3503008
15/08/21 13:51:20 INFO InternalParquetRecordReader: block read in memory in 1389 ms. row count = 3500100
15/08/21 13:51:24 INFO InternalParquetRecordReader: Assembled and processed 3500939 records from 2 columns in 4804 ms: 728.755 rec/ms, 1457.51 cell/ms
15/08/21 13:51:24 INFO InternalParquetRecordReader: time spent so far 5% reading (304 ms) and 94% processing (4804 ms)
15/08/21 13:51:24 INFO InternalParquetRecordReader: at row 3500939. reading next block
15/08/21 13:51:24 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 71904
15/08/21 13:51:24 INFO Executor: Finished task 0.0 in stage 7.0 (TID 847). 2125 bytes result sent to driver
15/08/21 13:51:24 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 863, localhost, ANY, 1759 bytes)
15/08/21 13:51:24 INFO Executor: Running task 16.0 in stage 7.0 (TID 863)
15/08/21 13:51:24 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 847) in 5962 ms on localhost (1/170)
15/08/21 13:51:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501183 records.
15/08/21 13:51:24 INFO InternalParquetRecordReader: Assembled and processed 3502678 records from 2 columns in 5571 ms: 628.73413 rec/ms, 1257.4683 cell/ms
15/08/21 13:51:24 INFO InternalParquetRecordReader: time spent so far 4% reading (244 ms) and 95% processing (5571 ms)
15/08/21 13:51:24 INFO InternalParquetRecordReader: at row 3502678. reading next block
15/08/21 13:51:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:24 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 70255
15/08/21 13:51:24 INFO Executor: Finished task 1.0 in stage 7.0 (TID 848). 2125 bytes result sent to driver
15/08/21 13:51:24 INFO InternalParquetRecordReader: block read in memory in 108 ms. row count = 3501183
15/08/21 13:51:24 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 864, localhost, ANY, 1773 bytes)
15/08/21 13:51:24 INFO Executor: Running task 17.0 in stage 7.0 (TID 864)
15/08/21 13:51:24 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 848) in 6158 ms on localhost (2/170)
15/08/21 13:51:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 134217728 end: 261799262 length: 127581534 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3689969 records.
15/08/21 13:51:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:24 INFO Executor: Finished task 8.0 in stage 7.0 (TID 855). 2125 bytes result sent to driver
15/08/21 13:51:24 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 865, localhost, ANY, 1762 bytes)
15/08/21 13:51:24 INFO Executor: Running task 18.0 in stage 7.0 (TID 865)
15/08/21 13:51:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:24 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 855) in 5920 ms on localhost (3/170)
15/08/21 13:51:25 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 3501351
15/08/21 13:51:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:25 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5542 ms: 631.559 rec/ms, 1263.118 cell/ms
15/08/21 13:51:25 INFO InternalParquetRecordReader: time spent so far 4% reading (237 ms) and 95% processing (5542 ms)
15/08/21 13:51:25 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:25 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 72945
15/08/21 13:51:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:25 INFO InternalParquetRecordReader: block read in memory in 142 ms. row count = 3500100
15/08/21 13:51:25 INFO Executor: Finished task 10.0 in stage 7.0 (TID 857). 2125 bytes result sent to driver
15/08/21 13:51:25 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 866, localhost, ANY, 1773 bytes)
15/08/21 13:51:25 INFO Executor: Running task 19.0 in stage 7.0 (TID 866)
15/08/21 13:51:25 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 857) in 6200 ms on localhost (4/170)
15/08/21 13:51:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 134217728 end: 257435404 length: 123217676 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573923 records.
15/08/21 13:51:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:25 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5287 ms: 662.02 rec/ms, 1324.04 cell/ms
15/08/21 13:51:25 INFO InternalParquetRecordReader: time spent so far 14% reading (871 ms) and 85% processing (5287 ms)
15/08/21 13:51:25 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:25 INFO InternalParquetRecordReader: block read in memory in 121 ms. row count = 3501239
15/08/21 13:51:25 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 72684
15/08/21 13:51:25 INFO Executor: Finished task 3.0 in stage 7.0 (TID 850). 2125 bytes result sent to driver
15/08/21 13:51:25 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 867, localhost, ANY, 1762 bytes)
15/08/21 13:51:25 INFO Executor: Running task 20.0 in stage 7.0 (TID 867)
15/08/21 13:51:25 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 850) in 6841 ms on localhost (5/170)
15/08/21 13:51:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:25 INFO Executor: Finished task 2.0 in stage 7.0 (TID 849). 2125 bytes result sent to driver
15/08/21 13:51:25 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 868, localhost, ANY, 1775 bytes)
15/08/21 13:51:25 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 849) in 6951 ms on localhost (6/170)
15/08/21 13:51:25 INFO Executor: Running task 21.0 in stage 7.0 (TID 868)
15/08/21 13:51:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 134217728 end: 257347114 length: 123129386 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:25 INFO InternalParquetRecordReader: block read in memory in 82 ms. row count = 3500100
15/08/21 13:51:25 INFO InternalParquetRecordReader: Assembled and processed 3500780 records from 2 columns in 5112 ms: 684.8161 rec/ms, 1369.6322 cell/ms
15/08/21 13:51:25 INFO InternalParquetRecordReader: time spent so far 11% reading (676 ms) and 88% processing (5112 ms)
15/08/21 13:51:25 INFO InternalParquetRecordReader: at row 3500780. reading next block
15/08/21 13:51:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
15/08/21 13:51:25 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 73286
15/08/21 13:51:25 INFO Executor: Finished task 5.0 in stage 7.0 (TID 852). 2125 bytes result sent to driver
15/08/21 13:51:25 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 869, localhost, ANY, 1762 bytes)
15/08/21 13:51:25 INFO Executor: Running task 22.0 in stage 7.0 (TID 869)
15/08/21 13:51:25 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 852) in 6905 ms on localhost (7/170)
15/08/21 13:51:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501423 records.
15/08/21 13:51:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:25 INFO InternalParquetRecordReader: block read in memory in 127 ms. row count = 3500100
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 176 ms. row count = 3501423
15/08/21 13:51:26 INFO Executor: Finished task 12.0 in stage 7.0 (TID 859). 2125 bytes result sent to driver
15/08/21 13:51:26 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 870, localhost, ANY, 1772 bytes)
15/08/21 13:51:26 INFO Executor: Running task 23.0 in stage 7.0 (TID 870)
15/08/21 13:51:26 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 859) in 6808 ms on localhost (8/170)
15/08/21 13:51:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 134217728 end: 257844810 length: 123627082 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573035 records.
15/08/21 13:51:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 3500100
15/08/21 13:51:26 INFO Executor: Finished task 9.0 in stage 7.0 (TID 856). 2125 bytes result sent to driver
15/08/21 13:51:26 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 871, localhost, ANY, 1762 bytes)
15/08/21 13:51:26 INFO Executor: Running task 24.0 in stage 7.0 (TID 871)
15/08/21 13:51:26 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 856) in 7175 ms on localhost (9/170)
15/08/21 13:51:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:26 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5595 ms: 625.5764 rec/ms, 1251.1528 cell/ms
15/08/21 13:51:26 INFO InternalParquetRecordReader: time spent so far 19% reading (1389 ms) and 80% processing (5595 ms)
15/08/21 13:51:26 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 127897
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 108 ms. row count = 3500100
15/08/21 13:51:26 INFO InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 5723 ms: 612.09296 rec/ms, 1224.1859 cell/ms
15/08/21 13:51:26 INFO InternalParquetRecordReader: time spent so far 19% reading (1374 ms) and 80% processing (5723 ms)
15/08/21 13:51:26 INFO InternalParquetRecordReader: at row 3503008. reading next block
15/08/21 13:51:26 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5743 ms: 609.455 rec/ms, 1218.91 cell/ms
15/08/21 13:51:26 INFO InternalParquetRecordReader: time spent so far 21% reading (1550 ms) and 78% processing (5743 ms)
15/08/21 13:51:26 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:26 INFO Executor: Finished task 14.0 in stage 7.0 (TID 861). 2125 bytes result sent to driver
15/08/21 13:51:26 INFO Executor: Finished task 15.0 in stage 7.0 (TID 862). 2125 bytes result sent to driver
15/08/21 13:51:26 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 872, localhost, ANY, 1771 bytes)
15/08/21 13:51:26 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 873, localhost, ANY, 1762 bytes)
15/08/21 13:51:26 INFO Executor: Running task 25.0 in stage 7.0 (TID 872)
15/08/21 13:51:26 INFO Executor: Running task 26.0 in stage 7.0 (TID 873)
15/08/21 13:51:26 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 861) in 6715 ms on localhost (10/170)
15/08/21 13:51:26 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 862) in 6687 ms on localhost (11/170)
15/08/21 13:51:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 134217728 end: 257827380 length: 123609652 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 70884
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 73883
15/08/21 13:51:26 INFO Executor: Finished task 6.0 in stage 7.0 (TID 853). 2125 bytes result sent to driver
15/08/21 13:51:26 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 874, localhost, ANY, 1773 bytes)
15/08/21 13:51:26 INFO Executor: Running task 27.0 in stage 7.0 (TID 874)
15/08/21 13:51:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 134217728 end: 257458294 length: 123240566 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573921 records.
15/08/21 13:51:26 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 853) in 7580 ms on localhost (12/170)
15/08/21 13:51:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501667 records.
15/08/21 13:51:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572747 records.
15/08/21 13:51:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 134 ms. row count = 3501667
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 200 ms. row count = 3501305
15/08/21 13:51:26 INFO InternalParquetRecordReader: block read in memory in 218 ms. row count = 3500100
15/08/21 13:51:27 INFO Executor: Finished task 4.0 in stage 7.0 (TID 851). 2125 bytes result sent to driver
15/08/21 13:51:27 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 875, localhost, ANY, 1762 bytes)
15/08/21 13:51:27 INFO Executor: Running task 28.0 in stage 7.0 (TID 875)
15/08/21 13:51:27 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 851) in 9101 ms on localhost (13/170)
15/08/21 13:51:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:27 INFO Executor: Finished task 11.0 in stage 7.0 (TID 858). 2125 bytes result sent to driver
15/08/21 13:51:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:28 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 876, localhost, ANY, 1773 bytes)
15/08/21 13:51:28 INFO Executor: Running task 29.0 in stage 7.0 (TID 876)
15/08/21 13:51:28 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 858) in 8749 ms on localhost (14/170)
15/08/21 13:51:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 134217728 end: 257369456 length: 123151728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:28 INFO Executor: Finished task 7.0 in stage 7.0 (TID 854). 2125 bytes result sent to driver
15/08/21 13:51:28 INFO Executor: Finished task 13.0 in stage 7.0 (TID 860). 2125 bytes result sent to driver
15/08/21 13:51:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
15/08/21 13:51:28 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 877, localhost, ANY, 1762 bytes)
15/08/21 13:51:28 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 878, localhost, ANY, 1776 bytes)
15/08/21 13:51:28 INFO Executor: Running task 31.0 in stage 7.0 (TID 878)
15/08/21 13:51:28 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 854) in 9015 ms on localhost (15/170)
15/08/21 13:51:28 INFO Executor: Running task 30.0 in stage 7.0 (TID 877)
15/08/21 13:51:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 134217728 end: 257340601 length: 123122873 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:28 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 860) in 8776 ms on localhost (16/170)
15/08/21 13:51:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574247 records.
15/08/21 13:51:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:28 INFO InternalParquetRecordReader: block read in memory in 154 ms. row count = 3500100
15/08/21 13:51:28 INFO InternalParquetRecordReader: block read in memory in 131 ms. row count = 3500100
15/08/21 13:51:28 INFO InternalParquetRecordReader: block read in memory in 304 ms. row count = 3500100
15/08/21 13:51:28 INFO InternalParquetRecordReader: block read in memory in 250 ms. row count = 3500100
15/08/21 13:51:30 INFO InternalParquetRecordReader: Assembled and processed 3501351 records from 2 columns in 5342 ms: 655.43823 rec/ms, 1310.8765 cell/ms
15/08/21 13:51:30 INFO InternalParquetRecordReader: time spent so far 1% reading (91 ms) and 98% processing (5342 ms)
15/08/21 13:51:30 INFO InternalParquetRecordReader: at row 3501351. reading next block
15/08/21 13:51:30 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 188618
15/08/21 13:51:31 INFO Executor: Finished task 18.0 in stage 7.0 (TID 865). 2125 bytes result sent to driver
15/08/21 13:51:31 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 879, localhost, ANY, 1761 bytes)
15/08/21 13:51:31 INFO Executor: Running task 32.0 in stage 7.0 (TID 879)
15/08/21 13:51:31 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 865) in 6077 ms on localhost (17/170)
15/08/21 13:51:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:31 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500100
15/08/21 13:51:31 INFO Executor: Finished task 17.0 in stage 7.0 (TID 864). 2125 bytes result sent to driver
15/08/21 13:51:31 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 880, localhost, ANY, 1771 bytes)
15/08/21 13:51:31 INFO Executor: Finished task 16.0 in stage 7.0 (TID 863). 2125 bytes result sent to driver
15/08/21 13:51:31 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 881, localhost, ANY, 1762 bytes)
15/08/21 13:51:31 INFO Executor: Running task 34.0 in stage 7.0 (TID 881)
15/08/21 13:51:31 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 864) in 7070 ms on localhost (18/170)
15/08/21 13:51:31 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 863) in 7286 ms on localhost (19/170)
15/08/21 13:51:31 INFO Executor: Running task 33.0 in stage 7.0 (TID 880)
15/08/21 13:51:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 134217728 end: 259459156 length: 125241428 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627517 records.
15/08/21 13:51:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501180 records.
15/08/21 13:51:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:32 INFO InternalParquetRecordReader: block read in memory in 181 ms. row count = 3501180
15/08/21 13:51:32 INFO InternalParquetRecordReader: block read in memory in 200 ms. row count = 3500968
15/08/21 13:51:32 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6407 ms: 546.2931 rec/ms, 1092.5862 cell/ms
15/08/21 13:51:32 INFO InternalParquetRecordReader: time spent so far 1% reading (127 ms) and 98% processing (6407 ms)
15/08/21 13:51:32 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:32 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 74238
15/08/21 13:51:32 INFO InternalParquetRecordReader: Assembled and processed 3501239 records from 2 columns in 7176 ms: 487.90955 rec/ms, 975.8191 cell/ms
15/08/21 13:51:32 INFO InternalParquetRecordReader: time spent so far 1% reading (121 ms) and 98% processing (7176 ms)
15/08/21 13:51:32 INFO InternalParquetRecordReader: at row 3501239. reading next block
15/08/21 13:51:32 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 72684
15/08/21 13:51:32 INFO InternalParquetRecordReader: Assembled and processed 3501305 records from 2 columns in 6141 ms: 570.1523 rec/ms, 1140.3046 cell/ms
15/08/21 13:51:32 INFO InternalParquetRecordReader: time spent so far 3% reading (200 ms) and 96% processing (6141 ms)
15/08/21 13:51:32 INFO InternalParquetRecordReader: at row 3501305. reading next block
15/08/21 13:51:32 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 72616
15/08/21 13:51:33 INFO Executor: Finished task 26.0 in stage 7.0 (TID 873). 2125 bytes result sent to driver
15/08/21 13:51:33 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 882, localhost, ANY, 1772 bytes)
15/08/21 13:51:33 INFO Executor: Running task 35.0 in stage 7.0 (TID 882)
15/08/21 13:51:33 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 873) in 6694 ms on localhost (20/170)
15/08/21 13:51:33 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6384 ms: 548.2613 rec/ms, 1096.5226 cell/ms
15/08/21 13:51:33 INFO InternalParquetRecordReader: time spent so far 3% reading (218 ms) and 96% processing (6384 ms)
15/08/21 13:51:33 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 134217728 end: 257580347 length: 123362619 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:33 INFO Executor: Finished task 20.0 in stage 7.0 (TID 867). 2125 bytes result sent to driver
15/08/21 13:51:33 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 883, localhost, ANY, 1761 bytes)
15/08/21 13:51:33 INFO Executor: Running task 36.0 in stage 7.0 (TID 883)
15/08/21 13:51:33 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 867) in 7600 ms on localhost (21/170)
15/08/21 13:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:33 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 72647
15/08/21 13:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573027 records.
15/08/21 13:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:33 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 7048 ms: 496.60898 rec/ms, 993.21796 cell/ms
15/08/21 13:51:33 INFO InternalParquetRecordReader: time spent so far 1% reading (98 ms) and 98% processing (7048 ms)
15/08/21 13:51:33 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:33 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 72935
15/08/21 13:51:33 INFO InternalParquetRecordReader: block read in memory in 186 ms. row count = 3500100
15/08/21 13:51:33 INFO InternalParquetRecordReader: block read in memory in 251 ms. row count = 3500932
15/08/21 13:51:33 INFO Executor: Finished task 22.0 in stage 7.0 (TID 869). 2125 bytes result sent to driver
15/08/21 13:51:33 INFO Executor: Finished task 21.0 in stage 7.0 (TID 868). 2125 bytes result sent to driver
15/08/21 13:51:33 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 884, localhost, ANY, 1773 bytes)
15/08/21 13:51:33 INFO Executor: Running task 37.0 in stage 7.0 (TID 884)
15/08/21 13:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 134217728 end: 257857124 length: 123639396 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:33 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 885, localhost, ANY, 1761 bytes)
15/08/21 13:51:33 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 869) in 7940 ms on localhost (22/170)
15/08/21 13:51:33 INFO Executor: Running task 38.0 in stage 7.0 (TID 885)
15/08/21 13:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:33 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 868) in 8011 ms on localhost (23/170)
15/08/21 13:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574216 records.
15/08/21 13:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
15/08/21 13:51:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:33 INFO Executor: Finished task 19.0 in stage 7.0 (TID 866). 2125 bytes result sent to driver
15/08/21 13:51:33 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 886, localhost, ANY, 1773 bytes)
15/08/21 13:51:33 INFO Executor: Running task 39.0 in stage 7.0 (TID 886)
15/08/21 13:51:33 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 866) in 8577 ms on localhost (24/170)
15/08/21 13:51:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 134217728 end: 259199417 length: 124981689 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:33 INFO InternalParquetRecordReader: block read in memory in 162 ms. row count = 3501121
15/08/21 13:51:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626725 records.
15/08/21 13:51:33 INFO InternalParquetRecordReader: block read in memory in 234 ms. row count = 3501165
15/08/21 13:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:34 INFO InternalParquetRecordReader: block read in memory in 112 ms. row count = 3502670
15/08/21 13:51:34 INFO Executor: Finished task 25.0 in stage 7.0 (TID 872). 2125 bytes result sent to driver
15/08/21 13:51:34 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 887, localhost, ANY, 1761 bytes)
15/08/21 13:51:34 INFO Executor: Running task 40.0 in stage 7.0 (TID 887)
15/08/21 13:51:34 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 872) in 7765 ms on localhost (25/170)
15/08/21 13:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501407 records.
15/08/21 13:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:34 INFO Executor: Finished task 27.0 in stage 7.0 (TID 874). 2125 bytes result sent to driver
15/08/21 13:51:34 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 888, localhost, ANY, 1773 bytes)
15/08/21 13:51:34 INFO Executor: Running task 41.0 in stage 7.0 (TID 888)
15/08/21 13:51:34 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 874) in 7917 ms on localhost (26/170)
15/08/21 13:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 134217728 end: 257564901 length: 123347173 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572791 records.
15/08/21 13:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:34 INFO InternalParquetRecordReader: block read in memory in 185 ms. row count = 3501407
15/08/21 13:51:34 INFO Executor: Finished task 24.0 in stage 7.0 (TID 871). 2125 bytes result sent to driver
15/08/21 13:51:34 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 889, localhost, ANY, 1761 bytes)
15/08/21 13:51:34 INFO Executor: Running task 42.0 in stage 7.0 (TID 889)
15/08/21 13:51:34 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 871) in 8249 ms on localhost (27/170)
15/08/21 13:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501143 records.
15/08/21 13:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:34 INFO Executor: Finished task 23.0 in stage 7.0 (TID 870). 2125 bytes result sent to driver
15/08/21 13:51:34 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 890, localhost, ANY, 1772 bytes)
15/08/21 13:51:34 INFO Executor: Running task 43.0 in stage 7.0 (TID 890)
15/08/21 13:51:34 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 870) in 8540 ms on localhost (28/170)
15/08/21 13:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 134217728 end: 257460000 length: 123242272 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573047 records.
15/08/21 13:51:34 INFO InternalParquetRecordReader: block read in memory in 104 ms. row count = 3501143
15/08/21 13:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:34 INFO Executor: Finished task 28.0 in stage 7.0 (TID 875). 2125 bytes result sent to driver
15/08/21 13:51:34 INFO InternalParquetRecordReader: block read in memory in 269 ms. row count = 3500841
15/08/21 13:51:34 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 891, localhost, ANY, 1762 bytes)
15/08/21 13:51:34 INFO Executor: Running task 44.0 in stage 7.0 (TID 891)
15/08/21 13:51:34 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 875) in 6827 ms on localhost (29/170)
15/08/21 13:51:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501260 records.
15/08/21 13:51:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:34 INFO InternalParquetRecordReader: block read in memory in 181 ms. row count = 3500100
15/08/21 13:51:34 INFO InternalParquetRecordReader: block read in memory in 168 ms. row count = 3501260
15/08/21 13:51:35 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6813 ms: 513.73846 rec/ms, 1027.4769 cell/ms
15/08/21 13:51:35 INFO InternalParquetRecordReader: time spent so far 2% reading (154 ms) and 97% processing (6813 ms)
15/08/21 13:51:35 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:35 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 74054
15/08/21 13:51:35 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6799 ms: 514.79626 rec/ms, 1029.5925 cell/ms
15/08/21 13:51:35 INFO InternalParquetRecordReader: time spent so far 1% reading (131 ms) and 98% processing (6799 ms)
15/08/21 13:51:35 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:35 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 74147
15/08/21 13:51:35 INFO Executor: Finished task 30.0 in stage 7.0 (TID 877). 2125 bytes result sent to driver
15/08/21 13:51:35 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 892, localhost, ANY, 1772 bytes)
15/08/21 13:51:35 INFO Executor: Running task 45.0 in stage 7.0 (TID 892)
15/08/21 13:51:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 134217728 end: 257842743 length: 123625015 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:35 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 877) in 7790 ms on localhost (30/170)
15/08/21 13:51:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573207 records.
15/08/21 13:51:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:36 INFO Executor: Finished task 31.0 in stage 7.0 (TID 878). 2125 bytes result sent to driver
15/08/21 13:51:36 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 893, localhost, ANY, 1762 bytes)
15/08/21 13:51:36 INFO Executor: Running task 46.0 in stage 7.0 (TID 893)
15/08/21 13:51:36 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 878) in 8104 ms on localhost (31/170)
15/08/21 13:51:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:36 INFO Executor: Finished task 29.0 in stage 7.0 (TID 876). 2125 bytes result sent to driver
15/08/21 13:51:36 INFO InternalParquetRecordReader: block read in memory in 239 ms. row count = 3500100
15/08/21 13:51:36 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 894, localhost, ANY, 1773 bytes)
15/08/21 13:51:36 INFO Executor: Running task 47.0 in stage 7.0 (TID 894)
15/08/21 13:51:36 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 876) in 8192 ms on localhost (32/170)
15/08/21 13:51:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501217 records.
15/08/21 13:51:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 134217728 end: 257576472 length: 123358744 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573258 records.
15/08/21 13:51:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:36 INFO InternalParquetRecordReader: block read in memory in 150 ms. row count = 3501508
15/08/21 13:51:36 INFO InternalParquetRecordReader: block read in memory in 229 ms. row count = 3501217
15/08/21 13:51:37 INFO Executor: Finished task 32.0 in stage 7.0 (TID 879). 2125 bytes result sent to driver
15/08/21 13:51:37 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 895, localhost, ANY, 1762 bytes)
15/08/21 13:51:37 INFO Executor: Running task 48.0 in stage 7.0 (TID 895)
15/08/21 13:51:37 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 879) in 6630 ms on localhost (33/170)
15/08/21 13:51:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503050 records.
15/08/21 13:51:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:37 INFO InternalParquetRecordReader: block read in memory in 109 ms. row count = 3503050
15/08/21 13:51:38 INFO InternalParquetRecordReader: Assembled and processed 3500968 records from 2 columns in 5840 ms: 599.48083 rec/ms, 1198.9617 cell/ms
15/08/21 13:51:38 INFO InternalParquetRecordReader: time spent so far 3% reading (200 ms) and 96% processing (5840 ms)
15/08/21 13:51:38 INFO InternalParquetRecordReader: at row 3500968. reading next block
15/08/21 13:51:38 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 126549
15/08/21 13:51:39 INFO Executor: Finished task 33.0 in stage 7.0 (TID 880). 2125 bytes result sent to driver
15/08/21 13:51:39 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 896, localhost, ANY, 1773 bytes)
15/08/21 13:51:39 INFO Executor: Running task 49.0 in stage 7.0 (TID 896)
15/08/21 13:51:39 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 880) in 7514 ms on localhost (34/170)
15/08/21 13:51:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 134217728 end: 256733155 length: 122515427 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3570986 records.
15/08/21 13:51:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:39 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 3503008
15/08/21 13:51:39 INFO InternalParquetRecordReader: Assembled and processed 3500932 records from 2 columns in 6345 ms: 551.7623 rec/ms, 1103.5247 cell/ms
15/08/21 13:51:39 INFO InternalParquetRecordReader: time spent so far 3% reading (251 ms) and 96% processing (6345 ms)
15/08/21 13:51:39 INFO InternalParquetRecordReader: at row 3500932. reading next block
15/08/21 13:51:39 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 72095
15/08/21 13:51:40 INFO InternalParquetRecordReader: Assembled and processed 3501165 records from 2 columns in 6021 ms: 581.49225 rec/ms, 1162.9845 cell/ms
15/08/21 13:51:40 INFO InternalParquetRecordReader: time spent so far 3% reading (234 ms) and 96% processing (6021 ms)
15/08/21 13:51:40 INFO InternalParquetRecordReader: at row 3501165. reading next block
15/08/21 13:51:40 INFO Executor: Finished task 34.0 in stage 7.0 (TID 881). 2125 bytes result sent to driver
15/08/21 13:51:40 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 897, localhost, ANY, 1761 bytes)
15/08/21 13:51:40 INFO Executor: Running task 50.0 in stage 7.0 (TID 897)
15/08/21 13:51:40 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 73051
15/08/21 13:51:40 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 881) in 8153 ms on localhost (35/170)
15/08/21 13:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501200 records.
15/08/21 13:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:40 INFO InternalParquetRecordReader: Assembled and processed 3502670 records from 2 columns in 5980 ms: 585.7308 rec/ms, 1171.4615 cell/ms
15/08/21 13:51:40 INFO InternalParquetRecordReader: time spent so far 1% reading (112 ms) and 98% processing (5980 ms)
15/08/21 13:51:40 INFO InternalParquetRecordReader: at row 3502670. reading next block
15/08/21 13:51:40 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 124055
15/08/21 13:51:40 INFO InternalParquetRecordReader: block read in memory in 119 ms. row count = 3501200
15/08/21 13:51:40 INFO InternalParquetRecordReader: Assembled and processed 3500841 records from 2 columns in 5646 ms: 620.0569 rec/ms, 1240.1138 cell/ms
15/08/21 13:51:40 INFO InternalParquetRecordReader: time spent so far 4% reading (269 ms) and 95% processing (5646 ms)
15/08/21 13:51:40 INFO InternalParquetRecordReader: at row 3500841. reading next block
15/08/21 13:51:40 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 71950
15/08/21 13:51:40 INFO Executor: Finished task 40.0 in stage 7.0 (TID 887). 2125 bytes result sent to driver
15/08/21 13:51:40 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 898, localhost, ANY, 1771 bytes)
15/08/21 13:51:40 INFO Executor: Running task 51.0 in stage 7.0 (TID 898)
15/08/21 13:51:40 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 887) in 6347 ms on localhost (36/170)
15/08/21 13:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 134217728 end: 257568635 length: 123350907 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573168 records.
15/08/21 13:51:40 INFO Executor: Finished task 36.0 in stage 7.0 (TID 883). 2125 bytes result sent to driver
15/08/21 13:51:40 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 899, localhost, ANY, 1760 bytes)
15/08/21 13:51:40 INFO Executor: Running task 52.0 in stage 7.0 (TID 899)
15/08/21 13:51:40 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 883) in 7408 ms on localhost (37/170)
15/08/21 13:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500823 records.
15/08/21 13:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:40 INFO Executor: Finished task 35.0 in stage 7.0 (TID 882). 2125 bytes result sent to driver
15/08/21 13:51:40 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 900, localhost, ANY, 1771 bytes)
15/08/21 13:51:40 INFO Executor: Running task 53.0 in stage 7.0 (TID 900)
15/08/21 13:51:40 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 882) in 7520 ms on localhost (38/170)
15/08/21 13:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 134217728 end: 257584480 length: 123366752 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573143 records.
15/08/21 13:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:40 INFO InternalParquetRecordReader: block read in memory in 92 ms. row count = 3501035
15/08/21 13:51:40 INFO InternalParquetRecordReader: block read in memory in 194 ms. row count = 3500823
15/08/21 13:51:40 INFO InternalParquetRecordReader: block read in memory in 253 ms. row count = 3501341
15/08/21 13:51:40 INFO Executor: Finished task 37.0 in stage 7.0 (TID 884). 2125 bytes result sent to driver
15/08/21 13:51:40 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 901, localhost, ANY, 1760 bytes)
15/08/21 13:51:40 INFO Executor: Running task 54.0 in stage 7.0 (TID 901)
15/08/21 13:51:40 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 884) in 7197 ms on localhost (39/170)
15/08/21 13:51:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503144 records.
15/08/21 13:51:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:41 INFO InternalParquetRecordReader: block read in memory in 80 ms. row count = 3503144
15/08/21 13:51:41 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6186 ms: 565.8099 rec/ms, 1131.6198 cell/ms
15/08/21 13:51:41 INFO InternalParquetRecordReader: time spent so far 2% reading (181 ms) and 97% processing (6186 ms)
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:41 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 72947
15/08/21 13:51:41 INFO Executor: Finished task 39.0 in stage 7.0 (TID 886). 2125 bytes result sent to driver
15/08/21 13:51:41 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 902, localhost, ANY, 1773 bytes)
15/08/21 13:51:41 INFO Executor: Running task 55.0 in stage 7.0 (TID 902)
15/08/21 13:51:41 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 886) in 7204 ms on localhost (40/170)
15/08/21 13:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 134217728 end: 259050527 length: 124832799 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3624571 records.
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:41 INFO InternalParquetRecordReader: Assembled and processed 3501508 records from 2 columns in 4805 ms: 728.72174 rec/ms, 1457.4435 cell/ms
15/08/21 13:51:41 INFO InternalParquetRecordReader: time spent so far 3% reading (150 ms) and 96% processing (4805 ms)
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 3501508. reading next block
15/08/21 13:51:41 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 71750
15/08/21 13:51:41 INFO Executor: Finished task 42.0 in stage 7.0 (TID 889). 2125 bytes result sent to driver
15/08/21 13:51:41 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 903, localhost, ANY, 1761 bytes)
15/08/21 13:51:41 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 889) in 6759 ms on localhost (41/170)
15/08/21 13:51:41 INFO InternalParquetRecordReader: block read in memory in 96 ms. row count = 3500100
15/08/21 13:51:41 INFO Executor: Running task 56.0 in stage 7.0 (TID 903)
15/08/21 13:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:41 INFO Executor: Finished task 41.0 in stage 7.0 (TID 888). 2125 bytes result sent to driver
15/08/21 13:51:41 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 904, localhost, ANY, 1775 bytes)
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:41 INFO Executor: Running task 57.0 in stage 7.0 (TID 904)
15/08/21 13:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 134217728 end: 257461890 length: 123244162 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:41 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 888) in 6955 ms on localhost (42/170)
15/08/21 13:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573988 records.
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:41 INFO Executor: Finished task 44.0 in stage 7.0 (TID 891). 2125 bytes result sent to driver
15/08/21 13:51:41 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 3500824
15/08/21 13:51:41 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 905, localhost, ANY, 1762 bytes)
15/08/21 13:51:41 INFO Executor: Running task 58.0 in stage 7.0 (TID 905)
15/08/21 13:51:41 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 891) in 6785 ms on localhost (43/170)
15/08/21 13:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:41 INFO Executor: Finished task 38.0 in stage 7.0 (TID 885). 2125 bytes result sent to driver
15/08/21 13:51:41 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 906, localhost, ANY, 1777 bytes)
15/08/21 13:51:41 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 885) in 7864 ms on localhost (44/170)
15/08/21 13:51:41 INFO Executor: Running task 59.0 in stage 7.0 (TID 906)
15/08/21 13:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 134217728 end: 257416343 length: 123198615 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:41 INFO InternalParquetRecordReader: block read in memory in 267 ms. row count = 3500100
15/08/21 13:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574223 records.
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:41 INFO InternalParquetRecordReader: block read in memory in 120 ms. row count = 3501614
15/08/21 13:51:41 INFO InternalParquetRecordReader: block read in memory in 219 ms. row count = 3500100
15/08/21 13:51:41 INFO Executor: Finished task 43.0 in stage 7.0 (TID 890). 2125 bytes result sent to driver
15/08/21 13:51:41 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 907, localhost, ANY, 1761 bytes)
15/08/21 13:51:41 INFO Executor: Running task 60.0 in stage 7.0 (TID 907)
15/08/21 13:51:41 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 890) in 7230 ms on localhost (45/170)
15/08/21 13:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:41 INFO Executor: Finished task 47.0 in stage 7.0 (TID 894). 2125 bytes result sent to driver
15/08/21 13:51:41 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 908, localhost, ANY, 1773 bytes)
15/08/21 13:51:41 INFO Executor: Running task 61.0 in stage 7.0 (TID 908)
15/08/21 13:51:41 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 134217728 end: 257883468 length: 123665740 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:41 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 894) in 5699 ms on localhost (46/170)
15/08/21 13:51:41 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
15/08/21 13:51:41 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573362 records.
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:41 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:42 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5829 ms: 600.4632 rec/ms, 1200.9264 cell/ms
15/08/21 13:51:42 INFO InternalParquetRecordReader: time spent so far 3% reading (239 ms) and 96% processing (5829 ms)
15/08/21 13:51:42 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:42 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 73107
15/08/21 13:51:42 INFO InternalParquetRecordReader: block read in memory in 294 ms. row count = 3500100
15/08/21 13:51:42 INFO InternalParquetRecordReader: block read in memory in 299 ms. row count = 3501191
15/08/21 13:51:43 INFO Executor: Finished task 46.0 in stage 7.0 (TID 893). 2125 bytes result sent to driver
15/08/21 13:51:43 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 909, localhost, ANY, 1761 bytes)
15/08/21 13:51:43 INFO Executor: Running task 62.0 in stage 7.0 (TID 909)
15/08/21 13:51:43 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 893) in 7129 ms on localhost (47/170)
15/08/21 13:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:43 INFO Executor: Finished task 45.0 in stage 7.0 (TID 892). 2125 bytes result sent to driver
15/08/21 13:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:43 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 910, localhost, ANY, 1775 bytes)
15/08/21 13:51:43 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 892) in 7508 ms on localhost (48/170)
15/08/21 13:51:43 INFO Executor: Running task 63.0 in stage 7.0 (TID 910)
15/08/21 13:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 134217728 end: 257592803 length: 123375075 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501059 records.
15/08/21 13:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573291 records.
15/08/21 13:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:43 INFO InternalParquetRecordReader: block read in memory in 129 ms. row count = 3501059
15/08/21 13:51:43 INFO InternalParquetRecordReader: block read in memory in 165 ms. row count = 3501221
15/08/21 13:51:43 INFO Executor: Finished task 48.0 in stage 7.0 (TID 895). 2125 bytes result sent to driver
15/08/21 13:51:43 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 911, localhost, ANY, 1761 bytes)
15/08/21 13:51:43 INFO Executor: Running task 64.0 in stage 7.0 (TID 911)
15/08/21 13:51:43 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 895) in 6116 ms on localhost (49/170)
15/08/21 13:51:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:43 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:43 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:43 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:43 INFO InternalParquetRecordReader: block read in memory in 103 ms. row count = 3500100
15/08/21 13:51:44 INFO InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 4954 ms: 707.107 rec/ms, 1414.214 cell/ms
15/08/21 13:51:44 INFO InternalParquetRecordReader: time spent so far 1% reading (100 ms) and 98% processing (4954 ms)
15/08/21 13:51:44 INFO InternalParquetRecordReader: at row 3503008. reading next block
15/08/21 13:51:44 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 67978
15/08/21 13:51:44 INFO Executor: Finished task 49.0 in stage 7.0 (TID 896). 2125 bytes result sent to driver
15/08/21 13:51:44 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 912, localhost, ANY, 1773 bytes)
15/08/21 13:51:44 INFO Executor: Running task 65.0 in stage 7.0 (TID 912)
15/08/21 13:51:44 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 896) in 5567 ms on localhost (50/170)
15/08/21 13:51:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 134217728 end: 258145407 length: 123927679 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574053 records.
15/08/21 13:51:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:45 INFO InternalParquetRecordReader: block read in memory in 122 ms. row count = 3500100
15/08/21 13:51:45 INFO InternalParquetRecordReader: Assembled and processed 3501035 records from 2 columns in 4496 ms: 778.69995 rec/ms, 1557.3999 cell/ms
15/08/21 13:51:45 INFO InternalParquetRecordReader: time spent so far 2% reading (92 ms) and 97% processing (4496 ms)
15/08/21 13:51:45 INFO InternalParquetRecordReader: at row 3501035. reading next block
15/08/21 13:51:45 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 72108
15/08/21 13:51:45 INFO Executor: Finished task 50.0 in stage 7.0 (TID 897). 2125 bytes result sent to driver
15/08/21 13:51:45 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 913, localhost, ANY, 1760 bytes)
15/08/21 13:51:45 INFO Executor: Running task 66.0 in stage 7.0 (TID 913)
15/08/21 13:51:45 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 897) in 5952 ms on localhost (51/170)
15/08/21 13:51:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:46 INFO InternalParquetRecordReader: block read in memory in 116 ms. row count = 3500100
15/08/21 13:51:46 INFO InternalParquetRecordReader: Assembled and processed 3501341 records from 2 columns in 5277 ms: 663.50977 rec/ms, 1327.0195 cell/ms
15/08/21 13:51:46 INFO InternalParquetRecordReader: time spent so far 4% reading (253 ms) and 95% processing (5277 ms)
15/08/21 13:51:46 INFO InternalParquetRecordReader: at row 3501341. reading next block
15/08/21 13:51:46 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 71827
15/08/21 13:51:46 INFO Executor: Finished task 52.0 in stage 7.0 (TID 899). 2125 bytes result sent to driver
15/08/21 13:51:46 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 914, localhost, ANY, 1773 bytes)
15/08/21 13:51:46 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 899) in 5629 ms on localhost (52/170)
15/08/21 13:51:46 INFO Executor: Running task 67.0 in stage 7.0 (TID 914)
15/08/21 13:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 134217728 end: 259344407 length: 125126679 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:46 INFO Executor: Finished task 53.0 in stage 7.0 (TID 900). 2125 bytes result sent to driver
15/08/21 13:51:46 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 915, localhost, ANY, 1762 bytes)
15/08/21 13:51:46 INFO Executor: Running task 68.0 in stage 7.0 (TID 915)
15/08/21 13:51:46 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 900) in 5597 ms on localhost (53/170)
15/08/21 13:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627630 records.
15/08/21 13:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:46 INFO InternalParquetRecordReader: block read in memory in 130 ms. row count = 3500100
15/08/21 13:51:46 INFO InternalParquetRecordReader: block read in memory in 198 ms. row count = 3500100
15/08/21 13:51:46 INFO Executor: Finished task 51.0 in stage 7.0 (TID 898). 2125 bytes result sent to driver
15/08/21 13:51:46 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 916, localhost, ANY, 1773 bytes)
15/08/21 13:51:46 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 898) in 6281 ms on localhost (54/170)
15/08/21 13:51:46 INFO Executor: Running task 69.0 in stage 7.0 (TID 916)
15/08/21 13:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 134217728 end: 257768338 length: 123550610 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574075 records.
15/08/21 13:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:46 INFO Executor: Finished task 54.0 in stage 7.0 (TID 901). 2125 bytes result sent to driver
15/08/21 13:51:46 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 917, localhost, ANY, 1761 bytes)
15/08/21 13:51:46 INFO Executor: Running task 70.0 in stage 7.0 (TID 917)
15/08/21 13:51:46 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 901) in 6048 ms on localhost (55/170)
15/08/21 13:51:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502727 records.
15/08/21 13:51:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:47 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500100
15/08/21 13:51:47 INFO InternalParquetRecordReader: Assembled and processed 3500824 records from 2 columns in 5463 ms: 640.82446 rec/ms, 1281.6489 cell/ms
15/08/21 13:51:47 INFO InternalParquetRecordReader: time spent so far 1% reading (102 ms) and 98% processing (5463 ms)
15/08/21 13:51:47 INFO InternalParquetRecordReader: at row 3500824. reading next block
15/08/21 13:51:47 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 73164
15/08/21 13:51:47 INFO InternalParquetRecordReader: block read in memory in 109 ms. row count = 3502727
15/08/21 13:51:47 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5839 ms: 599.4348 rec/ms, 1198.8696 cell/ms
15/08/21 13:51:47 INFO InternalParquetRecordReader: time spent so far 1% reading (96 ms) and 98% processing (5839 ms)
15/08/21 13:51:47 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:47 INFO InternalParquetRecordReader: Assembled and processed 3501614 records from 2 columns in 5361 ms: 653.1643 rec/ms, 1306.3286 cell/ms
15/08/21 13:51:47 INFO InternalParquetRecordReader: time spent so far 2% reading (120 ms) and 97% processing (5361 ms)
15/08/21 13:51:47 INFO InternalParquetRecordReader: at row 3501614. reading next block
15/08/21 13:51:47 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 72609
15/08/21 13:51:47 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 124471
15/08/21 13:51:47 INFO Executor: Finished task 58.0 in stage 7.0 (TID 905). 2125 bytes result sent to driver
15/08/21 13:51:47 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 918, localhost, ANY, 1775 bytes)
15/08/21 13:51:47 INFO Executor: Running task 71.0 in stage 7.0 (TID 918)
15/08/21 13:51:47 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 905) in 5790 ms on localhost (56/170)
15/08/21 13:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 134217728 end: 257102581 length: 122884853 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571405 records.
15/08/21 13:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:47 INFO InternalParquetRecordReader: block read in memory in 89 ms. row count = 3500100
15/08/21 13:51:47 INFO Executor: Finished task 57.0 in stage 7.0 (TID 904). 2125 bytes result sent to driver
15/08/21 13:51:47 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 919, localhost, ANY, 1762 bytes)
15/08/21 13:51:47 INFO Executor: Running task 72.0 in stage 7.0 (TID 919)
15/08/21 13:51:47 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 904) in 6117 ms on localhost (57/170)
15/08/21 13:51:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501229 records.
15/08/21 13:51:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:47 INFO InternalParquetRecordReader: block read in memory in 137 ms. row count = 3501229
15/08/21 13:51:47 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5682 ms: 615.99786 rec/ms, 1231.9957 cell/ms
15/08/21 13:51:47 INFO InternalParquetRecordReader: time spent so far 4% reading (294 ms) and 95% processing (5682 ms)
15/08/21 13:51:47 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:48 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 73262
15/08/21 13:51:48 INFO Executor: Finished task 56.0 in stage 7.0 (TID 903). 2125 bytes result sent to driver
15/08/21 13:51:48 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 920, localhost, ANY, 1775 bytes)
15/08/21 13:51:48 INFO Executor: Running task 73.0 in stage 7.0 (TID 920)
15/08/21 13:51:48 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 903) in 6963 ms on localhost (58/170)
15/08/21 13:51:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 134217728 end: 257873629 length: 123655901 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572739 records.
15/08/21 13:51:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:48 INFO InternalParquetRecordReader: block read in memory in 145 ms. row count = 3500100
15/08/21 13:51:48 INFO Executor: Finished task 55.0 in stage 7.0 (TID 902). 2125 bytes result sent to driver
15/08/21 13:51:48 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 921, localhost, ANY, 1760 bytes)
15/08/21 13:51:48 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 902) in 7600 ms on localhost (59/170)
15/08/21 13:51:48 INFO Executor: Running task 74.0 in stage 7.0 (TID 921)
15/08/21 13:51:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:48 INFO Executor: Finished task 60.0 in stage 7.0 (TID 907). 2125 bytes result sent to driver
15/08/21 13:51:48 INFO Executor: Finished task 59.0 in stage 7.0 (TID 906). 2125 bytes result sent to driver
15/08/21 13:51:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:48 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 922, localhost, ANY, 1770 bytes)
15/08/21 13:51:48 INFO Executor: Running task 75.0 in stage 7.0 (TID 922)
15/08/21 13:51:48 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 923, localhost, ANY, 1761 bytes)
15/08/21 13:51:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 134217728 end: 259470450 length: 125252722 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:48 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 906) in 7265 ms on localhost (60/170)
15/08/21 13:51:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:48 INFO Executor: Running task 76.0 in stage 7.0 (TID 923)
15/08/21 13:51:48 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 907) in 6999 ms on localhost (61/170)
15/08/21 13:51:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627768 records.
15/08/21 13:51:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:49 INFO InternalParquetRecordReader: block read in memory in 134 ms. row count = 3501462
15/08/21 13:51:49 INFO Executor: Finished task 61.0 in stage 7.0 (TID 908). 2125 bytes result sent to driver
15/08/21 13:51:49 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 924, localhost, ANY, 1774 bytes)
15/08/21 13:51:49 INFO Executor: Running task 77.0 in stage 7.0 (TID 924)
15/08/21 13:51:49 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 908) in 7146 ms on localhost (62/170)
15/08/21 13:51:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 134217728 end: 259842205 length: 125624477 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627712 records.
15/08/21 13:51:49 INFO InternalParquetRecordReader: block read in memory in 183 ms. row count = 3500100
15/08/21 13:51:49 INFO Executor: Finished task 62.0 in stage 7.0 (TID 909). 2125 bytes result sent to driver
15/08/21 13:51:49 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 925, localhost, ANY, 1761 bytes)
15/08/21 13:51:49 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 909) in 5848 ms on localhost (63/170)
15/08/21 13:51:49 INFO Executor: Running task 78.0 in stage 7.0 (TID 925)
15/08/21 13:51:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503221 records.
15/08/21 13:51:49 INFO InternalParquetRecordReader: block read in memory in 335 ms. row count = 3500100
15/08/21 13:51:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:49 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3503221
15/08/21 13:51:49 INFO InternalParquetRecordReader: block read in memory in 204 ms. row count = 3501302
15/08/21 13:51:49 INFO InternalParquetRecordReader: Assembled and processed 3501221 records from 2 columns in 6155 ms: 568.84174 rec/ms, 1137.6835 cell/ms
15/08/21 13:51:49 INFO InternalParquetRecordReader: time spent so far 2% reading (165 ms) and 97% processing (6155 ms)
15/08/21 13:51:49 INFO InternalParquetRecordReader: at row 3501221. reading next block
15/08/21 13:51:49 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 72070
15/08/21 13:51:50 INFO Executor: Finished task 64.0 in stage 7.0 (TID 911). 2125 bytes result sent to driver
15/08/21 13:51:50 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 926, localhost, ANY, 1776 bytes)
15/08/21 13:51:50 INFO Executor: Running task 79.0 in stage 7.0 (TID 926)
15/08/21 13:51:50 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 911) in 6372 ms on localhost (64/170)
15/08/21 13:51:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 134217728 end: 257071082 length: 122853354 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572089 records.
15/08/21 13:51:50 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:50 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 3500100
15/08/21 13:51:50 INFO Executor: Finished task 63.0 in stage 7.0 (TID 910). 2125 bytes result sent to driver
15/08/21 13:51:50 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 927, localhost, ANY, 1762 bytes)
15/08/21 13:51:50 INFO Executor: Running task 80.0 in stage 7.0 (TID 927)
15/08/21 13:51:50 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 910) in 7141 ms on localhost (65/170)
15/08/21 13:51:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:50 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5271 ms: 664.0296 rec/ms, 1328.0592 cell/ms
15/08/21 13:51:50 INFO InternalParquetRecordReader: time spent so far 2% reading (122 ms) and 97% processing (5271 ms)
15/08/21 13:51:50 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:50 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:50 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 73953
15/08/21 13:51:50 INFO InternalParquetRecordReader: block read in memory in 104 ms. row count = 3500100
15/08/21 13:51:51 INFO Executor: Finished task 65.0 in stage 7.0 (TID 912). 2125 bytes result sent to driver
15/08/21 13:51:51 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 928, localhost, ANY, 1774 bytes)
15/08/21 13:51:51 INFO Executor: Running task 81.0 in stage 7.0 (TID 928)
15/08/21 13:51:51 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 912) in 6112 ms on localhost (66/170)
15/08/21 13:51:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 134217728 end: 257075001 length: 122857273 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574581 records.
15/08/21 13:51:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:51 INFO InternalParquetRecordReader: block read in memory in 103 ms. row count = 3503231
15/08/21 13:51:51 INFO Executor: Finished task 66.0 in stage 7.0 (TID 913). 2125 bytes result sent to driver
15/08/21 13:51:51 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 929, localhost, ANY, 1761 bytes)
15/08/21 13:51:51 INFO Executor: Running task 82.0 in stage 7.0 (TID 929)
15/08/21 13:51:51 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 913) in 5848 ms on localhost (67/170)
15/08/21 13:51:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:51 INFO InternalParquetRecordReader: block read in memory in 118 ms. row count = 3500100
15/08/21 13:51:52 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4975 ms: 703.53766 rec/ms, 1407.0753 cell/ms
15/08/21 13:51:52 INFO InternalParquetRecordReader: time spent so far 1% reading (101 ms) and 98% processing (4975 ms)
15/08/21 13:51:52 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:52 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 73975
15/08/21 13:51:52 INFO Executor: Finished task 70.0 in stage 7.0 (TID 917). 2125 bytes result sent to driver
15/08/21 13:51:52 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 930, localhost, ANY, 1773 bytes)
15/08/21 13:51:52 INFO Executor: Running task 83.0 in stage 7.0 (TID 930)
15/08/21 13:51:52 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 917) in 5366 ms on localhost (68/170)
15/08/21 13:51:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 134217728 end: 257427527 length: 123209799 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574316 records.
15/08/21 13:51:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:52 INFO Executor: Finished task 68.0 in stage 7.0 (TID 915). 2125 bytes result sent to driver
15/08/21 13:51:52 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 931, localhost, ANY, 1761 bytes)
15/08/21 13:51:52 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 915) in 6078 ms on localhost (69/170)
15/08/21 13:51:52 INFO Executor: Running task 84.0 in stage 7.0 (TID 931)
15/08/21 13:51:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:52 INFO InternalParquetRecordReader: block read in memory in 551 ms. row count = 3503274
15/08/21 13:51:52 INFO Executor: Finished task 69.0 in stage 7.0 (TID 916). 2125 bytes result sent to driver
15/08/21 13:51:52 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 932, localhost, ANY, 1774 bytes)
15/08/21 13:51:52 INFO InternalParquetRecordReader: block read in memory in 115 ms. row count = 3500100
15/08/21 13:51:52 INFO Executor: Running task 85.0 in stage 7.0 (TID 932)
15/08/21 13:51:52 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 916) in 6111 ms on localhost (70/170)
15/08/21 13:51:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 134217728 end: 257349334 length: 123131606 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574256 records.
15/08/21 13:51:52 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5530 ms: 632.9295 rec/ms, 1265.859 cell/ms
15/08/21 13:51:52 INFO InternalParquetRecordReader: time spent so far 1% reading (89 ms) and 98% processing (5530 ms)
15/08/21 13:51:52 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:53 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 71305
15/08/21 13:51:53 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500100
15/08/21 13:51:53 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6883 ms: 508.51373 rec/ms, 1017.02747 cell/ms
15/08/21 13:51:53 INFO InternalParquetRecordReader: time spent so far 2% reading (198 ms) and 97% processing (6883 ms)
15/08/21 13:51:53 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:53 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 127530
15/08/21 13:51:53 INFO Executor: Finished task 71.0 in stage 7.0 (TID 918). 2125 bytes result sent to driver
15/08/21 13:51:53 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 933, localhost, ANY, 1761 bytes)
15/08/21 13:51:53 INFO Executor: Running task 86.0 in stage 7.0 (TID 933)
15/08/21 13:51:53 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 918) in 6231 ms on localhost (71/170)
15/08/21 13:51:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:53 INFO Executor: Finished task 72.0 in stage 7.0 (TID 919). 2125 bytes result sent to driver
15/08/21 13:51:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:53 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 934, localhost, ANY, 1773 bytes)
15/08/21 13:51:53 INFO Executor: Running task 87.0 in stage 7.0 (TID 934)
15/08/21 13:51:53 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 919) in 6121 ms on localhost (72/170)
15/08/21 13:51:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 134217728 end: 257709471 length: 123491743 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
15/08/21 13:51:53 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500100
15/08/21 13:51:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:53 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 3500100
15/08/21 13:51:53 INFO Executor: Finished task 67.0 in stage 7.0 (TID 914). 2125 bytes result sent to driver
15/08/21 13:51:53 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 935, localhost, ANY, 1760 bytes)
15/08/21 13:51:53 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 914) in 7632 ms on localhost (73/170)
15/08/21 13:51:53 INFO Executor: Running task 88.0 in stage 7.0 (TID 935)
15/08/21 13:51:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:53 INFO InternalParquetRecordReader: Assembled and processed 3501462 records from 2 columns in 4897 ms: 715.02185 rec/ms, 1430.0437 cell/ms
15/08/21 13:51:53 INFO InternalParquetRecordReader: time spent so far 2% reading (134 ms) and 97% processing (4897 ms)
15/08/21 13:51:53 INFO InternalParquetRecordReader: at row 3501462. reading next block
15/08/21 13:51:53 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 126306
15/08/21 13:51:54 INFO InternalParquetRecordReader: block read in memory in 106 ms. row count = 3500100
15/08/21 13:51:54 INFO Executor: Finished task 76.0 in stage 7.0 (TID 923). 2125 bytes result sent to driver
15/08/21 13:51:54 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 936, localhost, ANY, 1772 bytes)
15/08/21 13:51:54 INFO Executor: Running task 89.0 in stage 7.0 (TID 936)
15/08/21 13:51:54 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 923) in 5327 ms on localhost (74/170)
15/08/21 13:51:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 134217728 end: 259741355 length: 125523627 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:54 INFO InternalParquetRecordReader: Assembled and processed 3501302 records from 2 columns in 4815 ms: 727.1655 rec/ms, 1454.331 cell/ms
15/08/21 13:51:54 INFO InternalParquetRecordReader: time spent so far 4% reading (204 ms) and 95% processing (4815 ms)
15/08/21 13:51:54 INFO InternalParquetRecordReader: at row 3501302. reading next block
15/08/21 13:51:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627697 records.
15/08/21 13:51:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:54 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 126410
15/08/21 13:51:54 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3500100
15/08/21 13:51:54 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5879 ms: 595.3563 rec/ms, 1190.7126 cell/ms
15/08/21 13:51:54 INFO InternalParquetRecordReader: time spent so far 2% reading (145 ms) and 97% processing (5879 ms)
15/08/21 13:51:54 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:54 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 72639
15/08/21 13:51:54 INFO Executor: Finished task 75.0 in stage 7.0 (TID 922). 2125 bytes result sent to driver
15/08/21 13:51:54 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 937, localhost, ANY, 1761 bytes)
15/08/21 13:51:54 INFO Executor: Running task 90.0 in stage 7.0 (TID 937)
15/08/21 13:51:54 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 922) in 5602 ms on localhost (75/170)
15/08/21 13:51:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:54 INFO InternalParquetRecordReader: block read in memory in 122 ms. row count = 3500100
15/08/21 13:51:54 INFO Executor: Finished task 77.0 in stage 7.0 (TID 924). 2125 bytes result sent to driver
15/08/21 13:51:54 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 938, localhost, ANY, 1776 bytes)
15/08/21 13:51:54 INFO Executor: Running task 91.0 in stage 7.0 (TID 938)
15/08/21 13:51:54 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 924) in 5693 ms on localhost (76/170)
15/08/21 13:51:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 134217728 end: 257333395 length: 123115667 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573967 records.
15/08/21 13:51:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:54 INFO InternalParquetRecordReader: block read in memory in 94 ms. row count = 3500100
15/08/21 13:51:54 INFO Executor: Finished task 78.0 in stage 7.0 (TID 925). 2125 bytes result sent to driver
15/08/21 13:51:54 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 939, localhost, ANY, 1761 bytes)
15/08/21 13:51:54 INFO Executor: Running task 92.0 in stage 7.0 (TID 939)
15/08/21 13:51:54 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 925) in 5806 ms on localhost (77/170)
15/08/21 13:51:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:54 INFO Executor: Finished task 73.0 in stage 7.0 (TID 920). 2125 bytes result sent to driver
15/08/21 13:51:54 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 940, localhost, ANY, 1776 bytes)
15/08/21 13:51:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:54 INFO Executor: Running task 93.0 in stage 7.0 (TID 940)
15/08/21 13:51:54 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 920) in 6717 ms on localhost (78/170)
15/08/21 13:51:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 134217728 end: 257330329 length: 123112601 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574008 records.
15/08/21 13:51:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:55 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500100
15/08/21 13:51:55 INFO InternalParquetRecordReader: block read in memory in 193 ms. row count = 3500100
15/08/21 13:51:55 INFO Executor: Finished task 74.0 in stage 7.0 (TID 921). 2125 bytes result sent to driver
15/08/21 13:51:55 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 941, localhost, ANY, 1762 bytes)
15/08/21 13:51:55 INFO Executor: Running task 94.0 in stage 7.0 (TID 941)
15/08/21 13:51:55 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 921) in 6458 ms on localhost (79/170)
15/08/21 13:51:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:55 INFO InternalParquetRecordReader: block read in memory in 103 ms. row count = 3500100
15/08/21 13:51:56 INFO Executor: Finished task 80.0 in stage 7.0 (TID 927). 2125 bytes result sent to driver
15/08/21 13:51:56 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 942, localhost, ANY, 1772 bytes)
15/08/21 13:51:56 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 927) in 6165 ms on localhost (80/170)
15/08/21 13:51:56 INFO Executor: Running task 95.0 in stage 7.0 (TID 942)
15/08/21 13:51:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 134217728 end: 257756022 length: 123538294 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574308 records.
15/08/21 13:51:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:56 INFO InternalParquetRecordReader: Assembled and processed 3503231 records from 2 columns in 5393 ms: 649.58856 rec/ms, 1299.1771 cell/ms
15/08/21 13:51:56 INFO InternalParquetRecordReader: time spent so far 1% reading (103 ms) and 98% processing (5393 ms)
15/08/21 13:51:56 INFO InternalParquetRecordReader: at row 3503231. reading next block
15/08/21 13:51:56 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 71350
15/08/21 13:51:56 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 3500100
15/08/21 13:51:56 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6588 ms: 531.2842 rec/ms, 1062.5684 cell/ms
15/08/21 13:51:56 INFO InternalParquetRecordReader: time spent so far 1% reading (105 ms) and 98% processing (6588 ms)
15/08/21 13:51:56 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:56 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 71989
15/08/21 13:51:57 INFO InternalParquetRecordReader: Assembled and processed 3503274 records from 2 columns in 4318 ms: 811.31866 rec/ms, 1622.6373 cell/ms
15/08/21 13:51:57 INFO InternalParquetRecordReader: time spent so far 11% reading (551 ms) and 88% processing (4318 ms)
15/08/21 13:51:57 INFO InternalParquetRecordReader: at row 3503274. reading next block
15/08/21 13:51:57 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 71042
15/08/21 13:51:57 INFO Executor: Finished task 81.0 in stage 7.0 (TID 928). 2125 bytes result sent to driver
15/08/21 13:51:57 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 943, localhost, ANY, 1761 bytes)
15/08/21 13:51:57 INFO Executor: Running task 96.0 in stage 7.0 (TID 943)
15/08/21 13:51:57 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 928) in 6206 ms on localhost (81/170)
15/08/21 13:51:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:57 INFO Executor: Finished task 82.0 in stage 7.0 (TID 929). 2125 bytes result sent to driver
15/08/21 13:51:57 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 944, localhost, ANY, 1774 bytes)
15/08/21 13:51:57 INFO Executor: Running task 97.0 in stage 7.0 (TID 944)
15/08/21 13:51:57 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 929) in 5612 ms on localhost (82/170)
15/08/21 13:51:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 134217728 end: 257832393 length: 123614665 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:57 INFO InternalParquetRecordReader: block read in memory in 82 ms. row count = 3500100
15/08/21 13:51:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573690 records.
15/08/21 13:51:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:57 INFO Executor: Finished task 79.0 in stage 7.0 (TID 926). 2125 bytes result sent to driver
15/08/21 13:51:57 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 945, localhost, ANY, 1762 bytes)
15/08/21 13:51:57 INFO Executor: Running task 98.0 in stage 7.0 (TID 945)
15/08/21 13:51:57 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 926) in 7413 ms on localhost (83/170)
15/08/21 13:51:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501124 records.
15/08/21 13:51:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:57 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500612
15/08/21 13:51:57 INFO InternalParquetRecordReader: block read in memory in 122 ms. row count = 3501124
15/08/21 13:51:57 INFO Executor: Finished task 83.0 in stage 7.0 (TID 930). 2125 bytes result sent to driver
15/08/21 13:51:57 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 946, localhost, ANY, 1775 bytes)
15/08/21 13:51:57 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 930) in 5460 ms on localhost (84/170)
15/08/21 13:51:57 INFO Executor: Running task 99.0 in stage 7.0 (TID 946)
15/08/21 13:51:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 134217728 end: 257486081 length: 123268353 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573157 records.
15/08/21 13:51:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:57 INFO InternalParquetRecordReader: block read in memory in 97 ms. row count = 3500100
15/08/21 13:51:58 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4955 ms: 706.3774 rec/ms, 1412.7548 cell/ms
15/08/21 13:51:58 INFO InternalParquetRecordReader: time spent so far 1% reading (101 ms) and 98% processing (4955 ms)
15/08/21 13:51:58 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:58 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 74156
15/08/21 13:51:58 INFO Executor: Finished task 84.0 in stage 7.0 (TID 931). 2125 bytes result sent to driver
15/08/21 13:51:58 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 947, localhost, ANY, 1761 bytes)
15/08/21 13:51:58 INFO Executor: Running task 100.0 in stage 7.0 (TID 947)
15/08/21 13:51:58 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 931) in 6088 ms on localhost (85/170)
15/08/21 13:51:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:58 INFO InternalParquetRecordReader: block read in memory in 87 ms. row count = 3500100
15/08/21 13:51:58 INFO Executor: Finished task 85.0 in stage 7.0 (TID 932). 2125 bytes result sent to driver
15/08/21 13:51:58 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 948, localhost, ANY, 1773 bytes)
15/08/21 13:51:58 INFO Executor: Running task 101.0 in stage 7.0 (TID 948)
15/08/21 13:51:58 INFO Executor: Finished task 86.0 in stage 7.0 (TID 933). 2125 bytes result sent to driver
15/08/21 13:51:58 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 949, localhost, ANY, 1762 bytes)
15/08/21 13:51:58 INFO Executor: Running task 102.0 in stage 7.0 (TID 949)
15/08/21 13:51:58 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 932) in 5648 ms on localhost (86/170)
15/08/21 13:51:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 134217728 end: 257368738 length: 123151010 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:58 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 933) in 5080 ms on localhost (87/170)
15/08/21 13:51:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
15/08/21 13:51:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:51:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:59 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5286 ms: 662.14526 rec/ms, 1324.2905 cell/ms
15/08/21 13:51:59 INFO InternalParquetRecordReader: time spent so far 1% reading (75 ms) and 98% processing (5286 ms)
15/08/21 13:51:59 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:59 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 74054
15/08/21 13:51:59 INFO Executor: Finished task 88.0 in stage 7.0 (TID 935). 2125 bytes result sent to driver
15/08/21 13:51:59 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 950, localhost, ANY, 1774 bytes)
15/08/21 13:51:59 INFO Executor: Running task 103.0 in stage 7.0 (TID 950)
15/08/21 13:51:59 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 935) in 5193 ms on localhost (88/170)
15/08/21 13:51:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 134217728 end: 257317174 length: 123099446 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573826 records.
15/08/21 13:51:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:59 INFO InternalParquetRecordReader: block read in memory in 438 ms. row count = 3500100
15/08/21 13:51:59 INFO InternalParquetRecordReader: block read in memory in 168 ms. row count = 3500100
15/08/21 13:51:59 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 3500100
15/08/21 13:51:59 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4937 ms: 708.9528 rec/ms, 1417.9056 cell/ms
15/08/21 13:51:59 INFO InternalParquetRecordReader: time spent so far 1% reading (65 ms) and 98% processing (4937 ms)
15/08/21 13:51:59 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:59 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 127597
15/08/21 13:51:59 INFO Executor: Finished task 87.0 in stage 7.0 (TID 934). 2125 bytes result sent to driver
15/08/21 13:51:59 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 951, localhost, ANY, 1761 bytes)
15/08/21 13:51:59 INFO Executor: Running task 104.0 in stage 7.0 (TID 951)
15/08/21 13:51:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:59 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 934) in 5952 ms on localhost (89/170)
15/08/21 13:51:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500786 records.
15/08/21 13:51:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:59 INFO Executor: Finished task 89.0 in stage 7.0 (TID 936). 2125 bytes result sent to driver
15/08/21 13:51:59 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 952, localhost, ANY, 1773 bytes)
15/08/21 13:51:59 INFO Executor: Running task 105.0 in stage 7.0 (TID 952)
15/08/21 13:51:59 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 936) in 5515 ms on localhost (90/170)
15/08/21 13:51:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 134217728 end: 257566042 length: 123348314 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:51:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:51:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573221 records.
15/08/21 13:51:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:51:59 INFO InternalParquetRecordReader: block read in memory in 88 ms. row count = 3500786
15/08/21 13:51:59 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4829 ms: 724.8085 rec/ms, 1449.617 cell/ms
15/08/21 13:51:59 INFO InternalParquetRecordReader: time spent so far 1% reading (94 ms) and 98% processing (4829 ms)
15/08/21 13:51:59 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:51:59 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 73867
15/08/21 13:51:59 INFO InternalParquetRecordReader: block read in memory in 227 ms. row count = 3501171
15/08/21 13:52:00 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5037 ms: 694.8779 rec/ms, 1389.7559 cell/ms
15/08/21 13:52:00 INFO InternalParquetRecordReader: time spent so far 1% reading (91 ms) and 98% processing (5037 ms)
15/08/21 13:52:00 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:00 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 73908
15/08/21 13:52:00 INFO Executor: Finished task 90.0 in stage 7.0 (TID 937). 2125 bytes result sent to driver
15/08/21 13:52:00 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 953, localhost, ANY, 1762 bytes)
15/08/21 13:52:00 INFO Executor: Running task 106.0 in stage 7.0 (TID 953)
15/08/21 13:52:00 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 937) in 5812 ms on localhost (91/170)
15/08/21 13:52:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502806 records.
15/08/21 13:52:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:00 INFO InternalParquetRecordReader: block read in memory in 132 ms. row count = 3502806
15/08/21 13:52:00 INFO Executor: Finished task 94.0 in stage 7.0 (TID 941). 2125 bytes result sent to driver
15/08/21 13:52:00 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 954, localhost, ANY, 1775 bytes)
15/08/21 13:52:00 INFO Executor: Running task 107.0 in stage 7.0 (TID 954)
15/08/21 13:52:00 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 941) in 5316 ms on localhost (92/170)
15/08/21 13:52:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 134217728 end: 257458240 length: 123240512 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571385 records.
15/08/21 13:52:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:00 INFO Executor: Finished task 92.0 in stage 7.0 (TID 939). 2125 bytes result sent to driver
15/08/21 13:52:00 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 955, localhost, ANY, 1762 bytes)
15/08/21 13:52:00 INFO Executor: Running task 108.0 in stage 7.0 (TID 955)
15/08/21 13:52:00 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 939) in 5657 ms on localhost (93/170)
15/08/21 13:52:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:00 INFO InternalParquetRecordReader: block read in memory in 193 ms. row count = 3500100
15/08/21 13:52:00 INFO Executor: Finished task 93.0 in stage 7.0 (TID 940). 2125 bytes result sent to driver
15/08/21 13:52:00 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 956, localhost, ANY, 1773 bytes)
15/08/21 13:52:00 INFO Executor: Running task 109.0 in stage 7.0 (TID 956)
15/08/21 13:52:00 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 940) in 5855 ms on localhost (94/170)
15/08/21 13:52:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 134217728 end: 257458739 length: 123241011 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574219 records.
15/08/21 13:52:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:00 INFO InternalParquetRecordReader: block read in memory in 125 ms. row count = 3501364
15/08/21 13:52:01 INFO InternalParquetRecordReader: block read in memory in 467 ms. row count = 3500100
15/08/21 13:52:01 INFO Executor: Finished task 91.0 in stage 7.0 (TID 938). 2125 bytes result sent to driver
15/08/21 13:52:01 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 957, localhost, ANY, 1762 bytes)
15/08/21 13:52:01 INFO Executor: Running task 110.0 in stage 7.0 (TID 957)
15/08/21 13:52:01 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 938) in 6415 ms on localhost (95/170)
15/08/21 13:52:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501448 records.
15/08/21 13:52:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:01 INFO InternalParquetRecordReader: block read in memory in 139 ms. row count = 3501448
15/08/21 13:52:01 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5225 ms: 669.8756 rec/ms, 1339.7512 cell/ms
15/08/21 13:52:01 INFO InternalParquetRecordReader: time spent so far 1% reading (74 ms) and 98% processing (5225 ms)
15/08/21 13:52:01 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:01 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 74208
15/08/21 13:52:02 INFO InternalParquetRecordReader: Assembled and processed 3500612 records from 2 columns in 4753 ms: 736.5058 rec/ms, 1473.0116 cell/ms
15/08/21 13:52:02 INFO InternalParquetRecordReader: time spent so far 2% reading (101 ms) and 97% processing (4753 ms)
15/08/21 13:52:02 INFO InternalParquetRecordReader: at row 3500612. reading next block
15/08/21 13:52:02 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 73078
15/08/21 13:52:02 INFO Executor: Finished task 95.0 in stage 7.0 (TID 942). 2125 bytes result sent to driver
15/08/21 13:52:02 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 958, localhost, ANY, 1774 bytes)
15/08/21 13:52:02 INFO Executor: Running task 111.0 in stage 7.0 (TID 958)
15/08/21 13:52:02 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 942) in 5853 ms on localhost (96/170)
15/08/21 13:52:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 134217728 end: 257439181 length: 123221453 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572926 records.
15/08/21 13:52:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:02 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 3500100
15/08/21 13:52:02 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4831 ms: 724.50836 rec/ms, 1449.0167 cell/ms
15/08/21 13:52:02 INFO InternalParquetRecordReader: time spent so far 1% reading (97 ms) and 98% processing (4831 ms)
15/08/21 13:52:02 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:02 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 73057
15/08/21 13:52:02 INFO Executor: Finished task 97.0 in stage 7.0 (TID 944). 2125 bytes result sent to driver
15/08/21 13:52:02 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 959, localhost, ANY, 1762 bytes)
15/08/21 13:52:02 INFO Executor: Running task 112.0 in stage 7.0 (TID 959)
15/08/21 13:52:02 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 944) in 5385 ms on localhost (97/170)
15/08/21 13:52:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:02 INFO InternalParquetRecordReader: block read in memory in 94 ms. row count = 3500100
15/08/21 13:52:03 INFO Executor: Finished task 99.0 in stage 7.0 (TID 946). 2125 bytes result sent to driver
15/08/21 13:52:03 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 960, localhost, ANY, 1773 bytes)
15/08/21 13:52:03 INFO Executor: Running task 113.0 in stage 7.0 (TID 960)
15/08/21 13:52:03 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 946) in 5400 ms on localhost (98/170)
15/08/21 13:52:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 134217728 end: 181459518 length: 47241790 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1466882 records.
15/08/21 13:52:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:03 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 1466882
15/08/21 13:52:03 INFO Executor: Finished task 96.0 in stage 7.0 (TID 943). 2125 bytes result sent to driver
15/08/21 13:52:03 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 961, localhost, ANY, 1761 bytes)
15/08/21 13:52:03 INFO Executor: Running task 114.0 in stage 7.0 (TID 961)
15/08/21 13:52:03 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 943) in 6080 ms on localhost (99/170)
15/08/21 13:52:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:03 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 3500100
15/08/21 13:52:03 INFO Executor: Finished task 98.0 in stage 7.0 (TID 945). 2125 bytes result sent to driver
15/08/21 13:52:03 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 962, localhost, ANY, 1772 bytes)
15/08/21 13:52:03 INFO Executor: Running task 115.0 in stage 7.0 (TID 962)
15/08/21 13:52:03 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 945) in 5961 ms on localhost (100/170)
15/08/21 13:52:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 134217728 end: 259458210 length: 125240482 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627682 records.
15/08/21 13:52:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:03 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 3503132
15/08/21 13:52:03 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4475 ms: 782.14526 rec/ms, 1564.2905 cell/ms
15/08/21 13:52:03 INFO InternalParquetRecordReader: time spent so far 8% reading (438 ms) and 91% processing (4475 ms)
15/08/21 13:52:03 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:03 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 73871
15/08/21 13:52:03 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4446 ms: 787.24695 rec/ms, 1574.4939 cell/ms
15/08/21 13:52:03 INFO InternalParquetRecordReader: time spent so far 2% reading (95 ms) and 97% processing (4446 ms)
15/08/21 13:52:03 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:03 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 73726
15/08/21 13:52:03 INFO Executor: Finished task 101.0 in stage 7.0 (TID 948). 2125 bytes result sent to driver
15/08/21 13:52:03 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 963, localhost, ANY, 1761 bytes)
15/08/21 13:52:03 INFO Executor: Running task 116.0 in stage 7.0 (TID 963)
15/08/21 13:52:03 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 948) in 5341 ms on localhost (101/170)
15/08/21 13:52:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:04 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500100
15/08/21 13:52:04 INFO Executor: Finished task 103.0 in stage 7.0 (TID 950). 2125 bytes result sent to driver
15/08/21 13:52:04 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 964, localhost, ANY, 1772 bytes)
15/08/21 13:52:04 INFO Executor: Running task 117.0 in stage 7.0 (TID 964)
15/08/21 13:52:04 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 950) in 5053 ms on localhost (102/170)
15/08/21 13:52:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 134217728 end: 257467186 length: 123249458 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574192 records.
15/08/21 13:52:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:04 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3501130
15/08/21 13:52:04 INFO InternalParquetRecordReader: Assembled and processed 3501171 records from 2 columns in 4444 ms: 787.8423 rec/ms, 1575.6846 cell/ms
15/08/21 13:52:04 INFO InternalParquetRecordReader: time spent so far 4% reading (227 ms) and 95% processing (4444 ms)
15/08/21 13:52:04 INFO InternalParquetRecordReader: at row 3501171. reading next block
15/08/21 13:52:04 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 72050
15/08/21 13:52:04 INFO Executor: Finished task 104.0 in stage 7.0 (TID 951). 2125 bytes result sent to driver
15/08/21 13:52:04 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 965, localhost, ANY, 1760 bytes)
15/08/21 13:52:04 INFO Executor: Running task 118.0 in stage 7.0 (TID 965)
15/08/21 13:52:04 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 951) in 5084 ms on localhost (103/170)
15/08/21 13:52:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500631 records.
15/08/21 13:52:04 INFO Executor: Finished task 100.0 in stage 7.0 (TID 947). 2125 bytes result sent to driver
15/08/21 13:52:04 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 966, localhost, ANY, 1772 bytes)
15/08/21 13:52:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:04 INFO Executor: Running task 119.0 in stage 7.0 (TID 966)
15/08/21 13:52:04 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 947) in 6212 ms on localhost (104/170)
15/08/21 13:52:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 134217728 end: 260141700 length: 125923972 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3648307 records.
15/08/21 13:52:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:04 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500631
15/08/21 13:52:04 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 3500100
15/08/21 13:52:04 INFO Executor: Finished task 102.0 in stage 7.0 (TID 949). 2125 bytes result sent to driver
15/08/21 13:52:04 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 967, localhost, ANY, 1760 bytes)
15/08/21 13:52:04 INFO Executor: Running task 120.0 in stage 7.0 (TID 967)
15/08/21 13:52:04 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 949) in 6148 ms on localhost (105/170)
15/08/21 13:52:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501115 records.
15/08/21 13:52:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:04 INFO Executor: Finished task 105.0 in stage 7.0 (TID 952). 2125 bytes result sent to driver
15/08/21 13:52:04 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 968, localhost, ANY, 1772 bytes)
15/08/21 13:52:04 INFO Executor: Running task 121.0 in stage 7.0 (TID 968)
15/08/21 13:52:04 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 952) in 5147 ms on localhost (106/170)
15/08/21 13:52:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 134217728 end: 259582440 length: 125364712 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626900 records.
15/08/21 13:52:04 INFO InternalParquetRecordReader: block read in memory in 47 ms. row count = 3501115
15/08/21 13:52:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:04 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3501235
15/08/21 13:52:05 INFO Executor: Finished task 106.0 in stage 7.0 (TID 953). 2125 bytes result sent to driver
15/08/21 13:52:05 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 969, localhost, ANY, 1761 bytes)
15/08/21 13:52:05 INFO Executor: Running task 122.0 in stage 7.0 (TID 969)
15/08/21 13:52:05 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 953) in 5466 ms on localhost (107/170)
15/08/21 13:52:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:05 INFO Executor: Finished task 113.0 in stage 7.0 (TID 960). 2125 bytes result sent to driver
15/08/21 13:52:05 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 970, localhost, ANY, 1774 bytes)
15/08/21 13:52:05 INFO Executor: Running task 123.0 in stage 7.0 (TID 970)
15/08/21 13:52:05 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 960) in 2620 ms on localhost (108/170)
15/08/21 13:52:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 134217728 end: 257838232 length: 123620504 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574036 records.
15/08/21 13:52:05 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 3500100
15/08/21 13:52:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:05 INFO InternalParquetRecordReader: Assembled and processed 3501364 records from 2 columns in 4891 ms: 715.87897 rec/ms, 1431.7579 cell/ms
15/08/21 13:52:05 INFO InternalParquetRecordReader: time spent so far 2% reading (125 ms) and 97% processing (4891 ms)
15/08/21 13:52:05 INFO InternalParquetRecordReader: at row 3501364. reading next block
15/08/21 13:52:05 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 72855
15/08/21 13:52:05 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 3501487
15/08/21 13:52:06 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5269 ms: 664.2817 rec/ms, 1328.5634 cell/ms
15/08/21 13:52:06 INFO InternalParquetRecordReader: time spent so far 3% reading (193 ms) and 96% processing (5269 ms)
15/08/21 13:52:06 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 71285
15/08/21 13:52:06 INFO Executor: Finished task 109.0 in stage 7.0 (TID 956). 2125 bytes result sent to driver
15/08/21 13:52:06 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 971, localhost, ANY, 1761 bytes)
15/08/21 13:52:06 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 956) in 5740 ms on localhost (109/170)
15/08/21 13:52:06 INFO Executor: Running task 124.0 in stage 7.0 (TID 971)
15/08/21 13:52:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501235 records.
15/08/21 13:52:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:06 INFO Executor: Finished task 107.0 in stage 7.0 (TID 954). 2125 bytes result sent to driver
15/08/21 13:52:06 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 972, localhost, ANY, 1773 bytes)
15/08/21 13:52:06 INFO Executor: Running task 125.0 in stage 7.0 (TID 972)
15/08/21 13:52:06 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 954) in 6147 ms on localhost (110/170)
15/08/21 13:52:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 134217728 end: 257849235 length: 123631507 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:06 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 3501235
15/08/21 13:52:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573259 records.
15/08/21 13:52:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:06 INFO InternalParquetRecordReader: block read in memory in 77 ms. row count = 3500100
15/08/21 13:52:07 INFO Executor: Finished task 110.0 in stage 7.0 (TID 957). 2125 bytes result sent to driver
15/08/21 13:52:07 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 973, localhost, ANY, 1761 bytes)
15/08/21 13:52:07 INFO Executor: Running task 126.0 in stage 7.0 (TID 973)
15/08/21 13:52:07 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 957) in 6016 ms on localhost (111/170)
15/08/21 13:52:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502933 records.
15/08/21 13:52:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:07 INFO Executor: Finished task 108.0 in stage 7.0 (TID 955). 2125 bytes result sent to driver
15/08/21 13:52:07 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 974, localhost, ANY, 1774 bytes)
15/08/21 13:52:07 INFO Executor: Running task 127.0 in stage 7.0 (TID 974)
15/08/21 13:52:07 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 955) in 6642 ms on localhost (112/170)
15/08/21 13:52:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 134217728 end: 257181348 length: 122963620 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571471 records.
15/08/21 13:52:07 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 3502933
15/08/21 13:52:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:07 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4715 ms: 742.333 rec/ms, 1484.666 cell/ms
15/08/21 13:52:07 INFO InternalParquetRecordReader: time spent so far 1% reading (86 ms) and 98% processing (4715 ms)
15/08/21 13:52:07 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:07 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 72826
15/08/21 13:52:07 INFO InternalParquetRecordReader: block read in memory in 99 ms. row count = 3501332
15/08/21 13:52:07 INFO Executor: Finished task 111.0 in stage 7.0 (TID 958). 2125 bytes result sent to driver
15/08/21 13:52:07 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 975, localhost, ANY, 1760 bytes)
15/08/21 13:52:07 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 958) in 5392 ms on localhost (113/170)
15/08/21 13:52:07 INFO Executor: Running task 128.0 in stage 7.0 (TID 975)
15/08/21 13:52:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500833 records.
15/08/21 13:52:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:08 INFO InternalParquetRecordReader: block read in memory in 121 ms. row count = 3500833
15/08/21 13:52:08 INFO Executor: Finished task 112.0 in stage 7.0 (TID 959). 2125 bytes result sent to driver
15/08/21 13:52:08 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 976, localhost, ANY, 1772 bytes)
15/08/21 13:52:08 INFO Executor: Running task 129.0 in stage 7.0 (TID 976)
15/08/21 13:52:08 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 959) in 5436 ms on localhost (114/170)
15/08/21 13:52:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 134217728 end: 257473792 length: 123256064 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573329 records.
15/08/21 13:52:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:08 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 3500100
15/08/21 13:52:08 INFO InternalParquetRecordReader: Assembled and processed 3503132 records from 2 columns in 5037 ms: 695.47986 rec/ms, 1390.9597 cell/ms
15/08/21 13:52:08 INFO InternalParquetRecordReader: time spent so far 0% reading (32 ms) and 99% processing (5037 ms)
15/08/21 13:52:08 INFO InternalParquetRecordReader: at row 3503132. reading next block
15/08/21 13:52:08 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 124550
15/08/21 13:52:08 INFO InternalParquetRecordReader: Assembled and processed 3501130 records from 2 columns in 4757 ms: 735.99536 rec/ms, 1471.9907 cell/ms
15/08/21 13:52:08 INFO InternalParquetRecordReader: time spent so far 1% reading (65 ms) and 98% processing (4757 ms)
15/08/21 13:52:08 INFO InternalParquetRecordReader: at row 3501130. reading next block
15/08/21 13:52:08 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 73062
15/08/21 13:52:09 INFO Executor: Finished task 115.0 in stage 7.0 (TID 962). 2125 bytes result sent to driver
15/08/21 13:52:09 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 977, localhost, ANY, 1761 bytes)
15/08/21 13:52:09 INFO Executor: Running task 130.0 in stage 7.0 (TID 977)
15/08/21 13:52:09 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 962) in 5881 ms on localhost (115/170)
15/08/21 13:52:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502559 records.
15/08/21 13:52:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:09 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 3502559
15/08/21 13:52:09 INFO Executor: Finished task 117.0 in stage 7.0 (TID 964). 2125 bytes result sent to driver
15/08/21 13:52:09 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 978, localhost, ANY, 1774 bytes)
15/08/21 13:52:09 INFO Executor: Running task 131.0 in stage 7.0 (TID 978)
15/08/21 13:52:09 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 964) in 5671 ms on localhost (116/170)
15/08/21 13:52:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 134217728 end: 257455806 length: 123238078 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:09 INFO Executor: Finished task 114.0 in stage 7.0 (TID 961). 2125 bytes result sent to driver
15/08/21 13:52:09 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 979, localhost, ANY, 1762 bytes)
15/08/21 13:52:09 INFO Executor: Running task 132.0 in stage 7.0 (TID 979)
15/08/21 13:52:09 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 961) in 6433 ms on localhost (117/170)
15/08/21 13:52:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571559 records.
15/08/21 13:52:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501043 records.
15/08/21 13:52:09 INFO Executor: Finished task 116.0 in stage 7.0 (TID 963). 2125 bytes result sent to driver
15/08/21 13:52:09 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 980, localhost, ANY, 1776 bytes)
15/08/21 13:52:09 INFO Executor: Running task 133.0 in stage 7.0 (TID 980)
15/08/21 13:52:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 134217728 end: 257547934 length: 123330206 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:09 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 963) in 5871 ms on localhost (118/170)
15/08/21 13:52:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573366 records.
15/08/21 13:52:09 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 3501043
15/08/21 13:52:09 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 3500100
15/08/21 13:52:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:09 INFO InternalParquetRecordReader: block read in memory in 47 ms. row count = 3501786
15/08/21 13:52:10 INFO InternalParquetRecordReader: Assembled and processed 3501235 records from 2 columns in 5072 ms: 690.3066 rec/ms, 1380.6132 cell/ms
15/08/21 13:52:10 INFO InternalParquetRecordReader: time spent so far 1% reading (63 ms) and 98% processing (5072 ms)
15/08/21 13:52:10 INFO InternalParquetRecordReader: at row 3501235. reading next block
15/08/21 13:52:10 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 125665
15/08/21 13:52:10 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5349 ms: 654.3466 rec/ms, 1308.6932 cell/ms
15/08/21 13:52:10 INFO InternalParquetRecordReader: time spent so far 0% reading (38 ms) and 99% processing (5349 ms)
15/08/21 13:52:10 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:10 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 148207
15/08/21 13:52:10 INFO Executor: Finished task 118.0 in stage 7.0 (TID 965). 2125 bytes result sent to driver
15/08/21 13:52:10 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 981, localhost, ANY, 1762 bytes)
15/08/21 13:52:10 INFO Executor: Running task 134.0 in stage 7.0 (TID 981)
15/08/21 13:52:10 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 965) in 5532 ms on localhost (119/170)
15/08/21 13:52:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:10 INFO Executor: Finished task 120.0 in stage 7.0 (TID 967). 2125 bytes result sent to driver
15/08/21 13:52:10 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 982, localhost, ANY, 1773 bytes)
15/08/21 13:52:10 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 967) in 5445 ms on localhost (120/170)
15/08/21 13:52:10 INFO Executor: Running task 135.0 in stage 7.0 (TID 982)
15/08/21 13:52:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 134217728 end: 257790576 length: 123572848 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574145 records.
15/08/21 13:52:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:10 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 3500100
15/08/21 13:52:10 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
15/08/21 13:52:10 INFO Executor: Finished task 121.0 in stage 7.0 (TID 968). 2125 bytes result sent to driver
15/08/21 13:52:10 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 983, localhost, ANY, 1761 bytes)
15/08/21 13:52:10 INFO Executor: Running task 136.0 in stage 7.0 (TID 983)
15/08/21 13:52:10 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 968) in 5696 ms on localhost (121/170)
15/08/21 13:52:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501169 records.
15/08/21 13:52:10 INFO InternalParquetRecordReader: Assembled and processed 3501487 records from 2 columns in 4626 ms: 756.9146 rec/ms, 1513.8292 cell/ms
15/08/21 13:52:10 INFO InternalParquetRecordReader: time spent so far 2% reading (98 ms) and 97% processing (4626 ms)
15/08/21 13:52:10 INFO InternalParquetRecordReader: at row 3501487. reading next block
15/08/21 13:52:10 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 72549
15/08/21 13:52:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:10 INFO Executor: Finished task 119.0 in stage 7.0 (TID 966). 2125 bytes result sent to driver
15/08/21 13:52:10 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 984, localhost, ANY, 1773 bytes)
15/08/21 13:52:10 INFO Executor: Running task 137.0 in stage 7.0 (TID 984)
15/08/21 13:52:10 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 966) in 5924 ms on localhost (122/170)
15/08/21 13:52:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 134217728 end: 257571649 length: 123353921 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573313 records.
15/08/21 13:52:10 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 3501169
15/08/21 13:52:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:10 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 3501584
15/08/21 13:52:10 INFO Executor: Finished task 123.0 in stage 7.0 (TID 970). 2125 bytes result sent to driver
15/08/21 13:52:10 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 985, localhost, ANY, 1761 bytes)
15/08/21 13:52:10 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 970) in 5132 ms on localhost (123/170)
15/08/21 13:52:10 INFO Executor: Running task 138.0 in stage 7.0 (TID 985)
15/08/21 13:52:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501195 records.
15/08/21 13:52:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:11 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 3501195
15/08/21 13:52:11 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4496 ms: 778.492 rec/ms, 1556.984 cell/ms
15/08/21 13:52:11 INFO InternalParquetRecordReader: time spent so far 1% reading (77 ms) and 98% processing (4496 ms)
15/08/21 13:52:11 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 73159
15/08/21 13:52:12 INFO InternalParquetRecordReader: Assembled and processed 3501332 records from 2 columns in 4684 ms: 747.509 rec/ms, 1495.018 cell/ms
15/08/21 13:52:12 INFO InternalParquetRecordReader: time spent so far 2% reading (99 ms) and 97% processing (4684 ms)
15/08/21 13:52:12 INFO InternalParquetRecordReader: at row 3501332. reading next block
15/08/21 13:52:12 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 70139
15/08/21 13:52:12 INFO Executor: Finished task 125.0 in stage 7.0 (TID 972). 2125 bytes result sent to driver
15/08/21 13:52:12 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 986, localhost, ANY, 1774 bytes)
15/08/21 13:52:12 INFO Executor: Running task 139.0 in stage 7.0 (TID 986)
15/08/21 13:52:12 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 972) in 5522 ms on localhost (124/170)
15/08/21 13:52:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 134217728 end: 257573201 length: 123355473 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:12 INFO Executor: Finished task 124.0 in stage 7.0 (TID 971). 2125 bytes result sent to driver
15/08/21 13:52:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572913 records.
15/08/21 13:52:12 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 987, localhost, ANY, 1761 bytes)
15/08/21 13:52:12 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 971) in 5687 ms on localhost (125/170)
15/08/21 13:52:12 INFO Executor: Running task 140.0 in stage 7.0 (TID 987)
15/08/21 13:52:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501339 records.
15/08/21 13:52:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:12 INFO Executor: Finished task 122.0 in stage 7.0 (TID 969). 2125 bytes result sent to driver
15/08/21 13:52:12 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 988, localhost, ANY, 1773 bytes)
15/08/21 13:52:12 INFO Executor: Running task 141.0 in stage 7.0 (TID 988)
15/08/21 13:52:12 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 969) in 6597 ms on localhost (126/170)
15/08/21 13:52:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 134217728 end: 257466118 length: 123248390 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:12 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 3501339
15/08/21 13:52:12 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500728
15/08/21 13:52:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572985 records.
15/08/21 13:52:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:12 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500100
15/08/21 13:52:12 INFO Executor: Finished task 126.0 in stage 7.0 (TID 973). 2125 bytes result sent to driver
15/08/21 13:52:12 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 989, localhost, ANY, 1761 bytes)
15/08/21 13:52:12 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 973) in 5482 ms on localhost (127/170)
15/08/21 13:52:12 INFO Executor: Running task 142.0 in stage 7.0 (TID 989)
15/08/21 13:52:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:12 INFO Executor: Finished task 127.0 in stage 7.0 (TID 974). 2125 bytes result sent to driver
15/08/21 13:52:12 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 990, localhost, ANY, 1773 bytes)
15/08/21 13:52:12 INFO Executor: Running task 143.0 in stage 7.0 (TID 990)
15/08/21 13:52:12 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 974) in 5411 ms on localhost (128/170)
15/08/21 13:52:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 134217728 end: 257888240 length: 123670512 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
15/08/21 13:52:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:12 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500100
15/08/21 13:52:12 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3501076
15/08/21 13:52:13 INFO Executor: Finished task 128.0 in stage 7.0 (TID 975). 2125 bytes result sent to driver
15/08/21 13:52:13 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 991, localhost, ANY, 1761 bytes)
15/08/21 13:52:13 INFO Executor: Running task 144.0 in stage 7.0 (TID 991)
15/08/21 13:52:13 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 975) in 5486 ms on localhost (129/170)
15/08/21 13:52:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:13 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
15/08/21 13:52:13 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5402 ms: 647.9267 rec/ms, 1295.8534 cell/ms
15/08/21 13:52:13 INFO InternalParquetRecordReader: time spent so far 1% reading (100 ms) and 98% processing (5402 ms)
15/08/21 13:52:13 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:13 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 73229
15/08/21 13:52:14 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4197 ms: 833.9528 rec/ms, 1667.9056 cell/ms
15/08/21 13:52:14 INFO InternalParquetRecordReader: time spent so far 1% reading (72 ms) and 98% processing (4197 ms)
15/08/21 13:52:14 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:14 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 71459
15/08/21 13:52:14 INFO Executor: Finished task 129.0 in stage 7.0 (TID 976). 2125 bytes result sent to driver
15/08/21 13:52:14 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 992, localhost, ANY, 1772 bytes)
15/08/21 13:52:14 INFO Executor: Running task 145.0 in stage 7.0 (TID 992)
15/08/21 13:52:14 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 976) in 6059 ms on localhost (130/170)
15/08/21 13:52:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 134217728 end: 258178393 length: 123960665 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573846 records.
15/08/21 13:52:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:14 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 3500100
15/08/21 13:52:14 INFO Executor: Finished task 130.0 in stage 7.0 (TID 977). 2125 bytes result sent to driver
15/08/21 13:52:14 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 993, localhost, ANY, 1762 bytes)
15/08/21 13:52:14 INFO Executor: Running task 146.0 in stage 7.0 (TID 993)
15/08/21 13:52:14 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 977) in 5392 ms on localhost (131/170)
15/08/21 13:52:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:14 INFO Executor: Finished task 132.0 in stage 7.0 (TID 979). 2125 bytes result sent to driver
15/08/21 13:52:14 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 994, localhost, ANY, 1775 bytes)
15/08/21 13:52:14 INFO Executor: Running task 147.0 in stage 7.0 (TID 994)
15/08/21 13:52:14 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4521 ms: 774.18713 rec/ms, 1548.3743 cell/ms
15/08/21 13:52:14 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 979) in 5083 ms on localhost (132/170)
15/08/21 13:52:14 INFO InternalParquetRecordReader: time spent so far 1% reading (70 ms) and 98% processing (4521 ms)
15/08/21 13:52:14 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 134217728 end: 257798680 length: 123580952 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501395 records.
15/08/21 13:52:14 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 74045
15/08/21 13:52:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572381 records.
15/08/21 13:52:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:14 INFO InternalParquetRecordReader: Assembled and processed 3501786 records from 2 columns in 5003 ms: 699.93726 rec/ms, 1399.8745 cell/ms
15/08/21 13:52:14 INFO InternalParquetRecordReader: time spent so far 0% reading (47 ms) and 99% processing (5003 ms)
15/08/21 13:52:14 INFO InternalParquetRecordReader: at row 3501786. reading next block
15/08/21 13:52:14 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 71580
15/08/21 13:52:14 INFO InternalParquetRecordReader: block read in memory in 93 ms. row count = 3500100
15/08/21 13:52:14 INFO InternalParquetRecordReader: block read in memory in 104 ms. row count = 3501395
15/08/21 13:52:15 INFO Executor: Finished task 131.0 in stage 7.0 (TID 978). 2125 bytes result sent to driver
15/08/21 13:52:15 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 995, localhost, ANY, 1761 bytes)
15/08/21 13:52:15 INFO Executor: Running task 148.0 in stage 7.0 (TID 995)
15/08/21 13:52:15 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 978) in 5259 ms on localhost (133/170)
15/08/21 13:52:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:15 INFO InternalParquetRecordReader: Assembled and processed 3501584 records from 2 columns in 4359 ms: 803.29987 rec/ms, 1606.5997 cell/ms
15/08/21 13:52:15 INFO InternalParquetRecordReader: time spent so far 1% reading (59 ms) and 98% processing (4359 ms)
15/08/21 13:52:15 INFO InternalParquetRecordReader: at row 3501584. reading next block
15/08/21 13:52:15 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 71729
15/08/21 13:52:15 INFO Executor: Finished task 134.0 in stage 7.0 (TID 981). 2125 bytes result sent to driver
15/08/21 13:52:15 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 996, localhost, ANY, 1774 bytes)
15/08/21 13:52:15 INFO Executor: Running task 149.0 in stage 7.0 (TID 996)
15/08/21 13:52:15 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 981) in 4976 ms on localhost (134/170)
15/08/21 13:52:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 134217728 end: 257837778 length: 123620050 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
15/08/21 13:52:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573120 records.
15/08/21 13:52:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:15 INFO InternalParquetRecordReader: block read in memory in 80 ms. row count = 3500100
15/08/21 13:52:15 INFO InternalParquetRecordReader: block read in memory in 103 ms. row count = 3501121
15/08/21 13:52:15 INFO Executor: Finished task 135.0 in stage 7.0 (TID 982). 2125 bytes result sent to driver
15/08/21 13:52:15 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 997, localhost, ANY, 1762 bytes)
15/08/21 13:52:15 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 982) in 5200 ms on localhost (135/170)
15/08/21 13:52:15 INFO Executor: Running task 150.0 in stage 7.0 (TID 997)
15/08/21 13:52:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:15 INFO Executor: Finished task 133.0 in stage 7.0 (TID 980). 2125 bytes result sent to driver
15/08/21 13:52:15 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 998, localhost, ANY, 1775 bytes)
15/08/21 13:52:15 INFO Executor: Running task 151.0 in stage 7.0 (TID 998)
15/08/21 13:52:15 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 980) in 5888 ms on localhost (136/170)
15/08/21 13:52:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 134217728 end: 257748250 length: 123530522 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:15 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500100
15/08/21 13:52:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573993 records.
15/08/21 13:52:15 INFO Executor: Finished task 137.0 in stage 7.0 (TID 984). 2125 bytes result sent to driver
15/08/21 13:52:15 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 999, localhost, ANY, 1761 bytes)
15/08/21 13:52:15 INFO Executor: Running task 152.0 in stage 7.0 (TID 999)
15/08/21 13:52:15 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 984) in 5254 ms on localhost (137/170)
15/08/21 13:52:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:15 INFO InternalParquetRecordReader: block read in memory in 89 ms. row count = 3500100
15/08/21 13:52:15 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 3500100
15/08/21 13:52:16 INFO Executor: Finished task 138.0 in stage 7.0 (TID 985). 2125 bytes result sent to driver
15/08/21 13:52:16 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 1000, localhost, ANY, 1776 bytes)
15/08/21 13:52:16 INFO Executor: Running task 153.0 in stage 7.0 (TID 1000)
15/08/21 13:52:16 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 985) in 5521 ms on localhost (138/170)
15/08/21 13:52:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 134217728 end: 257331238 length: 123113510 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
15/08/21 13:52:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:16 INFO Executor: Finished task 136.0 in stage 7.0 (TID 983). 2125 bytes result sent to driver
15/08/21 13:52:16 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 1001, localhost, ANY, 1762 bytes)
15/08/21 13:52:16 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 983) in 5980 ms on localhost (139/170)
15/08/21 13:52:16 INFO Executor: Running task 154.0 in stage 7.0 (TID 1001)
15/08/21 13:52:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:16 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 3500100
15/08/21 13:52:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503161 records.
15/08/21 13:52:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:16 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 3503161
15/08/21 13:52:16 INFO InternalParquetRecordReader: Assembled and processed 3500728 records from 2 columns in 4377 ms: 799.8008 rec/ms, 1599.6016 cell/ms
15/08/21 13:52:16 INFO InternalParquetRecordReader: time spent so far 2% reading (101 ms) and 97% processing (4377 ms)
15/08/21 13:52:16 INFO InternalParquetRecordReader: at row 3500728. reading next block
15/08/21 13:52:16 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 72185
15/08/21 13:52:16 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4593 ms: 762.05096 rec/ms, 1524.1019 cell/ms
15/08/21 13:52:16 INFO InternalParquetRecordReader: time spent so far 0% reading (39 ms) and 99% processing (4593 ms)
15/08/21 13:52:16 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:16 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 72885
15/08/21 13:52:17 INFO Executor: Finished task 139.0 in stage 7.0 (TID 986). 2125 bytes result sent to driver
15/08/21 13:52:17 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 1002, localhost, ANY, 1775 bytes)
15/08/21 13:52:17 INFO Executor: Running task 155.0 in stage 7.0 (TID 1002)
15/08/21 13:52:17 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 986) in 5189 ms on localhost (140/170)
15/08/21 13:52:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 134217728 end: 257176539 length: 122958811 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571357 records.
15/08/21 13:52:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:17 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 3501317
15/08/21 13:52:17 INFO Executor: Finished task 141.0 in stage 7.0 (TID 988). 2125 bytes result sent to driver
15/08/21 13:52:17 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 1003, localhost, ANY, 1761 bytes)
15/08/21 13:52:17 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 988) in 5338 ms on localhost (141/170)
15/08/21 13:52:17 INFO Executor: Running task 156.0 in stage 7.0 (TID 1003)
15/08/21 13:52:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501689 records.
15/08/21 13:52:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:17 INFO Executor: Finished task 142.0 in stage 7.0 (TID 989). 2125 bytes result sent to driver
15/08/21 13:52:17 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 1004, localhost, ANY, 1774 bytes)
15/08/21 13:52:17 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 989) in 5077 ms on localhost (142/170)
15/08/21 13:52:17 INFO Executor: Running task 157.0 in stage 7.0 (TID 1004)
15/08/21 13:52:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 134217728 end: 257446174 length: 123228446 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:17 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 3501689
15/08/21 13:52:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572541 records.
15/08/21 13:52:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:17 INFO Executor: Finished task 140.0 in stage 7.0 (TID 987). 2125 bytes result sent to driver
15/08/21 13:52:17 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 1005, localhost, ANY, 1760 bytes)
15/08/21 13:52:17 INFO Executor: Running task 158.0 in stage 7.0 (TID 1005)
15/08/21 13:52:17 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 987) in 5576 ms on localhost (143/170)
15/08/21 13:52:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:17 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 3500100
15/08/21 13:52:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501046 records.
15/08/21 13:52:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:17 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 3501046
15/08/21 13:52:18 INFO InternalParquetRecordReader: Assembled and processed 3501076 records from 2 columns in 5920 ms: 591.39795 rec/ms, 1182.7959 cell/ms
15/08/21 13:52:18 INFO InternalParquetRecordReader: time spent so far 1% reading (61 ms) and 98% processing (5920 ms)
15/08/21 13:52:18 INFO InternalParquetRecordReader: at row 3501076. reading next block
15/08/21 13:52:18 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 73262
15/08/21 13:52:18 INFO Executor: Finished task 144.0 in stage 7.0 (TID 991). 2125 bytes result sent to driver
15/08/21 13:52:18 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 1006, localhost, ANY, 1772 bytes)
15/08/21 13:52:18 INFO Executor: Running task 159.0 in stage 7.0 (TID 1006)
15/08/21 13:52:18 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 991) in 5597 ms on localhost (144/170)
15/08/21 13:52:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 134217728 end: 259884384 length: 125666656 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626733 records.
15/08/21 13:52:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:18 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 3500100
15/08/21 13:52:19 INFO Executor: Finished task 143.0 in stage 7.0 (TID 990). 2125 bytes result sent to driver
15/08/21 13:52:19 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 1007, localhost, ANY, 1760 bytes)
15/08/21 13:52:19 INFO Executor: Running task 160.0 in stage 7.0 (TID 1007)
15/08/21 13:52:19 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 990) in 6525 ms on localhost (145/170)
15/08/21 13:52:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501221 records.
15/08/21 13:52:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:19 INFO InternalParquetRecordReader: block read in memory in 67 ms. row count = 3501221
15/08/21 13:52:19 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5394 ms: 648.88763 rec/ms, 1297.7753 cell/ms
15/08/21 13:52:19 INFO InternalParquetRecordReader: time spent so far 0% reading (44 ms) and 99% processing (5394 ms)
15/08/21 13:52:19 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 73746
15/08/21 13:52:19 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4858 ms: 720.4817 rec/ms, 1440.9634 cell/ms
15/08/21 13:52:19 INFO InternalParquetRecordReader: time spent so far 1% reading (93 ms) and 98% processing (4858 ms)
15/08/21 13:52:19 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 72281
15/08/21 13:52:19 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4645 ms: 753.5199 rec/ms, 1507.0398 cell/ms
15/08/21 13:52:19 INFO InternalParquetRecordReader: time spent so far 1% reading (80 ms) and 98% processing (4645 ms)
15/08/21 13:52:19 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:19 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 73020
15/08/21 13:52:20 INFO Executor: Finished task 146.0 in stage 7.0 (TID 993). 2125 bytes result sent to driver
15/08/21 13:52:20 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 1008, localhost, ANY, 1774 bytes)
15/08/21 13:52:20 INFO Executor: Running task 161.0 in stage 7.0 (TID 1008)
15/08/21 13:52:20 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 993) in 5507 ms on localhost (146/170)
15/08/21 13:52:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 134217728 end: 259183553 length: 124965825 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627071 records.
15/08/21 13:52:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:20 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 3503161
15/08/21 13:52:20 INFO Executor: Finished task 147.0 in stage 7.0 (TID 994). 2125 bytes result sent to driver
15/08/21 13:52:20 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 1009, localhost, ANY, 1761 bytes)
15/08/21 13:52:20 INFO Executor: Running task 162.0 in stage 7.0 (TID 1009)
15/08/21 13:52:20 INFO Executor: Finished task 149.0 in stage 7.0 (TID 996). 2125 bytes result sent to driver
15/08/21 13:52:20 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 994) in 5712 ms on localhost (147/170)
15/08/21 13:52:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:20 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 1010, localhost, ANY, 1774 bytes)
15/08/21 13:52:20 INFO Executor: Running task 163.0 in stage 7.0 (TID 1010)
15/08/21 13:52:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 134217728 end: 257504450 length: 123286722 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:20 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 996) in 5484 ms on localhost (148/170)
15/08/21 13:52:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573920 records.
15/08/21 13:52:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:20 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 3500779
15/08/21 13:52:20 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 3500100
15/08/21 13:52:20 INFO Executor: Finished task 145.0 in stage 7.0 (TID 992). 2125 bytes result sent to driver
15/08/21 13:52:20 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 1011, localhost, ANY, 1761 bytes)
15/08/21 13:52:20 INFO Executor: Running task 164.0 in stage 7.0 (TID 1011)
15/08/21 13:52:20 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 992) in 6387 ms on localhost (149/170)
15/08/21 13:52:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501110 records.
15/08/21 13:52:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:20 INFO Executor: Finished task 148.0 in stage 7.0 (TID 995). 2125 bytes result sent to driver
15/08/21 13:52:20 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 1012, localhost, ANY, 1773 bytes)
15/08/21 13:52:20 INFO Executor: Running task 165.0 in stage 7.0 (TID 1012)
15/08/21 13:52:20 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 995) in 5801 ms on localhost (150/170)
15/08/21 13:52:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 134217728 end: 257173847 length: 122956119 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573142 records.
15/08/21 13:52:20 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 3501110
15/08/21 13:52:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:20 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 3502917
15/08/21 13:52:20 INFO Executor: Finished task 150.0 in stage 7.0 (TID 997). 2125 bytes result sent to driver
15/08/21 13:52:20 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 1013, localhost, ANY, 1761 bytes)
15/08/21 13:52:20 INFO Executor: Running task 166.0 in stage 7.0 (TID 1013)
15/08/21 13:52:20 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 997) in 5576 ms on localhost (151/170)
15/08/21 13:52:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500519 records.
15/08/21 13:52:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:21 INFO InternalParquetRecordReader: block read in memory in 109 ms. row count = 3500519
15/08/21 13:52:21 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5180 ms: 675.695 rec/ms, 1351.39 cell/ms
15/08/21 13:52:21 INFO InternalParquetRecordReader: time spent so far 0% reading (33 ms) and 99% processing (5180 ms)
15/08/21 13:52:21 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 73893
15/08/21 13:52:21 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4822 ms: 725.86066 rec/ms, 1451.7213 cell/ms
15/08/21 13:52:21 INFO InternalParquetRecordReader: time spent so far 0% reading (36 ms) and 99% processing (4822 ms)
15/08/21 13:52:21 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 73871
15/08/21 13:52:22 INFO Executor: Finished task 151.0 in stage 7.0 (TID 998). 2125 bytes result sent to driver
15/08/21 13:52:22 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 1014, localhost, ANY, 1774 bytes)
15/08/21 13:52:22 INFO Executor: Running task 167.0 in stage 7.0 (TID 1014)
15/08/21 13:52:22 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 998) in 6488 ms on localhost (152/170)
15/08/21 13:52:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 134217728 end: 257494956 length: 123277228 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573577 records.
15/08/21 13:52:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:22 INFO Executor: Finished task 152.0 in stage 7.0 (TID 999). 2125 bytes result sent to driver
15/08/21 13:52:22 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 1015, localhost, ANY, 1762 bytes)
15/08/21 13:52:22 INFO Executor: Running task 168.0 in stage 7.0 (TID 1015)
15/08/21 13:52:22 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 999) in 6444 ms on localhost (153/170)
15/08/21 13:52:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:22 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 3500100
15/08/21 13:52:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 13:52:22 INFO Executor: Finished task 153.0 in stage 7.0 (TID 1000). 2125 bytes result sent to driver
15/08/21 13:52:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:22 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 1016, localhost, ANY, 1775 bytes)
15/08/21 13:52:22 INFO Executor: Running task 169.0 in stage 7.0 (TID 1016)
15/08/21 13:52:22 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 1000) in 5958 ms on localhost (154/170)
15/08/21 13:52:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 134217728 end: 257035718 length: 122817990 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 13:52:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 13:52:22 INFO Executor: Finished task 154.0 in stage 7.0 (TID 1001). 2125 bytes result sent to driver
15/08/21 13:52:22 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 1001) in 5908 ms on localhost (155/170)
15/08/21 13:52:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574089 records.
15/08/21 13:52:22 INFO InternalParquetRecordReader: Assembled and processed 3501317 records from 2 columns in 4930 ms: 710.2063 rec/ms, 1420.4126 cell/ms
15/08/21 13:52:22 INFO InternalParquetRecordReader: time spent so far 0% reading (41 ms) and 99% processing (4930 ms)
15/08/21 13:52:22 INFO InternalParquetRecordReader: at row 3501317. reading next block
15/08/21 13:52:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 13:52:22 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 70040
15/08/21 13:52:22 INFO InternalParquetRecordReader: block read in memory in 55 ms. row count = 3500100
15/08/21 13:52:22 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 3503378
15/08/21 13:52:22 INFO Executor: Finished task 155.0 in stage 7.0 (TID 1002). 2125 bytes result sent to driver
15/08/21 13:52:22 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 1002) in 5491 ms on localhost (156/170)
15/08/21 13:52:22 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5032 ms: 695.56836 rec/ms, 1391.1367 cell/ms
15/08/21 13:52:22 INFO InternalParquetRecordReader: time spent so far 1% reading (75 ms) and 98% processing (5032 ms)
15/08/21 13:52:22 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:22 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 72441
15/08/21 13:52:22 INFO Executor: Finished task 156.0 in stage 7.0 (TID 1003). 2125 bytes result sent to driver
15/08/21 13:52:22 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 1003) in 5306 ms on localhost (157/170)
15/08/21 13:52:23 INFO Executor: Finished task 157.0 in stage 7.0 (TID 1004). 2125 bytes result sent to driver
15/08/21 13:52:23 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 1004) in 5603 ms on localhost (158/170)
15/08/21 13:52:23 INFO Executor: Finished task 158.0 in stage 7.0 (TID 1005). 2125 bytes result sent to driver
15/08/21 13:52:23 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 1005) in 5816 ms on localhost (159/170)
15/08/21 13:52:23 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4655 ms: 751.9012 rec/ms, 1503.8024 cell/ms
15/08/21 13:52:23 INFO InternalParquetRecordReader: time spent so far 1% reading (50 ms) and 98% processing (4655 ms)
15/08/21 13:52:23 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:23 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 126633
15/08/21 13:52:24 INFO Executor: Finished task 159.0 in stage 7.0 (TID 1006). 2125 bytes result sent to driver
15/08/21 13:52:24 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 1006) in 5243 ms on localhost (160/170)
15/08/21 13:52:24 INFO Executor: Finished task 160.0 in stage 7.0 (TID 1007). 2125 bytes result sent to driver
15/08/21 13:52:24 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 1007) in 5050 ms on localhost (161/170)
15/08/21 13:52:24 INFO InternalParquetRecordReader: Assembled and processed 3503161 records from 2 columns in 4463 ms: 784.93414 rec/ms, 1569.8683 cell/ms
15/08/21 13:52:24 INFO InternalParquetRecordReader: time spent so far 0% reading (39 ms) and 99% processing (4463 ms)
15/08/21 13:52:24 INFO InternalParquetRecordReader: at row 3503161. reading next block
15/08/21 13:52:24 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 123910
15/08/21 13:52:25 INFO InternalParquetRecordReader: Assembled and processed 3500779 records from 2 columns in 4564 ms: 767.0419 rec/ms, 1534.0837 cell/ms
15/08/21 13:52:25 INFO InternalParquetRecordReader: time spent so far 1% reading (49 ms) and 98% processing (4564 ms)
15/08/21 13:52:25 INFO InternalParquetRecordReader: at row 3500779. reading next block
15/08/21 13:52:25 INFO InternalParquetRecordReader: Assembled and processed 3502917 records from 2 columns in 4325 ms: 809.92303 rec/ms, 1619.8461 cell/ms
15/08/21 13:52:25 INFO InternalParquetRecordReader: time spent so far 0% reading (34 ms) and 99% processing (4325 ms)
15/08/21 13:52:25 INFO InternalParquetRecordReader: at row 3502917. reading next block
15/08/21 13:52:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 73141
15/08/21 13:52:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 70225
15/08/21 13:52:25 INFO Executor: Finished task 161.0 in stage 7.0 (TID 1008). 2125 bytes result sent to driver
15/08/21 13:52:25 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 1008) in 5027 ms on localhost (162/170)
15/08/21 13:52:25 INFO Executor: Finished task 162.0 in stage 7.0 (TID 1009). 2125 bytes result sent to driver
15/08/21 13:52:25 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 1009) in 4970 ms on localhost (163/170)
15/08/21 13:52:25 INFO Executor: Finished task 163.0 in stage 7.0 (TID 1010). 2125 bytes result sent to driver
15/08/21 13:52:25 INFO Executor: Finished task 165.0 in stage 7.0 (TID 1012). 2125 bytes result sent to driver
15/08/21 13:52:25 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 1012) in 4840 ms on localhost (164/170)
15/08/21 13:52:25 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 1010) in 5077 ms on localhost (165/170)
15/08/21 13:52:25 INFO Executor: Finished task 166.0 in stage 7.0 (TID 1013). 2125 bytes result sent to driver
15/08/21 13:52:25 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 1013) in 4723 ms on localhost (166/170)
15/08/21 13:52:25 INFO Executor: Finished task 164.0 in stage 7.0 (TID 1011). 2125 bytes result sent to driver
15/08/21 13:52:25 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 1011) in 5078 ms on localhost (167/170)
15/08/21 13:52:26 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4050 ms: 864.2222 rec/ms, 1728.4445 cell/ms
15/08/21 13:52:26 INFO InternalParquetRecordReader: time spent so far 1% reading (53 ms) and 98% processing (4050 ms)
15/08/21 13:52:26 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 13:52:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 73477
15/08/21 13:52:26 INFO InternalParquetRecordReader: Assembled and processed 3503378 records from 2 columns in 4075 ms: 859.7247 rec/ms, 1719.4493 cell/ms
15/08/21 13:52:26 INFO InternalParquetRecordReader: time spent so far 0% reading (38 ms) and 99% processing (4075 ms)
15/08/21 13:52:26 INFO InternalParquetRecordReader: at row 3503378. reading next block
15/08/21 13:52:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 70711
15/08/21 13:52:26 INFO Executor: Finished task 167.0 in stage 7.0 (TID 1014). 2125 bytes result sent to driver
15/08/21 13:52:26 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 1014) in 4534 ms on localhost (168/170)
15/08/21 13:52:26 INFO Executor: Finished task 168.0 in stage 7.0 (TID 1015). 2125 bytes result sent to driver
15/08/21 13:52:26 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 1015) in 4472 ms on localhost (169/170)
15/08/21 13:52:26 INFO Executor: Finished task 169.0 in stage 7.0 (TID 1016). 2125 bytes result sent to driver
15/08/21 13:52:26 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 1016) in 4447 ms on localhost (170/170)
15/08/21 13:52:26 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/21 13:52:26 INFO DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:423) finished in 229.730 s
15/08/21 13:52:26 INFO DAGScheduler: looking for newly runnable stages
15/08/21 13:52:26 INFO DAGScheduler: running: Set()
15/08/21 13:52:26 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/21 13:52:26 INFO DAGScheduler: failed: Set()
15/08/21 13:52:26 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@795ddd9e
15/08/21 13:52:26 INFO StatsReportListener: task runtime:(count: 170, mean: 6192.664706, stdev: 1018.075797, max: 9101.000000, min: 2620.000000)
15/08/21 13:52:26 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:52:26 INFO StatsReportListener: 	2.6 s	5.0 s	5.1 s	5.5 s	6.0 s	6.8 s	7.6 s	8.2 s	9.1 s
15/08/21 13:52:26 INFO StatsReportListener: shuffle bytes written:(count: 170, mean: 22160815.064706, stdev: 1033869.959364, max: 23161542.000000, min: 9212204.000000)
15/08/21 13:52:26 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:52:26 INFO StatsReportListener: 	8.8 MB	20.9 MB	20.9 MB	21.0 MB	21.0 MB	21.4 MB	21.4 MB	21.7 MB	22.1 MB
15/08/21 13:52:26 INFO StatsReportListener: task result size:(count: 170, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 13:52:26 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:52:26 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 13:52:26 INFO DAGScheduler: Missing parents for ResultStage 8: List()
15/08/21 13:52:26 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 13:52:26 INFO StatsReportListener: executor (non-fetch) time pct: (count: 170, mean: 99.566680, stdev: 0.263920, max: 99.805593, min: 96.915352)
15/08/21 13:52:26 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:52:26 INFO StatsReportListener: 	97 %	99 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %
15/08/21 13:52:26 INFO StatsReportListener: other time pct: (count: 170, mean: 0.433320, stdev: 0.263920, max: 3.084648, min: 0.194407)
15/08/21 13:52:26 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:52:26 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 1 %	 3 %
15/08/21 13:52:26 INFO MemoryStore: ensureFreeSpace(16720) called with curMem=1913105, maxMem=22226833244
15/08/21 13:52:26 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.3 KB, free 20.7 GB)
15/08/21 13:52:26 INFO MemoryStore: ensureFreeSpace(7754) called with curMem=1929825, maxMem=22226833244
15/08/21 13:52:26 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.6 KB, free 20.7 GB)
15/08/21 13:52:26 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:45550 (size: 7.6 KB, free: 20.7 GB)
15/08/21 13:52:26 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874
15/08/21 13:52:26 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 8 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423)
15/08/21 13:52:26 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/21 13:52:26 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 1017, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 1018, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 1019, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 1020, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 1021, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 1022, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 1023, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 1024, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 1025, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 1026, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 1027, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 1028, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 1029, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 1030, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 1031, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 1032, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:26 INFO Executor: Running task 2.0 in stage 8.0 (TID 1019)
15/08/21 13:52:26 INFO Executor: Running task 6.0 in stage 8.0 (TID 1023)
15/08/21 13:52:26 INFO Executor: Running task 8.0 in stage 8.0 (TID 1025)
15/08/21 13:52:26 INFO Executor: Running task 3.0 in stage 8.0 (TID 1020)
15/08/21 13:52:26 INFO Executor: Running task 11.0 in stage 8.0 (TID 1028)
15/08/21 13:52:26 INFO Executor: Running task 0.0 in stage 8.0 (TID 1017)
15/08/21 13:52:26 INFO Executor: Running task 14.0 in stage 8.0 (TID 1031)
15/08/21 13:52:26 INFO Executor: Running task 5.0 in stage 8.0 (TID 1022)
15/08/21 13:52:26 INFO Executor: Running task 4.0 in stage 8.0 (TID 1021)
15/08/21 13:52:26 INFO Executor: Running task 15.0 in stage 8.0 (TID 1032)
15/08/21 13:52:26 INFO Executor: Running task 12.0 in stage 8.0 (TID 1029)
15/08/21 13:52:26 INFO Executor: Running task 13.0 in stage 8.0 (TID 1030)
15/08/21 13:52:26 INFO Executor: Running task 10.0 in stage 8.0 (TID 1027)
15/08/21 13:52:26 INFO Executor: Running task 7.0 in stage 8.0 (TID 1024)
15/08/21 13:52:26 INFO Executor: Running task 9.0 in stage 8.0 (TID 1026)
15/08/21 13:52:26 INFO Executor: Running task 1.0 in stage 8.0 (TID 1018)
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:52:48 INFO Executor: Finished task 13.0 in stage 8.0 (TID 1030). 1737 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 8.0 in stage 8.0 (TID 1025). 1941 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 14.0 in stage 8.0 (TID 1031). 1874 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 1.0 in stage 8.0 (TID 1018). 1602 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 2.0 in stage 8.0 (TID 1019). 1738 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 10.0 in stage 8.0 (TID 1027). 1943 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 0.0 in stage 8.0 (TID 1017). 1805 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 9.0 in stage 8.0 (TID 1026). 1738 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 3.0 in stage 8.0 (TID 1020). 1668 bytes result sent to driver
15/08/21 13:52:48 INFO Executor: Finished task 5.0 in stage 8.0 (TID 1022). 1806 bytes result sent to driver
15/08/21 13:52:49 INFO Executor: Finished task 6.0 in stage 8.0 (TID 1023). 1802 bytes result sent to driver
15/08/21 13:52:49 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 1033, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 16.0 in stage 8.0 (TID 1033)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 1034, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 17.0 in stage 8.0 (TID 1034)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 1035, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 18.0 in stage 8.0 (TID 1035)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 1036, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 1037, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 19.0 in stage 8.0 (TID 1036)
15/08/21 13:52:49 INFO Executor: Running task 20.0 in stage 8.0 (TID 1037)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 1038, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 21.0 in stage 8.0 (TID 1038)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 1039, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 22.0 in stage 8.0 (TID 1039)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 1040, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 23.0 in stage 8.0 (TID 1040)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 1041, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 24.0 in stage 8.0 (TID 1041)
15/08/21 13:52:49 INFO Executor: Finished task 15.0 in stage 8.0 (TID 1032). 1738 bytes result sent to driver
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 1042, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO Executor: Running task 25.0 in stage 8.0 (TID 1042)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 1043, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO Executor: Running task 26.0 in stage 8.0 (TID 1043)
15/08/21 13:52:49 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 1044, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO Executor: Running task 27.0 in stage 8.0 (TID 1044)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
15/08/21 13:52:49 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 1018) in 22199 ms on localhost (1/200)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 1031) in 22199 ms on localhost (2/200)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 13:52:49 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 1030) in 22203 ms on localhost (3/200)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO Executor: Finished task 7.0 in stage 8.0 (TID 1024). 1805 bytes result sent to driver
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 1045, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO Executor: Running task 28.0 in stage 8.0 (TID 1045)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 13:52:49 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 1025) in 22214 ms on localhost (4/200)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 1027) in 22235 ms on localhost (5/200)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 1026) in 22243 ms on localhost (6/200)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 1017) in 22250 ms on localhost (7/200)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 1019) in 22249 ms on localhost (8/200)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 1020) in 22264 ms on localhost (9/200)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 1022) in 22271 ms on localhost (10/200)
15/08/21 13:52:49 INFO Executor: Finished task 12.0 in stage 8.0 (TID 1029). 1737 bytes result sent to driver
15/08/21 13:52:49 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 1046, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 1032) in 22273 ms on localhost (11/200)
15/08/21 13:52:49 INFO Executor: Running task 29.0 in stage 8.0 (TID 1046)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 1023) in 22277 ms on localhost (12/200)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 1024) in 22285 ms on localhost (13/200)
15/08/21 13:52:49 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 1029) in 22285 ms on localhost (14/200)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:52:49 INFO Executor: Finished task 11.0 in stage 8.0 (TID 1028). 1737 bytes result sent to driver
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 1047, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:52:49 INFO Executor: Running task 30.0 in stage 8.0 (TID 1047)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 13:52:49 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 1028) in 22317 ms on localhost (15/200)
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:52:50 INFO Executor: Finished task 4.0 in stage 8.0 (TID 1021). 1602 bytes result sent to driver
15/08/21 13:52:50 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 1048, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:52:50 INFO Executor: Running task 31.0 in stage 8.0 (TID 1048)
15/08/21 13:52:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:52:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 13:52:50 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 1021) in 24000 ms on localhost (16/200)
15/08/21 13:52:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:52:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/21 13:52:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:52:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:17 INFO Executor: Finished task 16.0 in stage 8.0 (TID 1033). 2077 bytes result sent to driver
15/08/21 13:53:17 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 1049, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:17 INFO Executor: Running task 32.0 in stage 8.0 (TID 1049)
15/08/21 13:53:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:17 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 1033) in 28396 ms on localhost (17/200)
15/08/21 13:53:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:18 INFO Executor: Finished task 17.0 in stage 8.0 (TID 1034). 1426 bytes result sent to driver
15/08/21 13:53:18 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 1050, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:18 INFO Executor: Running task 33.0 in stage 8.0 (TID 1050)
15/08/21 13:53:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:18 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 1034) in 29893 ms on localhost (18/200)
15/08/21 13:53:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO Executor: Finished task 23.0 in stage 8.0 (TID 1040). 1669 bytes result sent to driver
15/08/21 13:53:19 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 1051, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:19 INFO Executor: Running task 34.0 in stage 8.0 (TID 1051)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 1040) in 30731 ms on localhost (19/200)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO Executor: Finished task 18.0 in stage 8.0 (TID 1035). 1806 bytes result sent to driver
15/08/21 13:53:19 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 1052, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:19 INFO Executor: Running task 35.0 in stage 8.0 (TID 1052)
15/08/21 13:53:19 INFO Executor: Finished task 24.0 in stage 8.0 (TID 1041). 1738 bytes result sent to driver
15/08/21 13:53:19 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 1053, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:19 INFO Executor: Running task 36.0 in stage 8.0 (TID 1053)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 1035) in 30856 ms on localhost (20/200)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:19 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 1041) in 30850 ms on localhost (21/200)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO Executor: Finished task 19.0 in stage 8.0 (TID 1036). 1670 bytes result sent to driver
15/08/21 13:53:19 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 1054, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:19 INFO Executor: Running task 37.0 in stage 8.0 (TID 1054)
15/08/21 13:53:19 INFO Executor: Finished task 21.0 in stage 8.0 (TID 1038). 1426 bytes result sent to driver
15/08/21 13:53:19 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 1055, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:19 INFO Executor: Running task 38.0 in stage 8.0 (TID 1055)
15/08/21 13:53:19 INFO Executor: Finished task 25.0 in stage 8.0 (TID 1042). 1426 bytes result sent to driver
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 1056, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:19 INFO Executor: Running task 39.0 in stage 8.0 (TID 1056)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:19 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 1036) in 30904 ms on localhost (22/200)
15/08/21 13:53:19 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 1038) in 30900 ms on localhost (23/200)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 1042) in 30901 ms on localhost (24/200)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:19 INFO Executor: Finished task 22.0 in stage 8.0 (TID 1039). 1602 bytes result sent to driver
15/08/21 13:53:19 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 1057, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:19 INFO Executor: Running task 40.0 in stage 8.0 (TID 1057)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:19 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 1039) in 30931 ms on localhost (25/200)
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO Executor: Finished task 20.0 in stage 8.0 (TID 1037). 1602 bytes result sent to driver
15/08/21 13:53:20 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 1058, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:20 INFO Executor: Running task 41.0 in stage 8.0 (TID 1058)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 1037) in 31022 ms on localhost (26/200)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO Executor: Finished task 26.0 in stage 8.0 (TID 1043). 1874 bytes result sent to driver
15/08/21 13:53:20 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 1059, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:20 INFO Executor: Running task 42.0 in stage 8.0 (TID 1059)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 1043) in 31024 ms on localhost (27/200)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO Executor: Finished task 27.0 in stage 8.0 (TID 1044). 1601 bytes result sent to driver
15/08/21 13:53:20 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 1060, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:20 INFO Executor: Running task 43.0 in stage 8.0 (TID 1060)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 1044) in 31157 ms on localhost (28/200)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO Executor: Finished task 28.0 in stage 8.0 (TID 1045). 1875 bytes result sent to driver
15/08/21 13:53:20 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 1061, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:20 INFO Executor: Running task 44.0 in stage 8.0 (TID 1061)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 1045) in 31284 ms on localhost (29/200)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO Executor: Finished task 29.0 in stage 8.0 (TID 1046). 1806 bytes result sent to driver
15/08/21 13:53:20 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 1062, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:20 INFO Executor: Running task 45.0 in stage 8.0 (TID 1062)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 1046) in 31324 ms on localhost (30/200)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO Executor: Finished task 30.0 in stage 8.0 (TID 1047). 1601 bytes result sent to driver
15/08/21 13:53:20 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 1063, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:20 INFO Executor: Running task 46.0 in stage 8.0 (TID 1063)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 1047) in 31404 ms on localhost (31/200)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:20 INFO Executor: Finished task 31.0 in stage 8.0 (TID 1048). 1601 bytes result sent to driver
15/08/21 13:53:20 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 1064, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:20 INFO Executor: Running task 47.0 in stage 8.0 (TID 1064)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:20 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 1048) in 29947 ms on localhost (32/200)
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:34 INFO Executor: Finished task 32.0 in stage 8.0 (TID 1049). 1670 bytes result sent to driver
15/08/21 13:53:34 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 1065, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:34 INFO Executor: Running task 48.0 in stage 8.0 (TID 1065)
15/08/21 13:53:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:34 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 1049) in 17040 ms on localhost (33/200)
15/08/21 13:53:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:35 INFO Executor: Finished task 33.0 in stage 8.0 (TID 1050). 1874 bytes result sent to driver
15/08/21 13:53:35 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 1066, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:35 INFO Executor: Running task 49.0 in stage 8.0 (TID 1066)
15/08/21 13:53:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:35 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 1050) in 17113 ms on localhost (34/200)
15/08/21 13:53:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:41 INFO Executor: Finished task 34.0 in stage 8.0 (TID 1051). 1738 bytes result sent to driver
15/08/21 13:53:41 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 1067, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:41 INFO Executor: Running task 50.0 in stage 8.0 (TID 1067)
15/08/21 13:53:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:41 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 1051) in 22110 ms on localhost (35/200)
15/08/21 13:53:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:43 INFO Executor: Finished task 35.0 in stage 8.0 (TID 1052). 1873 bytes result sent to driver
15/08/21 13:53:43 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 1068, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:43 INFO Executor: Running task 51.0 in stage 8.0 (TID 1068)
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:43 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 1052) in 23460 ms on localhost (36/200)
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:43 INFO Executor: Finished task 36.0 in stage 8.0 (TID 1053). 1804 bytes result sent to driver
15/08/21 13:53:43 INFO Executor: Finished task 37.0 in stage 8.0 (TID 1054). 1602 bytes result sent to driver
15/08/21 13:53:43 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 1069, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:43 INFO Executor: Running task 52.0 in stage 8.0 (TID 1069)
15/08/21 13:53:43 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 1070, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:43 INFO Executor: Running task 53.0 in stage 8.0 (TID 1070)
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:43 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 1054) in 23630 ms on localhost (37/200)
15/08/21 13:53:43 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 1053) in 23666 ms on localhost (38/200)
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:53:44 INFO Executor: Finished task 38.0 in stage 8.0 (TID 1055). 1670 bytes result sent to driver
15/08/21 13:53:44 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 1071, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:44 INFO Executor: Running task 54.0 in stage 8.0 (TID 1071)
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:44 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 1055) in 24542 ms on localhost (39/200)
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:44 INFO Executor: Finished task 39.0 in stage 8.0 (TID 1056). 1736 bytes result sent to driver
15/08/21 13:53:44 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 1072, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:44 INFO Executor: Running task 55.0 in stage 8.0 (TID 1072)
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:44 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 1056) in 24938 ms on localhost (40/200)
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO Executor: Finished task 40.0 in stage 8.0 (TID 1057). 1737 bytes result sent to driver
15/08/21 13:53:49 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 1073, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:49 INFO Executor: Running task 56.0 in stage 8.0 (TID 1073)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:49 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 1057) in 29454 ms on localhost (41/200)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO Executor: Finished task 41.0 in stage 8.0 (TID 1058). 1873 bytes result sent to driver
15/08/21 13:53:49 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 1074, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:49 INFO Executor: Running task 57.0 in stage 8.0 (TID 1074)
15/08/21 13:53:49 INFO Executor: Finished task 42.0 in stage 8.0 (TID 1059). 1670 bytes result sent to driver
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:49 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 1075, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO Executor: Running task 58.0 in stage 8.0 (TID 1075)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 1058) in 29504 ms on localhost (42/200)
15/08/21 13:53:49 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 1059) in 29481 ms on localhost (43/200)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:49 INFO Executor: Finished task 43.0 in stage 8.0 (TID 1060). 1806 bytes result sent to driver
15/08/21 13:53:49 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 1076, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:49 INFO Executor: Running task 59.0 in stage 8.0 (TID 1076)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 1060) in 29610 ms on localhost (44/200)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:49 INFO Executor: Finished task 47.0 in stage 8.0 (TID 1064). 1806 bytes result sent to driver
15/08/21 13:53:49 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 1077, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:49 INFO Executor: Running task 60.0 in stage 8.0 (TID 1077)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:49 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 1064) in 29205 ms on localhost (45/200)
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:50 INFO Executor: Finished task 44.0 in stage 8.0 (TID 1061). 1943 bytes result sent to driver
15/08/21 13:53:50 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 1078, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:50 INFO Executor: Running task 61.0 in stage 8.0 (TID 1078)
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:50 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 1061) in 29769 ms on localhost (46/200)
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:50 INFO Executor: Finished task 45.0 in stage 8.0 (TID 1062). 1738 bytes result sent to driver
15/08/21 13:53:50 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 1079, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:50 INFO Executor: Running task 62.0 in stage 8.0 (TID 1079)
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:50 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 1062) in 29797 ms on localhost (47/200)
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:50 INFO Executor: Finished task 46.0 in stage 8.0 (TID 1063). 1602 bytes result sent to driver
15/08/21 13:53:50 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 1080, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:50 INFO Executor: Running task 63.0 in stage 8.0 (TID 1080)
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:50 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 1063) in 29720 ms on localhost (48/200)
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:53 INFO Executor: Finished task 48.0 in stage 8.0 (TID 1065). 1943 bytes result sent to driver
15/08/21 13:53:53 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 1081, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:53 INFO Executor: Running task 64.0 in stage 8.0 (TID 1081)
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:53 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 1065) in 19194 ms on localhost (49/200)
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:53 INFO Executor: Finished task 49.0 in stage 8.0 (TID 1066). 1602 bytes result sent to driver
15/08/21 13:53:53 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 1082, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:53 INFO Executor: Running task 65.0 in stage 8.0 (TID 1082)
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:53 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 1066) in 17801 ms on localhost (50/200)
15/08/21 13:53:54 INFO Executor: Finished task 50.0 in stage 8.0 (TID 1067). 1670 bytes result sent to driver
15/08/21 13:53:54 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 1083, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:54 INFO Executor: Running task 66.0 in stage 8.0 (TID 1083)
15/08/21 13:53:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:54 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 1067) in 13087 ms on localhost (51/200)
15/08/21 13:53:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:59 INFO Executor: Finished task 51.0 in stage 8.0 (TID 1068). 1669 bytes result sent to driver
15/08/21 13:53:59 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 1084, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:59 INFO Executor: Running task 67.0 in stage 8.0 (TID 1084)
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:59 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 1068) in 16139 ms on localhost (52/200)
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:59 INFO Executor: Finished task 52.0 in stage 8.0 (TID 1069). 1805 bytes result sent to driver
15/08/21 13:53:59 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 1085, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:53:59 INFO Executor: Running task 68.0 in stage 8.0 (TID 1085)
15/08/21 13:53:59 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 1069) in 16421 ms on localhost (53/200)
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:53:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:45550 in memory (size: 4.9 KB, free: 20.7 GB)
15/08/21 13:54:15 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:45550 in memory (size: 3.7 KB, free: 20.7 GB)
15/08/21 13:54:15 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:45550 in memory (size: 3.8 KB, free: 20.7 GB)
15/08/21 13:54:15 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:45550 in memory (size: 3.6 KB, free: 20.7 GB)
15/08/21 13:54:15 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:45550 in memory (size: 3.5 KB, free: 20.7 GB)
15/08/21 13:54:15 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:45550 in memory (size: 29.3 KB, free: 20.7 GB)
15/08/21 13:54:15 INFO Executor: Finished task 53.0 in stage 8.0 (TID 1070). 1805 bytes result sent to driver
15/08/21 13:54:15 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 1086, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:15 INFO Executor: Running task 69.0 in stage 8.0 (TID 1086)
15/08/21 13:54:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:15 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:16 INFO Executor: Finished task 55.0 in stage 8.0 (TID 1072). 1804 bytes result sent to driver
15/08/21 13:54:16 INFO Executor: Finished task 54.0 in stage 8.0 (TID 1071). 1669 bytes result sent to driver
15/08/21 13:54:16 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 1087, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:16 INFO Executor: Running task 70.0 in stage 8.0 (TID 1087)
15/08/21 13:54:16 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 1088, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:16 INFO Executor: Running task 71.0 in stage 8.0 (TID 1088)
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:16 INFO ContextCleaner: Cleaned shuffle 0
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:45550 in memory (size: 22.3 KB, free: 20.7 GB)
15/08/21 13:54:16 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 1070) in 32978 ms on localhost (54/200)
15/08/21 13:54:16 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 1072) in 31666 ms on localhost (55/200)
15/08/21 13:54:16 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 1071) in 32075 ms on localhost (56/200)
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:54:17 INFO Executor: Finished task 56.0 in stage 8.0 (TID 1073). 1805 bytes result sent to driver
15/08/21 13:54:17 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 1089, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:17 INFO Executor: Running task 72.0 in stage 8.0 (TID 1089)
15/08/21 13:54:17 INFO Executor: Finished task 57.0 in stage 8.0 (TID 1074). 1737 bytes result sent to driver
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:17 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 1090, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:17 INFO Executor: Running task 73.0 in stage 8.0 (TID 1090)
15/08/21 13:54:17 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 1073) in 28472 ms on localhost (57/200)
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:17 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 1074) in 28350 ms on localhost (58/200)
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:20 INFO Executor: Finished task 58.0 in stage 8.0 (TID 1075). 1668 bytes result sent to driver
15/08/21 13:54:20 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 1091, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:20 INFO Executor: Running task 74.0 in stage 8.0 (TID 1091)
15/08/21 13:54:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:20 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 1075) in 31019 ms on localhost (59/200)
15/08/21 13:54:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO Executor: Finished task 59.0 in stage 8.0 (TID 1076). 1875 bytes result sent to driver
15/08/21 13:54:24 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 1092, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:24 INFO Executor: Running task 75.0 in stage 8.0 (TID 1092)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 1076) in 35033 ms on localhost (60/200)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO Executor: Finished task 60.0 in stage 8.0 (TID 1077). 1805 bytes result sent to driver
15/08/21 13:54:24 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 1093, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:24 INFO Executor: Running task 76.0 in stage 8.0 (TID 1093)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 1077) in 34881 ms on localhost (61/200)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO Executor: Finished task 61.0 in stage 8.0 (TID 1078). 2009 bytes result sent to driver
15/08/21 13:54:24 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 1094, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:24 INFO Executor: Running task 77.0 in stage 8.0 (TID 1094)
15/08/21 13:54:24 INFO Executor: Finished task 62.0 in stage 8.0 (TID 1079). 1737 bytes result sent to driver
15/08/21 13:54:24 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 1095, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:24 INFO Executor: Running task 78.0 in stage 8.0 (TID 1095)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 1078) in 34854 ms on localhost (62/200)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 1079) in 34730 ms on localhost (63/200)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO Executor: Finished task 63.0 in stage 8.0 (TID 1080). 1804 bytes result sent to driver
15/08/21 13:54:24 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 1096, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:24 INFO Executor: Running task 79.0 in stage 8.0 (TID 1096)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:24 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 1080) in 34725 ms on localhost (64/200)
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:25 INFO Executor: Finished task 64.0 in stage 8.0 (TID 1081). 1873 bytes result sent to driver
15/08/21 13:54:25 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 1097, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:25 INFO Executor: Running task 80.0 in stage 8.0 (TID 1097)
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:25 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 1081) in 31536 ms on localhost (65/200)
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:25 INFO Executor: Finished task 65.0 in stage 8.0 (TID 1082). 1803 bytes result sent to driver
15/08/21 13:54:25 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 1098, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:25 INFO Executor: Running task 81.0 in stage 8.0 (TID 1098)
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:25 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 1082) in 31957 ms on localhost (66/200)
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:33 INFO Executor: Finished task 66.0 in stage 8.0 (TID 1083). 1876 bytes result sent to driver
15/08/21 13:54:33 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 1099, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:33 INFO Executor: Finished task 67.0 in stage 8.0 (TID 1084). 2078 bytes result sent to driver
15/08/21 13:54:33 INFO Executor: Running task 82.0 in stage 8.0 (TID 1099)
15/08/21 13:54:33 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 1100, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:33 INFO Executor: Running task 83.0 in stage 8.0 (TID 1100)
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:54:33 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 1084) in 33906 ms on localhost (67/200)
15/08/21 13:54:33 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 1083) in 38419 ms on localhost (68/200)
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:33 INFO Executor: Finished task 68.0 in stage 8.0 (TID 1085). 1670 bytes result sent to driver
15/08/21 13:54:33 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 1101, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:33 INFO Executor: Running task 84.0 in stage 8.0 (TID 1101)
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:33 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 1085) in 33792 ms on localhost (69/200)
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:33 INFO Executor: Finished task 69.0 in stage 8.0 (TID 1086). 1426 bytes result sent to driver
15/08/21 13:54:33 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 1102, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:33 INFO Executor: Running task 85.0 in stage 8.0 (TID 1102)
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:34 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 1086) in 18447 ms on localhost (70/200)
15/08/21 13:54:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:41 INFO Executor: Finished task 70.0 in stage 8.0 (TID 1087). 1426 bytes result sent to driver
15/08/21 13:54:41 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 1103, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:41 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 1087) in 25211 ms on localhost (71/200)
15/08/21 13:54:41 INFO Executor: Running task 86.0 in stage 8.0 (TID 1103)
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:41 INFO Executor: Finished task 71.0 in stage 8.0 (TID 1088). 1805 bytes result sent to driver
15/08/21 13:54:41 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 1104, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:41 INFO Executor: Running task 87.0 in stage 8.0 (TID 1104)
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:54:41 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 1088) in 25389 ms on localhost (72/200)
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:41 INFO Executor: Finished task 72.0 in stage 8.0 (TID 1089). 1426 bytes result sent to driver
15/08/21 13:54:41 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 1105, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:41 INFO Executor: Running task 88.0 in stage 8.0 (TID 1105)
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:41 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 1089) in 24136 ms on localhost (73/200)
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:42 INFO Executor: Finished task 73.0 in stage 8.0 (TID 1090). 1942 bytes result sent to driver
15/08/21 13:54:42 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 1106, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:42 INFO Executor: Running task 89.0 in stage 8.0 (TID 1106)
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:42 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 1090) in 24276 ms on localhost (74/200)
15/08/21 13:54:42 INFO Executor: Finished task 74.0 in stage 8.0 (TID 1091). 2150 bytes result sent to driver
15/08/21 13:54:42 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 1107, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:42 INFO Executor: Running task 90.0 in stage 8.0 (TID 1107)
15/08/21 13:54:42 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 1091) in 21644 ms on localhost (75/200)
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:45 INFO Executor: Finished task 75.0 in stage 8.0 (TID 1092). 1876 bytes result sent to driver
15/08/21 13:54:45 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 1108, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:45 INFO Executor: Running task 91.0 in stage 8.0 (TID 1108)
15/08/21 13:54:45 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 1092) in 20548 ms on localhost (76/200)
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:45 INFO Executor: Finished task 76.0 in stage 8.0 (TID 1093). 1805 bytes result sent to driver
15/08/21 13:54:45 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 1109, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:45 INFO Executor: Running task 92.0 in stage 8.0 (TID 1109)
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:45 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 1093) in 20661 ms on localhost (77/200)
15/08/21 13:54:46 INFO Executor: Finished task 77.0 in stage 8.0 (TID 1094). 2010 bytes result sent to driver
15/08/21 13:54:46 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 1110, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:46 INFO Executor: Running task 93.0 in stage 8.0 (TID 1110)
15/08/21 13:54:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:46 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 1094) in 21269 ms on localhost (78/200)
15/08/21 13:54:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:54:51 INFO Executor: Finished task 78.0 in stage 8.0 (TID 1095). 1669 bytes result sent to driver
15/08/21 13:54:51 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 1111, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:51 INFO Executor: Running task 94.0 in stage 8.0 (TID 1111)
15/08/21 13:54:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:51 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 1095) in 26974 ms on localhost (79/200)
15/08/21 13:54:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:52 INFO Executor: Finished task 79.0 in stage 8.0 (TID 1096). 1939 bytes result sent to driver
15/08/21 13:54:52 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 1112, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:52 INFO Executor: Running task 95.0 in stage 8.0 (TID 1112)
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:52 INFO Executor: Finished task 80.0 in stage 8.0 (TID 1097). 1738 bytes result sent to driver
15/08/21 13:54:52 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 1113, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:52 INFO Executor: Running task 96.0 in stage 8.0 (TID 1113)
15/08/21 13:54:52 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 1096) in 27161 ms on localhost (80/200)
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:52 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 1097) in 27081 ms on localhost (81/200)
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:52 INFO Executor: Finished task 81.0 in stage 8.0 (TID 1098). 1736 bytes result sent to driver
15/08/21 13:54:52 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 1114, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:52 INFO Executor: Running task 97.0 in stage 8.0 (TID 1114)
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:54:52 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 1098) in 26673 ms on localhost (82/200)
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:57 INFO Executor: Finished task 82.0 in stage 8.0 (TID 1099). 1804 bytes result sent to driver
15/08/21 13:54:57 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 1115, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:57 INFO Executor: Running task 98.0 in stage 8.0 (TID 1115)
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:57 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 1099) in 24156 ms on localhost (83/200)
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:57 INFO Executor: Finished task 83.0 in stage 8.0 (TID 1100). 1426 bytes result sent to driver
15/08/21 13:54:57 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 1116, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:57 INFO Executor: Running task 99.0 in stage 8.0 (TID 1116)
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:57 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 1100) in 24392 ms on localhost (84/200)
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:58 INFO Executor: Finished task 84.0 in stage 8.0 (TID 1101). 1738 bytes result sent to driver
15/08/21 13:54:58 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 1117, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:58 INFO Executor: Running task 100.0 in stage 8.0 (TID 1117)
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:58 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 1101) in 24601 ms on localhost (85/200)
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:58 INFO Executor: Finished task 85.0 in stage 8.0 (TID 1102). 1602 bytes result sent to driver
15/08/21 13:54:58 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 1118, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:58 INFO Executor: Running task 101.0 in stage 8.0 (TID 1118)
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:58 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 1102) in 24791 ms on localhost (86/200)
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:58 INFO Executor: Finished task 86.0 in stage 8.0 (TID 1103). 1805 bytes result sent to driver
15/08/21 13:54:58 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 1119, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:58 INFO Executor: Running task 102.0 in stage 8.0 (TID 1119)
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:58 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 1103) in 17521 ms on localhost (87/200)
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:59 INFO Executor: Finished task 87.0 in stage 8.0 (TID 1104). 1669 bytes result sent to driver
15/08/21 13:54:59 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 1120, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:54:59 INFO Executor: Running task 103.0 in stage 8.0 (TID 1120)
15/08/21 13:54:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:54:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:59 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 1104) in 17755 ms on localhost (88/200)
15/08/21 13:54:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:54:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:54:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:54:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:04 INFO Executor: Finished task 88.0 in stage 8.0 (TID 1105). 2009 bytes result sent to driver
15/08/21 13:55:04 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 1121, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:04 INFO Executor: Finished task 89.0 in stage 8.0 (TID 1106). 1670 bytes result sent to driver
15/08/21 13:55:04 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 1122, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:04 INFO Executor: Running task 104.0 in stage 8.0 (TID 1121)
15/08/21 13:55:04 INFO Executor: Running task 105.0 in stage 8.0 (TID 1122)
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:04 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 1105) in 22267 ms on localhost (89/200)
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:04 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 1106) in 22121 ms on localhost (90/200)
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:04 INFO Executor: Finished task 90.0 in stage 8.0 (TID 1107). 1941 bytes result sent to driver
15/08/21 13:55:04 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 1123, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:04 INFO Executor: Running task 106.0 in stage 8.0 (TID 1123)
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:04 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 1107) in 22145 ms on localhost (91/200)
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO Executor: Finished task 91.0 in stage 8.0 (TID 1108). 2009 bytes result sent to driver
15/08/21 13:55:19 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 1124, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:19 INFO Executor: Running task 107.0 in stage 8.0 (TID 1124)
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 1108) in 34279 ms on localhost (92/200)
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:55:19 INFO Executor: Finished task 92.0 in stage 8.0 (TID 1109). 1670 bytes result sent to driver
15/08/21 13:55:19 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 1125, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:19 INFO Executor: Running task 108.0 in stage 8.0 (TID 1125)
15/08/21 13:55:19 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 1109) in 34265 ms on localhost (93/200)
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO Executor: Finished task 93.0 in stage 8.0 (TID 1110). 1736 bytes result sent to driver
15/08/21 13:55:19 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 1126, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:19 INFO Executor: Running task 109.0 in stage 8.0 (TID 1126)
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:19 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 1110) in 33623 ms on localhost (94/200)
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO Executor: Finished task 94.0 in stage 8.0 (TID 1111). 1738 bytes result sent to driver
15/08/21 13:55:19 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 1127, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:19 INFO Executor: Running task 110.0 in stage 8.0 (TID 1127)
15/08/21 13:55:19 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 1111) in 27937 ms on localhost (95/200)
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:20 INFO Executor: Finished task 95.0 in stage 8.0 (TID 1112). 1805 bytes result sent to driver
15/08/21 13:55:20 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 1128, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:20 INFO Executor: Running task 111.0 in stage 8.0 (TID 1128)
15/08/21 13:55:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:20 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 1112) in 27983 ms on localhost (96/200)
15/08/21 13:55:23 INFO Executor: Finished task 96.0 in stage 8.0 (TID 1113). 1738 bytes result sent to driver
15/08/21 13:55:23 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 1129, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:23 INFO Executor: Running task 112.0 in stage 8.0 (TID 1129)
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:23 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 1113) in 31533 ms on localhost (97/200)
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:23 INFO Executor: Finished task 97.0 in stage 8.0 (TID 1114). 1876 bytes result sent to driver
15/08/21 13:55:23 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 1130, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:23 INFO Executor: Running task 113.0 in stage 8.0 (TID 1130)
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:23 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 1114) in 31624 ms on localhost (98/200)
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:24 INFO Executor: Finished task 98.0 in stage 8.0 (TID 1115). 1668 bytes result sent to driver
15/08/21 13:55:24 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 1131, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:24 INFO Executor: Running task 114.0 in stage 8.0 (TID 1131)
15/08/21 13:55:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:24 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 1115) in 26682 ms on localhost (99/200)
15/08/21 13:55:24 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:28 INFO Executor: Finished task 99.0 in stage 8.0 (TID 1116). 1737 bytes result sent to driver
15/08/21 13:55:28 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 1132, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:28 INFO Executor: Running task 115.0 in stage 8.0 (TID 1132)
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:28 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 1116) in 30552 ms on localhost (100/200)
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:28 INFO Executor: Finished task 100.0 in stage 8.0 (TID 1117). 1669 bytes result sent to driver
15/08/21 13:55:28 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 1133, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:28 INFO Executor: Running task 116.0 in stage 8.0 (TID 1133)
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:28 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 1117) in 30549 ms on localhost (101/200)
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:29 INFO Executor: Finished task 101.0 in stage 8.0 (TID 1118). 1670 bytes result sent to driver
15/08/21 13:55:29 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 1134, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:29 INFO Executor: Running task 117.0 in stage 8.0 (TID 1134)
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:29 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 1118) in 30823 ms on localhost (102/200)
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:29 INFO Executor: Finished task 102.0 in stage 8.0 (TID 1119). 1602 bytes result sent to driver
15/08/21 13:55:29 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 1135, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:29 INFO Executor: Running task 118.0 in stage 8.0 (TID 1135)
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:29 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 1119) in 30938 ms on localhost (103/200)
15/08/21 13:55:29 INFO Executor: Finished task 103.0 in stage 8.0 (TID 1120). 1426 bytes result sent to driver
15/08/21 13:55:29 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 1136, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:29 INFO Executor: Running task 119.0 in stage 8.0 (TID 1136)
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:29 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 1120) in 30565 ms on localhost (104/200)
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:30 INFO Executor: Finished task 104.0 in stage 8.0 (TID 1121). 1602 bytes result sent to driver
15/08/21 13:55:30 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 1137, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:30 INFO Executor: Running task 120.0 in stage 8.0 (TID 1137)
15/08/21 13:55:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:30 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:30 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 1121) in 25883 ms on localhost (105/200)
15/08/21 13:55:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:35 INFO Executor: Finished task 105.0 in stage 8.0 (TID 1122). 1426 bytes result sent to driver
15/08/21 13:55:35 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 1138, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:35 INFO Executor: Running task 121.0 in stage 8.0 (TID 1138)
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:35 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 1122) in 31030 ms on localhost (106/200)
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:35 INFO Executor: Finished task 106.0 in stage 8.0 (TID 1123). 1670 bytes result sent to driver
15/08/21 13:55:35 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 1139, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:35 INFO Executor: Running task 122.0 in stage 8.0 (TID 1139)
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:35 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 1123) in 31736 ms on localhost (107/200)
15/08/21 13:55:36 INFO Executor: Finished task 107.0 in stage 8.0 (TID 1124). 1738 bytes result sent to driver
15/08/21 13:55:36 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 1140, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:36 INFO Executor: Running task 123.0 in stage 8.0 (TID 1140)
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:36 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 1124) in 16509 ms on localhost (108/200)
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:36 INFO Executor: Finished task 108.0 in stage 8.0 (TID 1125). 1806 bytes result sent to driver
15/08/21 13:55:36 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 1141, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:36 INFO Executor: Running task 124.0 in stage 8.0 (TID 1141)
15/08/21 13:55:36 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 1125) in 16740 ms on localhost (109/200)
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:36 INFO Executor: Finished task 109.0 in stage 8.0 (TID 1126). 1669 bytes result sent to driver
15/08/21 13:55:36 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 1142, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:41 INFO Executor: Running task 125.0 in stage 8.0 (TID 1142)
15/08/21 13:55:41 INFO Executor: Finished task 110.0 in stage 8.0 (TID 1127). 1426 bytes result sent to driver
15/08/21 13:55:41 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 1143, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:41 INFO Executor: Running task 126.0 in stage 8.0 (TID 1143)
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:41 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 1126) in 21307 ms on localhost (110/200)
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:41 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 1127) in 21280 ms on localhost (111/200)
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:41 INFO Executor: Finished task 111.0 in stage 8.0 (TID 1128). 1738 bytes result sent to driver
15/08/21 13:55:41 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 1144, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:41 INFO Executor: Running task 127.0 in stage 8.0 (TID 1144)
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:41 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 1128) in 21105 ms on localhost (112/200)
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:45 INFO Executor: Finished task 112.0 in stage 8.0 (TID 1129). 1874 bytes result sent to driver
15/08/21 13:55:46 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 1145, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:46 INFO Executor: Running task 128.0 in stage 8.0 (TID 1145)
15/08/21 13:55:46 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 1129) in 22371 ms on localhost (113/200)
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:46 INFO Executor: Finished task 113.0 in stage 8.0 (TID 1130). 1737 bytes result sent to driver
15/08/21 13:55:46 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 1146, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:46 INFO Executor: Running task 129.0 in stage 8.0 (TID 1146)
15/08/21 13:55:46 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 1130) in 22166 ms on localhost (114/200)
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:46 INFO Executor: Finished task 114.0 in stage 8.0 (TID 1131). 1806 bytes result sent to driver
15/08/21 13:55:46 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 1147, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:46 INFO Executor: Running task 130.0 in stage 8.0 (TID 1147)
15/08/21 13:55:46 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 1131) in 22543 ms on localhost (115/200)
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:46 INFO Executor: Finished task 115.0 in stage 8.0 (TID 1132). 1670 bytes result sent to driver
15/08/21 13:55:46 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 1148, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:46 INFO Executor: Running task 131.0 in stage 8.0 (TID 1148)
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:46 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 1132) in 18522 ms on localhost (116/200)
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:46 INFO Executor: Finished task 116.0 in stage 8.0 (TID 1133). 1737 bytes result sent to driver
15/08/21 13:55:46 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 1149, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:46 INFO Executor: Running task 132.0 in stage 8.0 (TID 1149)
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:46 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 1133) in 18145 ms on localhost (117/200)
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:54 INFO Executor: Finished task 117.0 in stage 8.0 (TID 1134). 1737 bytes result sent to driver
15/08/21 13:55:54 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 1150, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:54 INFO Executor: Running task 133.0 in stage 8.0 (TID 1150)
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:54 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 1134) in 24626 ms on localhost (118/200)
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:54 INFO Executor: Finished task 118.0 in stage 8.0 (TID 1135). 1736 bytes result sent to driver
15/08/21 13:55:54 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 1151, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:54 INFO Executor: Running task 134.0 in stage 8.0 (TID 1151)
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:54 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 1135) in 24627 ms on localhost (119/200)
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:58 INFO Executor: Finished task 119.0 in stage 8.0 (TID 1136). 1738 bytes result sent to driver
15/08/21 13:55:58 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 1152, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:58 INFO Executor: Running task 135.0 in stage 8.0 (TID 1152)
15/08/21 13:55:58 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 1136) in 29030 ms on localhost (120/200)
15/08/21 13:55:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:55:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:59 INFO Executor: Finished task 120.0 in stage 8.0 (TID 1137). 1670 bytes result sent to driver
15/08/21 13:55:59 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 1153, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:59 INFO Executor: Running task 136.0 in stage 8.0 (TID 1153)
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:59 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 1137) in 29538 ms on localhost (121/200)
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:59 INFO Executor: Finished task 121.0 in stage 8.0 (TID 1138). 1737 bytes result sent to driver
15/08/21 13:55:59 INFO Executor: Finished task 123.0 in stage 8.0 (TID 1140). 1737 bytes result sent to driver
15/08/21 13:55:59 INFO Executor: Finished task 122.0 in stage 8.0 (TID 1139). 1804 bytes result sent to driver
15/08/21 13:55:59 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 1154, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:59 INFO Executor: Running task 137.0 in stage 8.0 (TID 1154)
15/08/21 13:55:59 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 1155, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:59 INFO Executor: Running task 138.0 in stage 8.0 (TID 1155)
15/08/21 13:55:59 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 1156, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:55:59 INFO Executor: Running task 139.0 in stage 8.0 (TID 1156)
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:59 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 1138) in 24659 ms on localhost (122/200)
15/08/21 13:55:59 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 1139) in 23921 ms on localhost (123/200)
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:55:59 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 1140) in 23855 ms on localhost (124/200)
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO Executor: Finished task 124.0 in stage 8.0 (TID 1141). 1805 bytes result sent to driver
15/08/21 13:56:00 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 1157, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:00 INFO Executor: Running task 140.0 in stage 8.0 (TID 1157)
15/08/21 13:56:00 INFO Executor: Finished task 125.0 in stage 8.0 (TID 1142). 1806 bytes result sent to driver
15/08/21 13:56:00 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 1158, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:00 INFO Executor: Running task 141.0 in stage 8.0 (TID 1158)
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 1141) in 23700 ms on localhost (125/200)
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 1142) in 23219 ms on localhost (126/200)
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO Executor: Finished task 126.0 in stage 8.0 (TID 1143). 1670 bytes result sent to driver
15/08/21 13:56:00 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 1159, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:00 INFO Executor: Running task 142.0 in stage 8.0 (TID 1159)
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 1143) in 19115 ms on localhost (127/200)
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:00 INFO Executor: Finished task 127.0 in stage 8.0 (TID 1144). 1670 bytes result sent to driver
15/08/21 13:56:00 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 1160, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:00 INFO Executor: Running task 143.0 in stage 8.0 (TID 1160)
15/08/21 13:56:00 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 1144) in 19139 ms on localhost (128/200)
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:20 INFO Executor: Finished task 129.0 in stage 8.0 (TID 1146). 1426 bytes result sent to driver
15/08/21 13:56:20 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 1161, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:20 INFO Executor: Finished task 128.0 in stage 8.0 (TID 1145). 1669 bytes result sent to driver
15/08/21 13:56:20 INFO Executor: Running task 144.0 in stage 8.0 (TID 1161)
15/08/21 13:56:20 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 1162, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:20 INFO Executor: Running task 145.0 in stage 8.0 (TID 1162)
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:20 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 1146) in 34095 ms on localhost (129/200)
15/08/21 13:56:20 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 1145) in 34209 ms on localhost (130/200)
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:20 INFO Executor: Finished task 130.0 in stage 8.0 (TID 1147). 1669 bytes result sent to driver
15/08/21 13:56:20 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 1163, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:20 INFO Executor: Running task 146.0 in stage 8.0 (TID 1163)
15/08/21 13:56:20 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 1147) in 33709 ms on localhost (131/200)
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:20 INFO Executor: Finished task 131.0 in stage 8.0 (TID 1148). 1738 bytes result sent to driver
15/08/21 13:56:20 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 1164, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:20 INFO Executor: Running task 147.0 in stage 8.0 (TID 1164)
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:20 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 1148) in 33951 ms on localhost (132/200)
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:21 INFO Executor: Finished task 132.0 in stage 8.0 (TID 1149). 1602 bytes result sent to driver
15/08/21 13:56:21 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 1165, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:21 INFO Executor: Running task 148.0 in stage 8.0 (TID 1165)
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:21 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 1149) in 34586 ms on localhost (133/200)
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:21 INFO Executor: Finished task 133.0 in stage 8.0 (TID 1150). 1806 bytes result sent to driver
15/08/21 13:56:21 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 1166, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:21 INFO Executor: Running task 149.0 in stage 8.0 (TID 1166)
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:21 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 1150) in 27462 ms on localhost (134/200)
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:23 INFO Executor: Finished task 134.0 in stage 8.0 (TID 1151). 1602 bytes result sent to driver
15/08/21 13:56:23 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 1167, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:23 INFO Executor: Running task 150.0 in stage 8.0 (TID 1167)
15/08/21 13:56:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:23 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 1151) in 29305 ms on localhost (135/200)
15/08/21 13:56:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:56:25 INFO Executor: Finished task 135.0 in stage 8.0 (TID 1152). 1670 bytes result sent to driver
15/08/21 13:56:25 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 1168, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:25 INFO Executor: Running task 151.0 in stage 8.0 (TID 1168)
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:25 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 1152) in 26236 ms on localhost (136/200)
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:25 INFO Executor: Finished task 136.0 in stage 8.0 (TID 1153). 1874 bytes result sent to driver
15/08/21 13:56:25 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 1169, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:25 INFO Executor: Running task 152.0 in stage 8.0 (TID 1169)
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:25 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 1153) in 25805 ms on localhost (137/200)
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:30 INFO Executor: Finished task 137.0 in stage 8.0 (TID 1154). 1737 bytes result sent to driver
15/08/21 13:56:30 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 1170, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:30 INFO Executor: Running task 153.0 in stage 8.0 (TID 1170)
15/08/21 13:56:30 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 1154) in 30333 ms on localhost (138/200)
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:30 INFO Executor: Finished task 138.0 in stage 8.0 (TID 1155). 1738 bytes result sent to driver
15/08/21 13:56:30 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 1171, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:30 INFO Executor: Running task 154.0 in stage 8.0 (TID 1171)
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:30 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 1155) in 31028 ms on localhost (139/200)
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:30 INFO Executor: Finished task 139.0 in stage 8.0 (TID 1156). 1670 bytes result sent to driver
15/08/21 13:56:30 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 1172, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:30 INFO Executor: Running task 155.0 in stage 8.0 (TID 1172)
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:30 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 1156) in 31121 ms on localhost (140/200)
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:31 INFO Executor: Finished task 140.0 in stage 8.0 (TID 1157). 1737 bytes result sent to driver
15/08/21 13:56:31 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 1173, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:31 INFO Executor: Running task 156.0 in stage 8.0 (TID 1173)
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:31 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 1157) in 31206 ms on localhost (141/200)
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:31 INFO Executor: Finished task 141.0 in stage 8.0 (TID 1158). 1670 bytes result sent to driver
15/08/21 13:56:31 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 1174, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:31 INFO Executor: Running task 157.0 in stage 8.0 (TID 1174)
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:31 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 1158) in 31513 ms on localhost (142/200)
15/08/21 13:56:36 INFO Executor: Finished task 142.0 in stage 8.0 (TID 1159). 1736 bytes result sent to driver
15/08/21 13:56:36 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 1175, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:36 INFO Executor: Running task 158.0 in stage 8.0 (TID 1175)
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:36 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 1159) in 36424 ms on localhost (143/200)
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:36 INFO Executor: Finished task 143.0 in stage 8.0 (TID 1160). 1737 bytes result sent to driver
15/08/21 13:56:36 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 1176, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:36 INFO Executor: Running task 159.0 in stage 8.0 (TID 1176)
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:36 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 1160) in 36389 ms on localhost (144/200)
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:37 INFO Executor: Finished task 144.0 in stage 8.0 (TID 1161). 1805 bytes result sent to driver
15/08/21 13:56:37 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 1177, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:37 INFO Executor: Running task 160.0 in stage 8.0 (TID 1177)
15/08/21 13:56:37 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 1161) in 16841 ms on localhost (145/200)
15/08/21 13:56:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:56:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:38 INFO Executor: Finished task 145.0 in stage 8.0 (TID 1162). 1874 bytes result sent to driver
15/08/21 13:56:38 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 1178, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:38 INFO Executor: Running task 161.0 in stage 8.0 (TID 1178)
15/08/21 13:56:38 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:38 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:38 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 1162) in 17947 ms on localhost (146/200)
15/08/21 13:56:39 INFO Executor: Finished task 146.0 in stage 8.0 (TID 1163). 1601 bytes result sent to driver
15/08/21 13:56:39 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 1179, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:39 INFO Executor: Running task 162.0 in stage 8.0 (TID 1179)
15/08/21 13:56:39 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:39 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 1163) in 18784 ms on localhost (147/200)
15/08/21 13:56:39 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:43 INFO Executor: Finished task 148.0 in stage 8.0 (TID 1165). 1942 bytes result sent to driver
15/08/21 13:56:43 INFO Executor: Finished task 147.0 in stage 8.0 (TID 1164). 1669 bytes result sent to driver
15/08/21 13:56:43 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 1180, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:43 INFO Executor: Running task 163.0 in stage 8.0 (TID 1180)
15/08/21 13:56:43 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 1181, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:43 INFO Executor: Running task 164.0 in stage 8.0 (TID 1181)
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:56:43 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 1165) in 22048 ms on localhost (148/200)
15/08/21 13:56:43 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 1164) in 22908 ms on localhost (149/200)
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:43 INFO Executor: Finished task 149.0 in stage 8.0 (TID 1166). 2010 bytes result sent to driver
15/08/21 13:56:43 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 1182, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:43 INFO Executor: Running task 165.0 in stage 8.0 (TID 1182)
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:43 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 1166) in 22217 ms on localhost (150/200)
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:44 INFO Executor: Finished task 150.0 in stage 8.0 (TID 1167). 1941 bytes result sent to driver
15/08/21 13:56:44 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 1183, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:44 INFO Executor: Running task 166.0 in stage 8.0 (TID 1183)
15/08/21 13:56:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:44 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 1167) in 21267 ms on localhost (151/200)
15/08/21 13:56:44 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:50 INFO Executor: Finished task 151.0 in stage 8.0 (TID 1168). 1805 bytes result sent to driver
15/08/21 13:56:50 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 1184, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:50 INFO Executor: Running task 167.0 in stage 8.0 (TID 1184)
15/08/21 13:56:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:50 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 1168) in 25575 ms on localhost (152/200)
15/08/21 13:56:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:51 INFO Executor: Finished task 152.0 in stage 8.0 (TID 1169). 1738 bytes result sent to driver
15/08/21 13:56:51 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 1185, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:51 INFO Executor: Running task 168.0 in stage 8.0 (TID 1185)
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:51 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 1169) in 26038 ms on localhost (153/200)
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:51 INFO Executor: Finished task 153.0 in stage 8.0 (TID 1170). 1737 bytes result sent to driver
15/08/21 13:56:51 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 1186, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:51 INFO Executor: Running task 169.0 in stage 8.0 (TID 1186)
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:51 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 1170) in 21293 ms on localhost (154/200)
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:52 INFO Executor: Finished task 154.0 in stage 8.0 (TID 1171). 1876 bytes result sent to driver
15/08/21 13:56:52 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 1187, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:52 INFO Executor: Running task 170.0 in stage 8.0 (TID 1187)
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:52 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 1171) in 21836 ms on localhost (155/200)
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:52 INFO Executor: Finished task 155.0 in stage 8.0 (TID 1172). 1601 bytes result sent to driver
15/08/21 13:56:52 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 1188, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:52 INFO Executor: Running task 171.0 in stage 8.0 (TID 1188)
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:52 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 1172) in 21847 ms on localhost (156/200)
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:58 INFO Executor: Finished task 156.0 in stage 8.0 (TID 1173). 1737 bytes result sent to driver
15/08/21 13:56:58 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 1189, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:58 INFO Executor: Running task 172.0 in stage 8.0 (TID 1189)
15/08/21 13:56:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:58 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 1173) in 27505 ms on localhost (157/200)
15/08/21 13:56:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO Executor: Finished task 157.0 in stage 8.0 (TID 1174). 1805 bytes result sent to driver
15/08/21 13:56:59 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 1190, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:59 INFO Executor: Running task 173.0 in stage 8.0 (TID 1190)
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 1174) in 27445 ms on localhost (158/200)
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO Executor: Finished task 158.0 in stage 8.0 (TID 1175). 1940 bytes result sent to driver
15/08/21 13:56:59 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 1191, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:59 INFO Executor: Running task 174.0 in stage 8.0 (TID 1191)
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 1175) in 22725 ms on localhost (159/200)
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO Executor: Finished task 159.0 in stage 8.0 (TID 1176). 1670 bytes result sent to driver
15/08/21 13:56:59 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 1192, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:59 INFO Executor: Running task 175.0 in stage 8.0 (TID 1192)
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:56:59 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 1176) in 22803 ms on localhost (160/200)
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO Executor: Finished task 160.0 in stage 8.0 (TID 1177). 1602 bytes result sent to driver
15/08/21 13:56:59 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 1193, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:56:59 INFO Executor: Running task 176.0 in stage 8.0 (TID 1193)
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 1177) in 22461 ms on localhost (161/200)
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:56:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:01 INFO Executor: Finished task 161.0 in stage 8.0 (TID 1178). 1670 bytes result sent to driver
15/08/21 13:57:01 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 1194, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:01 INFO Executor: Running task 177.0 in stage 8.0 (TID 1194)
15/08/21 13:57:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:01 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 1178) in 23673 ms on localhost (162/200)
15/08/21 13:57:01 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:07 INFO Executor: Finished task 162.0 in stage 8.0 (TID 1179). 1738 bytes result sent to driver
15/08/21 13:57:07 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 1195, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:07 INFO Executor: Running task 178.0 in stage 8.0 (TID 1195)
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:07 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 1179) in 28163 ms on localhost (163/200)
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:07 INFO Executor: Finished task 163.0 in stage 8.0 (TID 1180). 1670 bytes result sent to driver
15/08/21 13:57:07 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 1196, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:07 INFO Executor: Running task 179.0 in stage 8.0 (TID 1196)
15/08/21 13:57:07 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 1180) in 24397 ms on localhost (164/200)
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:08 INFO Executor: Finished task 164.0 in stage 8.0 (TID 1181). 1602 bytes result sent to driver
15/08/21 13:57:08 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 1197, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:08 INFO Executor: Running task 180.0 in stage 8.0 (TID 1197)
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:08 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 1181) in 24603 ms on localhost (165/200)
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:08 INFO Executor: Finished task 165.0 in stage 8.0 (TID 1182). 1669 bytes result sent to driver
15/08/21 13:57:08 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 1198, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:08 INFO Executor: Running task 181.0 in stage 8.0 (TID 1198)
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:08 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 1182) in 24750 ms on localhost (166/200)
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:08 INFO Executor: Finished task 166.0 in stage 8.0 (TID 1183). 1737 bytes result sent to driver
15/08/21 13:57:08 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 1199, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:08 INFO Executor: Running task 182.0 in stage 8.0 (TID 1199)
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:08 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 1183) in 24023 ms on localhost (167/200)
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:09 INFO Executor: Finished task 167.0 in stage 8.0 (TID 1184). 1738 bytes result sent to driver
15/08/21 13:57:09 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 1200, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:09 INFO Executor: Running task 183.0 in stage 8.0 (TID 1200)
15/08/21 13:57:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:09 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 1184) in 18678 ms on localhost (168/200)
15/08/21 13:57:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:26 INFO Executor: Finished task 168.0 in stage 8.0 (TID 1185). 1668 bytes result sent to driver
15/08/21 13:57:26 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 1201, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:26 INFO Executor: Running task 184.0 in stage 8.0 (TID 1201)
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:26 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 1185) in 34882 ms on localhost (169/200)
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:26 INFO Executor: Finished task 169.0 in stage 8.0 (TID 1186). 1942 bytes result sent to driver
15/08/21 13:57:26 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 1202, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:26 INFO Executor: Running task 185.0 in stage 8.0 (TID 1202)
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:26 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 1186) in 35023 ms on localhost (170/200)
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:27 INFO Executor: Finished task 171.0 in stage 8.0 (TID 1188). 1804 bytes result sent to driver
15/08/21 13:57:27 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 1203, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:27 INFO Executor: Running task 186.0 in stage 8.0 (TID 1203)
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:27 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 1188) in 34830 ms on localhost (171/200)
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:27 INFO Executor: Finished task 170.0 in stage 8.0 (TID 1187). 1736 bytes result sent to driver
15/08/21 13:57:27 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 1204, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:27 INFO Executor: Running task 187.0 in stage 8.0 (TID 1204)
15/08/21 13:57:27 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 1187) in 35163 ms on localhost (172/200)
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:28 INFO Executor: Finished task 172.0 in stage 8.0 (TID 1189). 1736 bytes result sent to driver
15/08/21 13:57:28 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 1205, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:28 INFO Executor: Running task 188.0 in stage 8.0 (TID 1205)
15/08/21 13:57:28 INFO Executor: Finished task 173.0 in stage 8.0 (TID 1190). 1738 bytes result sent to driver
15/08/21 13:57:28 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 1206, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:28 INFO Executor: Running task 189.0 in stage 8.0 (TID 1206)
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:28 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 1190) in 29922 ms on localhost (173/200)
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:28 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 1189) in 30171 ms on localhost (174/200)
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:31 INFO Executor: Finished task 174.0 in stage 8.0 (TID 1191). 1875 bytes result sent to driver
15/08/21 13:57:31 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 1207, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:31 INFO Executor: Running task 190.0 in stage 8.0 (TID 1207)
15/08/21 13:57:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:31 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 1191) in 32571 ms on localhost (175/200)
15/08/21 13:57:31 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:31 INFO Executor: Finished task 175.0 in stage 8.0 (TID 1192). 2011 bytes result sent to driver
15/08/21 13:57:31 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 1208, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:31 INFO Executor: Running task 191.0 in stage 8.0 (TID 1208)
15/08/21 13:57:31 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 1192) in 32563 ms on localhost (176/200)
15/08/21 13:57:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:32 INFO Executor: Finished task 176.0 in stage 8.0 (TID 1193). 1602 bytes result sent to driver
15/08/21 13:57:32 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 1209, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:32 INFO Executor: Running task 192.0 in stage 8.0 (TID 1209)
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:32 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 1193) in 33114 ms on localhost (177/200)
15/08/21 13:57:34 INFO Executor: Finished task 177.0 in stage 8.0 (TID 1194). 1670 bytes result sent to driver
15/08/21 13:57:34 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 1210, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:34 INFO Executor: Running task 193.0 in stage 8.0 (TID 1210)
15/08/21 13:57:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 13:57:34 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 1194) in 32813 ms on localhost (178/200)
15/08/21 13:57:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:40 INFO Executor: Finished task 178.0 in stage 8.0 (TID 1195). 1806 bytes result sent to driver
15/08/21 13:57:40 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 1211, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:40 INFO Executor: Running task 194.0 in stage 8.0 (TID 1211)
15/08/21 13:57:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:40 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 1195) in 33018 ms on localhost (179/200)
15/08/21 13:57:40 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:40 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:42 INFO Executor: Finished task 179.0 in stage 8.0 (TID 1196). 1738 bytes result sent to driver
15/08/21 13:57:42 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 1212, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:42 INFO Executor: Running task 195.0 in stage 8.0 (TID 1212)
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:42 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 1196) in 34326 ms on localhost (180/200)
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:42 INFO Executor: Finished task 180.0 in stage 8.0 (TID 1197). 1737 bytes result sent to driver
15/08/21 13:57:42 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 1213, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:42 INFO Executor: Running task 196.0 in stage 8.0 (TID 1213)
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:42 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 1197) in 34151 ms on localhost (181/200)
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:43 INFO Executor: Finished task 182.0 in stage 8.0 (TID 1199). 1802 bytes result sent to driver
15/08/21 13:57:43 INFO Executor: Finished task 181.0 in stage 8.0 (TID 1198). 1736 bytes result sent to driver
15/08/21 13:57:43 INFO Executor: Finished task 183.0 in stage 8.0 (TID 1200). 1600 bytes result sent to driver
15/08/21 13:57:43 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 1214, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:43 INFO Executor: Running task 197.0 in stage 8.0 (TID 1214)
15/08/21 13:57:43 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 1215, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:43 INFO Executor: Running task 198.0 in stage 8.0 (TID 1215)
15/08/21 13:57:43 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 1216, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 13:57:43 INFO Executor: Running task 199.0 in stage 8.0 (TID 1216)
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:43 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 1198) in 35256 ms on localhost (182/200)
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:43 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 1199) in 34880 ms on localhost (183/200)
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 13:57:43 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 1200) in 34495 ms on localhost (184/200)
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 13:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 13:57:49 INFO Executor: Finished task 186.0 in stage 8.0 (TID 1203). 1670 bytes result sent to driver
15/08/21 13:57:49 INFO Executor: Finished task 184.0 in stage 8.0 (TID 1201). 1737 bytes result sent to driver
15/08/21 13:57:49 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 1201) in 22980 ms on localhost (185/200)
15/08/21 13:57:49 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 1203) in 21654 ms on localhost (186/200)
15/08/21 13:57:49 INFO Executor: Finished task 185.0 in stage 8.0 (TID 1202). 1602 bytes result sent to driver
15/08/21 13:57:49 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 1202) in 22863 ms on localhost (187/200)
15/08/21 13:57:49 INFO Executor: Finished task 187.0 in stage 8.0 (TID 1204). 1602 bytes result sent to driver
15/08/21 13:57:49 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 1204) in 21961 ms on localhost (188/200)
15/08/21 13:57:50 INFO Executor: Finished task 188.0 in stage 8.0 (TID 1205). 1601 bytes result sent to driver
15/08/21 13:57:50 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 1205) in 21426 ms on localhost (189/200)
15/08/21 13:57:51 INFO Executor: Finished task 189.0 in stage 8.0 (TID 1206). 1668 bytes result sent to driver
15/08/21 13:57:51 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 1206) in 22196 ms on localhost (190/200)
15/08/21 13:57:51 INFO Executor: Finished task 190.0 in stage 8.0 (TID 1207). 1941 bytes result sent to driver
15/08/21 13:57:51 INFO Executor: Finished task 191.0 in stage 8.0 (TID 1208). 1669 bytes result sent to driver
15/08/21 13:57:51 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 1207) in 19531 ms on localhost (191/200)
15/08/21 13:57:51 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 1208) in 19400 ms on localhost (192/200)
15/08/21 13:57:51 INFO Executor: Finished task 192.0 in stage 8.0 (TID 1209). 1669 bytes result sent to driver
15/08/21 13:57:51 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 1209) in 19029 ms on localhost (193/200)
15/08/21 13:57:52 INFO Executor: Finished task 193.0 in stage 8.0 (TID 1210). 1669 bytes result sent to driver
15/08/21 13:57:52 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 1210) in 17813 ms on localhost (194/200)
15/08/21 13:57:52 INFO Executor: Finished task 194.0 in stage 8.0 (TID 1211). 1426 bytes result sent to driver
15/08/21 13:57:52 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 1211) in 12243 ms on localhost (195/200)
15/08/21 13:57:58 INFO Executor: Finished task 195.0 in stage 8.0 (TID 1212). 1602 bytes result sent to driver
15/08/21 13:57:58 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 1212) in 16654 ms on localhost (196/200)
15/08/21 13:57:59 INFO Executor: Finished task 196.0 in stage 8.0 (TID 1213). 1670 bytes result sent to driver
15/08/21 13:57:59 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 1213) in 16984 ms on localhost (197/200)
15/08/21 13:57:59 INFO Executor: Finished task 197.0 in stage 8.0 (TID 1214). 1667 bytes result sent to driver
15/08/21 13:57:59 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 1214) in 15750 ms on localhost (198/200)
15/08/21 13:57:59 INFO Executor: Finished task 198.0 in stage 8.0 (TID 1215). 1601 bytes result sent to driver
15/08/21 13:57:59 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 1215) in 15799 ms on localhost (199/200)
15/08/21 13:57:59 INFO Executor: Finished task 199.0 in stage 8.0 (TID 1216). 1874 bytes result sent to driver
15/08/21 13:57:59 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 1216) in 15856 ms on localhost (200/200)
15/08/21 13:57:59 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/21 13:57:59 INFO DAGScheduler: ResultStage 8 (processCmd at CliDriver.java:423) finished in 332.805 s
15/08/21 13:57:59 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3ce322af
15/08/21 13:57:59 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:423, took 563.867085 s
15/08/21 13:57:59 INFO StatsReportListener: task runtime:(count: 200, mean: 26176.080000, stdev: 5747.884051, max: 38419.000000, min: 12243.000000)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	12.2 s	16.8 s	18.4 s	22.2 s	25.4 s	31.0 s	34.2 s	34.9 s	38.4 s
15/08/21 13:57:59 INFO StatsReportListener: fetch wait time:(count: 200, mean: 1.485000, stdev: 2.220310, max: 15.000000, min: 0.000000)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	3.0 ms	7.0 ms	15.0 ms
15/08/21 13:57:59 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 13:57:59 INFO StatsReportListener: task result size:(count: 200, mean: 1735.070000, stdev: 135.216179, max: 2150.000000, min: 1426.000000)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	1426.0 B	1426.0 B	1602.0 B	1669.0 B	1737.0 B	1805.0 B	1940.0 B	1943.0 B	2.1 KB
15/08/21 13:57:59 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 99.109780, stdev: 4.531198, max: 99.933765, min: 57.815195)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	58 %	98 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 13:57:59 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.005812, stdev: 0.008812, max: 0.057010, min: 0.000000)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 13:57:59 INFO StatsReportListener: other time pct: (count: 200, mean: 0.884408, stdev: 4.531334, max: 42.178959, min: 0.066235)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 2 %	42 %
15/08/21 13:57:59 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 13:57:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:57:59 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 13:57:59 INFO DAGScheduler: Got job 3 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 13:57:59 INFO DAGScheduler: Final stage: ResultStage 9(processCmd at CliDriver.java:423)
15/08/21 13:57:59 INFO DAGScheduler: Parents of final stage: List()
15/08/21 13:57:59 INFO DAGScheduler: Missing parents: List()
15/08/21 13:57:59 INFO DAGScheduler: Submitting ResultStage 9 (ParallelCollectionRDD[45] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 13:57:59 INFO MemoryStore: ensureFreeSpace(71408) called with curMem=1422078, maxMem=22226833244
15/08/21 13:57:59 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 69.7 KB, free 20.7 GB)
15/08/21 13:57:59 INFO MemoryStore: ensureFreeSpace(26159) called with curMem=1493486, maxMem=22226833244
15/08/21 13:57:59 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.5 KB, free 20.7 GB)
15/08/21 13:57:59 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:45550 (size: 25.5 KB, free: 20.7 GB)
15/08/21 13:57:59 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874
15/08/21 13:57:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (ParallelCollectionRDD[45] at processCmd at CliDriver.java:423)
15/08/21 13:57:59 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/08/21 13:57:59 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 1217, localhost, PROCESS_LOCAL, 8222 bytes)
15/08/21 13:57:59 INFO Executor: Running task 0.0 in stage 9.0 (TID 1217)
15/08/21 13:57:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 13:57:59 INFO CodecConfig: Compression: GZIP
15/08/21 13:57:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 13:57:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 13:57:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 13:57:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 13:57:59 INFO ParquetOutputFormat: Validation is off
15/08/21 13:57:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 13:57:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 13:57:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 40,692,456
15/08/21 13:57:59 INFO ColumnChunkPageWriteStore: written 615B for [c_name] BINARY: 100 values, 2,207B raw, 551B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:57:59 INFO ColumnChunkPageWriteStore: written 455B for [c_custkey] INT32: 100 values, 407B raw, 419B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:57:59 INFO ColumnChunkPageWriteStore: written 466B for [o_orderkey] INT32: 100 values, 407B raw, 430B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:57:59 INFO ColumnChunkPageWriteStore: written 441B for [o_orderdate] BINARY: 100 values, 1,407B raw, 393B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:57:59 INFO ColumnChunkPageWriteStore: written 622B for [o_totalprice] DOUBLE: 100 values, 807B raw, 578B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/21 13:57:59 INFO ColumnChunkPageWriteStore: written 141B for [sum_quantity] DOUBLE: 100 values, 74B raw, 97B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE, BIT_PACKED], dic { 18 entries, 144B raw, 18B comp}
15/08/21 13:57:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508211357_0009_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_large_volume_customer_par/_temporary/0/task_201508211357_0009_m_000000
15/08/21 13:57:59 INFO SparkHadoopMapRedUtil: attempt_201508211357_0009_m_000000_0: Committed
15/08/21 13:57:59 INFO Executor: Finished task 0.0 in stage 9.0 (TID 1217). 577 bytes result sent to driver
15/08/21 13:57:59 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 1217) in 193 ms on localhost (1/1)
15/08/21 13:57:59 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/21 13:57:59 INFO DAGScheduler: ResultStage 9 (processCmd at CliDriver.java:423) finished in 0.193 s
15/08/21 13:57:59 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@285f0a62
15/08/21 13:57:59 INFO DAGScheduler: Job 3 finished: processCmd at CliDriver.java:423, took 0.252388 s
15/08/21 13:57:59 INFO StatsReportListener: task runtime:(count: 1, mean: 193.000000, stdev: 0.000000, max: 193.000000, min: 193.000000)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	193.0 ms	193.0 ms	193.0 ms	193.0 ms	193.0 ms	193.0 ms	193.0 ms	193.0 ms	193.0 ms
15/08/21 13:57:59 INFO StatsReportListener: task result size:(count: 1, mean: 577.000000, stdev: 0.000000, max: 577.000000, min: 577.000000)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B
15/08/21 13:57:59 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 68.393782, stdev: 0.000000, max: 68.393782, min: 68.393782)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	68 %	68 %	68 %	68 %	68 %	68 %	68 %	68 %	68 %
15/08/21 13:57:59 INFO StatsReportListener: other time pct: (count: 1, mean: 31.606218, stdev: 0.000000, max: 31.606218, min: 31.606218)
15/08/21 13:57:59 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:57:59 INFO StatsReportListener: 	32 %	32 %	32 %	32 %	32 %	32 %	32 %	32 %	32 %
15/08/21 13:58:00 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:58:00 INFO DefaultWriterContainer: Job job_201508211357_0000 committed.
15/08/21 13:58:00 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 13:58:00 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_large_volume_customer_par/_common_metadata
15/08/21 13:58:00 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 13:58:00 INFO DAGScheduler: Got job 4 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 13:58:00 INFO DAGScheduler: Final stage: ResultStage 10(processCmd at CliDriver.java:423)
15/08/21 13:58:00 INFO DAGScheduler: Parents of final stage: List()
15/08/21 13:58:00 INFO DAGScheduler: Missing parents: List()
15/08/21 13:58:00 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 13:58:00 INFO MemoryStore: ensureFreeSpace(3104) called with curMem=1519645, maxMem=22226833244
15/08/21 13:58:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.0 KB, free 20.7 GB)
15/08/21 13:58:00 INFO MemoryStore: ensureFreeSpace(1802) called with curMem=1522749, maxMem=22226833244
15/08/21 13:58:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1802.0 B, free 20.7 GB)
15/08/21 13:58:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:45550 (size: 1802.0 B, free: 20.7 GB)
15/08/21 13:58:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874
15/08/21 13:58:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423)
15/08/21 13:58:00 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
15/08/21 13:58:00 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 1218, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 13:58:00 INFO Executor: Running task 0.0 in stage 10.0 (TID 1218)
15/08/21 13:58:00 INFO Executor: Finished task 0.0 in stage 10.0 (TID 1218). 606 bytes result sent to driver
15/08/21 13:58:00 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 1218) in 11 ms on localhost (1/1)
15/08/21 13:58:00 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/21 13:58:00 INFO DAGScheduler: ResultStage 10 (processCmd at CliDriver.java:423) finished in 0.012 s
15/08/21 13:58:00 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@57507e65
15/08/21 13:58:00 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:423, took 0.025099 s
15/08/21 13:58:00 INFO StatsReportListener: task runtime:(count: 1, mean: 11.000000, stdev: 0.000000, max: 11.000000, min: 11.000000)
Time taken: 566.416 seconds15/08/21 13:58:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%

15/08/21 13:58:00 INFO StatsReportListener: 	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms
15/08/21 13:58:00 INFO CliDriver: Time taken: 566.416 seconds
15/08/21 13:58:00 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 13:58:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:58:00 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 13:58:00 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 13:58:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:58:00 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 13:58:00 INFO StatsReportListener: other time pct: (count: 1, mean: 100.000000, stdev: 0.000000, max: 100.000000, min: 100.000000)
15/08/21 13:58:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 13:58:00 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/21 13:58:00 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/21 13:58:00 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/21 13:58:00 INFO DAGScheduler: Stopping DAGScheduler
15/08/21 13:58:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/21 13:58:00 INFO Utils: path = /tmp/spark-f0275731-dec3-43ab-821d-4d526d08464c/blockmgr-74647b95-16db-4827-8c41-296cefe8ca3b, already present as root for deletion.
15/08/21 13:58:00 INFO MemoryStore: MemoryStore cleared
15/08/21 13:58:00 INFO BlockManager: BlockManager stopped
15/08/21 13:58:00 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/21 13:58:00 INFO SparkContext: Successfully stopped SparkContext
15/08/21 13:58:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/21 13:58:00 INFO Utils: Shutdown hook called
15/08/21 13:58:00 INFO Utils: Deleting directory /tmp/spark-f0275731-dec3-43ab-821d-4d526d08464c
15/08/21 13:58:00 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/21 13:58:00 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/21 13:58:00 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/08/21 13:58:04 INFO Utils: Deleting directory /tmp/spark-962f33d0-e7b6-4c38-ab3a-4d1f5c0448e9
15/08/21 13:58:04 INFO Utils: Deleting directory /tmp/spark-46f8825c-2857-4d4b-90a5-c57344cdf31b
