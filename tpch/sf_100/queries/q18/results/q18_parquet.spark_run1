 -- the query
insert into table q18_tmp_par
select 
  l_orderkey, sum(l_quantity) as t_sum_quantity
from 
  lineitem_par
group by l_orderkey;

insert into table q18_large_volume_customer_par
select 
  c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice,sum(l_quantity)
from 
  customer_par c join orders_par o 
  on 
    c.c_custkey = o.o_custkey
  join q18_tmp_par t 
  on 
    o.o_orderkey = t.l_orderkey and t.t_sum_quantity > 315
  join lineitem_par l 
  on 
    o.o_orderkey = l.l_orderkey
group by c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice
order by o_totalprice desc,o_orderdate
limit 100;
15/08/19 18:11:42 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/19 18:11:42 INFO metastore: Connected to metastore.
15/08/19 18:11:43 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/19 18:11:44 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:44 INFO SparkContext: Running Spark version 1.4.1
15/08/19 18:11:44 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:44 INFO SecurityManager: Changing view acls to: hive
15/08/19 18:11:44 INFO SecurityManager: Changing modify acls to: hive
15/08/19 18:11:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/19 18:11:45 INFO Slf4jLogger: Slf4jLogger started
15/08/19 18:11:45 INFO Remoting: Starting remoting
15/08/19 18:11:45 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:37801]
15/08/19 18:11:45 INFO Utils: Successfully started service 'sparkDriver' on port 37801.
15/08/19 18:11:45 INFO SparkEnv: Registering MapOutputTracker
15/08/19 18:11:45 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:45 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:45 INFO SparkEnv: Registering BlockManagerMaster
15/08/19 18:11:45 INFO DiskBlockManager: Created local directory at /tmp/spark-5bca4007-eaeb-411d-91ad-eee4fafc7e75/blockmgr-81695391-cdd1-4610-bfcd-5ca0a755c1a3
15/08/19 18:11:45 INFO MemoryStore: MemoryStore started with capacity 20.7 GB
15/08/19 18:11:45 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:45 INFO HttpFileServer: HTTP File server directory is /tmp/spark-5bca4007-eaeb-411d-91ad-eee4fafc7e75/httpd-5a12d097-a7f5-4933-9e4b-4f00090ca255
15/08/19 18:11:45 INFO HttpServer: Starting HTTP Server
15/08/19 18:11:46 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/19 18:11:46 INFO AbstractConnector: Started SocketConnector@0.0.0.0:59297
15/08/19 18:11:46 INFO Utils: Successfully started service 'HTTP file server' on port 59297.
15/08/19 18:11:46 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/19 18:11:46 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/19 18:11:46 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/19 18:11:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/19 18:11:46 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/19 18:11:46 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:46 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:46 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:46 INFO Executor: Starting executor ID driver on host localhost
15/08/19 18:11:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36176.
15/08/19 18:11:46 INFO NettyBlockTransferService: Server created on 36176
15/08/19 18:11:46 INFO BlockManagerMaster: Trying to register BlockManager
15/08/19 18:11:46 INFO BlockManagerMasterEndpoint: Registering block manager localhost:36176 with 20.7 GB RAM, BlockManagerId(driver, localhost, 36176)
15/08/19 18:11:46 INFO BlockManagerMaster: Registered BlockManager
15/08/19 18:11:47 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/19 18:11:47 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/19 18:11:47 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/19 18:11:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/19 18:11:48 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/19 18:11:48 INFO metastore: Connected to metastore.
15/08/19 18:11:49 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/19 18:11:49 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/19 18:11:49 INFO ParseDriver: Parsing command: -- the query
insert into table q18_tmp_par
select 
  l_orderkey, sum(l_quantity) as t_sum_quantity
from 
  lineitem_par
group by l_orderkey
15/08/19 18:11:50 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/19 18:11:53 INFO MemoryStore: ensureFreeSpace(326528) called with curMem=0, maxMem=22226833244
15/08/19 18:11:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/19 18:11:53 INFO MemoryStore: ensureFreeSpace(22794) called with curMem=326528, maxMem=22226833244
15/08/19 18:11:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/19 18:11:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:36176 (size: 22.3 KB, free: 20.7 GB)
15/08/19 18:11:53 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/19 18:11:54 INFO Exchange: Using SparkSqlSerializer2.
15/08/19 18:11:54 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/19 18:11:54 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/19 18:11:54 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/19 18:11:54 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/19 18:11:54 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/19 18:11:54 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/19 18:11:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:11:54 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/19 18:11:54 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/19 18:11:54 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/19 18:11:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/19 18:11:54 INFO DAGScheduler: Registering RDD 3 (processCmd at CliDriver.java:423)
15/08/19 18:11:54 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/19 18:11:54 INFO DAGScheduler: Final stage: ResultStage 1(processCmd at CliDriver.java:423)
15/08/19 18:11:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15/08/19 18:11:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15/08/19 18:11:54 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423), which has no missing parents
15/08/19 18:11:54 INFO MemoryStore: ensureFreeSpace(9208) called with curMem=349322, maxMem=22226833244
15/08/19 18:11:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 20.7 GB)
15/08/19 18:11:54 INFO MemoryStore: ensureFreeSpace(4552) called with curMem=358530, maxMem=22226833244
15/08/19 18:11:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 20.7 GB)
15/08/19 18:11:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:36176 (size: 4.4 KB, free: 20.7 GB)
15/08/19 18:11:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/08/19 18:11:55 INFO DAGScheduler: Submitting 85 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423)
15/08/19 18:11:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 85 tasks
15/08/19 18:11:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1757 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1770 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, ANY, 1757 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, ANY, 1770 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, ANY, 1755 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, ANY, 1771 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, ANY, 1758 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, ANY, 1769 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, ANY, 1758 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, ANY, 1769 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, localhost, ANY, 1758 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, localhost, ANY, 1770 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, localhost, ANY, 1757 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, localhost, ANY, 1771 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, localhost, ANY, 1757 bytes)
15/08/19 18:11:55 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, localhost, ANY, 1771 bytes)
15/08/19 18:11:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/19 18:11:55 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
15/08/19 18:11:55 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
15/08/19 18:11:55 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
15/08/19 18:11:55 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
15/08/19 18:11:55 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
15/08/19 18:11:55 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
15/08/19 18:11:55 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/08/19 18:11:55 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
15/08/19 18:11:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/19 18:11:55 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/08/19 18:11:55 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
15/08/19 18:11:55 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
15/08/19 18:11:55 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
15/08/19 18:11:55 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
15/08/19 18:11:55 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 134217728 end: 260413554 length: 126195826 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 134217728 end: 258497939 length: 124280211 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 134217728 end: 260663202 length: 126445474 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 134217728 end: 262730160 length: 128512432 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 134217728 end: 258365400 length: 124147672 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 134217728 end: 258787350 length: 124569622 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 134217728 end: 261069615 length: 126851887 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 134217728 end: 259054175 length: 124836447 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/19 18:11:55 INFO CodecPool: Got brand-new decompressor [.snappy]
19-Aug-2015 18:11:51 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
19-Aug-2015 18:11:51 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
19-Aug-2015 18:11:51 INFO: parquet.hadoop.ParquetFileReader: reading another 42 footers
19-Aug-2015 18:11:51 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3663081 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501536 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3606822 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3663748 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609919 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502843 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609688 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3662666 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609665 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3726777 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501133 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501210 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 200 ms. row count = 3501210
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 207 ms. row count = 3502843
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 208 ms. row count = 3500100
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 209 ms. row count = 3500949
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 212 ms. row count = 15/08/19 18:12:25 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2125 bytes result sent to driver
15/08/19 18:12:25 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 2125 bytes result sent to driver
15/08/19 18:12:25 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, localhost, ANY, 1758 bytes)
15/08/19 18:12:25 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
15/08/19 18:12:25 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, localhost, ANY, 1769 bytes)
15/08/19 18:12:25 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
15/08/19 18:12:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 134217728 end: 258769491 length: 124551763 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:25 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 30531 ms on localhost (1/85)
15/08/19 18:12:25 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 30523 ms on localhost (2/85)
15/08/19 18:12:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2125 bytes result sent to driver
15/08/19 18:12:26 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, localhost, ANY, 1757 bytes)
15/08/19 18:12:26 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
15/08/19 18:12:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 31587 ms on localhost (3/85)
15/08/19 18:12:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:27 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 2125 bytes result sent to driver
15/08/19 18:12:27 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, localhost, ANY, 1770 bytes)
15/08/19 18:12:27 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 2125 bytes result sent to driver
15/08/19 18:12:27 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
15/08/19 18:12:27 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20, localhost, ANY, 1757 bytes)
15/08/19 18:12:27 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
15/08/19 18:12:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 134217728 end: 258780135 length: 124562407 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:27 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 31937 ms on localhost (4/85)
15/08/19 18:12:27 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 31943 ms on localhost (5/85)
15/08/19 18:12:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:27 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 2125 bytes result sent to driver
15/08/19 18:12:27 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21, localhost, ANY, 1770 bytes)
15/08/19 18:12:27 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
15/08/19 18:12:27 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 32146 ms on localhost (6/85)
15/08/19 18:12:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 134217728 end: 258792739 length: 124575011 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
3500100
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 213 ms. row count = 3500741
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 219 ms. row count = 3501536
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 219 ms. row count = 3500100
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 223 ms. row count = 3503026
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 230 ms. row count = 3500100
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 230 ms. row count = 3500100
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 230 ms. row count = 3501133
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 229 ms. row count = 3501123
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 231 ms. row count = 3500100
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 230 ms. row count = 3502953
19-Aug-2015 18:11:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 232 ms. row count = 3503264
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502953 records from 2 columns in 27037 ms: 129.56145 rec/ms, 259.1229 cell/ms
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (230 ms) and 99% processing (27037 ms)
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502953. reading next block
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 160128
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501123 records from 2 columns in 27600 ms: 126.85228 rec/ms, 253.70456 cell/ms
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (229 ms) and 99% processing (27600 ms)
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501123. reading next block
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 27607 ms: 126.783066 rec/ms, 253.56613 cell/ms
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (231 ms) and 99% processing (27607 ms)
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 108565
19-Aug-2015 18:12:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 109819
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500741 records from 2 columns in 28950 ms: 120.9237 rec/ms, 241.8474 cell/ms
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (213 ms) and 99% processing (28950 ms)
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500741. reading next block
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 163007
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503264 records from 2 columns in 29203 ms: 119.96247 rec/ms, 239.92494 cell/ms
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (232 ms) and 99% processing (29203 ms)
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503264. reading next block
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 223513
19-Aug-2015 18:12:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609845 records.
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 44 ms. row count = 3500974
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 3500100
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503026 records from 2 columns in 29660 ms: 118.10607 rec/ms, 236.21214 cell/ms
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (223 ms) and 99% processing (29660 ms)
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503026. reading next block
19-Aug-2015 18:12:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 106639
19-Aug-2015 18:12:26 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 30300 ms: 115.514854 rec/ms, 231.02971 cell/ms
19-Aug-2015 18:12:26 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (230 ms) and 99% processing (30300 ms)
19-Aug-2015 18:12:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:12:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 162566
19-Aug-2015 18:12:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501014 records.
19-Aug-2015 18:12:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 44 ms. row count = 3501014
19-Aug-2015 18:12:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608767 records.
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 47 ms. row count = 3500100
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 74 ms. row count = 3500100
19-Aug-2015 18:12:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.tas15/08/19 18:12:27 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 2125 bytes result sent to driver
15/08/19 18:12:27 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22, localhost, ANY, 1757 bytes)
15/08/19 18:12:27 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
15/08/19 18:12:27 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 32205 ms on localhost (7/85)
15/08/19 18:12:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:27 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 2125 bytes result sent to driver
15/08/19 18:12:27 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23, localhost, ANY, 1769 bytes)
15/08/19 18:12:27 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
15/08/19 18:12:27 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 32785 ms on localhost (8/85)
15/08/19 18:12:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 134217728 end: 260267299 length: 126049571 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:28 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2125 bytes result sent to driver
15/08/19 18:12:28 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24, localhost, ANY, 1758 bytes)
15/08/19 18:12:28 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
15/08/19 18:12:28 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 33255 ms on localhost (9/85)
15/08/19 18:12:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:34 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 2125 bytes result sent to driver
15/08/19 18:12:34 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25, localhost, ANY, 1771 bytes)
15/08/19 18:12:34 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
15/08/19 18:12:34 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 39053 ms on localhost (10/85)
15/08/19 18:12:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 134217728 end: 258509806 length: 124292078 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:34 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 2125 bytes result sent to driver
15/08/19 18:12:34 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26, localhost, ANY, 1758 bytes)
15/08/19 18:12:34 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
15/08/19 18:12:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:34 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 39162 ms on localhost (11/85)
15/08/19 18:12:34 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2125 bytes result sent to driver
15/08/19 18:12:34 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 2125 bytes result sent to driver
15/08/19 18:12:34 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27, localhost, ANY, 1770 bytes)
15/08/19 18:12:34 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
15/08/19 18:12:34 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28, localhost, ANY, 1758 bytes)
15/08/19 18:12:34 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
15/08/19 18:12:34 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 39869 ms on localhost (12/85)
15/08/19 18:12:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 134217728 end: 259215293 length: 124997565 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:34 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 39884 ms on localhost (13/85)
15/08/19 18:12:35 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2125 bytes result sent to driver
15/08/19 18:12:35 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29, localhost, ANY, 1770 bytes)
15/08/19 18:12:35 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
15/08/19 18:12:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 134217728 end: 258903039 length: 124685311 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:35 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 40293 ms on localhost (14/85)
15/08/19 18:12:35 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2125 bytes result sent to driver
15/08/19 18:12:35 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30, localhost, ANY, 1757 bytes)
15/08/19 18:12:35 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
15/08/19 18:12:35 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 40661 ms on localhost (15/85)
15/08/19 18:12:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
k.TaskAttemptContextImpl
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609476 records.
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3501078
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503194 records.
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 49 ms. row count = 3503194
19-Aug-2015 18:12:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3660722 records.
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 49 ms. row count = 3500100
19-Aug-2015 18:12:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500966 records.
19-Aug-2015 18:12:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 62 ms. row count = 3500966
19-Aug-2015 18:12:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608800 records.
19-Aug-2015 18:12:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501036 records.
19-Aug-2015 18:12:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 3502755
19-Aug-2015 18:12:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 56 ms. row count = 3501036
19-Aug-2015 18:12:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:35 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500807 records.
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609239 records.
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 100 ms. row count = 3500807
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 157 ms. row count = 3500100
19-Aug-2015 18:12:35 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609008 records.
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 143 ms. row count = 3501316
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500949 records from 2 columns in 39331 ms: 89.01246 rec/ms, 178.02492 cell/ms
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (209 ms) and 99% processing (39331 ms)
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500949. reading next block
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14 ms. row count = 105873
19-Aug-2015 18:12:35 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503166 records.
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 109 ms. row count = 3503166
19-Aug-2015 18:12:48 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500974 records from 2 columns in 23098 ms: 151.57043 rec/ms, 303.14087 cell/ms
19-Aug-2015 18:12:48 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (44 ms) and 99% processing (23098 ms)
19-Aug-2015 18:12:48 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500974. reading next block
19-Aug-2015 18:12:48 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 108871
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501078 records from 2 columns in 25898 ms: 135.1872 rec/ms, 270.3744 cell/ms
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (46 ms) and 99% processing (25898 ms)
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501078. reading next block
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 108398
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 26473 ms: 132.21396 rec/ms, 264.42792 cell/ms
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (74 ms) and 99% processing (26473 ms)
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 108667
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 25722 ms: 136.07417 rec/ms, 272.14835 cell/ms
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (49 ms) and 99% processing (25722 ms)
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:12:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in15/08/19 18:12:54 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2125 bytes result sent to driver
15/08/19 18:12:54 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31, localhost, ANY, 1770 bytes)
15/08/19 18:12:54 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
15/08/19 18:12:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 134217728 end: 259962410 length: 125744682 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:54 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 58990 ms on localhost (16/85)
15/08/19 18:12:58 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 2125 bytes result sent to driver
15/08/19 18:12:58 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32, localhost, ANY, 1757 bytes)
15/08/19 18:12:58 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 32996 ms on localhost (17/85)
15/08/19 18:12:58 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
15/08/19 18:12:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:58 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 2125 bytes result sent to driver
15/08/19 18:12:58 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33, localhost, ANY, 1771 bytes)
15/08/19 18:12:58 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
15/08/19 18:12:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 134217728 end: 260302148 length: 126084420 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:58 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 33234 ms on localhost (18/85)
15/08/19 18:12:59 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 2125 bytes result sent to driver
15/08/19 18:12:59 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34, localhost, ANY, 1757 bytes)
15/08/19 18:12:59 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
15/08/19 18:12:59 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 32122 ms on localhost (19/85)
15/08/19 18:12:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:59 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 2125 bytes result sent to driver
15/08/19 18:12:59 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35, localhost, ANY, 1773 bytes)
15/08/19 18:12:59 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
15/08/19 18:12:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 134217728 end: 261087966 length: 126870238 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:12:59 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 32074 ms on localhost (20/85)
15/08/19 18:12:59 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 2125 bytes result sent to driver
15/08/19 18:12:59 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36, localhost, ANY, 1758 bytes)
15/08/19 18:12:59 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
15/08/19 18:12:59 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 32212 ms on localhost (21/85)
15/08/19 18:12:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:04 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 2125 bytes result sent to driver
15/08/19 18:13:04 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37, localhost, ANY, 1771 bytes)
15/08/19 18:13:04 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 37607 ms on localhost (22/85)
15/08/19 18:13:04 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
15/08/19 18:13:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 134217728 end: 258061335 length: 123843607 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:04 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 2125 bytes result sent to driver
15/08/19 18:13:04 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38, localhost, ANY, 1757 bytes)
15/08/19 18:13:04 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
15/08/19 18:13:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:04 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 37325 ms on localhost (23/85)
15/08/19 18:13:04 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 2125 bytes result sent to driver
15/08/19 18:13:04 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39, localhost, ANY, 1770 bytes)
15/08/19 18:13:04 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
15/08/19 18:13:04 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 36162 ms on localhost (24/85)
15/08/19 18:13:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 134217728 end: 258877467 length: 124659739 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:04 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 2125 bytes result sent to driver
15/08/19 18:13:04 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, ANY, 1757 bytes)
15/08/19 18:13:04 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
15/08/19 18:13:04 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 36819 ms on localhost (25/85)
15/08/19 18:13:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:21 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 2125 bytes result sent to driver
15/08/19 18:13:21 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41, localhost, ANY, 1771 bytes)
15/08/19 18:13:21 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
15/08/19 18:13:21 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 47573 ms on localhost (26/85)
15/08/19 18:13:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 134217728 end: 258786694 length: 124568966 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:22 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 2125 bytes result sent to driver
15/08/19 18:13:22 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42, localhost, ANY, 1757 bytes)
15/08/19 18:13:22 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
15/08/19 18:13:22 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 47886 ms on localhost (27/85)
15/08/19 18:13:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
 memory in 24 ms. row count = 160622
19-Aug-2015 18:12:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3660809 records.
19-Aug-2015 18:12:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3502900
19-Aug-2015 18:12:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
19-Aug-2015 18:12:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3664018 records.
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502755 records from 2 columns in 24576 ms: 142.52747 rec/ms, 285.05493 cell/ms
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (89 ms) and 99% processing (24576 ms)
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502755. reading next block
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 106045
19-Aug-2015 18:12:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 49 ms. row count = 3503024
19-Aug-2015 18:12:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 70 ms. row count = 3500100
19-Aug-2015 18:12:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3663964 records.
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 64 ms. row count = 3501325
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 24140 ms: 144.99171 rec/ms, 289.98343 cell/ms
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (157 ms) and 99% processing (24140 ms)
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:12:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 109139
19-Aug-2015 18:12:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503694 records.
19-Aug-2015 18:13:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 3503694
19-Aug-2015 18:13:03 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501316 records from 2 columns in 28327 ms: 123.603485 rec/ms, 247.20697 cell/ms
19-Aug-2015 18:13:03 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (143 ms) and 99% processing (28327 ms)
19-Aug-2015 18:13:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501316. reading next block
19-Aug-2015 18:13:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 107692
19-Aug-2015 18:13:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3606669 records.
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 62 ms. row count = 3503512
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501619 records.
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3501619
19-Aug-2015 18:13:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608467 records.
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 3501092
19-Aug-2015 18:13:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
19-Aug-2015 18:13:21 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609766 records.
19-Aug-2015 18:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 96 ms. row count = 3500891
19-Aug-2015 18:13:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:22 INFO15/08/19 18:13:22 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 2125 bytes result sent to driver
15/08/19 18:13:22 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43, localhost, ANY, 1768 bytes)
15/08/19 18:13:22 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
15/08/19 18:13:22 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 48020 ms on localhost (28/85)
15/08/19 18:13:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 134217728 end: 260666224 length: 126448496 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:23 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 2125 bytes result sent to driver
15/08/19 18:13:23 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44, localhost, ANY, 1758 bytes)
15/08/19 18:13:23 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
15/08/19 18:13:23 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 48123 ms on localhost (29/85)
15/08/19 18:13:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:24 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 2125 bytes result sent to driver
15/08/19 18:13:24 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45, localhost, ANY, 1764 bytes)
15/08/19 18:13:24 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
15/08/19 18:13:24 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 48803 ms on localhost (30/85)
15/08/19 18:13:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:24 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 2125 bytes result sent to driver
15/08/19 18:13:24 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46, localhost, ANY, 1770 bytes)
15/08/19 18:13:24 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
15/08/19 18:13:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 268435456 end: 279508675 length: 11073219 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:24 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 49214 ms on localhost (31/85)
15/08/19 18:13:24 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 2125 bytes result sent to driver
15/08/19 18:13:24 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47, localhost, ANY, 1757 bytes)
15/08/19 18:13:24 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 119 ms on localhost (32/85)
15/08/19 18:13:24 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
15/08/19 18:13:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:39 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 2125 bytes result sent to driver
15/08/19 18:13:39 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48, localhost, ANY, 1770 bytes)
15/08/19 18:13:39 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
15/08/19 18:13:39 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 45267 ms on localhost (33/85)
15/08/19 18:13:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 134217728 end: 258866877 length: 124649149 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:40 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 2125 bytes result sent to driver
15/08/19 18:13:40 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49, localhost, ANY, 1758 bytes)
15/08/19 18:13:40 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
15/08/19 18:13:40 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 41503 ms on localhost (34/85)
15/08/19 18:13:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501324 records.
19-Aug-2015 18:13:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 98 ms. row count = 3501324
19-Aug-2015 18:13:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3662650 records.
19-Aug-2015 18:13:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 1203 ms. row count = 3500100
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 41 ms. row count = 3500100
19-Aug-2015 18:13:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 4159434 records.
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 0 records.
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 63 ms. row count = 3501026
19-Aug-2015 18:13:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501151 records.
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 58 ms. row count = 3501151
19-Aug-2015 18:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502900 records from 2 columns in 33374 ms: 104.958954 rec/ms, 209.91791 cell/ms
19-Aug-2015 18:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (46 ms) and 99% processing (33374 ms)
19-Aug-2015 18:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502900. reading next block
19-Aug-2015 18:13:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 157909
19-Aug-2015 18:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503024 records from 2 columns in 36080 ms: 97.09047 rec/ms, 194.18094 cell/ms
19-Aug-2015 18:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (49 ms) and 99% processing (36080 ms)
19-Aug-2015 18:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503024. reading next block
19-Aug-2015 18:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 23 ms. row count = 160994
19-Aug-2015 18:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501325 records from 2 columns in 36121 ms: 96.93323 rec/ms, 193.86646 cell/ms
19-Aug-2015 18:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (64 ms) and 99% processing (36121 ms)
19-Aug-2015 18:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501325. reading next block
19-Aug-2015 18:13:35 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 162639
19-Aug-2015 18:13:38 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503512 records from 2 columns in 34343 ms: 102.01532 rec/ms, 204.03064 cell/ms
19-Aug-2015 18:13:38 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (62 ms) and 99% processing (34343 ms)
19-Aug-2015 18:13:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503512. reading next block
19-Aug-2015 18:13:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 16 ms. row count = 103157
19-Aug-2015 18:13:38 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501092 records from 2 columns in 34279 ms: 102.135185 rec/ms, 204.27037 cell/ms
19-Aug-2015 18:13:38 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (53 ms) and 99% processing (34279 ms)
19-Aug-2015 18:13:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501092. reading next block
19-Aug-2015 18:13:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 107375
19-Aug-2015 18:13:39 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608451 records.
19-Aug-2015 18:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 68 ms. row count = 3501003
19-Aug-2015 18:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500891 records from 2 columns in 17714 ms: 197.63412 rec/ms, 395.26825 cell/ms
19-Aug-2015 18:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (96 ms) and 99% processing (17714 ms)
19-Aug-2015 18:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500891. reading next block
19-Aug-2015 18:13:39 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 108875
19-Aug-2015 18:13:40 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:40 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500783 records.
19-Aug-2015 18:13:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:42 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2820 ms. row count = 3500783
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501026 records from 2 columns in 18411 ms: 190.15947 rec/ms, 380.31894 cell/ms
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (63 ms) and 99% processing (18411 ms)
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501026. reading next block
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 18782 ms: 186.35396 rec/ms, 372.70792 cell/ms
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 6% reading (1203 ms) and 93% processing (18782 ms)
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-A15/08/19 18:13:43 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 2125 bytes result sent to driver
15/08/19 18:13:43 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50, localhost, ANY, 1771 bytes)
15/08/19 18:13:43 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
15/08/19 18:13:43 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 44022 ms on localhost (35/85)
15/08/19 18:13:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 134217728 end: 258790728 length: 124573000 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:43 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 2125 bytes result sent to driver
15/08/19 18:13:43 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51, localhost, ANY, 1757 bytes)
15/08/19 18:13:43 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
15/08/19 18:13:43 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 44702 ms on localhost (36/85)
15/08/19 18:13:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:43 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 2125 bytes result sent to driver
15/08/19 18:13:43 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52, localhost, ANY, 1772 bytes)
15/08/19 18:13:43 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
15/08/19 18:13:43 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 44300 ms on localhost (37/85)
15/08/19 18:13:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 134217728 end: 260690571 length: 126472843 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:43 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 2125 bytes result sent to driver
15/08/19 18:13:43 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 2125 bytes result sent to driver
15/08/19 18:13:43 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53, localhost, ANY, 1758 bytes)
15/08/19 18:13:43 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
15/08/19 18:13:43 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54, localhost, ANY, 1768 bytes)
15/08/19 18:13:43 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
15/08/19 18:13:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:43 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 39464 ms on localhost (38/85)
15/08/19 18:13:43 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 134217728 end: 258899835 length: 124682107 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:43 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 44297 ms on localhost (39/85)
15/08/19 18:13:44 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 2125 bytes result sent to driver
15/08/19 18:13:44 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55, localhost, ANY, 1756 bytes)
15/08/19 18:13:44 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
15/08/19 18:13:44 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 39877 ms on localhost (40/85)
15/08/19 18:13:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:44 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 2125 bytes result sent to driver
15/08/19 18:13:44 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56, localhost, ANY, 1769 bytes)
15/08/19 18:13:44 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
15/08/19 18:13:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 134217728 end: 261449270 length: 127231542 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:44 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 39614 ms on localhost (41/85)
15/08/19 18:13:44 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 2125 bytes result sent to driver
15/08/19 18:13:44 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57, localhost, ANY, 1756 bytes)
15/08/19 18:13:44 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
15/08/19 18:13:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:44 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 39457 ms on localhost (42/85)
15/08/19 18:13:47 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 2125 bytes result sent to driver
15/08/19 18:13:47 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58, localhost, ANY, 1772 bytes)
15/08/19 18:13:47 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
15/08/19 18:13:47 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 25592 ms on localhost (43/85)
15/08/19 18:13:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 134217728 end: 260804804 length: 126587076 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:50 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 2125 bytes result sent to driver
15/08/19 18:13:50 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59, localhost, ANY, 1758 bytes)
15/08/19 18:13:50 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
15/08/19 18:13:50 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 27110 ms on localhost (44/85)
15/08/19 18:13:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:50 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 2125 bytes result sent to driver
15/08/19 18:13:50 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60, localhost, ANY, 1769 bytes)
15/08/19 18:13:50 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
15/08/19 18:13:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 134217728 end: 258898835 length: 124681107 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:50 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 28228 ms on localhost (45/85)
15/08/19 18:13:50 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 2125 bytes result sent to driver
15/08/19 18:13:50 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61, localhost, ANY, 1757 bytes)
15/08/19 18:13:50 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
15/08/19 18:13:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:50 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 27367 ms on localhost (46/85)
15/08/19 18:13:50 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 2125 bytes result sent to driver
15/08/19 18:13:50 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62, localhost, ANY, 1770 bytes)
15/08/19 18:13:50 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
15/08/19 18:13:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 134217728 end: 258817871 length: 124600143 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:13:50 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 25984 ms on localhost (47/85)
15/08/19 18:13:53 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 2125 bytes result sent to driver
15/08/19 18:13:53 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63, localhost, ANY, 1758 bytes)
15/08/19 18:13:53 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
15/08/19 18:13:53 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 29274 ms on localhost (48/85)
15/08/19 18:13:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
ug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 658408
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 162550
19-Aug-2015 18:13:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608955 records.
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3500100
19-Aug-2015 18:13:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501135 records.
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3501135
19-Aug-2015 18:13:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3662957 records.
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 36 ms. row count = 3500100
19-Aug-2015 18:13:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:43 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501088 records.
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608995 records.
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 41 ms. row count = 3501407
19-Aug-2015 18:13:43 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 3501088
19-Aug-2015 18:13:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500719 records.
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:44 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3684271 records.
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501051 records.
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500100
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 58 ms. row count = 3501051
19-Aug-2015 18:13:44 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 85 ms. row count = 3500719
19-Aug-2015 18:13:47 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:47 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3663150 records.
19-Aug-2015 18:13:47 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:47 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 3501200
19-Aug-2015 18:13:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500780 records.
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 57 ms. row count = 3500780
19-Aug-2015 18:13:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609136 records.
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 3501367
19-Aug-2015 18:13:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 93 ms. row count = 3500100
19-Aug-2015 18:13:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3610161 records.
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 43 ms. row count = 3501357
19-Aug-2015 18:13:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:13:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501165 records.
19-Aug-2015 18:13:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:13:53 INFO: parquet.hadoop.InternalParquetRecordReader15/08/19 18:14:15 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 2125 bytes result sent to driver
15/08/19 18:14:15 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64, localhost, ANY, 1771 bytes)
15/08/19 18:14:15 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
15/08/19 18:14:15 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 36466 ms on localhost (49/85)
15/08/19 18:14:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 134217728 end: 258510998 length: 124293270 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:18 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 2125 bytes result sent to driver
15/08/19 18:14:18 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65, localhost, ANY, 1757 bytes)
15/08/19 18:14:18 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
15/08/19 18:14:18 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 38853 ms on localhost (50/85)
15/08/19 18:14:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:19 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 2125 bytes result sent to driver
15/08/19 18:14:19 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66, localhost, ANY, 1770 bytes)
15/08/19 18:14:19 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
15/08/19 18:14:19 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 36600 ms on localhost (51/85)
15/08/19 18:14:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 134217728 end: 258077510 length: 123859782 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:20 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 2125 bytes result sent to driver
15/08/19 18:14:20 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67, localhost, ANY, 1758 bytes)
15/08/19 18:14:20 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
15/08/19 18:14:20 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 36707 ms on localhost (52/85)
15/08/19 18:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:20 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 2125 bytes result sent to driver
15/08/19 18:14:20 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68, localhost, ANY, 1772 bytes)
15/08/19 18:14:20 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
15/08/19 18:14:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 134217728 end: 258658247 length: 124440519 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:20 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 36712 ms on localhost (53/85)
15/08/19 18:14:23 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 2125 bytes result sent to driver
15/08/19 18:14:23 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69, localhost, ANY, 1758 bytes)
15/08/19 18:14:23 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
15/08/19 18:14:23 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 39682 ms on localhost (54/85)
15/08/19 18:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:23 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 2125 bytes result sent to driver
15/08/19 18:14:23 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70, localhost, ANY, 1768 bytes)
15/08/19 18:14:23 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
15/08/19 18:14:23 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 39201 ms on localhost (55/85)
15/08/19 18:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 134217728 end: 258890765 length: 124673037 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
: block read in memory in 48 ms. row count = 3501165
19-Aug-2015 18:13:56 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501003 records from 2 columns in 17160 ms: 204.02115 rec/ms, 408.0423 cell/ms
19-Aug-2015 18:13:56 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (68 ms) and 99% processing (17160 ms)
19-Aug-2015 18:13:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501003. reading next block
19-Aug-2015 18:13:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 107448
19-Aug-2015 18:14:12 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29555 ms: 118.42666 rec/ms, 236.85332 cell/ms
19-Aug-2015 18:14:12 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (46 ms) and 99% processing (29555 ms)
19-Aug-2015 18:14:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:14:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 108855
19-Aug-2015 18:14:13 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29601 ms: 118.24263 rec/ms, 236.48526 cell/ms
19-Aug-2015 18:14:13 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (36 ms) and 99% processing (29601 ms)
19-Aug-2015 18:14:13 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:14:13 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 162857
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 31257 ms: 111.97812 rec/ms, 223.95624 cell/ms
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (31257 ms)
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 184171
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501407 records from 2 columns in 31645 ms: 110.646454 rec/ms, 221.29291 cell/ms
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (41 ms) and 99% processing (31645 ms)
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501407. reading next block
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 107588
19-Aug-2015 18:14:15 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608780 records.
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 32 ms. row count = 3502929
19-Aug-2015 18:14:18 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501200 records from 2 columns in 31378 ms: 111.58136 rec/ms, 223.16272 cell/ms
19-Aug-2015 18:14:18 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (54 ms) and 99% processing (31378 ms)
19-Aug-2015 18:14:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501200. reading next block
19-Aug-2015 18:14:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 161950
19-Aug-2015 18:14:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502743 records.
19-Aug-2015 18:14:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 65 ms. row count = 3502743
19-Aug-2015 18:14:19 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501367 records from 2 columns in 28958 ms: 120.9119 rec/ms, 241.8238 cell/ms
19-Aug-2015 18:14:19 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (54 ms) and 99% processing (28958 ms)
19-Aug-2015 18:14:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501367. reading next block
19-Aug-2015 18:14:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 9 ms. row count = 107769
19-Aug-2015 18:14:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3607229 records.
19-Aug-2015 18:14:19 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 3503385
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501357 records from 2 columns in 29373 ms: 119.20325 rec/ms, 238.4065 cell/ms
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (43 ms) and 99% processing (29373 ms)
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501357. reading next block
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 6 ms. row count = 108804
19-Aug-2015 18:14:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500100
19-Aug-2015 18:14:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609673 records.
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500100
19-Aug-2015 18:14:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500967 records.
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 41 ms. row count = 3500967
19-Aug-2015 18:14:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608672 records.
19-Aug-2015 18:14:23 INFO: parquet.hadoop.Int15/08/19 18:14:23 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 2125 bytes result sent to driver
15/08/19 18:14:23 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71, localhost, ANY, 1757 bytes)
15/08/19 18:14:23 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
15/08/19 18:14:23 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 39195 ms on localhost (56/85)
15/08/19 18:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:23 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 2125 bytes result sent to driver
15/08/19 18:14:23 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72, localhost, ANY, 1768 bytes)
15/08/19 18:14:23 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
15/08/19 18:14:23 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 39562 ms on localhost (57/85)
15/08/19 18:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 134217728 end: 258681625 length: 124463897 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:23 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 2125 bytes result sent to driver
15/08/19 18:14:23 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73, localhost, ANY, 1758 bytes)
15/08/19 18:14:23 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 39281 ms on localhost (58/85)
15/08/19 18:14:23 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
15/08/19 18:14:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:26 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 2125 bytes result sent to driver
15/08/19 18:14:26 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74, localhost, ANY, 1771 bytes)
15/08/19 18:14:26 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
15/08/19 18:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 134217728 end: 258792347 length: 124574619 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:26 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 39264 ms on localhost (59/85)
15/08/19 18:14:26 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 2125 bytes result sent to driver
15/08/19 18:14:26 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75, localhost, ANY, 1757 bytes)
15/08/19 18:14:26 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
15/08/19 18:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:26 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 36438 ms on localhost (60/85)
15/08/19 18:14:26 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 2125 bytes result sent to driver
15/08/19 18:14:26 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76, localhost, ANY, 1769 bytes)
15/08/19 18:14:26 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
15/08/19 18:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 134217728 end: 258482455 length: 124264727 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:26 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 36654 ms on localhost (61/85)
15/08/19 18:14:26 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 2125 bytes result sent to driver
15/08/19 18:14:26 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77, localhost, ANY, 1758 bytes)
15/08/19 18:14:26 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
15/08/19 18:14:26 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 36536 ms on localhost (62/85)
15/08/19 18:14:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:27 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 2125 bytes result sent to driver
15/08/19 18:14:27 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78, localhost, ANY, 1771 bytes)
15/08/19 18:14:27 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
15/08/19 18:14:27 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 36515 ms on localhost (63/85)
15/08/19 18:14:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 134217728 end: 260809124 length: 126591396 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:30 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 2125 bytes result sent to driver
15/08/19 18:14:30 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79, localhost, ANY, 1758 bytes)
15/08/19 18:14:30 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
15/08/19 18:14:30 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 36649 ms on localhost (64/85)
15/08/19 18:14:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:45 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 2125 bytes result sent to driver
15/08/19 18:14:45 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80, localhost, ANY, 1770 bytes)
15/08/19 18:14:45 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
15/08/19 18:14:45 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 26186 ms on localhost (65/85)
15/08/19 18:14:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 134217728 end: 259851958 length: 125634230 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
ernalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 34 ms. row count = 3500860
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3609901 records.
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 3500100
19-Aug-2015 18:14:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 45 ms. row count = 3500100
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501368 records.
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3501368
19-Aug-2015 18:14:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608782 records.
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501297 records.
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 44 ms. row count = 3500100
19-Aug-2015 18:14:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3608268 records.
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 45 ms. row count = 3501297
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 58 ms. row count = 3502399
19-Aug-2015 18:14:26 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501079 records.
19-Aug-2015 18:14:26 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 55 ms. row count = 3501079
19-Aug-2015 18:14:27 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:27 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3663023 records.
19-Aug-2015 18:14:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 3501178
19-Aug-2015 18:14:30 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502929 records from 2 columns in 14342 ms: 244.2427 rec/ms, 488.4854 cell/ms
19-Aug-2015 18:14:30 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (32 ms) and 99% processing (14342 ms)
19-Aug-2015 18:14:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502929. reading next block
19-Aug-2015 18:14:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 105851
19-Aug-2015 18:14:30 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
19-Aug-2015 18:14:30 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503367 records.
19-Aug-2015 18:14:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
19-Aug-2015 18:14:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2644 ms. row count = 3503367
19-Aug-2015 18:14:37 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503385 records from 2 columns in 17335 ms: 202.09894 rec/ms, 404.19788 cell/ms
19-Aug-2015 18:14:37 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (42 ms) and 99% processing (17335 ms)
19-Aug-2015 18:14:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503385. reading next block
19-Aug-2015 18:14:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 103844
19-Aug-2015 18:14:41 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500860 records from 2 columns in 17926 ms: 195.2951 rec/ms, 390.5902 cell/ms
19-Aug-2015 18:14:41 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (34 ms) and 99% processing (17926 ms)
19-Aug-2015 18:14:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500860. reading next block
19-Aug-2015 18:14:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 38 ms. row count = 107812
19-Aug-2015 18:14:41 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 17973 ms: 194.74211 rec/ms, 389.48422 cell/ms
19-Aug-2015 18:14:41 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (45 ms) and 99% processing (17973 ms)
19-Aug-2015 18:14:41 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:14:41 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 17 ms. row count = 109801
19-Aug-2015 18:14:45 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 24493 ms: 142.90205 rec/ms, 285.8041 cell/ms
19-Aug-2015 18:14:45 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (24493 ms)
19-Aug-2015 18:14:45 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
19-Aug-2015 18:14:45 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 109573
19-Aug-2015 18:14:45 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.15/08/19 18:14:45 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 2125 bytes result sent to driver
15/08/19 18:14:45 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81, localhost, ANY, 1757 bytes)
15/08/19 18:14:45 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
15/08/19 18:14:45 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 29777 ms on localhost (66/85)
15/08/19 18:14:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:46 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 2125 bytes result sent to driver
15/08/19 18:14:46 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82, localhost, ANY, 1768 bytes)
15/08/19 18:14:46 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
15/08/19 18:14:46 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 26571 ms on localhost (67/85)
15/08/19 18:14:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 134217728 end: 259121277 length: 124903549 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:46 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 2125 bytes result sent to driver
15/08/19 18:14:46 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83, localhost, ANY, 1758 bytes)
15/08/19 18:14:47 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
15/08/19 18:14:47 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 23702 ms on localhost (68/85)
15/08/19 18:14:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:47 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 2125 bytes result sent to driver
15/08/19 18:14:47 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 2125 bytes result sent to driver
15/08/19 18:14:47 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84, localhost, ANY, 1769 bytes)
15/08/19 18:14:47 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
15/08/19 18:14:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 134217728 end: 258781787 length: 124564059 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:14:47 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 23851 ms on localhost (69/85)
15/08/19 18:14:47 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 23758 ms on localhost (70/85)
15/08/19 18:14:47 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 2125 bytes result sent to driver
15/08/19 18:14:47 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 26922 ms on localhost (71/85)
15/08/19 18:14:47 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 2125 bytes result sent to driver
15/08/19 18:14:47 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 23969 ms on localhost (72/85)
15/08/19 18:14:47 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 2125 bytes result sent to driver
15/08/19 18:14:47 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 24126 ms on localhost (73/85)
15/08/19 18:14:50 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 2125 bytes result sent to driver
15/08/19 18:14:50 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 2125 bytes result sent to driver
15/08/19 18:14:50 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 30469 ms on localhost (74/85)
15/08/19 18:14:50 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 24347 ms on localhost (75/85)
15/08/19 18:14:51 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 2125 bytes result sent to driver
15/08/19 18:14:51 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 24147 ms on localhost (76/85)
15/08/19 18:14:51 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 2125 bytes result sent to driver
15/08/19 18:14:51 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 24503 ms on localhost (77/85)
15/08/19 18:14:51 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 2125 bytes result sent to driver
15/08/19 18:14:51 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 24601 ms on localhost (78/85)
15/08/19 18:14:51 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 2125 bytes result sent to driver
15/08/19 18:14:51 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 24245 ms on localhost (79/85)
15/08/19 18:14:52 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 2125 bytes result sent to driver
15/08/19 18:14:52 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 21938 ms on localhost (80/85)
15/08/19 18:15:04 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 2125 bytes result sent to driver
15/08/19 18:15:04 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 19151 ms on localhost (81/85)
15/08/19 18:15:04 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 2125 bytes result sent to driver
15/08/19 18:15:04 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 18743 ms on localhost (82/85)
15/08/19 18:15:04 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 2125 bytes result sent to driver
15/08/19 18:15:04 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 18642 ms on localhost (83/85)
15/08/19 18:15:05 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 2125 bytes result sent to driver
15/08/19 18:15:05 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 18419 ms on localhost (84/85)
15/08/19 18:15:05 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 2125 bytes result sent to driver
15/08/19 18:15:05 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 18469 ms on localhost (85/85)
15/08/19 18:15:05 INFO DAGScheduler: ShuffleMapStage 0 (processCmd at CliDriver.java:423) finished in 190.532 s
15/08/19 18:15:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/19 18:15:05 INFO DAGScheduler: looking for newly runnable stages
15/08/19 18:15:05 INFO DAGScheduler: running: Set()
15/08/19 18:15:05 INFO DAGScheduler: waiting: Set(ResultStage 1)
15/08/19 18:15:05 INFO DAGScheduler: failed: Set()
15/08/19 18:15:05 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1b6d4488
15/08/19 18:15:05 INFO DAGScheduler: Missing parents for ResultStage 1: List()
15/08/19 18:15:05 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423), which is now runnable
15/08/19 18:15:05 INFO StatsReportListener: task runtime:(count: 85, mean: 33788.494118, stdev: 9113.276485, max: 58990.000000, min: 119.000000)
15/08/19 18:15:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:15:05 INFO StatsReportListener: 	119.0 ms	18.7 s	23.8 s	26.9 s	36.2 s	39.5 s	44.7 s	48.0 s	59.0 s
15/08/19 18:15:05 INFO StatsReportListener: shuffle bytes written:(count: 85, mean: 9626448.305882, stdev: 1091378.807366, max: 11271823.000000, min: 0.000000)
15/08/19 18:15:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:15:05 INFO StatsReportListener: 	0.0 B	9.0 MB	9.0 MB	9.1 MB	9.3 MB	9.4 MB	9.6 MB	9.8 MB	10.7 MB
15/08/19 18:15:05 INFO StatsReportListener: task result size:(count: 85, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/19 18:15:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:15:05 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/19 18:15:05 INFO StatsReportListener: executor (non-fetch) time pct: (count: 85, mean: 99.288680, stdev: 3.079492, max: 99.943870, min: 71.428571)
15/08/19 18:15:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:15:05 INFO StatsReportListener: 	71 %	98 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/19 18:15:05 INFO StatsReportListener: other time pct: (count: 85, mean: 0.711320, stdev: 3.079492, max: 28.571429, min: 0.056130)
15/08/19 18:15:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:15:05 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 2 %	29 %
15/08/19 18:15:05 INFO MemoryStore: ensureFreeSpace(78872) called with curMem=363082, maxMem=22226833244
15/08/19 18:15:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 77.0 KB, free 20.7 GB)
15/08/19 18:15:05 INFO MemoryStore: ensureFreeSpace(30004) called with curMem=441954, maxMem=22226833244
15/08/19 18:15:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.3 KB, free 20.7 GB)
15/08/19 18:15:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:36176 (size: 29.3 KB, free: 20.7 GB)
15/08/19 18:15:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
15/08/19 18:15:05 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423)
15/08/19 18:15:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
15/08/19 18:15:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 85, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 86, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 87, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 88, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 89, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 90, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 91, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 92, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 93, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 94, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 95, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 96, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 97, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 98, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 99, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 100, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:05 INFO Executor: Running task 1.0 in stage 1.0 (TID 86)
15/08/19 18:15:05 INFO Executor: Running task 3.0 in stage 1.0 (TID 88)
15/08/19 18:15:05 INFO Executor: Running task 6.0 in stage 1.0 (TID 91)
15/08/19 18:15:05 INFO Executor: Running task 7.0 in stage 1.0 (TID 92)
15/08/19 18:15:05 INFO Executor: Running task 10.0 in stage 1.0 (TID 95)
15/08/19 18:15:05 INFO Executor: Running task 14.0 in stage 1.0 (TID 99)
15/08/19 18:15:05 INFO Executor: Running task 8.0 in stage 1.0 (TID 93)
15/08/19 18:15:05 INFO Executor: Running task 11.0 in stage 1.0 (TID 96)
15/08/19 18:15:05 INFO Executor: Running task 5.0 in stage 1.0 (TID 90)
15/08/19 18:15:05 INFO Executor: Running task 4.0 in stage 1.0 (TID 89)
15/08/19 18:15:05 INFO Executor: Running task 2.0 in stage 1.0 (TID 87)
15/08/19 18:15:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 85)
15/08/19 18:15:05 INFO Executor: Running task 15.0 in stage 1.0 (TID 100)
15/08/19 18:15:05 INFO Executor: Running task 12.0 in stage 1.0 (TID 97)
15/08/19 18:15:05 INFO Executor: Running task 13.0 in stage 1.0 (TID 98)
15/08/19 18:15:05 INFO Executor: Running task 9.0 in stage 1.0 (TID 94)
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:15 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:15 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:15 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,691B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,612B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,191B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,050B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,804B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,725B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,355B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,214B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,462B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,383B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,686B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,545B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,527B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,448B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,290B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,211B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,527B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,448B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,261B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,120B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,817B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,676B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 415,751B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,610B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,533
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,599B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,520B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,054B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,913B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,529B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,450B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 415,970B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,829B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,635B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,556B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,929B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,788B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,448B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,369B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,843B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,702B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000004
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000005
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000005_0: Committed
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000004_0: Committed
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000002
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000002_0: Committed
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000015
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000015_0: Committed
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000007
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000010
15/08/19 18:15:16 INFO Executor: Finished task 5.0 in stage 1.0 (TID 90). 843 bytes result sent to driver
15/08/19 18:15:16 INFO Executor: Finished task 4.0 in stage 1.0 (TID 89). 843 bytes result sent to driver
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000007_0: Committed
15/08/19 18:15:16 INFO Executor: Finished task 2.0 in stage 1.0 (TID 87). 843 bytes result sent to driver
15/08/19 18:15:16 INFO Executor: Finished task 15.0 in stage 1.0 (TID 100). 843 bytes result sent to driver
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000010_0: Committed
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000009
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000009_0: Committed
15/08/19 18:15:16 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 101, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Running task 16.0 in stage 1.0 (TID 101)
15/08/19 18:15:16 INFO Executor: Finished task 10.0 in stage 1.0 (TID 95). 843 bytes result sent to driver
15/08/19 18:15:16 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 102, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Running task 17.0 in stage 1.0 (TID 102)
15/08/19 18:15:16 INFO Executor: Finished task 7.0 in stage 1.0 (TID 92). 843 bytes result sent to driver
15/08/19 18:15:16 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 103, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Running task 18.0 in stage 1.0 (TID 103)
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000003
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000003_0: Committed
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000000
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000000_0: Committed
15/08/19 18:15:16 INFO Executor: Finished task 9.0 in stage 1.0 (TID 94). 843 bytes result sent to driver
15/08/19 18:15:16 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 104, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Running task 19.0 in stage 1.0 (TID 104)
15/08/19 18:15:16 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 90) in 11246 ms on localhost (1/200)
15/08/19 18:15:16 INFO Executor: Finished task 3.0 in stage 1.0 (TID 88). 843 bytes result sent to driver
15/08/19 18:15:16 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 89) in 11248 ms on localhost (2/200)
15/08/19 18:15:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000014
15/08/19 18:15:16 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000014_0: Committed
15/08/19 18:15:16 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 87) in 11249 ms on localhost (3/200)
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 85). 843 bytes result sent to driver
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,391B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,312B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 105, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Finished task 14.0 in stage 1.0 (TID 99). 843 bytes result sent to driver
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 417,180B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,039B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:15:16 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 106, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Running task 20.0 in stage 1.0 (TID 105)
15/08/19 18:15:16 INFO Executor: Running task 21.0 in stage 1.0 (TID 106)
15/08/19 18:15:16 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 100) in 11246 ms on localhost (4/200)
15/08/19 18:15:16 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 107, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 108, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Running task 22.0 in stage 1.0 (TID 107)
15/08/19 18:15:16 INFO Executor: Running task 23.0 in stage 1.0 (TID 108)
15/08/19 18:15:16 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 94) in 11255 ms on localhost (5/200)
15/08/19 18:15:16 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 88) in 11261 ms on localhost (6/200)
15/08/19 18:15:16 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 95) in 11257 ms on localhost (7/200)
15/08/19 18:15:16 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 109, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Running task 24.0 in stage 1.0 (TID 109)
15/08/19 18:15:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 85) in 11268 ms on localhost (8/200)
15/08/19 18:15:16 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 92) in 11262 ms on localhost (9/200)
15/08/19 18:15:16 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 110, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:16 INFO Executor: Running task 25.0 in stage 1.0 (TID 110)
15/08/19 18:15:16 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 99) in 11264 ms on localhost (10/200)
15/08/19 18:15:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 1,308,298B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,219B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:16 INFO ColumnChunkPageWriteStore: written 416,341B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,200B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,509
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000013
15/08/19 18:15:17 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000013_0: Committed
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:17 INFO Executor: Finished task 13.0 in stage 1.0 (TID 98). 843 bytes result sent to driver
15/08/19 18:15:17 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 111, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:17 INFO Executor: Running task 26.0 in stage 1.0 (TID 111)
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:15:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:15:17 INFO ColumnChunkPageWriteStore: written 1,308,474B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,395B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:17 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 98) in 11335 ms on localhost (11/200)
15/08/19 18:15:17 INFO ColumnChunkPageWriteStore: written 417,217B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,076B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:15:17 INFO ColumnChunkPageWriteStore: written 1,308,507B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,428B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:17 INFO ColumnChunkPageWriteStore: written 416,530B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,389B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/19 18:15:17 INFO ColumnChunkPageWriteStore: written 1,308,631B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,552B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000001
15/08/19 18:15:17 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000001_0: Committed
15/08/19 18:15:17 INFO ColumnChunkPageWriteStore: written 417,425B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,284B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:15:17 INFO Executor: Finished task 1.0 in stage 1.0 (TID 86). 843 bytes result sent to driver
15/08/19 18:15:17 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 112, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:17 INFO Executor: Running task 27.0 in stage 1.0 (TID 112)
15/08/19 18:15:17 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 86) in 11392 ms on localhost (12/200)
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000008
15/08/19 18:15:17 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000008_0: Committed
15/08/19 18:15:17 INFO Executor: Finished task 8.0 in stage 1.0 (TID 93). 843 bytes result sent to driver
15/08/19 18:15:17 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 113, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:17 INFO Executor: Running task 28.0 in stage 1.0 (TID 113)
15/08/19 18:15:17 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 93) in 11437 ms on localhost (13/200)
15/08/19 18:15:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000011
15/08/19 18:15:17 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000011_0: Committed
15/08/19 18:15:17 INFO Executor: Finished task 11.0 in stage 1.0 (TID 96). 843 bytes result sent to driver
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:17 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 114, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:17 INFO Executor: Running task 29.0 in stage 1.0 (TID 114)
15/08/19 18:15:17 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 96) in 11451 ms on localhost (14/200)
15/08/19 18:15:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000006
15/08/19 18:15:17 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000006_0: Committed
15/08/19 18:15:17 INFO Executor: Finished task 6.0 in stage 1.0 (TID 91). 843 bytes result sent to driver
15/08/19 18:15:17 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 115, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:17 INFO Executor: Running task 30.0 in stage 1.0 (TID 115)
15/08/19 18:15:17 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 91) in 11481 ms on localhost (15/200)
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:15:17 INFO ColumnChunkPageWriteStore: written 1,308,710B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,631B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:17 INFO ColumnChunkPageWriteStore: written 415,811B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,670B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:15:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000012
15/08/19 18:15:17 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000012_0: Committed
15/08/19 18:15:17 INFO Executor: Finished task 12.0 in stage 1.0 (TID 97). 843 bytes result sent to driver
15/08/19 18:15:17 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 116, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:17 INFO Executor: Running task 31.0 in stage 1.0 (TID 116)
15/08/19 18:15:17 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 97) in 11750 ms on localhost (16/200)
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:27 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:27 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:27 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:27 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:27 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:27 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:27 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:27 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:27 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO ColumnChunkPageWriteStore: written 1,308,653B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,574B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:28 INFO ColumnChunkPageWriteStore: written 417,199B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,058B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000027
15/08/19 18:15:28 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000027_0: Committed
15/08/19 18:15:28 INFO Executor: Finished task 27.0 in stage 1.0 (TID 112). 843 bytes result sent to driver
15/08/19 18:15:28 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 117, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:28 INFO Executor: Running task 32.0 in stage 1.0 (TID 117)
15/08/19 18:15:28 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 112) in 11796 ms on localhost (17/200)
15/08/19 18:15:28 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:29 INFO ColumnChunkPageWriteStore: written 1,308,528B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,449B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:29 INFO ColumnChunkPageWriteStore: written 418,141B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 418,000B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000018
15/08/19 18:15:29 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000018_0: Committed
15/08/19 18:15:29 INFO Executor: Finished task 18.0 in stage 1.0 (TID 103). 843 bytes result sent to driver
15/08/19 18:15:29 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 118, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:29 INFO Executor: Running task 33.0 in stage 1.0 (TID 118)
15/08/19 18:15:29 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 103) in 12253 ms on localhost (18/200)
15/08/19 18:15:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:29 INFO ColumnChunkPageWriteStore: written 1,308,598B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,519B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:29 INFO ColumnChunkPageWriteStore: written 416,707B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,566B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000019
15/08/19 18:15:29 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000019_0: Committed
15/08/19 18:15:29 INFO Executor: Finished task 19.0 in stage 1.0 (TID 104). 843 bytes result sent to driver
15/08/19 18:15:29 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 119, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:29 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 104) in 12685 ms on localhost (19/200)
15/08/19 18:15:29 INFO Executor: Running task 34.0 in stage 1.0 (TID 119)
15/08/19 18:15:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:29 INFO ColumnChunkPageWriteStore: written 1,308,659B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,580B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:29 INFO ColumnChunkPageWriteStore: written 416,976B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,835B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000024
15/08/19 18:15:29 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000024_0: Committed
15/08/19 18:15:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:29 INFO Executor: Finished task 24.0 in stage 1.0 (TID 109). 843 bytes result sent to driver
15/08/19 18:15:29 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 120, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:29 INFO Executor: Running task 35.0 in stage 1.0 (TID 120)
15/08/19 18:15:29 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 109) in 13035 ms on localhost (20/200)
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,437
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,146B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,067B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 416,109B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,968B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,357B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,278B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 417,964B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,823B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,621B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,542B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 416,798B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,657B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 305 entries, 2,440B raw, 305B comp}
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000017
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000017_0: Committed
15/08/19 18:15:30 INFO Executor: Finished task 17.0 in stage 1.0 (TID 102). 843 bytes result sent to driver
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000025
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000025_0: Committed
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 121, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:30 INFO Executor: Running task 36.0 in stage 1.0 (TID 121)
15/08/19 18:15:30 INFO Executor: Finished task 25.0 in stage 1.0 (TID 110). 843 bytes result sent to driver
15/08/19 18:15:30 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 122, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO Executor: Running task 37.0 in stage 1.0 (TID 122)
15/08/19 18:15:30 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 102) in 13219 ms on localhost (21/200)
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:15:30 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 110) in 13200 ms on localhost (22/200)
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000022
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000022_0: Committed
15/08/19 18:15:30 INFO Executor: Finished task 22.0 in stage 1.0 (TID 107). 843 bytes result sent to driver
15/08/19 18:15:30 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 123, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:30 INFO Executor: Running task 38.0 in stage 1.0 (TID 123)
15/08/19 18:15:30 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 107) in 13288 ms on localhost (23/200)
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,604B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,525B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 416,869B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,728B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,437
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000028
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000028_0: Committed
15/08/19 18:15:30 INFO Executor: Finished task 28.0 in stage 1.0 (TID 113). 843 bytes result sent to driver
15/08/19 18:15:30 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 124, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO Executor: Running task 39.0 in stage 1.0 (TID 124)
15/08/19 18:15:30 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 113) in 13261 ms on localhost (24/200)
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,509
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,567B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,488B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,562B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,483B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,661B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,582B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 416,278B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,137B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 417,094B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,953B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 418,004B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,863B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 305 entries, 2,440B raw, 305B comp}
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000016
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000016_0: Committed
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,642B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,563B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 417,915B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,774B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,608B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,529B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 417,660B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,519B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:15:30 INFO Executor: Finished task 16.0 in stage 1.0 (TID 101). 843 bytes result sent to driver
15/08/19 18:15:30 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 125, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO Executor: Running task 40.0 in stage 1.0 (TID 125)
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000026
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000026_0: Committed
15/08/19 18:15:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:15:30 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 101) in 13669 ms on localhost (25/200)
15/08/19 18:15:30 INFO Executor: Finished task 26.0 in stage 1.0 (TID 111). 843 bytes result sent to driver
15/08/19 18:15:30 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 126, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 111) in 13595 ms on localhost (26/200)
15/08/19 18:15:30 INFO Executor: Running task 41.0 in stage 1.0 (TID 126)
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000030
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000030_0: Committed
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,423B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,344B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO Executor: Finished task 30.0 in stage 1.0 (TID 115). 843 bytes result sent to driver
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 416,416B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,275B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:30 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 127, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO Executor: Running task 42.0 in stage 1.0 (TID 127)
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,501B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,422B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 1,308,500B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,421B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 416,657B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,516B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:15:30 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 115) in 13500 ms on localhost (27/200)
15/08/19 18:15:30 INFO ColumnChunkPageWriteStore: written 415,814B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,673B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000031
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000031_0: Committed
15/08/19 18:15:30 INFO Executor: Finished task 31.0 in stage 1.0 (TID 116). 843 bytes result sent to driver
15/08/19 18:15:30 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 128, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO Executor: Running task 43.0 in stage 1.0 (TID 128)
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000029
15/08/19 18:15:30 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 116) in 13262 ms on localhost (28/200)
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000029_0: Committed
15/08/19 18:15:30 INFO Executor: Finished task 29.0 in stage 1.0 (TID 114). 843 bytes result sent to driver
15/08/19 18:15:30 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 129, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO Executor: Running task 44.0 in stage 1.0 (TID 129)
15/08/19 18:15:30 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 114) in 13579 ms on localhost (29/200)
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000021
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000021_0: Committed
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000023
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000023_0: Committed
15/08/19 18:15:30 INFO Executor: Finished task 21.0 in stage 1.0 (TID 106). 843 bytes result sent to driver
15/08/19 18:15:30 INFO Executor: Finished task 23.0 in stage 1.0 (TID 108). 843 bytes result sent to driver
15/08/19 18:15:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000020
15/08/19 18:15:30 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000020_0: Committed
15/08/19 18:15:30 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 130, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 131, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO Executor: Running task 45.0 in stage 1.0 (TID 130)
15/08/19 18:15:30 INFO Executor: Running task 46.0 in stage 1.0 (TID 131)
15/08/19 18:15:30 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 106) in 13860 ms on localhost (30/200)
15/08/19 18:15:30 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 108) in 13857 ms on localhost (31/200)
15/08/19 18:15:30 INFO Executor: Finished task 20.0 in stage 1.0 (TID 105). 843 bytes result sent to driver
15/08/19 18:15:30 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 132, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:30 INFO Executor: Running task 47.0 in stage 1.0 (TID 132)
15/08/19 18:15:30 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 105) in 13870 ms on localhost (32/200)
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3826 ms
15/08/19 18:15:40 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:40 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:40 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:40 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:40 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:41 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:41 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:41 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:41 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:41 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:41 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:41 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:41 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:41 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:15:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,509
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO ColumnChunkPageWriteStore: written 1,308,321B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,242B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:42 INFO ColumnChunkPageWriteStore: written 417,699B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,558B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:15:42 INFO ColumnChunkPageWriteStore: written 1,308,616B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,537B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO ColumnChunkPageWriteStore: written 416,424B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,283B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/19 18:15:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000032
15/08/19 18:15:42 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000032_0: Committed
15/08/19 18:15:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000033
15/08/19 18:15:42 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000033_0: Committed
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO Executor: Finished task 32.0 in stage 1.0 (TID 117). 843 bytes result sent to driver
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 133, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:42 INFO Executor: Running task 48.0 in stage 1.0 (TID 133)
15/08/19 18:15:42 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 117) in 13273 ms on localhost (33/200)
15/08/19 18:15:42 INFO Executor: Finished task 33.0 in stage 1.0 (TID 118). 843 bytes result sent to driver
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 134, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO Executor: Running task 49.0 in stage 1.0 (TID 134)
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 118) in 12965 ms on localhost (34/200)
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO ColumnChunkPageWriteStore: written 1,308,645B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,566B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:42 INFO ColumnChunkPageWriteStore: written 416,700B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,559B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000035
15/08/19 18:15:42 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000035_0: Committed
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO Executor: Finished task 35.0 in stage 1.0 (TID 120). 843 bytes result sent to driver
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 135, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 120) in 12500 ms on localhost (35/200)
15/08/19 18:15:42 INFO Executor: Running task 50.0 in stage 1.0 (TID 135)
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:43 INFO ColumnChunkPageWriteStore: written 1,308,427B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,348B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:43 INFO ColumnChunkPageWriteStore: written 417,836B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,695B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000038
15/08/19 18:15:43 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000038_0: Committed
15/08/19 18:15:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,517
15/08/19 18:15:43 INFO Executor: Finished task 38.0 in stage 1.0 (TID 123). 843 bytes result sent to driver
15/08/19 18:15:43 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 136, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:43 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 123) in 13019 ms on localhost (36/200)
15/08/19 18:15:43 INFO Executor: Running task 51.0 in stage 1.0 (TID 136)
15/08/19 18:15:43 INFO ColumnChunkPageWriteStore: written 1,308,457B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,378B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:43 INFO ColumnChunkPageWriteStore: written 418,397B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 418,256B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/19 18:15:43 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000034
15/08/19 18:15:43 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000034_0: Committed
15/08/19 18:15:43 INFO Executor: Finished task 34.0 in stage 1.0 (TID 119). 843 bytes result sent to driver
15/08/19 18:15:43 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 137, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:43 INFO Executor: Running task 52.0 in stage 1.0 (TID 137)
15/08/19 18:15:43 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 119) in 13801 ms on localhost (37/200)
15/08/19 18:15:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,549B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,470B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 415,388B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,247B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,637B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,558B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 416,334B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,193B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000037
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000037_0: Committed
15/08/19 18:15:47 INFO Executor: Finished task 37.0 in stage 1.0 (TID 122). 843 bytes result sent to driver
15/08/19 18:15:47 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 138, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 53.0 in stage 1.0 (TID 138)
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000036
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000036_0: Committed
15/08/19 18:15:47 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 122) in 17111 ms on localhost (38/200)
15/08/19 18:15:47 INFO Executor: Finished task 36.0 in stage 1.0 (TID 121). 843 bytes result sent to driver
15/08/19 18:15:47 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 139, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 54.0 in stage 1.0 (TID 139)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 121) in 17124 ms on localhost (39/200)
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,408B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,329B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 416,583B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,442B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,361B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,282B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 416,489B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,348B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000041
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000041_0: Committed
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000042
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000042_0: Committed
15/08/19 18:15:47 INFO Executor: Finished task 41.0 in stage 1.0 (TID 126). 843 bytes result sent to driver
15/08/19 18:15:47 INFO Executor: Finished task 42.0 in stage 1.0 (TID 127). 843 bytes result sent to driver
15/08/19 18:15:47 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 140, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 55.0 in stage 1.0 (TID 140)
15/08/19 18:15:47 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 141, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 56.0 in stage 1.0 (TID 141)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 126) in 16785 ms on localhost (40/200)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 127) in 16725 ms on localhost (41/200)
15/08/19 18:15:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,380B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,301B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 418,423B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 418,282B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,424B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,345B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 416,368B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,227B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000046
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000046_0: Committed
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000047
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000047_0: Committed
15/08/19 18:15:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:47 INFO Executor: Finished task 46.0 in stage 1.0 (TID 131). 843 bytes result sent to driver
15/08/19 18:15:47 INFO Executor: Finished task 47.0 in stage 1.0 (TID 132). 843 bytes result sent to driver
15/08/19 18:15:47 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 142, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 57.0 in stage 1.0 (TID 142)
15/08/19 18:15:47 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 143, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 58.0 in stage 1.0 (TID 143)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 131) in 16702 ms on localhost (42/200)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 132) in 16694 ms on localhost (43/200)
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,614B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,535B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 417,413B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,272B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000045
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000045_0: Committed
15/08/19 18:15:47 INFO Executor: Finished task 45.0 in stage 1.0 (TID 130). 843 bytes result sent to driver
15/08/19 18:15:47 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 144, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 59.0 in stage 1.0 (TID 144)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 130) in 16883 ms on localhost (44/200)
15/08/19 18:15:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,339B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,260B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 417,255B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,114B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,515B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,436B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000039
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000039_0: Committed
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 415,336B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,195B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:47 INFO Executor: Finished task 39.0 in stage 1.0 (TID 124). 843 bytes result sent to driver
15/08/19 18:15:47 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 145, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 60.0 in stage 1.0 (TID 145)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 124) in 17420 ms on localhost (45/200)
15/08/19 18:15:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000040
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000040_0: Committed
15/08/19 18:15:47 INFO Executor: Finished task 40.0 in stage 1.0 (TID 125). 843 bytes result sent to driver
15/08/19 18:15:47 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 146, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 61.0 in stage 1.0 (TID 146)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 125) in 17278 ms on localhost (46/200)
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 1,308,556B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,477B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:47 INFO ColumnChunkPageWriteStore: written 416,083B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,942B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000043
15/08/19 18:15:47 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000043_0: Committed
15/08/19 18:15:47 INFO Executor: Finished task 43.0 in stage 1.0 (TID 128). 843 bytes result sent to driver
15/08/19 18:15:47 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 147, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:47 INFO Executor: Running task 62.0 in stage 1.0 (TID 147)
15/08/19 18:15:47 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 128) in 17225 ms on localhost (47/200)
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,509
15/08/19 18:15:48 INFO ColumnChunkPageWriteStore: written 1,308,531B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,452B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:48 INFO ColumnChunkPageWriteStore: written 417,414B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,273B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/19 18:15:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000044
15/08/19 18:15:48 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000044_0: Committed
15/08/19 18:15:48 INFO Executor: Finished task 44.0 in stage 1.0 (TID 129). 843 bytes result sent to driver
15/08/19 18:15:48 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 148, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:48 INFO Executor: Running task 63.0 in stage 1.0 (TID 148)
15/08/19 18:15:48 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 129) in 17523 ms on localhost (48/200)
15/08/19 18:15:48 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:53 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:53 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:53 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:15:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:15:55 INFO ColumnChunkPageWriteStore: written 1,308,235B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,156B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:55 INFO ColumnChunkPageWriteStore: written 416,401B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,260B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:15:55 INFO ColumnChunkPageWriteStore: written 1,308,501B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,422B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO ColumnChunkPageWriteStore: written 416,311B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,170B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000048
15/08/19 18:15:55 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000048_0: Committed
15/08/19 18:15:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000049
15/08/19 18:15:55 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000049_0: Committed
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO Executor: Finished task 48.0 in stage 1.0 (TID 133). 843 bytes result sent to driver
15/08/19 18:15:55 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 149, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:55 INFO Executor: Finished task 49.0 in stage 1.0 (TID 134). 843 bytes result sent to driver
15/08/19 18:15:55 INFO Executor: Running task 64.0 in stage 1.0 (TID 149)
15/08/19 18:15:55 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 150, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:55 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 134) in 13281 ms on localhost (49/200)
15/08/19 18:15:55 INFO Executor: Running task 65.0 in stage 1.0 (TID 150)
15/08/19 18:15:55 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 133) in 13298 ms on localhost (50/200)
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:55 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO ColumnChunkPageWriteStore: written 1,308,452B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,373B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:55 INFO ColumnChunkPageWriteStore: written 417,547B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,406B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:15:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000050
15/08/19 18:15:55 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000050_0: Committed
15/08/19 18:15:55 INFO Executor: Finished task 50.0 in stage 1.0 (TID 135). 843 bytes result sent to driver
15/08/19 18:15:55 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 151, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:55 INFO Executor: Running task 66.0 in stage 1.0 (TID 151)
15/08/19 18:15:55 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 135) in 13188 ms on localhost (51/200)
15/08/19 18:15:55 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:15:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:15:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,509
15/08/19 18:15:55 INFO ColumnChunkPageWriteStore: written 1,308,761B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,682B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:15:55 INFO ColumnChunkPageWriteStore: written 418,092B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,951B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/19 18:15:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000053
15/08/19 18:15:55 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000053_0: Committed
15/08/19 18:15:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:15:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:15:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:15:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:15:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:15:55 INFO Executor: Finished task 53.0 in stage 1.0 (TID 138). 843 bytes result sent to driver
15/08/19 18:15:55 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 152, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:15:55 INFO Executor: Running task 67.0 in stage 1.0 (TID 152)
15/08/19 18:15:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:07 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 138) in 19967 ms on localhost (52/200)
15/08/19 18:16:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:07 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:07 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:07 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:36176 in memory (size: 4.4 KB, free: 20.7 GB)
15/08/19 18:16:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:07 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:16:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:16:07 INFO ColumnChunkPageWriteStore: written 1,308,455B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,376B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:07 INFO ColumnChunkPageWriteStore: written 418,104B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,963B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:16:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:16:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,509
15/08/19 18:16:07 INFO ColumnChunkPageWriteStore: written 1,308,644B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,565B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:07 INFO ColumnChunkPageWriteStore: written 416,069B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,928B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:16:07 INFO ColumnChunkPageWriteStore: written 1,308,737B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,658B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:07 INFO ColumnChunkPageWriteStore: written 416,395B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,254B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/19 18:16:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000057
15/08/19 18:16:07 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000057_0: Committed
15/08/19 18:16:07 INFO Executor: Finished task 57.0 in stage 1.0 (TID 142). 843 bytes result sent to driver
15/08/19 18:16:07 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 153, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:07 INFO Executor: Running task 68.0 in stage 1.0 (TID 153)
15/08/19 18:16:07 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 142) in 20424 ms on localhost (53/200)
15/08/19 18:16:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000051
15/08/19 18:16:07 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000051_0: Committed
15/08/19 18:16:07 INFO Executor: Finished task 51.0 in stage 1.0 (TID 136). 843 bytes result sent to driver
15/08/19 18:16:07 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 154, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:07 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 136) in 24690 ms on localhost (54/200)
15/08/19 18:16:07 INFO Executor: Running task 69.0 in stage 1.0 (TID 154)
15/08/19 18:16:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000052
15/08/19 18:16:07 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000052_0: Committed
15/08/19 18:16:08 INFO Executor: Finished task 52.0 in stage 1.0 (TID 137). 843 bytes result sent to driver
15/08/19 18:16:08 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 155, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 70.0 in stage 1.0 (TID 155)
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:08 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 137) in 24597 ms on localhost (55/200)
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,693B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,614B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 416,489B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,348B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000054
15/08/19 18:16:08 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000054_0: Committed
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,473B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,394B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,593B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,514B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 415,624B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,483B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 417,258B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,117B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:16:08 INFO Executor: Finished task 54.0 in stage 1.0 (TID 139). 843 bytes result sent to driver
15/08/19 18:16:08 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 156, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 71.0 in stage 1.0 (TID 156)
15/08/19 18:16:08 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 139) in 21079 ms on localhost (56/200)
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,437B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,358B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,667B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,588B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 417,425B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,284B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 416,792B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,651B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000055
15/08/19 18:16:08 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000055_0: Committed
15/08/19 18:16:08 INFO Executor: Finished task 55.0 in stage 1.0 (TID 140). 843 bytes result sent to driver
15/08/19 18:16:08 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 157, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 72.0 in stage 1.0 (TID 157)
15/08/19 18:16:08 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 140) in 21086 ms on localhost (57/200)
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,513B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,434B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 416,041B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,900B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000060
15/08/19 18:16:08 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000060_0: Committed
15/08/19 18:16:08 INFO Executor: Finished task 60.0 in stage 1.0 (TID 145). 843 bytes result sent to driver
15/08/19 18:16:08 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 158, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 73.0 in stage 1.0 (TID 158)
15/08/19 18:16:08 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 145) in 20718 ms on localhost (58/200)
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000056
15/08/19 18:16:08 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000056_0: Committed
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,517
15/08/19 18:16:08 INFO Executor: Finished task 56.0 in stage 1.0 (TID 141). 843 bytes result sent to driver
15/08/19 18:16:08 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 159, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 74.0 in stage 1.0 (TID 159)
15/08/19 18:16:08 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 141) in 21180 ms on localhost (59/200)
15/08/19 18:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000062
15/08/19 18:16:08 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000062_0: Committed
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,467B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,388B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 416,880B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,739B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO Executor: Finished task 62.0 in stage 1.0 (TID 147). 843 bytes result sent to driver
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:08 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 160, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 75.0 in stage 1.0 (TID 160)
15/08/19 18:16:08 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 147) in 20684 ms on localhost (60/200)
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000061
15/08/19 18:16:08 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000061_0: Committed
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,511B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,432B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 415,430B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,289B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:16:08 INFO Executor: Finished task 61.0 in stage 1.0 (TID 146). 843 bytes result sent to driver
15/08/19 18:16:08 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 161, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 76.0 in stage 1.0 (TID 161)
15/08/19 18:16:08 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 146) in 20837 ms on localhost (61/200)
15/08/19 18:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000063
15/08/19 18:16:08 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000063_0: Committed
15/08/19 18:16:08 INFO Executor: Finished task 63.0 in stage 1.0 (TID 148). 843 bytes result sent to driver
15/08/19 18:16:08 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 162, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 77.0 in stage 1.0 (TID 162)
15/08/19 18:16:08 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 148) in 20551 ms on localhost (62/200)
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508191815_0001_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191815_0001_m_000058
15/08/19 18:16:08 INFO SparkHadoopMapRedUtil: attempt_201508191815_0001_m_000058_0: Committed
15/08/19 18:16:08 INFO Executor: Finished task 58.0 in stage 1.0 (TID 143). 843 bytes result sent to driver
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:08 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 163, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:08 INFO Executor: Running task 78.0 in stage 1.0 (TID 163)
15/08/19 18:16:08 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 143) in 21357 ms on localhost (63/200)
15/08/19 18:16:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 1,308,754B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,675B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:08 INFO ColumnChunkPageWriteStore: written 414,752B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 414,611B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:16:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000059
15/08/19 18:16:09 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000059_0: Committed
15/08/19 18:16:09 INFO Executor: Finished task 59.0 in stage 1.0 (TID 144). 843 bytes result sent to driver
15/08/19 18:16:09 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 164, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:09 INFO Executor: Running task 79.0 in stage 1.0 (TID 164)
15/08/19 18:16:09 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 144) in 21353 ms on localhost (64/200)
15/08/19 18:16:09 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:12 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:12 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:12 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:12 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:12 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:12 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:12 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:12 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:12 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:12 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:12 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:12 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:13 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:13 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:16:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:13 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:13 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:13 INFO ColumnChunkPageWriteStore: written 1,308,451B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,372B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:13 INFO ColumnChunkPageWriteStore: written 416,883B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,742B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:16:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:13 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:13 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000065
15/08/19 18:16:13 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000065_0: Committed
15/08/19 18:16:13 INFO Executor: Finished task 65.0 in stage 1.0 (TID 150). 843 bytes result sent to driver
15/08/19 18:16:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:13 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 165, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:13 INFO Executor: Running task 80.0 in stage 1.0 (TID 165)
15/08/19 18:16:13 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 150) in 18021 ms on localhost (65/200)
15/08/19 18:16:13 INFO ColumnChunkPageWriteStore: written 1,308,554B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,475B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:13 INFO ColumnChunkPageWriteStore: written 416,702B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,561B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000064
15/08/19 18:16:13 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000064_0: Committed
15/08/19 18:16:13 INFO Executor: Finished task 64.0 in stage 1.0 (TID 149). 843 bytes result sent to driver
15/08/19 18:16:13 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 166, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:13 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 149) in 18146 ms on localhost (66/200)
15/08/19 18:16:13 INFO Executor: Running task 81.0 in stage 1.0 (TID 166)
15/08/19 18:16:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:16:13 INFO ColumnChunkPageWriteStore: written 1,308,253B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,174B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:13 INFO ColumnChunkPageWriteStore: written 417,760B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,619B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:13 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:13 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:13 INFO ColumnChunkPageWriteStore: written 1,308,666B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,587B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:13 INFO ColumnChunkPageWriteStore: written 417,620B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,479B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:16:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:13 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:13 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:13 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000066
15/08/19 18:16:17 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000066_0: Committed
15/08/19 18:16:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:17 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:17 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:17 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:17 INFO Executor: Finished task 66.0 in stage 1.0 (TID 151). 843 bytes result sent to driver
15/08/19 18:16:17 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 167, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:17 INFO Executor: Running task 82.0 in stage 1.0 (TID 167)
15/08/19 18:16:17 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 151) in 22225 ms on localhost (67/200)
15/08/19 18:16:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000067
15/08/19 18:16:17 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000067_0: Committed
15/08/19 18:16:17 INFO Executor: Finished task 67.0 in stage 1.0 (TID 152). 843 bytes result sent to driver
15/08/19 18:16:17 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 168, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:17 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 152) in 22003 ms on localhost (68/200)
15/08/19 18:16:17 INFO Executor: Running task 83.0 in stage 1.0 (TID 168)
15/08/19 18:16:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:17 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:17 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:17 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:18 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:18 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:18 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:18 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:18 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:18 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:18 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:18 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:18 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:18 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:18 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:18 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:18 INFO ColumnChunkPageWriteStore: written 1,308,356B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,277B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:18 INFO ColumnChunkPageWriteStore: written 417,224B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,083B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:16:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000069
15/08/19 18:16:18 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000069_0: Committed
15/08/19 18:16:18 INFO ColumnChunkPageWriteStore: written 1,308,301B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,222B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:18 INFO ColumnChunkPageWriteStore: written 417,153B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,012B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:16:18 INFO Executor: Finished task 69.0 in stage 1.0 (TID 154). 843 bytes result sent to driver
15/08/19 18:16:18 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 169, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:18 INFO Executor: Running task 84.0 in stage 1.0 (TID 169)
15/08/19 18:16:18 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 154) in 10696 ms on localhost (69/200)
15/08/19 18:16:18 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:16:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000070
15/08/19 18:16:18 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000070_0: Committed
15/08/19 18:16:18 INFO Executor: Finished task 70.0 in stage 1.0 (TID 155). 843 bytes result sent to driver
15/08/19 18:16:18 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:18 INFO Executor: Running task 85.0 in stage 1.0 (TID 170)
15/08/19 18:16:18 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 155) in 10726 ms on localhost (70/200)
15/08/19 18:16:18 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:16:18 INFO ColumnChunkPageWriteStore: written 1,308,590B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,511B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:18 INFO ColumnChunkPageWriteStore: written 416,543B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,402B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:16:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000068
15/08/19 18:16:18 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000068_0: Committed
15/08/19 18:16:18 INFO Executor: Finished task 68.0 in stage 1.0 (TID 153). 843 bytes result sent to driver
15/08/19 18:16:18 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:18 INFO Executor: Running task 86.0 in stage 1.0 (TID 171)
15/08/19 18:16:18 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 153) in 11046 ms on localhost (71/200)
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,362B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,283B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 417,019B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,878B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,551B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,472B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 418,033B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,892B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000077
15/08/19 18:16:19 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000077_0: Committed
15/08/19 18:16:19 INFO Executor: Finished task 77.0 in stage 1.0 (TID 162). 843 bytes result sent to driver
15/08/19 18:16:19 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:19 INFO Executor: Running task 87.0 in stage 1.0 (TID 172)
15/08/19 18:16:19 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 162) in 10381 ms on localhost (72/200)
15/08/19 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000072
15/08/19 18:16:19 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000072_0: Committed
15/08/19 18:16:19 INFO Executor: Finished task 72.0 in stage 1.0 (TID 157). 843 bytes result sent to driver
15/08/19 18:16:19 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:19 INFO Executor: Running task 88.0 in stage 1.0 (TID 173)
15/08/19 18:16:19 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 157) in 10713 ms on localhost (73/200)
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,428B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,349B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 416,025B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,884B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,523B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,444B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 416,806B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,665B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,381B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,302B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 415,774B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,633B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,269B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,190B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 417,435B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,294B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000071
15/08/19 18:16:19 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000071_0: Committed
15/08/19 18:16:19 INFO Executor: Finished task 71.0 in stage 1.0 (TID 156). 843 bytes result sent to driver
15/08/19 18:16:19 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:19 INFO Executor: Running task 89.0 in stage 1.0 (TID 174)
15/08/19 18:16:19 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 156) in 11123 ms on localhost (74/200)
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000076
15/08/19 18:16:19 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000076_0: Committed
15/08/19 18:16:19 INFO Executor: Finished task 76.0 in stage 1.0 (TID 161). 843 bytes result sent to driver
15/08/19 18:16:19 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:19 INFO Executor: Running task 90.0 in stage 1.0 (TID 175)
15/08/19 18:16:19 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 161) in 10851 ms on localhost (75/200)
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,613B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,534B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 416,450B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,309B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000073
15/08/19 18:16:19 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000073_0: Committed
15/08/19 18:16:19 INFO Executor: Finished task 73.0 in stage 1.0 (TID 158). 843 bytes result sent to driver
15/08/19 18:16:19 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:19 INFO Executor: Running task 91.0 in stage 1.0 (TID 176)
15/08/19 18:16:19 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 158) in 11141 ms on localhost (76/200)
15/08/19 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000075
15/08/19 18:16:19 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000075_0: Committed
15/08/19 18:16:19 INFO Executor: Finished task 75.0 in stage 1.0 (TID 160). 843 bytes result sent to driver
15/08/19 18:16:19 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:19 INFO Executor: Running task 92.0 in stage 1.0 (TID 177)
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:19 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 160) in 11085 ms on localhost (77/200)
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,379B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,300B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 416,983B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,842B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000078
15/08/19 18:16:19 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000078_0: Committed
15/08/19 18:16:19 INFO Executor: Finished task 78.0 in stage 1.0 (TID 163). 843 bytes result sent to driver
15/08/19 18:16:19 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:19 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 163) in 11036 ms on localhost (78/200)
15/08/19 18:16:19 INFO Executor: Running task 93.0 in stage 1.0 (TID 178)
15/08/19 18:16:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000074
15/08/19 18:16:19 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000074_0: Committed
15/08/19 18:16:19 INFO Executor: Finished task 74.0 in stage 1.0 (TID 159). 843 bytes result sent to driver
15/08/19 18:16:19 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:19 INFO Executor: Running task 94.0 in stage 1.0 (TID 179)
15/08/19 18:16:19 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 159) in 11381 ms on localhost (79/200)
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 1,308,497B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,418B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:19 INFO ColumnChunkPageWriteStore: written 417,139B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,998B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000079
15/08/19 18:16:20 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000079_0: Committed
15/08/19 18:16:20 INFO Executor: Finished task 79.0 in stage 1.0 (TID 164). 843 bytes result sent to driver
15/08/19 18:16:20 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:20 INFO Executor: Running task 95.0 in stage 1.0 (TID 180)
15/08/19 18:16:20 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 164) in 11048 ms on localhost (80/200)
15/08/19 18:16:20 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:25 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:25 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:25 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:25 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:25 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:25 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:25 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:25 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:25 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:25 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:25 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:25 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:26 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:26 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:26 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:26 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:26 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:26 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:26 INFO ColumnChunkPageWriteStore: written 1,308,454B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,375B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:26 INFO ColumnChunkPageWriteStore: written 416,804B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,663B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:16:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000080
15/08/19 18:16:26 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000080_0: Committed
15/08/19 18:16:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:26 INFO Executor: Finished task 80.0 in stage 1.0 (TID 165). 843 bytes result sent to driver
15/08/19 18:16:26 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:26 INFO Executor: Running task 96.0 in stage 1.0 (TID 181)
15/08/19 18:16:26 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 165) in 13210 ms on localhost (81/200)
15/08/19 18:16:26 INFO ColumnChunkPageWriteStore: written 1,308,644B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,565B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:26 INFO ColumnChunkPageWriteStore: written 416,575B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,434B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:16:26 INFO ColumnChunkPageWriteStore: written 1,308,512B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,433B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:26 INFO ColumnChunkPageWriteStore: written 416,667B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,526B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:26 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000082
15/08/19 18:16:26 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000082_0: Committed
15/08/19 18:16:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000081
15/08/19 18:16:26 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000081_0: Committed
15/08/19 18:16:26 INFO Executor: Finished task 82.0 in stage 1.0 (TID 167). 843 bytes result sent to driver
15/08/19 18:16:26 INFO Executor: Finished task 81.0 in stage 1.0 (TID 166). 843 bytes result sent to driver
15/08/19 18:16:26 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:26 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:26 INFO Executor: Running task 98.0 in stage 1.0 (TID 183)
15/08/19 18:16:26 INFO Executor: Running task 97.0 in stage 1.0 (TID 182)
15/08/19 18:16:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:26 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:26 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:26 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:26 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 166) in 13217 ms on localhost (82/200)
15/08/19 18:16:26 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 167) in 8893 ms on localhost (83/200)
15/08/19 18:16:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:26 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:26 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:16:26 INFO ColumnChunkPageWriteStore: written 1,308,862B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,783B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:26 INFO ColumnChunkPageWriteStore: written 417,458B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,317B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:16:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000083
15/08/19 18:16:27 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000083_0: Committed
15/08/19 18:16:27 INFO Executor: Finished task 83.0 in stage 1.0 (TID 168). 843 bytes result sent to driver
15/08/19 18:16:27 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:27 INFO Executor: Running task 99.0 in stage 1.0 (TID 184)
15/08/19 18:16:27 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 168) in 9118 ms on localhost (84/200)
15/08/19 18:16:27 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:30 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:30 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:30 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:30 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:30 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:30 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:30 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:30 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:30 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:31 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:31 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:31 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:31 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:31 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:31 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 1,308,862B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,783B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 416,288B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,147B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 1,308,431B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,352B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 417,247B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,106B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000084
15/08/19 18:16:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000085
15/08/19 18:16:31 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000084_0: Committed
15/08/19 18:16:31 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000085_0: Committed
15/08/19 18:16:31 INFO Executor: Finished task 85.0 in stage 1.0 (TID 170). 843 bytes result sent to driver
15/08/19 18:16:31 INFO Executor: Finished task 84.0 in stage 1.0 (TID 169). 843 bytes result sent to driver
15/08/19 18:16:31 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:31 INFO Executor: Running task 100.0 in stage 1.0 (TID 185)
15/08/19 18:16:31 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:31 INFO Executor: Running task 101.0 in stage 1.0 (TID 186)
15/08/19 18:16:31 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 170) in 12557 ms on localhost (85/200)
15/08/19 18:16:31 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 169) in 12656 ms on localhost (86/200)
15/08/19 18:16:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/19 18:16:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,525
15/08/19 18:16:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:31 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:31 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 1,308,650B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,571B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 417,124B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,983B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/19 18:16:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000088
15/08/19 18:16:31 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000088_0: Committed
15/08/19 18:16:31 INFO Executor: Finished task 88.0 in stage 1.0 (TID 173). 843 bytes result sent to driver
15/08/19 18:16:31 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:31 INFO Executor: Running task 102.0 in stage 1.0 (TID 187)
15/08/19 18:16:31 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 173) in 12278 ms on localhost (87/200)
15/08/19 18:16:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:31 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:31 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:31 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:31 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:31 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 1,308,450B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,371B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 417,135B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,994B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000089
15/08/19 18:16:31 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000089_0: Committed
15/08/19 18:16:31 INFO Executor: Finished task 89.0 in stage 1.0 (TID 174). 843 bytes result sent to driver
15/08/19 18:16:31 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:31 INFO Executor: Running task 103.0 in stage 1.0 (TID 188)
15/08/19 18:16:31 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 174) in 12402 ms on localhost (88/200)
15/08/19 18:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:16:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/19 18:16:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 1,308,448B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,369B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 415,956B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,815B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 1,308,677B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,598B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:31 INFO ColumnChunkPageWriteStore: written 416,817B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,676B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000090
15/08/19 18:16:32 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000090_0: Committed
15/08/19 18:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000087
15/08/19 18:16:32 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000087_0: Committed
15/08/19 18:16:32 INFO Executor: Finished task 90.0 in stage 1.0 (TID 175). 843 bytes result sent to driver
15/08/19 18:16:32 INFO Executor: Finished task 87.0 in stage 1.0 (TID 172). 843 bytes result sent to driver
15/08/19 18:16:32 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:32 INFO Executor: Running task 104.0 in stage 1.0 (TID 189)
15/08/19 18:16:32 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,517
15/08/19 18:16:32 INFO Executor: Running task 105.0 in stage 1.0 (TID 190)
15/08/19 18:16:32 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 175) in 12507 ms on localhost (89/200)
15/08/19 18:16:32 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 172) in 12908 ms on localhost (90/200)
15/08/19 18:16:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,429
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 1,308,611B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,532B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 1,308,778B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,699B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 415,460B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,319B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 1,308,556B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,477B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 416,248B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,107B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 417,168B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,027B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 304 entries, 2,432B raw, 304B comp}
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000095
15/08/19 18:16:32 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000095_0: Committed
15/08/19 18:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000092
15/08/19 18:16:32 INFO Executor: Finished task 95.0 in stage 1.0 (TID 180). 843 bytes result sent to driver
15/08/19 18:16:32 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000092_0: Committed
15/08/19 18:16:32 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:32 INFO Executor: Running task 106.0 in stage 1.0 (TID 191)
15/08/19 18:16:32 INFO Executor: Finished task 92.0 in stage 1.0 (TID 177). 843 bytes result sent to driver
15/08/19 18:16:32 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:32 INFO Executor: Running task 107.0 in stage 1.0 (TID 192)
15/08/19 18:16:32 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 180) in 12182 ms on localhost (91/200)
15/08/19 18:16:32 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 177) in 12570 ms on localhost (92/200)
15/08/19 18:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000086
15/08/19 18:16:32 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000086_0: Committed
15/08/19 18:16:32 INFO Executor: Finished task 86.0 in stage 1.0 (TID 171). 843 bytes result sent to driver
15/08/19 18:16:32 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:32 INFO Executor: Running task 108.0 in stage 1.0 (TID 193)
15/08/19 18:16:32 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 171) in 13301 ms on localhost (93/200)
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/19 18:16:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 1,308,483B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,404B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 417,476B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,335B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000094
15/08/19 18:16:32 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000094_0: Committed
15/08/19 18:16:32 INFO Executor: Finished task 94.0 in stage 1.0 (TID 179). 843 bytes result sent to driver
15/08/19 18:16:32 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:32 INFO Executor: Running task 109.0 in stage 1.0 (TID 194)
15/08/19 18:16:32 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 179) in 12500 ms on localhost (94/200)
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,509
15/08/19 18:16:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 1,308,690B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,611B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 416,563B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,422B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 1,308,552B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,473B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:32 INFO ColumnChunkPageWriteStore: written 417,221B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,080B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000091
15/08/19 18:16:32 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000091_0: Committed
15/08/19 18:16:32 INFO Executor: Finished task 91.0 in stage 1.0 (TID 176). 843 bytes result sent to driver
15/08/19 18:16:32 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:32 INFO Executor: Running task 110.0 in stage 1.0 (TID 195)
15/08/19 18:16:32 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 176) in 13214 ms on localhost (95/200)
15/08/19 18:16:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000093
15/08/19 18:16:32 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000093_0: Committed
15/08/19 18:16:32 INFO Executor: Finished task 93.0 in stage 1.0 (TID 178). 843 bytes result sent to driver
15/08/19 18:16:32 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:32 INFO Executor: Running task 111.0 in stage 1.0 (TID 196)
15/08/19 18:16:32 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 178) in 12987 ms on localhost (96/200)
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:33 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:33 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:33 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:38 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:38 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:38 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:16:38 INFO ColumnChunkPageWriteStore: written 1,308,378B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,299B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:38 INFO ColumnChunkPageWriteStore: written 416,206B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,065B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:16:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:38 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:38 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:38 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:38 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:38 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:38 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000097
15/08/19 18:16:38 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000097_0: Committed
15/08/19 18:16:38 INFO Executor: Finished task 97.0 in stage 1.0 (TID 182). 843 bytes result sent to driver
15/08/19 18:16:38 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:38 INFO Executor: Running task 112.0 in stage 1.0 (TID 197)
15/08/19 18:16:38 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 182) in 11698 ms on localhost (97/200)
15/08/19 18:16:38 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:39 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:39 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:39 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:39 INFO ColumnChunkPageWriteStore: written 1,308,635B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,556B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:39 INFO ColumnChunkPageWriteStore: written 416,548B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,407B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:39 INFO ColumnChunkPageWriteStore: written 1,308,565B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,486B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:39 INFO ColumnChunkPageWriteStore: written 416,356B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,215B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:39 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:39 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:39 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:39 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:39 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:39 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:39 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:39 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000098
15/08/19 18:16:39 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000098_0: Committed
15/08/19 18:16:39 INFO Executor: Finished task 98.0 in stage 1.0 (TID 183). 843 bytes result sent to driver
15/08/19 18:16:39 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:39 INFO Executor: Running task 113.0 in stage 1.0 (TID 198)
15/08/19 18:16:39 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 183) in 13029 ms on localhost (98/200)
15/08/19 18:16:39 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000099
15/08/19 18:16:39 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000099_0: Committed
15/08/19 18:16:39 INFO Executor: Finished task 99.0 in stage 1.0 (TID 184). 843 bytes result sent to driver
15/08/19 18:16:39 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:39 INFO Executor: Running task 114.0 in stage 1.0 (TID 199)
15/08/19 18:16:39 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 184) in 12944 ms on localhost (99/200)
15/08/19 18:16:40 INFO ColumnChunkPageWriteStore: written 1,308,634B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,555B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:40 INFO ColumnChunkPageWriteStore: written 417,328B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,187B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:40 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:16:40 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:40 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:40 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:40 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:40 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000096
15/08/19 18:16:40 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000096_0: Committed
15/08/19 18:16:40 INFO Executor: Finished task 96.0 in stage 1.0 (TID 181). 843 bytes result sent to driver
15/08/19 18:16:40 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:40 INFO Executor: Running task 115.0 in stage 1.0 (TID 200)
15/08/19 18:16:40 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 181) in 13468 ms on localhost (100/200)
15/08/19 18:16:40 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:44 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:44 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:44 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:45 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:45 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:16:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:45 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:45 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:45 INFO ColumnChunkPageWriteStore: written 1,308,414B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,335B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:45 INFO ColumnChunkPageWriteStore: written 417,016B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,875B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:16:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000100
15/08/19 18:16:45 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000100_0: Committed
15/08/19 18:16:45 INFO Executor: Finished task 100.0 in stage 1.0 (TID 185). 843 bytes result sent to driver
15/08/19 18:16:45 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:45 INFO Executor: Running task 116.0 in stage 1.0 (TID 201)
15/08/19 18:16:45 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 185) in 14290 ms on localhost (101/200)
15/08/19 18:16:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:45 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:45 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:45 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:45 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:45 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:45 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:45 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:16:45 INFO ColumnChunkPageWriteStore: written 1,308,426B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,347B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:45 INFO ColumnChunkPageWriteStore: written 416,834B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,693B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:16:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:45 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:45 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:16:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:45 INFO ColumnChunkPageWriteStore: written 1,308,496B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,417B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:45 INFO ColumnChunkPageWriteStore: written 416,577B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,436B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:16:45 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:45 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:45 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:45 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:45 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000101
15/08/19 18:16:45 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000101_0: Committed
15/08/19 18:16:45 INFO Executor: Finished task 101.0 in stage 1.0 (TID 186). 843 bytes result sent to driver
15/08/19 18:16:45 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:45 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 186) in 14625 ms on localhost (102/200)
15/08/19 18:16:45 INFO Executor: Running task 117.0 in stage 1.0 (TID 202)
15/08/19 18:16:45 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:45 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000102
15/08/19 18:16:45 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000102_0: Committed
15/08/19 18:16:45 INFO Executor: Finished task 102.0 in stage 1.0 (TID 187). 843 bytes result sent to driver
15/08/19 18:16:45 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:45 INFO Executor: Running task 118.0 in stage 1.0 (TID 203)
15/08/19 18:16:45 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 187) in 14536 ms on localhost (103/200)
15/08/19 18:16:46 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:16:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:46 INFO ColumnChunkPageWriteStore: written 1,308,338B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,259B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:46 INFO ColumnChunkPageWriteStore: written 415,541B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,400B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:16:46 INFO ColumnChunkPageWriteStore: written 1,308,492B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,413B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:46 INFO ColumnChunkPageWriteStore: written 415,499B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,358B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:16:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000104
15/08/19 18:16:46 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000104_0: Committed
15/08/19 18:16:46 INFO Executor: Finished task 104.0 in stage 1.0 (TID 189). 843 bytes result sent to driver
15/08/19 18:16:46 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:46 INFO Executor: Running task 119.0 in stage 1.0 (TID 204)
15/08/19 18:16:46 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 189) in 14325 ms on localhost (104/200)
15/08/19 18:16:46 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000103
15/08/19 18:16:46 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000103_0: Committed
15/08/19 18:16:46 INFO Executor: Finished task 103.0 in stage 1.0 (TID 188). 843 bytes result sent to driver
15/08/19 18:16:46 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:46 INFO Executor: Running task 120.0 in stage 1.0 (TID 205)
15/08/19 18:16:46 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 188) in 14610 ms on localhost (105/200)
15/08/19 18:16:46 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:46 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:46 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:46 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:46 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:46 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:46 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:46 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:46 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 1,308,538B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,459B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 1,308,428B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,349B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 415,749B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,608B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 418,733B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 418,592B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 1,308,660B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,581B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 417,343B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,202B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 1,308,826B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,747B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 417,204B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,063B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:16:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000105
15/08/19 18:16:47 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000105_0: Committed
15/08/19 18:16:47 INFO Executor: Finished task 105.0 in stage 1.0 (TID 190). 843 bytes result sent to driver
15/08/19 18:16:47 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:47 INFO Executor: Running task 121.0 in stage 1.0 (TID 206)
15/08/19 18:16:47 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 190) in 15222 ms on localhost (106/200)
15/08/19 18:16:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000106
15/08/19 18:16:47 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000106_0: Committed
15/08/19 18:16:47 INFO Executor: Finished task 106.0 in stage 1.0 (TID 191). 843 bytes result sent to driver
15/08/19 18:16:47 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:47 INFO Executor: Running task 122.0 in stage 1.0 (TID 207)
15/08/19 18:16:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000107
15/08/19 18:16:47 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000107_0: Committed
15/08/19 18:16:47 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 191) in 15092 ms on localhost (107/200)
15/08/19 18:16:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000110
15/08/19 18:16:47 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000110_0: Committed
15/08/19 18:16:47 INFO Executor: Finished task 107.0 in stage 1.0 (TID 192). 843 bytes result sent to driver
15/08/19 18:16:47 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 208, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:47 INFO Executor: Finished task 110.0 in stage 1.0 (TID 195). 843 bytes result sent to driver
15/08/19 18:16:47 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 192) in 15104 ms on localhost (108/200)
15/08/19 18:16:47 INFO Executor: Running task 123.0 in stage 1.0 (TID 208)
15/08/19 18:16:47 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 195) in 14505 ms on localhost (109/200)
15/08/19 18:16:47 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 209, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:47 INFO Executor: Running task 124.0 in stage 1.0 (TID 209)
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,437
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 1,308,740B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,661B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 416,815B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,674B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 305 entries, 2,440B raw, 305B comp}
15/08/19 18:16:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:16:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000108
15/08/19 18:16:47 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000108_0: Committed
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 1,308,516B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,437B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:47 INFO Executor: Finished task 108.0 in stage 1.0 (TID 193). 843 bytes result sent to driver
15/08/19 18:16:47 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 210, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:47 INFO Executor: Running task 125.0 in stage 1.0 (TID 210)
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 415,769B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,628B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:16:47 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 193) in 15451 ms on localhost (110/200)
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000109
15/08/19 18:16:47 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000109_0: Committed
15/08/19 18:16:47 INFO Executor: Finished task 109.0 in stage 1.0 (TID 194). 843 bytes result sent to driver
15/08/19 18:16:47 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 211, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:47 INFO Executor: Running task 126.0 in stage 1.0 (TID 211)
15/08/19 18:16:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:16:47 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 194) in 15381 ms on localhost (111/200)
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 1,308,712B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,633B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:47 INFO ColumnChunkPageWriteStore: written 417,701B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,560B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000111
15/08/19 18:16:47 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000111_0: Committed
15/08/19 18:16:47 INFO Executor: Finished task 111.0 in stage 1.0 (TID 196). 843 bytes result sent to driver
15/08/19 18:16:47 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 212, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:47 INFO Executor: Running task 127.0 in stage 1.0 (TID 212)
15/08/19 18:16:47 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 196) in 15101 ms on localhost (112/200)
15/08/19 18:16:48 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:53 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:53 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:53 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:53 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:53 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:53 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:53 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:53 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:53 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:53 INFO ColumnChunkPageWriteStore: written 1,308,583B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,504B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:53 INFO ColumnChunkPageWriteStore: written 416,500B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,359B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:53 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000112
15/08/19 18:16:53 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000112_0: Committed
15/08/19 18:16:53 INFO Executor: Finished task 112.0 in stage 1.0 (TID 197). 843 bytes result sent to driver
15/08/19 18:16:53 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 213, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:53 INFO Executor: Running task 128.0 in stage 1.0 (TID 213)
15/08/19 18:16:53 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 197) in 14931 ms on localhost (113/200)
15/08/19 18:16:53 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:53 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:16:54 INFO ColumnChunkPageWriteStore: written 1,308,540B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,461B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:54 INFO ColumnChunkPageWriteStore: written 416,654B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,513B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:16:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000114
15/08/19 18:16:54 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000114_0: Committed
15/08/19 18:16:54 INFO Executor: Finished task 114.0 in stage 1.0 (TID 199). 843 bytes result sent to driver
15/08/19 18:16:54 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 214, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:54 INFO Executor: Running task 129.0 in stage 1.0 (TID 214)
15/08/19 18:16:54 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 199) in 14092 ms on localhost (114/200)
15/08/19 18:16:54 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:54 INFO ColumnChunkPageWriteStore: written 1,308,293B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,214B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:54 INFO ColumnChunkPageWriteStore: written 418,103B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,962B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:16:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000113
15/08/19 18:16:54 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000113_0: Committed
15/08/19 18:16:54 INFO Executor: Finished task 113.0 in stage 1.0 (TID 198). 843 bytes result sent to driver
15/08/19 18:16:54 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 215, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:54 INFO Executor: Running task 130.0 in stage 1.0 (TID 215)
15/08/19 18:16:54 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 198) in 14477 ms on localhost (115/200)
15/08/19 18:16:54 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:54 INFO ColumnChunkPageWriteStore: written 1,308,715B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,636B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:54 INFO ColumnChunkPageWriteStore: written 416,272B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,131B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:16:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000115
15/08/19 18:16:54 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000115_0: Committed
15/08/19 18:16:54 INFO Executor: Finished task 115.0 in stage 1.0 (TID 200). 843 bytes result sent to driver
15/08/19 18:16:54 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 216, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:54 INFO Executor: Running task 131.0 in stage 1.0 (TID 216)
15/08/19 18:16:54 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 200) in 14295 ms on localhost (116/200)
15/08/19 18:16:54 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:16:59 INFO ColumnChunkPageWriteStore: written 1,308,643B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,564B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:59 INFO ColumnChunkPageWriteStore: written 418,064B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,923B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:16:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000116
15/08/19 18:16:59 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000116_0: Committed
15/08/19 18:16:59 INFO Executor: Finished task 116.0 in stage 1.0 (TID 201). 843 bytes result sent to driver
15/08/19 18:16:59 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 217, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:59 INFO Executor: Running task 132.0 in stage 1.0 (TID 217)
15/08/19 18:16:59 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 201) in 13609 ms on localhost (117/200)
15/08/19 18:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:59 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:16:59 INFO ColumnChunkPageWriteStore: written 1,308,623B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,544B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:59 INFO ColumnChunkPageWriteStore: written 416,147B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,006B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:16:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000117
15/08/19 18:16:59 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000117_0: Committed
15/08/19 18:16:59 INFO Executor: Finished task 117.0 in stage 1.0 (TID 202). 843 bytes result sent to driver
15/08/19 18:16:59 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 218, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:16:59 INFO Executor: Running task 133.0 in stage 1.0 (TID 218)
15/08/19 18:16:59 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 202) in 13667 ms on localhost (118/200)
15/08/19 18:16:59 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:16:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:16:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:16:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:16:59 INFO ColumnChunkPageWriteStore: written 1,308,534B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,455B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:16:59 INFO ColumnChunkPageWriteStore: written 417,365B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,224B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:16:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:16:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:16:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:16:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:16:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000118
15/08/19 18:17:00 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000118_0: Committed
15/08/19 18:17:00 INFO Executor: Finished task 118.0 in stage 1.0 (TID 203). 843 bytes result sent to driver
15/08/19 18:17:00 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 219, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:00 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 203) in 14088 ms on localhost (119/200)
15/08/19 18:17:00 INFO Executor: Running task 134.0 in stage 1.0 (TID 219)
15/08/19 18:17:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:00 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:00 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:00 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,437
15/08/19 18:17:00 INFO ColumnChunkPageWriteStore: written 1,308,610B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,531B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:00 INFO ColumnChunkPageWriteStore: written 417,781B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,640B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 305 entries, 2,440B raw, 305B comp}
15/08/19 18:17:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:00 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:00 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:00 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000120
15/08/19 18:17:00 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000120_0: Committed
15/08/19 18:17:00 INFO Executor: Finished task 120.0 in stage 1.0 (TID 205). 843 bytes result sent to driver
15/08/19 18:17:00 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 220, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:00 INFO Executor: Running task 135.0 in stage 1.0 (TID 220)
15/08/19 18:17:00 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 205) in 14023 ms on localhost (120/200)
15/08/19 18:17:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:00 INFO ColumnChunkPageWriteStore: written 1,308,510B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,431B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:00 INFO ColumnChunkPageWriteStore: written 416,509B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,368B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:00 INFO ColumnChunkPageWriteStore: written 1,308,530B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,451B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:00 INFO ColumnChunkPageWriteStore: written 415,832B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,691B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000119
15/08/19 18:17:00 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000119_0: Committed
15/08/19 18:17:00 INFO Executor: Finished task 119.0 in stage 1.0 (TID 204). 843 bytes result sent to driver
15/08/19 18:17:00 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 221, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:00 INFO Executor: Running task 136.0 in stage 1.0 (TID 221)
15/08/19 18:17:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000121
15/08/19 18:17:00 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000121_0: Committed
15/08/19 18:17:00 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 204) in 14261 ms on localhost (121/200)
15/08/19 18:17:00 INFO Executor: Finished task 121.0 in stage 1.0 (TID 206). 843 bytes result sent to driver
15/08/19 18:17:00 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 222, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:00 INFO Executor: Running task 137.0 in stage 1.0 (TID 222)
15/08/19 18:17:00 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 206) in 13373 ms on localhost (122/200)
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:00 INFO ColumnChunkPageWriteStore: written 1,308,752B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,673B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:00 INFO ColumnChunkPageWriteStore: written 416,447B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,306B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000124
15/08/19 18:17:00 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000124_0: Committed
15/08/19 18:17:00 INFO Executor: Finished task 124.0 in stage 1.0 (TID 209). 843 bytes result sent to driver
15/08/19 18:17:00 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 223, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:00 INFO Executor: Running task 138.0 in stage 1.0 (TID 223)
15/08/19 18:17:00 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 209) in 13550 ms on localhost (123/200)
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:01 INFO ColumnChunkPageWriteStore: written 1,308,533B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,454B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:01 INFO ColumnChunkPageWriteStore: written 415,827B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,686B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:01 INFO ColumnChunkPageWriteStore: written 1,308,440B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,361B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:01 INFO ColumnChunkPageWriteStore: written 417,152B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,011B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:01 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:01 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:01 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000126
15/08/19 18:17:01 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000126_0: Committed
15/08/19 18:17:01 INFO ColumnChunkPageWriteStore: written 1,308,631B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,552B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:01 INFO ColumnChunkPageWriteStore: written 417,060B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,919B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:01 INFO Executor: Finished task 126.0 in stage 1.0 (TID 211). 843 bytes result sent to driver
15/08/19 18:17:01 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 224, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:01 INFO Executor: Running task 139.0 in stage 1.0 (TID 224)
15/08/19 18:17:01 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 211) in 13363 ms on localhost (124/200)
15/08/19 18:17:01 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:17:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000125
15/08/19 18:17:01 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000125_0: Committed
15/08/19 18:17:01 INFO Executor: Finished task 125.0 in stage 1.0 (TID 210). 843 bytes result sent to driver
15/08/19 18:17:01 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 225, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:01 INFO Executor: Running task 140.0 in stage 1.0 (TID 225)
15/08/19 18:17:01 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 210) in 13546 ms on localhost (125/200)
15/08/19 18:17:01 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:01 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:01 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:01 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:17:01 INFO ColumnChunkPageWriteStore: written 1,308,611B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,532B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:01 INFO ColumnChunkPageWriteStore: written 416,878B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,737B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:11 INFO DFSClient: Could not complete /apps/hive/warehouse/q18_tmp_par/_temporary/0/_temporary/attempt_201508191816_0001_m_000122_0/part-r-00122-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet retrying...
15/08/19 18:17:11 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:11 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:11 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:11 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:11 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:11 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:11 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:11 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:11 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:11 INFO ColumnChunkPageWriteStore: written 1,308,773B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,694B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:11 INFO ColumnChunkPageWriteStore: written 418,239B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 418,098B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:17:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000122
15/08/19 18:17:11 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000122_0: Committed
15/08/19 18:17:11 INFO Executor: Finished task 122.0 in stage 1.0 (TID 207). 843 bytes result sent to driver
15/08/19 18:17:11 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 226, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:11 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 207) in 24303 ms on localhost (126/200)
15/08/19 18:17:11 INFO Executor: Running task 141.0 in stage 1.0 (TID 226)
15/08/19 18:17:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508191816_0001_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191816_0001_m_000127
15/08/19 18:17:11 INFO SparkHadoopMapRedUtil: attempt_201508191816_0001_m_000127_0: Committed
15/08/19 18:17:11 INFO Executor: Finished task 127.0 in stage 1.0 (TID 212). 843 bytes result sent to driver
15/08/19 18:17:11 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 227, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:11 INFO Executor: Running task 142.0 in stage 1.0 (TID 227)
15/08/19 18:17:11 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 212) in 23686 ms on localhost (127/200)
15/08/19 18:17:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000123
15/08/19 18:17:11 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000123_0: Committed
15/08/19 18:17:11 INFO Executor: Finished task 123.0 in stage 1.0 (TID 208). 843 bytes result sent to driver
15/08/19 18:17:11 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 228, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:11 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 208) in 24332 ms on localhost (128/200)
15/08/19 18:17:11 INFO Executor: Running task 143.0 in stage 1.0 (TID 228)
15/08/19 18:17:11 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:11 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:11 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:12 INFO ColumnChunkPageWriteStore: written 1,308,422B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,343B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:12 INFO ColumnChunkPageWriteStore: written 417,100B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,959B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000128
15/08/19 18:17:12 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000128_0: Committed
15/08/19 18:17:12 INFO Executor: Finished task 128.0 in stage 1.0 (TID 213). 843 bytes result sent to driver
15/08/19 18:17:12 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 229, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:12 INFO Executor: Running task 144.0 in stage 1.0 (TID 229)
15/08/19 18:17:12 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 213) in 18694 ms on localhost (129/200)
15/08/19 18:17:12 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:12 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:12 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:12 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:12 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:12 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:12 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:12 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:12 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:12 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:12 INFO ColumnChunkPageWriteStore: written 1,308,394B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,315B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:12 INFO ColumnChunkPageWriteStore: written 417,978B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,837B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000130
15/08/19 18:17:12 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000130_0: Committed
15/08/19 18:17:12 INFO ColumnChunkPageWriteStore: written 1,308,504B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,425B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:12 INFO ColumnChunkPageWriteStore: written 416,212B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,071B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:12 INFO Executor: Finished task 130.0 in stage 1.0 (TID 215). 843 bytes result sent to driver
15/08/19 18:17:12 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 230, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:12 INFO Executor: Running task 145.0 in stage 1.0 (TID 230)
15/08/19 18:17:12 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 215) in 18460 ms on localhost (130/200)
15/08/19 18:17:12 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:17:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000131
15/08/19 18:17:12 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000131_0: Committed
15/08/19 18:17:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:12 INFO Executor: Finished task 131.0 in stage 1.0 (TID 216). 843 bytes result sent to driver
15/08/19 18:17:12 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 231, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:12 INFO Executor: Running task 146.0 in stage 1.0 (TID 231)
15/08/19 18:17:12 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 216) in 18396 ms on localhost (131/200)
15/08/19 18:17:12 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:12 INFO ColumnChunkPageWriteStore: written 1,308,200B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,121B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:12 INFO ColumnChunkPageWriteStore: written 417,266B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,125B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000129
15/08/19 18:17:12 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000129_0: Committed
15/08/19 18:17:12 INFO Executor: Finished task 129.0 in stage 1.0 (TID 214). 843 bytes result sent to driver
15/08/19 18:17:12 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 232, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:12 INFO Executor: Running task 147.0 in stage 1.0 (TID 232)
15/08/19 18:17:12 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 214) in 18851 ms on localhost (132/200)
15/08/19 18:17:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:13 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:13 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:13 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:13 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:13 INFO ColumnChunkPageWriteStore: written 1,308,550B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,471B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:13 INFO ColumnChunkPageWriteStore: written 416,957B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,816B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:13 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:13 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:13 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:13 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:13 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:13 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:13 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:13 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:13 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:13 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000132
15/08/19 18:17:13 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000132_0: Committed
15/08/19 18:17:13 INFO Executor: Finished task 132.0 in stage 1.0 (TID 217). 843 bytes result sent to driver
15/08/19 18:17:13 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 233, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:13 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 217) in 14605 ms on localhost (133/200)
15/08/19 18:17:13 INFO Executor: Running task 148.0 in stage 1.0 (TID 233)
15/08/19 18:17:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:16 INFO ColumnChunkPageWriteStore: written 1,308,272B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,193B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:16 INFO ColumnChunkPageWriteStore: written 416,636B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,495B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000133
15/08/19 18:17:16 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000133_0: Committed
15/08/19 18:17:16 INFO Executor: Finished task 133.0 in stage 1.0 (TID 218). 843 bytes result sent to driver
15/08/19 18:17:16 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 234, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:16 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 218) in 16941 ms on localhost (134/200)
15/08/19 18:17:16 INFO Executor: Running task 149.0 in stage 1.0 (TID 234)
15/08/19 18:17:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:16 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:16 INFO ColumnChunkPageWriteStore: written 1,308,466B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,387B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:16 INFO ColumnChunkPageWriteStore: written 417,484B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,343B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000134
15/08/19 18:17:16 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000134_0: Committed
15/08/19 18:17:16 INFO Executor: Finished task 134.0 in stage 1.0 (TID 219). 843 bytes result sent to driver
15/08/19 18:17:16 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 235, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:16 INFO Executor: Running task 150.0 in stage 1.0 (TID 235)
15/08/19 18:17:16 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 219) in 16852 ms on localhost (135/200)
15/08/19 18:17:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:16 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:16 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:16 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:16 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 1,308,456B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,377B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 416,611B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,470B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:17 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000137
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:17 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000137_0: Committed
15/08/19 18:17:17 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:17 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:17 INFO Executor: Finished task 137.0 in stage 1.0 (TID 222). 843 bytes result sent to driver
15/08/19 18:17:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:17 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:17 INFO Executor: Running task 151.0 in stage 1.0 (TID 236)
15/08/19 18:17:17 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 222) in 16815 ms on localhost (136/200)
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 1,308,221B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,142B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 416,273B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,132B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:17 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:17 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:17 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000135
15/08/19 18:17:17 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000135_0: Committed
15/08/19 18:17:17 INFO Executor: Finished task 135.0 in stage 1.0 (TID 220). 843 bytes result sent to driver
15/08/19 18:17:17 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 237, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:17 INFO Executor: Running task 152.0 in stage 1.0 (TID 237)
15/08/19 18:17:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:17 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 220) in 17090 ms on localhost (137/200)
15/08/19 18:17:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:17:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:17 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:17 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:17 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 1,308,325B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,246B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 416,263B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,122B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 1,308,724B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,645B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 417,239B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,098B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:17:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000138
15/08/19 18:17:17 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000138_0: Committed
15/08/19 18:17:17 INFO Executor: Finished task 138.0 in stage 1.0 (TID 223). 843 bytes result sent to driver
15/08/19 18:17:17 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 238, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:17 INFO Executor: Running task 153.0 in stage 1.0 (TID 238)
15/08/19 18:17:17 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 223) in 16829 ms on localhost (138/200)
15/08/19 18:17:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000140
15/08/19 18:17:17 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000140_0: Committed
15/08/19 18:17:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,501
15/08/19 18:17:17 INFO Executor: Finished task 140.0 in stage 1.0 (TID 225). 843 bytes result sent to driver
15/08/19 18:17:17 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 239, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:17 INFO Executor: Running task 154.0 in stage 1.0 (TID 239)
15/08/19 18:17:17 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 225) in 16504 ms on localhost (139/200)
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 1,308,713B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,634B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 417,791B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,650B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 1,308,591B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,512B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:17 INFO ColumnChunkPageWriteStore: written 416,532B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,391B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/19 18:17:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000139
15/08/19 18:17:17 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000139_0: Committed
15/08/19 18:17:17 INFO Executor: Finished task 139.0 in stage 1.0 (TID 224). 843 bytes result sent to driver
15/08/19 18:17:17 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 240, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:17 INFO Executor: Running task 155.0 in stage 1.0 (TID 240)
15/08/19 18:17:17 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 224) in 16677 ms on localhost (140/200)
15/08/19 18:17:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000136
15/08/19 18:17:17 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000136_0: Committed
15/08/19 18:17:17 INFO Executor: Finished task 136.0 in stage 1.0 (TID 221). 843 bytes result sent to driver
15/08/19 18:17:17 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 241, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:17 INFO Executor: Running task 156.0 in stage 1.0 (TID 241)
15/08/19 18:17:17 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 221) in 17236 ms on localhost (141/200)
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:18 INFO ColumnChunkPageWriteStore: written 1,308,358B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,279B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:18 INFO ColumnChunkPageWriteStore: written 417,733B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,592B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:18 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:18 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:18 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000142
15/08/19 18:17:18 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000142_0: Committed
15/08/19 18:17:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:18 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:18 INFO Executor: Finished task 142.0 in stage 1.0 (TID 227). 843 bytes result sent to driver
15/08/19 18:17:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:18 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:18 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:18 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 242, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:18 INFO Executor: Running task 157.0 in stage 1.0 (TID 242)
15/08/19 18:17:18 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 227) in 6492 ms on localhost (142/200)
15/08/19 18:17:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:18 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:17:21 INFO ColumnChunkPageWriteStore: written 1,308,400B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,321B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:21 INFO ColumnChunkPageWriteStore: written 416,656B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,515B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:17:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000141
15/08/19 18:17:21 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000141_0: Committed
15/08/19 18:17:21 INFO Executor: Finished task 141.0 in stage 1.0 (TID 226). 843 bytes result sent to driver
15/08/19 18:17:21 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 243, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:21 INFO Executor: Running task 158.0 in stage 1.0 (TID 243)
15/08/19 18:17:21 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 226) in 10362 ms on localhost (143/200)
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:22 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:22 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:22 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 1,308,633B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,554B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 416,445B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,304B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000143
15/08/19 18:17:22 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000143_0: Committed
15/08/19 18:17:22 INFO Executor: Finished task 143.0 in stage 1.0 (TID 228). 843 bytes result sent to driver
15/08/19 18:17:22 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 244, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:22 INFO Executor: Running task 159.0 in stage 1.0 (TID 244)
15/08/19 18:17:22 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 228) in 10602 ms on localhost (144/200)
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 1,308,418B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,339B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 416,827B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,686B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/19 18:17:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000144
15/08/19 18:17:22 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000144_0: Committed
15/08/19 18:17:22 INFO Executor: Finished task 144.0 in stage 1.0 (TID 229). 843 bytes result sent to driver
15/08/19 18:17:22 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 245, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:22 INFO Executor: Running task 160.0 in stage 1.0 (TID 245)
15/08/19 18:17:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:22 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 229) in 10275 ms on localhost (145/200)
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 1,308,429B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,350B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 416,377B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,236B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000146
15/08/19 18:17:22 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000146_0: Committed
15/08/19 18:17:22 INFO Executor: Finished task 146.0 in stage 1.0 (TID 231). 843 bytes result sent to driver
15/08/19 18:17:22 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 246, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:22 INFO Executor: Running task 161.0 in stage 1.0 (TID 246)
15/08/19 18:17:22 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 231) in 9700 ms on localhost (146/200)
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 1,308,458B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,379B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 416,642B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,501B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 1,308,494B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,415B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:22 INFO ColumnChunkPageWriteStore: written 416,125B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,984B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000147
15/08/19 18:17:22 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000147_0: Committed
15/08/19 18:17:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000145
15/08/19 18:17:22 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000145_0: Committed
15/08/19 18:17:22 INFO Executor: Finished task 147.0 in stage 1.0 (TID 232). 843 bytes result sent to driver
15/08/19 18:17:22 INFO Executor: Finished task 145.0 in stage 1.0 (TID 230). 843 bytes result sent to driver
15/08/19 18:17:22 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 247, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:22 INFO Executor: Running task 162.0 in stage 1.0 (TID 247)
15/08/19 18:17:22 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:22 INFO Executor: Running task 163.0 in stage 1.0 (TID 248)
15/08/19 18:17:22 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 230) in 9968 ms on localhost (147/200)
15/08/19 18:17:22 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 232) in 9779 ms on localhost (148/200)
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:22 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:22 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:22 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:17:23 INFO ColumnChunkPageWriteStore: written 1,308,555B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,476B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:23 INFO ColumnChunkPageWriteStore: written 416,420B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,279B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:17:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000148
15/08/19 18:17:23 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000148_0: Committed
15/08/19 18:17:23 INFO Executor: Finished task 148.0 in stage 1.0 (TID 233). 843 bytes result sent to driver
15/08/19 18:17:23 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 249, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:23 INFO Executor: Running task 164.0 in stage 1.0 (TID 249)
15/08/19 18:17:23 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 233) in 9367 ms on localhost (149/200)
15/08/19 18:17:23 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:17:23 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:23 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:23 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:23 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:23 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:23 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:23 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:23 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:23 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,493
15/08/19 18:17:27 INFO ColumnChunkPageWriteStore: written 1,308,486B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,407B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:27 INFO ColumnChunkPageWriteStore: written 416,766B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,625B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/19 18:17:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000149
15/08/19 18:17:27 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000149_0: Committed
15/08/19 18:17:27 INFO Executor: Finished task 149.0 in stage 1.0 (TID 234). 843 bytes result sent to driver
15/08/19 18:17:27 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 250, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:27 INFO Executor: Running task 165.0 in stage 1.0 (TID 250)
15/08/19 18:17:27 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 234) in 11082 ms on localhost (150/200)
15/08/19 18:17:27 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:27 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:27 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:27 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:27 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:27 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:27 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:28 INFO ColumnChunkPageWriteStore: written 1,308,373B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,294B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:28 INFO ColumnChunkPageWriteStore: written 416,217B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,076B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000150
15/08/19 18:17:28 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000150_0: Committed
15/08/19 18:17:28 INFO Executor: Finished task 150.0 in stage 1.0 (TID 235). 843 bytes result sent to driver
15/08/19 18:17:28 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 251, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:28 INFO Executor: Running task 166.0 in stage 1.0 (TID 251)
15/08/19 18:17:28 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 235) in 11670 ms on localhost (151/200)
15/08/19 18:17:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:28 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:28 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:28 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:28 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:28 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:28 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 1,308,602B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,523B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 417,208B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,067B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 1,308,383B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,304B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 416,976B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,835B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000152
15/08/19 18:17:29 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000152_0: Committed
15/08/19 18:17:29 INFO Executor: Finished task 152.0 in stage 1.0 (TID 237). 843 bytes result sent to driver
15/08/19 18:17:29 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 252, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:29 INFO Executor: Running task 167.0 in stage 1.0 (TID 252)
15/08/19 18:17:29 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 237) in 11727 ms on localhost (152/200)
15/08/19 18:17:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:29 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:29 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:29 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 1,308,381B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,302B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 415,805B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,664B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 1,308,470B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,391B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 1,308,595B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,516B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 417,351B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,210B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:29 INFO ColumnChunkPageWriteStore: written 416,739B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,598B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000153
15/08/19 18:17:29 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000153_0: Committed
15/08/19 18:17:29 INFO Executor: Finished task 153.0 in stage 1.0 (TID 238). 843 bytes result sent to driver
15/08/19 18:17:29 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 253, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:29 INFO Executor: Running task 168.0 in stage 1.0 (TID 253)
15/08/19 18:17:29 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 238) in 11792 ms on localhost (153/200)
15/08/19 18:17:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000155
15/08/19 18:17:29 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000155_0: Committed
15/08/19 18:17:29 INFO Executor: Finished task 155.0 in stage 1.0 (TID 240). 843 bytes result sent to driver
15/08/19 18:17:29 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 254, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:29 INFO Executor: Running task 169.0 in stage 1.0 (TID 254)
15/08/19 18:17:29 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 240) in 11707 ms on localhost (154/200)
15/08/19 18:17:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000151
15/08/19 18:17:29 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000151_0: Committed
15/08/19 18:17:29 INFO Executor: Finished task 151.0 in stage 1.0 (TID 236). 843 bytes result sent to driver
15/08/19 18:17:29 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 255, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:29 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:29 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:29 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:29 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:29 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:29 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:29 INFO Executor: Running task 170.0 in stage 1.0 (TID 255)
15/08/19 18:17:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:29 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:29 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:29 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:29 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 236) in 12222 ms on localhost (155/200)
15/08/19 18:17:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:17:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000154
15/08/19 18:17:29 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000154_0: Committed
15/08/19 18:17:29 INFO Executor: Finished task 154.0 in stage 1.0 (TID 239). 843 bytes result sent to driver
15/08/19 18:17:29 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 256, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:29 INFO Executor: Running task 171.0 in stage 1.0 (TID 256)
15/08/19 18:17:29 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 239) in 12224 ms on localhost (156/200)
15/08/19 18:17:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:30 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:30 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:30 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:30 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:30 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:30 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:30 INFO ColumnChunkPageWriteStore: written 1,308,678B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,599B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:30 INFO ColumnChunkPageWriteStore: written 416,306B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,165B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:30 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:30 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000156
15/08/19 18:17:30 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000156_0: Committed
15/08/19 18:17:30 INFO Executor: Finished task 156.0 in stage 1.0 (TID 241). 843 bytes result sent to driver
15/08/19 18:17:30 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 257, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:30 INFO Executor: Running task 172.0 in stage 1.0 (TID 257)
15/08/19 18:17:30 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 241) in 12303 ms on localhost (157/200)
15/08/19 18:17:30 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 1,308,360B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,281B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 416,930B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,789B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:34 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:17:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 1,308,464B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,385B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000157
15/08/19 18:17:34 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000157_0: Committed
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 416,921B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,780B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:34 INFO Executor: Finished task 157.0 in stage 1.0 (TID 242). 843 bytes result sent to driver
15/08/19 18:17:34 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 258, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:34 INFO Executor: Running task 173.0 in stage 1.0 (TID 258)
15/08/19 18:17:34 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 242) in 16167 ms on localhost (158/200)
15/08/19 18:17:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000158
15/08/19 18:17:34 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000158_0: Committed
15/08/19 18:17:34 INFO Executor: Finished task 158.0 in stage 1.0 (TID 243). 843 bytes result sent to driver
15/08/19 18:17:34 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 259, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:34 INFO Executor: Running task 174.0 in stage 1.0 (TID 259)
15/08/19 18:17:34 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 243) in 12418 ms on localhost (159/200)
15/08/19 18:17:34 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:34 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 1,308,524B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,445B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 416,651B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,510B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000160
15/08/19 18:17:34 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000160_0: Committed
15/08/19 18:17:34 INFO Executor: Finished task 160.0 in stage 1.0 (TID 245). 843 bytes result sent to driver
15/08/19 18:17:34 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 260, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:34 INFO Executor: Running task 175.0 in stage 1.0 (TID 260)
15/08/19 18:17:34 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 245) in 12340 ms on localhost (160/200)
15/08/19 18:17:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:34 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 1,308,520B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,441B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 416,336B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,195B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,437
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 1,308,476B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,397B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:34 INFO ColumnChunkPageWriteStore: written 416,246B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,105B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 305 entries, 2,440B raw, 305B comp}
15/08/19 18:17:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:35 INFO ColumnChunkPageWriteStore: written 1,308,367B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,288B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:35 INFO ColumnChunkPageWriteStore: written 417,216B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,075B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000159
15/08/19 18:17:35 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000159_0: Committed
15/08/19 18:17:35 INFO Executor: Finished task 159.0 in stage 1.0 (TID 244). 843 bytes result sent to driver
15/08/19 18:17:35 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 261, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:35 INFO Executor: Running task 176.0 in stage 1.0 (TID 261)
15/08/19 18:17:35 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 244) in 12842 ms on localhost (161/200)
15/08/19 18:17:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:35 INFO ColumnChunkPageWriteStore: written 1,308,727B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,648B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:35 INFO ColumnChunkPageWriteStore: written 416,286B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,145B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000161
15/08/19 18:17:35 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000161_0: Committed
15/08/19 18:17:35 INFO Executor: Finished task 161.0 in stage 1.0 (TID 246). 843 bytes result sent to driver
15/08/19 18:17:35 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 262, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:35 INFO Executor: Running task 177.0 in stage 1.0 (TID 262)
15/08/19 18:17:35 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 246) in 12710 ms on localhost (162/200)
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000163
15/08/19 18:17:35 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000163_0: Committed
15/08/19 18:17:35 INFO Executor: Finished task 163.0 in stage 1.0 (TID 248). 843 bytes result sent to driver
15/08/19 18:17:35 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 263, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:35 INFO Executor: Running task 178.0 in stage 1.0 (TID 263)
15/08/19 18:17:35 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 248) in 12543 ms on localhost (163/200)
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000162
15/08/19 18:17:35 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000162_0: Committed
15/08/19 18:17:35 INFO Executor: Finished task 162.0 in stage 1.0 (TID 247). 843 bytes result sent to driver
15/08/19 18:17:35 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 264, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:35 INFO Executor: Running task 179.0 in stage 1.0 (TID 264)
15/08/19 18:17:35 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 247) in 12599 ms on localhost (164/200)
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/19 18:17:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:17:35 INFO ColumnChunkPageWriteStore: written 1,308,626B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,547B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:35 INFO ColumnChunkPageWriteStore: written 418,549B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 418,408B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:17:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000164
15/08/19 18:17:35 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000164_0: Committed
15/08/19 18:17:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:35 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:35 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:35 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:35 INFO Executor: Finished task 164.0 in stage 1.0 (TID 249). 843 bytes result sent to driver
15/08/19 18:17:35 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 265, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:35 INFO Executor: Running task 180.0 in stage 1.0 (TID 265)
15/08/19 18:17:35 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 249) in 12397 ms on localhost (165/200)
15/08/19 18:17:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/19 18:17:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:36 INFO ColumnChunkPageWriteStore: written 1,308,441B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,362B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:36 INFO ColumnChunkPageWriteStore: written 417,445B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,304B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:36 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000165
15/08/19 18:17:36 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000165_0: Committed
15/08/19 18:17:36 INFO Executor: Finished task 165.0 in stage 1.0 (TID 250). 843 bytes result sent to driver
15/08/19 18:17:36 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 266, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:36 INFO Executor: Running task 181.0 in stage 1.0 (TID 266)
15/08/19 18:17:36 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 250) in 9074 ms on localhost (166/200)
15/08/19 18:17:36 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:36 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:36 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:36 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:36 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:36 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:36 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:36 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:36 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:36 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:41 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:41 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:41 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:41 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:41 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:41 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:42 INFO ColumnChunkPageWriteStore: written 1,308,448B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,369B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:42 INFO ColumnChunkPageWriteStore: written 415,491B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,350B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000166
15/08/19 18:17:42 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000166_0: Committed
15/08/19 18:17:42 INFO Executor: Finished task 166.0 in stage 1.0 (TID 251). 843 bytes result sent to driver
15/08/19 18:17:42 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 267, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:42 INFO Executor: Running task 182.0 in stage 1.0 (TID 267)
15/08/19 18:17:42 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 251) in 13530 ms on localhost (167/200)
15/08/19 18:17:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:42 INFO ColumnChunkPageWriteStore: written 1,308,437B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,358B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:42 INFO ColumnChunkPageWriteStore: written 416,680B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,539B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,509
15/08/19 18:17:42 INFO ColumnChunkPageWriteStore: written 1,308,345B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,266B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:42 INFO ColumnChunkPageWriteStore: written 417,446B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,305B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/19 18:17:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000169
15/08/19 18:17:42 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000169_0: Committed
15/08/19 18:17:42 INFO Executor: Finished task 169.0 in stage 1.0 (TID 254). 843 bytes result sent to driver
15/08/19 18:17:42 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 268, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:42 INFO Executor: Running task 183.0 in stage 1.0 (TID 268)
15/08/19 18:17:42 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 254) in 13321 ms on localhost (168/200)
15/08/19 18:17:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:42 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:42 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:42 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000167
15/08/19 18:17:43 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000167_0: Committed
15/08/19 18:17:43 INFO Executor: Finished task 167.0 in stage 1.0 (TID 252). 843 bytes result sent to driver
15/08/19 18:17:43 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 269, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:43 INFO Executor: Running task 184.0 in stage 1.0 (TID 269)
15/08/19 18:17:43 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 252) in 13918 ms on localhost (169/200)
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 1,308,630B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,551B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 416,430B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,289B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:43 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:43 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:43 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:43 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:43 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:43 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 1,308,424B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,345B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 416,312B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,171B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000168
15/08/19 18:17:43 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000168_0: Committed
15/08/19 18:17:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:43 INFO Executor: Finished task 168.0 in stage 1.0 (TID 253). 843 bytes result sent to driver
15/08/19 18:17:43 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 270, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:43 INFO Executor: Running task 185.0 in stage 1.0 (TID 270)
15/08/19 18:17:43 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 253) in 14094 ms on localhost (170/200)
15/08/19 18:17:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:43 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:43 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:43 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 1,308,693B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,614B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 416,438B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,297B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:43 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:43 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:43 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,429
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 1,308,621B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,542B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 418,064B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,923B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000171
15/08/19 18:17:43 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000171_0: Committed
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 1,308,463B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,384B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:43 INFO ColumnChunkPageWriteStore: written 418,012B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,871B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 304 entries, 2,432B raw, 304B comp}
15/08/19 18:17:43 INFO Executor: Finished task 171.0 in stage 1.0 (TID 256). 843 bytes result sent to driver
15/08/19 18:17:43 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 271, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:43 INFO Executor: Running task 186.0 in stage 1.0 (TID 271)
15/08/19 18:17:43 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 256) in 13846 ms on localhost (171/200)
15/08/19 18:17:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000170
15/08/19 18:17:43 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000170_0: Committed
15/08/19 18:17:43 INFO Executor: Finished task 170.0 in stage 1.0 (TID 255). 843 bytes result sent to driver
15/08/19 18:17:43 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 272, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:43 INFO Executor: Running task 187.0 in stage 1.0 (TID 272)
15/08/19 18:17:43 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 255) in 14183 ms on localhost (172/200)
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000172
15/08/19 18:17:43 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000172_0: Committed
15/08/19 18:17:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000174
15/08/19 18:17:43 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000174_0: Committed
15/08/19 18:17:43 INFO Executor: Finished task 172.0 in stage 1.0 (TID 257). 843 bytes result sent to driver
15/08/19 18:17:43 INFO Executor: Finished task 174.0 in stage 1.0 (TID 259). 843 bytes result sent to driver
15/08/19 18:17:43 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 273, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:43 INFO Executor: Running task 188.0 in stage 1.0 (TID 273)
15/08/19 18:17:43 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 274, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:43 INFO Executor: Running task 189.0 in stage 1.0 (TID 274)
15/08/19 18:17:43 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 257) in 13721 ms on localhost (173/200)
15/08/19 18:17:43 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 259) in 9472 ms on localhost (174/200)
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:44 INFO ColumnChunkPageWriteStore: written 1,308,429B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,350B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:44 INFO ColumnChunkPageWriteStore: written 416,803B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,662B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000175
15/08/19 18:17:44 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000175_0: Committed
15/08/19 18:17:47 INFO Executor: Finished task 175.0 in stage 1.0 (TID 260). 843 bytes result sent to driver
15/08/19 18:17:47 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 275, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:47 INFO Executor: Running task 190.0 in stage 1.0 (TID 275)
15/08/19 18:17:47 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 260) in 12704 ms on localhost (175/200)
15/08/19 18:17:47 INFO ColumnChunkPageWriteStore: written 1,308,476B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,397B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:47 INFO ColumnChunkPageWriteStore: written 416,248B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,107B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000173
15/08/19 18:17:47 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000173_0: Committed
15/08/19 18:17:47 INFO Executor: Finished task 173.0 in stage 1.0 (TID 258). 843 bytes result sent to driver
15/08/19 18:17:47 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 276, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:47 INFO Executor: Running task 191.0 in stage 1.0 (TID 276)
15/08/19 18:17:47 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 258) in 13177 ms on localhost (176/200)
15/08/19 18:17:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:47 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:47 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:47 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:47 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:47 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:47 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:47 INFO ColumnChunkPageWriteStore: written 1,308,574B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,495B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:47 INFO ColumnChunkPageWriteStore: written 415,972B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,831B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000178
15/08/19 18:17:48 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000178_0: Committed
15/08/19 18:17:48 INFO Executor: Finished task 178.0 in stage 1.0 (TID 263). 843 bytes result sent to driver
15/08/19 18:17:48 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 277, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:48 INFO Executor: Running task 192.0 in stage 1.0 (TID 277)
15/08/19 18:17:48 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 263) in 12801 ms on localhost (177/200)
15/08/19 18:17:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:48 INFO ColumnChunkPageWriteStore: written 1,308,654B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,575B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:48 INFO ColumnChunkPageWriteStore: written 418,260B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 418,119B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000179
15/08/19 18:17:48 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000179_0: Committed
15/08/19 18:17:48 INFO Executor: Finished task 179.0 in stage 1.0 (TID 264). 843 bytes result sent to driver
15/08/19 18:17:48 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 278, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:48 INFO Executor: Running task 193.0 in stage 1.0 (TID 278)
15/08/19 18:17:48 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 264) in 12991 ms on localhost (178/200)
15/08/19 18:17:48 INFO ColumnChunkPageWriteStore: written 1,308,489B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,410B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:48 INFO ColumnChunkPageWriteStore: written 415,788B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,647B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:48 INFO ColumnChunkPageWriteStore: written 1,308,524B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,445B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:48 INFO ColumnChunkPageWriteStore: written 417,303B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,162B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000177
15/08/19 18:17:48 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000177_0: Committed
15/08/19 18:17:48 INFO Executor: Finished task 177.0 in stage 1.0 (TID 262). 843 bytes result sent to driver
15/08/19 18:17:48 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 279, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:48 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 262) in 13374 ms on localhost (179/200)
15/08/19 18:17:48 INFO Executor: Running task 194.0 in stage 1.0 (TID 279)
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:48 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:48 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:48 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:48 INFO ColumnChunkPageWriteStore: written 1,308,688B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,609B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:48 INFO ColumnChunkPageWriteStore: written 416,845B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,704B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000180
15/08/19 18:17:48 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000180_0: Committed
15/08/19 18:17:48 INFO Executor: Finished task 180.0 in stage 1.0 (TID 265). 843 bytes result sent to driver
15/08/19 18:17:48 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 280, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:48 INFO Executor: Running task 195.0 in stage 1.0 (TID 280)
15/08/19 18:17:48 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 265) in 13307 ms on localhost (180/200)
15/08/19 18:17:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000176
15/08/19 18:17:48 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000176_0: Committed
15/08/19 18:17:48 INFO Executor: Finished task 176.0 in stage 1.0 (TID 261). 843 bytes result sent to driver
15/08/19 18:17:48 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 281, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:48 INFO Executor: Running task 196.0 in stage 1.0 (TID 281)
15/08/19 18:17:48 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 261) in 13726 ms on localhost (181/200)
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:49 INFO ColumnChunkPageWriteStore: written 1,308,714B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,635B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:49 INFO ColumnChunkPageWriteStore: written 416,897B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,756B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000181
15/08/19 18:17:49 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000181_0: Committed
15/08/19 18:17:49 INFO Executor: Finished task 181.0 in stage 1.0 (TID 266). 843 bytes result sent to driver
15/08/19 18:17:49 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 282, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:49 INFO Executor: Running task 197.0 in stage 1.0 (TID 282)
15/08/19 18:17:49 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 266) in 12589 ms on localhost (182/200)
15/08/19 18:17:49 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:49 INFO ColumnChunkPageWriteStore: written 1,308,445B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,366B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:49 INFO ColumnChunkPageWriteStore: written 417,926B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,785B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000182
15/08/19 18:17:49 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000182_0: Committed
15/08/19 18:17:49 INFO Executor: Finished task 182.0 in stage 1.0 (TID 267). 843 bytes result sent to driver
15/08/19 18:17:49 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 283, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:49 INFO Executor: Running task 198.0 in stage 1.0 (TID 283)
15/08/19 18:17:49 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 267) in 7724 ms on localhost (183/200)
15/08/19 18:17:49 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:17:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:53 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:53 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:53 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:53 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:53 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:53 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:53 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:53 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:53 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:53 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:53 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:53 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,485
15/08/19 18:17:54 INFO ColumnChunkPageWriteStore: written 1,308,407B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,328B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:54 INFO ColumnChunkPageWriteStore: written 416,522B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,381B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/19 18:17:54 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:54 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:54 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:54 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:54 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:54 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:54 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000184
15/08/19 18:17:54 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000184_0: Committed
15/08/19 18:17:54 INFO Executor: Finished task 184.0 in stage 1.0 (TID 269). 843 bytes result sent to driver
15/08/19 18:17:54 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 284, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/19 18:17:54 INFO Executor: Running task 199.0 in stage 1.0 (TID 284)
15/08/19 18:17:54 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 269) in 11563 ms on localhost (184/200)
15/08/19 18:17:54 INFO ColumnChunkPageWriteStore: written 1,308,399B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,320B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:54 INFO ColumnChunkPageWriteStore: written 417,135B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,994B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:54 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:17:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:17:54 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000183
15/08/19 18:17:54 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000183_0: Committed
15/08/19 18:17:54 INFO Executor: Finished task 183.0 in stage 1.0 (TID 268). 843 bytes result sent to driver
15/08/19 18:17:54 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 268) in 11983 ms on localhost (185/200)
15/08/19 18:17:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,453
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 1,308,670B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,591B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 417,361B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,220B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 307 entries, 2,456B raw, 307B comp}
15/08/19 18:17:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000186
15/08/19 18:17:55 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000186_0: Committed
15/08/19 18:17:55 INFO Executor: Finished task 186.0 in stage 1.0 (TID 271). 843 bytes result sent to driver
15/08/19 18:17:55 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 271) in 11529 ms on localhost (186/200)
15/08/19 18:17:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 1,308,705B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,626B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 416,815B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,674B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000189
15/08/19 18:17:55 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000189_0: Committed
15/08/19 18:17:55 INFO Executor: Finished task 189.0 in stage 1.0 (TID 274). 843 bytes result sent to driver
15/08/19 18:17:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:55 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 274) in 11639 ms on localhost (187/200)
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 1,308,535B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,456B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 416,233B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,092B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000185
15/08/19 18:17:55 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000185_0: Committed
15/08/19 18:17:55 INFO Executor: Finished task 185.0 in stage 1.0 (TID 270). 843 bytes result sent to driver
15/08/19 18:17:55 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 270) in 11987 ms on localhost (188/200)
15/08/19 18:17:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:17:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 1,308,873B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,794B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 416,774B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,633B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 1,308,789B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,710B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 417,441B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,300B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000187
15/08/19 18:17:55 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000187_0: Committed
15/08/19 18:17:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000188
15/08/19 18:17:55 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000188_0: Committed
15/08/19 18:17:55 INFO Executor: Finished task 188.0 in stage 1.0 (TID 273). 843 bytes result sent to driver
15/08/19 18:17:55 INFO Executor: Finished task 187.0 in stage 1.0 (TID 272). 843 bytes result sent to driver
15/08/19 18:17:55 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 273) in 11884 ms on localhost (189/200)
15/08/19 18:17:55 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 272) in 11928 ms on localhost (190/200)
15/08/19 18:17:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:55 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:55 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:55 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:17:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 1,308,641B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,562B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 416,821B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,680B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:17:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000191
15/08/19 18:17:55 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000191_0: Committed
15/08/19 18:17:55 INFO Executor: Finished task 191.0 in stage 1.0 (TID 276). 843 bytes result sent to driver
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 1,308,563B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,484B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:55 INFO ColumnChunkPageWriteStore: written 416,444B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,303B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:56 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 276) in 8533 ms on localhost (191/200)
15/08/19 18:17:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000190
15/08/19 18:17:56 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000190_0: Committed
15/08/19 18:17:56 INFO Executor: Finished task 190.0 in stage 1.0 (TID 275). 843 bytes result sent to driver
15/08/19 18:17:56 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 275) in 8660 ms on localhost (192/200)
15/08/19 18:17:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:56 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:56 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:56 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:17:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:17:56 INFO ColumnChunkPageWriteStore: written 1,308,454B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,375B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:56 INFO ColumnChunkPageWriteStore: written 417,419B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,278B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:17:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000192
15/08/19 18:17:56 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000192_0: Committed
15/08/19 18:17:56 INFO Executor: Finished task 192.0 in stage 1.0 (TID 277). 843 bytes result sent to driver
15/08/19 18:17:56 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 277) in 8421 ms on localhost (193/200)
15/08/19 18:17:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,445
15/08/19 18:17:56 INFO ColumnChunkPageWriteStore: written 1,308,379B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,300B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:56 INFO ColumnChunkPageWriteStore: written 417,580B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,439B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 306 entries, 2,448B raw, 306B comp}
15/08/19 18:17:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000193
15/08/19 18:17:56 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000193_0: Committed
15/08/19 18:17:56 INFO Executor: Finished task 193.0 in stage 1.0 (TID 278). 843 bytes result sent to driver
15/08/19 18:17:56 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 278) in 8288 ms on localhost (194/200)
15/08/19 18:17:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,429
15/08/19 18:17:56 INFO ColumnChunkPageWriteStore: written 1,308,724B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,645B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:56 INFO ColumnChunkPageWriteStore: written 416,609B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,468B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 304 entries, 2,432B raw, 304B comp}
15/08/19 18:17:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000195
15/08/19 18:17:56 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000195_0: Committed
15/08/19 18:17:56 INFO Executor: Finished task 195.0 in stage 1.0 (TID 280). 843 bytes result sent to driver
15/08/19 18:17:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,461
15/08/19 18:17:56 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 280) in 8056 ms on localhost (195/200)
15/08/19 18:17:56 INFO ColumnChunkPageWriteStore: written 1,308,687B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,608B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:56 INFO ColumnChunkPageWriteStore: written 418,314B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 418,173B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,469
15/08/19 18:17:59 INFO ColumnChunkPageWriteStore: written 1,308,342B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,263B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:59 INFO ColumnChunkPageWriteStore: written 417,975B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 417,834B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/19 18:17:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000196
15/08/19 18:17:59 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000196_0: Committed
15/08/19 18:17:59 INFO ColumnChunkPageWriteStore: written 1,308,559B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,480B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:17:59 INFO Executor: Finished task 196.0 in stage 1.0 (TID 281). 843 bytes result sent to driver
15/08/19 18:17:59 INFO ColumnChunkPageWriteStore: written 416,087B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,946B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 309 entries, 2,472B raw, 309B comp}
15/08/19 18:17:59 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 281) in 10923 ms on localhost (196/200)
15/08/19 18:17:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000197
15/08/19 18:17:59 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000197_0: Committed
15/08/19 18:17:59 INFO Executor: Finished task 197.0 in stage 1.0 (TID 282). 843 bytes result sent to driver
15/08/19 18:17:59 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 282) in 10581 ms on localhost (197/200)
15/08/19 18:17:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000194
15/08/19 18:17:59 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000194_0: Committed
15/08/19 18:17:59 INFO Executor: Finished task 194.0 in stage 1.0 (TID 279). 843 bytes result sent to driver
15/08/19 18:17:59 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 279) in 11244 ms on localhost (198/200)
15/08/19 18:17:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:17:59 INFO CodecConfig: Compression: GZIP
15/08/19 18:17:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:17:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:17:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:17:59 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:17:59 INFO ParquetOutputFormat: Validation is off
15/08/19 18:17:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:17:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:18:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,477
15/08/19 18:18:00 INFO ColumnChunkPageWriteStore: written 1,308,544B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,465B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:18:00 INFO ColumnChunkPageWriteStore: written 415,630B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 415,489B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/19 18:18:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000198
15/08/19 18:18:00 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000198_0: Committed
15/08/19 18:18:00 INFO Executor: Finished task 198.0 in stage 1.0 (TID 283). 843 bytes result sent to driver
15/08/19 18:18:00 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 283) in 10328 ms on localhost (199/200)
15/08/19 18:18:00 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,647,525
15/08/19 18:18:00 INFO ColumnChunkPageWriteStore: written 1,308,554B for [l_orderkey] INT32: 375,000 values, 1,500,016B raw, 1,308,475B comp, 2 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:18:00 INFO ColumnChunkPageWriteStore: written 416,186B for [t_sum_quantity] DOUBLE: 375,000 values, 422,666B raw, 416,045B comp, 3 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/19 18:18:00 INFO FileOutputCommitter: Saved output of task 'attempt_201508191817_0001_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508191817_0001_m_000199
15/08/19 18:18:00 INFO SparkHadoopMapRedUtil: attempt_201508191817_0001_m_000199_0: Committed
15/08/19 18:18:00 INFO Executor: Finished task 199.0 in stage 1.0 (TID 284). 843 bytes result sent to driver
15/08/19 18:18:00 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 284) in 5957 ms on localhost (200/200)
15/08/19 18:18:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/19 18:18:00 INFO DAGScheduler: ResultStage 1 (processCmd at CliDriver.java:423) finished in 175.019 s
15/08/19 18:18:00 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@79ce4b74
15/08/19 18:18:00 INFO StatsReportListener: task runtime:(count: 200, mean: 13780.210000, stdev: 3478.880625, max: 24690.000000, min: 5957.000000)
15/08/19 18:18:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:00 INFO StatsReportListener: 	6.0 s	9.1 s	10.6 s	11.5 s	13.2 s	15.1 s	18.7 s	21.2 s	24.7 s
15/08/19 18:18:00 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.695000, stdev: 1.507307, max: 13.000000, min: 0.000000)
15/08/19 18:18:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:00 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	3.0 ms	13.0 ms
15/08/19 18:18:00 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/19 18:18:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:00 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/19 18:18:00 INFO DAGScheduler: Job 0 finished: processCmd at CliDriver.java:423, took 366.108175 s
15/08/19 18:18:00 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/19 18:18:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:00 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/19 18:18:00 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 98.393056, stdev: 6.074180, max: 99.760818, min: 43.046026)
15/08/19 18:18:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:00 INFO StatsReportListener: 	43 %	98 %	98 %	99 %	99 %	100 %	100 %	100 %	100 %
15/08/19 18:18:00 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.005101, stdev: 0.010789, max: 0.097943, min: 0.000000)
15/08/19 18:18:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:00 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/19 18:18:00 INFO StatsReportListener: other time pct: (count: 200, mean: 1.601843, stdev: 6.074404, max: 56.948966, min: 0.239182)
15/08/19 18:18:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:00 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 2 %	 2 %	57 %
15/08/19 18:18:02 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:18:02 INFO DefaultWriterContainer: Job job_201508191811_0000 committed.
15/08/19 18:18:02 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:18:02 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_common_metadata
15/08/19 18:18:02 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/19 18:18:02 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/19 18:18:02 INFO DAGScheduler: Final stage: ResultStage 2(processCmd at CliDriver.java:423)
15/08/19 18:18:02 INFO DAGScheduler: Parents of final stage: List()
15/08/19 18:18:02 INFO DAGScheduler: Missing parents: List()
15/08/19 18:18:02 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423), which has no missing parents
15/08/19 18:18:02 INFO MemoryStore: ensureFreeSpace(2976) called with curMem=458198, maxMem=22226833244
15/08/19 18:18:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/19 18:18:02 INFO MemoryStore: ensureFreeSpace(1784) called with curMem=461174, maxMem=22226833244
15/08/19 18:18:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1784.0 B, free 20.7 GB)
15/08/19 18:18:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:36176 (size: 1784.0 B, free: 20.7 GB)
15/08/19 18:18:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/19 18:18:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423)
15/08/19 18:18:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/19 18:18:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 285, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/19 18:18:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 285)
15/08/19 18:18:02 INFO Executor: Finished task 0.0 in stage 2.0 (TID 285). 606 bytes result sent to driver
15/08/19 18:18:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 285) in 32 ms on localhost (1/1)
15/08/19 18:18:02 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/19 18:18:02 INFO DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:423) finished in 0.033 s
15/08/19 18:18:02 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7bf26c9e
15/08/19 18:18:02 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 0.058236 s
15/08/19 18:18:02 INFO StatsReportListener: task runtime:(count: 1, mean: 32.000000, stdev: 0.000000, max: 32.000000, min: 32.000000)
15/08/19 18:18:02 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:02 INFO StatsReportListener: 	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms
15/08/19 18:18:02 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/19 18:18:02 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:02 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/19 18:18:02 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 25.000000, stdev: 0.000000, max: 25.000000, min: 25.000000)
15/08/19 18:18:02 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:02 INFO StatsReportListener: 	25 %	25 %	25 %	25 %	25 %	25 %	25 %	25 %	25 %
15/08/19 18:18:02 INFO StatsReportListener: other time pct: (count: 1, mean: 75.000000, stdev: 0.000000, max: 75.000000, min: 75.000000)
15/08/19 18:18:02 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:02 INFO StatsReportListener: 	75 %	75 %	75 %	75 %	75 %	75 %	75 %	75 %	75 %
Time taken: 373.015 seconds
15/08/19 18:18:02 INFO CliDriver: Time taken: 373.015 seconds
15/08/19 18:18:02 INFO ParseDriver: Parsing command: insert into table q18_large_volume_customer_par
select 
  c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice,sum(l_quantity)
from 
  customer_par c join orders_par o 
  on 
    c.c_custkey = o.o_custkey
  join q18_tmp_par t 
  on 
    o.o_orderkey = t.l_orderkey and t.t_sum_quantity > 315
  join lineitem_par l 
  on 
    o.o_orderkey = l.l_orderkey
group by c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice
order by o_totalprice desc,o_orderdate
limit 100
15/08/19 18:18:02 INFO ParseDriver: Parse Completed
15/08/19 18:18:02 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:18:02 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:18:02 INFO ParquetFileReader: reading another 10 footers
15/08/19 18:18:02 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:18:02 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:18:02 INFO ParquetFileReader: reading another 10 footers
15/08/19 18:18:02 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:18:03 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=462958, maxMem=22226833244
15/08/19 18:18:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/19 18:18:03 INFO MemoryStore: ensureFreeSpace(22794) called with curMem=789566, maxMem=22226833244
15/08/19 18:18:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/19 18:18:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:36176 (size: 22.3 KB, free: 20.7 GB)
15/08/19 18:18:03 INFO SparkContext: Created broadcast 4 from processCmd at CliDriver.java:423
15/08/19 18:18:03 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=812360, maxMem=22226833244
15/08/19 18:18:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/19 18:18:03 INFO MemoryStore: ensureFreeSpace(22794) called with curMem=1138968, maxMem=22226833244
15/08/19 18:18:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/19 18:18:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:36176 (size: 22.3 KB, free: 20.7 GB)
15/08/19 18:18:03 INFO SparkContext: Created broadcast 5 from processCmd at CliDriver.java:423
15/08/19 18:18:03 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1161762, maxMem=22226833244
15/08/19 18:18:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/19 18:18:03 INFO MemoryStore: ensureFreeSpace(22794) called with curMem=1488370, maxMem=22226833244
15/08/19 18:18:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/19 18:18:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:36176 (size: 22.3 KB, free: 20.7 GB)
15/08/19 18:18:03 INFO SparkContext: Created broadcast 6 from processCmd at CliDriver.java:423
15/08/19 18:18:03 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1511164, maxMem=22226833244
15/08/19 18:18:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/19 18:18:03 INFO MemoryStore: ensureFreeSpace(22794) called with curMem=1837772, maxMem=22226833244
15/08/19 18:18:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/19 18:18:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:36176 (size: 22.3 KB, free: 20.7 GB)
15/08/19 18:18:03 INFO SparkContext: Created broadcast 7 from processCmd at CliDriver.java:423
15/08/19 18:18:03 INFO Exchange: Using SparkSqlSerializer2.
15/08/19 18:18:03 INFO Exchange: Using SparkSqlSerializer2.
15/08/19 18:18:03 INFO Exchange: Using SparkSqlSerializer2.
15/08/19 18:18:03 INFO Exchange: Using SparkSqlSerializer2.
15/08/19 18:18:03 INFO Exchange: Using SparkSqlSerializer2.
15/08/19 18:18:03 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/19 18:18:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/19 18:18:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/19 18:18:03 INFO DAGScheduler: Registering RDD 25 (processCmd at CliDriver.java:423)
15/08/19 18:18:03 INFO DAGScheduler: Registering RDD 28 (processCmd at CliDriver.java:423)
15/08/19 18:18:03 INFO DAGScheduler: Registering RDD 33 (processCmd at CliDriver.java:423)
15/08/19 18:18:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/19 18:18:04 INFO DAGScheduler: Registering RDD 22 (processCmd at CliDriver.java:423)
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/19 18:18:04 INFO DAGScheduler: Registering RDD 17 (processCmd at CliDriver.java:423)
15/08/19 18:18:04 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/19 18:18:04 INFO DAGScheduler: Final stage: ResultStage 8(processCmd at CliDriver.java:423)
15/08/19 18:18:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/19 18:18:04 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/19 18:18:04 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423), which has no missing parents
15/08/19 18:18:04 INFO MemoryStore: ensureFreeSpace(6632) called with curMem=1860566, maxMem=22226833244
15/08/19 18:18:04 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.5 KB, free 20.7 GB)
15/08/19 18:18:04 INFO MemoryStore: ensureFreeSpace(3618) called with curMem=1867198, maxMem=22226833244
15/08/19 18:18:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.5 KB, free 20.7 GB)
15/08/19 18:18:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:36176 (size: 3.5 KB, free: 20.7 GB)
15/08/19 18:18:04 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874
15/08/19 18:18:04 INFO DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423)
15/08/19 18:18:04 INFO TaskSchedulerImpl: Adding task set 3.0 with 10 tasks
15/08/19 18:18:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 286, localhost, ANY, 1712 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 287, localhost, ANY, 1711 bytes)
15/08/19 18:18:04 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423), which has no missing parents
15/08/19 18:18:04 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 288, localhost, ANY, 1715 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 289, localhost, ANY, 1715 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 290, localhost, ANY, 1716 bytes)
15/08/19 18:18:04 INFO MemoryStore: ensureFreeSpace(6728) called with curMem=1870816, maxMem=22226833244
15/08/19 18:18:04 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 6.6 KB, free 20.7 GB)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 291, localhost, ANY, 1711 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 292, localhost, ANY, 1713 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 293, localhost, ANY, 1712 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 294, localhost, ANY, 1716 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 295, localhost, ANY, 1713 bytes)
15/08/19 18:18:04 INFO Executor: Running task 0.0 in stage 3.0 (TID 286)
15/08/19 18:18:04 INFO Executor: Running task 1.0 in stage 3.0 (TID 287)
15/08/19 18:18:04 INFO Executor: Running task 3.0 in stage 3.0 (TID 289)
15/08/19 18:18:04 INFO Executor: Running task 5.0 in stage 3.0 (TID 291)
15/08/19 18:18:04 INFO Executor: Running task 2.0 in stage 3.0 (TID 288)
15/08/19 18:18:04 INFO Executor: Running task 8.0 in stage 3.0 (TID 294)
15/08/19 18:18:04 INFO Executor: Running task 6.0 in stage 3.0 (TID 292)
15/08/19 18:18:04 INFO Executor: Running task 4.0 in stage 3.0 (TID 290)
15/08/19 18:18:04 INFO Executor: Running task 7.0 in stage 3.0 (TID 293)
15/08/19 18:18:04 INFO Executor: Running task 9.0 in stage 3.0 (TID 295)
15/08/19 18:18:04 INFO MemoryStore: ensureFreeSpace(3657) called with curMem=1877544, maxMem=22226833244
15/08/19 18:18:04 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.6 KB, free 20.7 GB)
15/08/19 18:18:04 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:36176 (size: 3.6 KB, free: 20.7 GB)
15/08/19 18:18:04 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000004_0 start: 0 end: 69000681 length: 69000681 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000000_0 start: 0 end: 69498913 length: 69498913 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000008_0 start: 0 end: 69004441 length: 69004441 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO DAGScheduler: Submitting 28 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423)
15/08/19 18:18:04 INFO TaskSchedulerImpl: Adding task set 4.0 with 28 tasks
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000001_0 start: 0 end: 69100626 length: 69100626 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000006_0 start: 0 end: 69018199 length: 69018199 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000003_0 start: 0 end: 69013601 length: 69013601 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000009_0 start: 0 end: 10641565 length: 10641565 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000002_0 start: 0 end: 69012184 length: 69012184 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000005_0 start: 0 end: 69007547 length: 69007547 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000007_0 start: 0 end: 69015988 length: 69015988 hosts: [] requestedSchema: message root {
  optional binary c_name (UTF8);
  optional int32 c_custkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 296, localhost, ANY, 1742 bytes)
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 297, localhost, ANY, 1747 bytes)
15/08/19 18:18:04 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[22] at processCmd at CliDriver.java:423), which has no missing parents
15/08/19 18:18:04 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 298, localhost, ANY, 1755 bytes)
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 299, localhost, ANY, 1741 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 300, localhost, ANY, 1746 bytes)
15/08/19 18:18:04 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 301, localhost, ANY, 1749 bytes)
15/08/19 18:18:04 INFO Executor: Running task 1.0 in stage 4.0 (TID 297)
15/08/19 18:18:04 INFO Executor: Running task 4.0 in stage 4.0 (TID 300)
15/08/19 18:18:04 INFO Executor: Running task 0.0 in stage 4.0 (TID 296)
15/08/19 18:18:04 INFO Executor: Running task 3.0 in stage 4.0 (TID 299)
15/08/19 18:18:04 INFO MemoryStore: ensureFreeSpace(7320) called with curMem=1881201, maxMem=22226833244
15/08/19 18:18:04 INFO Executor: Running task 2.0 in stage 4.0 (TID 298)
15/08/19 18:18:04 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.1 KB, free 20.7 GB)
15/08/19 18:18:04 INFO Executor: Running task 5.0 in stage 4.0 (TID 301)
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 818334 records.
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 268435456 end: 306320849 length: 37885393 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 818535 records.
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 819698 records.
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 268435456 end: 309816172 length: 41380716 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 818401 records.
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 824096 records.
15/08/19 18:18:04 INFO MemoryStore: ensureFreeSpace(3874) called with curMem=1888521, maxMem=22226833244
15/08/19 18:18:04 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.8 KB, free 20.7 GB)
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126232 records.
15/08/19 18:18:04 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:36176 (size: 3.8 KB, free: 20.7 GB)
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 818681 records.
15/08/19 18:18:04 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:874
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 818661 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1095604 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3407267 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 818730 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 818632 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432254 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432101 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 126232
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432071 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1162588 records.
15/08/19 18:18:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:04 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[22] at processCmd at CliDriver.java:423)
15/08/19 18:18:04 INFO TaskSchedulerImpl: Adding task set 6.0 with 200 tasks
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 122 ms. row count = 819698
15/08/19 18:18:04 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[17] at processCmd at CliDriver.java:423), which has no missing parents
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 131 ms. row count = 818401
15/08/19 18:18:04 INFO MemoryStore: ensureFreeSpace(6952) called with curMem=1892395, maxMem=22226833244
15/08/19 18:18:04 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 6.8 KB, free 20.7 GB)
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 138 ms. row count = 824096
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 127 ms. row count = 818661
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 155 ms. row count = 818535
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 152 ms. row count = 818681
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 136 ms. row count = 818730
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 134 ms. row count = 818632
15/08/19 18:18:04 INFO MemoryStore: ensureFreeSpace(3752) called with curMem=1899347, maxMem=22226833244
15/08/19 18:18:04 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KB, free 20.7 GB)
15/08/19 18:18:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:36176 (size: 3.7 KB, free: 20.7 GB)
15/08/19 18:18:04 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:874
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 174 ms. row count = 818334
15/08/19 18:18:04 INFO DAGScheduler: Submitting 85 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[17] at processCmd at CliDriver.java:423)
15/08/19 18:18:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 85 tasks
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 168 ms. row count = 1162588
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 248 ms. row count = 1095604
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 416 ms. row count = 3432254
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 428 ms. row count = 3407267
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 409 ms. row count = 3432101
15/08/19 18:18:04 INFO InternalParquetRecordReader: block read in memory in 433 ms. row count = 3432071
15/08/19 18:18:05 INFO Executor: Finished task 1.0 in stage 3.0 (TID 287). 2125 bytes result sent to driver
15/08/19 18:18:05 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 302, localhost, ANY, 1742 bytes)
15/08/19 18:18:05 INFO Executor: Running task 6.0 in stage 4.0 (TID 302)
15/08/19 18:18:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:05 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 287) in 865 ms on localhost (1/10)
15/08/19 18:18:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3407444 records.
15/08/19 18:18:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:05 INFO InternalParquetRecordReader: block read in memory in 196 ms. row count = 3407444
15/08/19 18:18:06 INFO Executor: Finished task 2.0 in stage 3.0 (TID 288). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 303, localhost, ANY, 1747 bytes)
15/08/19 18:18:06 INFO Executor: Running task 7.0 in stage 4.0 (TID 303)
15/08/19 18:18:06 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 288) in 1869 ms on localhost (2/10)
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3407266 records.
15/08/19 18:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:06 INFO Executor: Finished task 3.0 in stage 3.0 (TID 289). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 304, localhost, ANY, 1754 bytes)
15/08/19 18:18:06 INFO Executor: Running task 8.0 in stage 4.0 (TID 304)
15/08/19 18:18:06 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 289) in 2090 ms on localhost (3/10)
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 268435456 end: 306618783 length: 38183327 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 INFO InternalParquetRecordReader: block read in memory in 166 ms. row count = 3407266
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1128127 records.
15/08/19 18:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:06 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 1128127
15/08/19 18:18:06 INFO Executor: Finished task 0.0 in stage 3.0 (TID 286). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 305, localhost, ANY, 1742 bytes)
15/08/19 18:18:06 INFO Executor: Running task 9.0 in stage 4.0 (TID 305)
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 286) in 2208 ms on localhost (4/10)
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3407629 records.
15/08/19 18:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:06 INFO Executor: Finished task 7.0 in stage 3.0 (TID 293). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 306, localhost, ANY, 1747 bytes)
15/08/19 18:18:06 INFO Executor: Running task 10.0 in stage 4.0 (TID 306)
15/08/19 18:18:06 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 293) in 2263 ms on localhost (5/10)
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO Executor: Finished task 5.0 in stage 3.0 (TID 291). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 307, localhost, ANY, 1752 bytes)
15/08/19 18:18:06 INFO Executor: Running task 11.0 in stage 4.0 (TID 307)
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3407832 records.
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 268435456 end: 308903374 length: 40467918 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 291) in 2283 ms on localhost (6/10)
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:06 INFO Executor: Finished task 9.0 in stage 3.0 (TID 295). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1187436 records.
15/08/19 18:18:06 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 308, localhost, ANY, 1743 bytes)
15/08/19 18:18:06 INFO Executor: Running task 12.0 in stage 4.0 (TID 308)
15/08/19 18:18:06 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 295) in 2309 ms on localhost (7/10)
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:06 INFO Executor: Finished task 4.0 in stage 3.0 (TID 290). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 309, localhost, ANY, 1748 bytes)
15/08/19 18:18:06 INFO Executor: Running task 13.0 in stage 4.0 (TID 309)
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3407768 records.
15/08/19 18:18:06 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 290) in 2339 ms on localhost (8/10)
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO Executor: Finished task 6.0 in stage 3.0 (TID 292). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 310, localhost, ANY, 1756 bytes)
15/08/19 18:18:06 INFO Executor: Running task 14.0 in stage 4.0 (TID 310)
15/08/19 18:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:06 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 292) in 2371 ms on localhost (9/10)
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 268435456 end: 306311828 length: 37876372 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432162 records.
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1095426 records.
15/08/19 18:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:06 INFO InternalParquetRecordReader: block read in memory in 141 ms. row count = 1187436
15/08/19 18:18:06 INFO InternalParquetRecordReader: block read in memory in 278 ms. row count = 3407629
15/08/19 18:18:06 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 1095426
15/08/19 18:18:06 INFO Executor: Finished task 8.0 in stage 3.0 (TID 294). 2125 bytes result sent to driver
15/08/19 18:18:06 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 311, localhost, ANY, 1742 bytes)
15/08/19 18:18:06 INFO Executor: Running task 15.0 in stage 4.0 (TID 311)
15/08/19 18:18:06 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 294) in 2573 ms on localhost (10/10)
15/08/19 18:18:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/19 18:18:06 INFO DAGScheduler: ShuffleMapStage 3 (processCmd at CliDriver.java:423) finished in 2.583 s
15/08/19 18:18:06 INFO DAGScheduler: looking for newly runnable stages
15/08/19 18:18:06 INFO DAGScheduler: running: Set(ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 4)
15/08/19 18:18:06 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/19 18:18:06 INFO DAGScheduler: failed: Set()
15/08/19 18:18:06 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7827f6dd
15/08/19 18:18:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:06 INFO StatsReportListener: task runtime:(count: 10, mean: 2117.000000, stdev: 452.270936, max: 2573.000000, min: 865.000000)
15/08/19 18:18:06 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:06 INFO StatsReportListener: 	865.0 ms	865.0 ms	1.9 s	2.1 s	2.3 s	2.3 s	2.6 s	2.6 s	2.6 s
15/08/19 18:18:06 INFO StatsReportListener: shuffle bytes written:(count: 10, mean: 8474379.000000, stdev: 2343210.247303, max: 9266936.000000, min: 1444786.000000)
15/08/19 18:18:06 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:06 INFO StatsReportListener: 	1410.9 KB	1410.9 KB	8.8 MB	8.8 MB	8.8 MB	8.8 MB	8.8 MB	8.8 MB	8.8 MB
15/08/19 18:18:06 INFO StatsReportListener: task result size:(count: 10, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/19 18:18:06 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:06 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/19 18:18:06 INFO StatsReportListener: executor (non-fetch) time pct: (count: 10, mean: 97.875925, stdev: 1.433906, max: 98.834046, min: 93.757225)
15/08/19 18:18:06 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:06 INFO StatsReportListener: 	94 %	94 %	98 %	98 %	98 %	99 %	99 %	99 %	99 %
15/08/19 18:18:06 INFO StatsReportListener: other time pct: (count: 10, mean: 2.124075, stdev: 1.433906, max: 6.242775, min: 1.165954)
15/08/19 18:18:06 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:06 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 2 %	 2 %	 6 %	 6 %	 6 %
15/08/19 18:18:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:06 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List(ShuffleMapStage 4)
15/08/19 18:18:06 INFO InternalParquetRecordReader: block read in memory in 311 ms. row count = 3407832
15/08/19 18:18:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3407020 records.
15/08/19 18:18:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:07 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/19 18:18:07 INFO InternalParquetRecordReader: block read in memory in 329 ms. row count = 3407768
15/08/19 18:18:07 INFO InternalParquetRecordReader: block read in memory in 279 ms. row count = 3432162
15/08/19 18:18:07 INFO InternalParquetRecordReader: block read in memory in 194 ms. row count = 3407020
15/08/19 18:18:08 INFO Executor: Finished task 2.0 in stage 4.0 (TID 298). 2125 bytes result sent to driver
15/08/19 18:18:08 INFO Executor: Finished task 5.0 in stage 4.0 (TID 301). 2125 bytes result sent to driver
15/08/19 18:18:08 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 312, localhost, ANY, 1748 bytes)
15/08/19 18:18:08 INFO Executor: Running task 16.0 in stage 4.0 (TID 312)
15/08/19 18:18:08 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 313, localhost, ANY, 1753 bytes)
15/08/19 18:18:08 INFO Executor: Running task 17.0 in stage 4.0 (TID 313)
15/08/19 18:18:08 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 298) in 3950 ms on localhost (1/28)
15/08/19 18:18:08 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 301) in 3948 ms on localhost (2/28)
15/08/19 18:18:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 268435456 end: 306320197 length: 37884741 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432048 records.
15/08/19 18:18:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1095672 records.
15/08/19 18:18:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:08 INFO InternalParquetRecordReader: block read in memory in 88 ms. row count = 1095672
15/08/19 18:18:08 INFO InternalParquetRecordReader: block read in memory in 219 ms. row count = 3432048
15/08/19 18:18:10 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:36176 in memory (size: 1784.0 B, free: 20.7 GB)
15/08/19 18:18:10 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:36176 in memory (size: 3.5 KB, free: 20.7 GB)
15/08/19 18:18:11 INFO Executor: Finished task 8.0 in stage 4.0 (TID 304). 2125 bytes result sent to driver
15/08/19 18:18:11 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 314, localhost, ANY, 1743 bytes)
15/08/19 18:18:11 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 304) in 5353 ms on localhost (3/28)
15/08/19 18:18:11 INFO Executor: Running task 18.0 in stage 4.0 (TID 314)
15/08/19 18:18:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432325 records.
15/08/19 18:18:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:11 INFO InternalParquetRecordReader: block read in memory in 171 ms. row count = 3432325
15/08/19 18:18:12 INFO Executor: Finished task 14.0 in stage 4.0 (TID 310). 2125 bytes result sent to driver
15/08/19 18:18:12 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 315, localhost, ANY, 1748 bytes)
15/08/19 18:18:12 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 310) in 5321 ms on localhost (4/28)
15/08/19 18:18:12 INFO Executor: Running task 19.0 in stage 4.0 (TID 315)
15/08/19 18:18:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3407468 records.
15/08/19 18:18:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:12 INFO Executor: Finished task 11.0 in stage 4.0 (TID 307). 2125 bytes result sent to driver
15/08/19 18:18:12 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 316, localhost, ANY, 1753 bytes)
15/08/19 18:18:12 INFO Executor: Running task 20.0 in stage 4.0 (TID 316)
15/08/19 18:18:12 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 307) in 5588 ms on localhost (5/28)
15/08/19 18:18:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 268435456 end: 306327339 length: 37891883 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:12 INFO InternalParquetRecordReader: block read in memory in 155 ms. row count = 3407468
15/08/19 18:18:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1095618 records.
15/08/19 18:18:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:12 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 1095618
15/08/19 18:18:13 INFO Executor: Finished task 17.0 in stage 4.0 (TID 313). 2125 bytes result sent to driver
15/08/19 18:18:13 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 317, localhost, ANY, 1749 bytes)
15/08/19 18:18:13 INFO Executor: Running task 21.0 in stage 4.0 (TID 317)
15/08/19 18:18:13 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 313) in 4862 ms on localhost (6/28)
15/08/19 18:18:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 0 end: 129160199 length: 129160199 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3347942 records.
15/08/19 18:18:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:13 INFO InternalParquetRecordReader: block read in memory in 169 ms. row count = 3347942
15/08/19 18:18:14 INFO Executor: Finished task 20.0 in stage 4.0 (TID 316). 2125 bytes result sent to driver
15/08/19 18:18:14 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 318, localhost, ANY, 1742 bytes)
15/08/19 18:18:14 INFO Executor: Running task 22.0 in stage 4.0 (TID 318)
15/08/19 18:18:14 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 316) in 2422 ms on localhost (7/28)
15/08/19 18:18:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432218 records.
15/08/19 18:18:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:14 INFO InternalParquetRecordReader: block read in memory in 171 ms. row count = 3432218
15/08/19 18:18:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:36176 in memory (size: 29.3 KB, free: 20.7 GB)
15/08/19 18:18:18 INFO ContextCleaner: Cleaned shuffle 0
15/08/19 18:18:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:36176 in memory (size: 22.3 KB, free: 20.7 GB)
15/08/19 18:18:18 INFO Executor: Finished task 0.0 in stage 4.0 (TID 296). 2125 bytes result sent to driver
15/08/19 18:18:18 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 319, localhost, ANY, 1747 bytes)
15/08/19 18:18:18 INFO Executor: Running task 23.0 in stage 4.0 (TID 319)
15/08/19 18:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:18 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 296) in 14028 ms on localhost (8/28)
15/08/19 18:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432788 records.
15/08/19 18:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:18 INFO InternalParquetRecordReader: block read in memory in 237 ms. row count = 3432788
15/08/19 18:18:18 INFO Executor: Finished task 3.0 in stage 4.0 (TID 299). 2125 bytes result sent to driver
15/08/19 18:18:18 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 320, localhost, ANY, 1751 bytes)
15/08/19 18:18:18 INFO Executor: Running task 24.0 in stage 4.0 (TID 320)
15/08/19 18:18:18 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 299) in 14316 ms on localhost (9/28)
15/08/19 18:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 268435456 end: 308902511 length: 40467055 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1138465 records.
15/08/19 18:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:18 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 1138465
15/08/19 18:18:18 INFO Executor: Finished task 1.0 in stage 4.0 (TID 297). 2125 bytes result sent to driver
15/08/19 18:18:18 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 321, localhost, ANY, 1742 bytes)
15/08/19 18:18:18 INFO Executor: Running task 25.0 in stage 4.0 (TID 321)
15/08/19 18:18:18 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 297) in 14481 ms on localhost (10/28)
15/08/19 18:18:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432465 records.
15/08/19 18:18:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:19 INFO Executor: Finished task 6.0 in stage 4.0 (TID 302). 2125 bytes result sent to driver
15/08/19 18:18:19 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 322, localhost, ANY, 1748 bytes)
15/08/19 18:18:19 INFO Executor: Running task 26.0 in stage 4.0 (TID 322)
15/08/19 18:18:19 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 302) in 13866 ms on localhost (11/28)
15/08/19 18:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3432120 records.
15/08/19 18:18:19 INFO Executor: Finished task 4.0 in stage 4.0 (TID 300). 2125 bytes result sent to driver
15/08/19 18:18:19 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 323, localhost, ANY, 1756 bytes)
15/08/19 18:18:19 INFO Executor: Running task 27.0 in stage 4.0 (TID 323)
15/08/19 18:18:19 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 300) in 14693 ms on localhost (12/28)
15/08/19 18:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 268435456 end: 306305663 length: 37870207 hosts: [] requestedSchema: message root {
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional int32 o_custkey;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:19 INFO InternalParquetRecordReader: block read in memory in 199 ms. row count = 3432465
15/08/19 18:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1070876 records.
15/08/19 18:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:19 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 1070876
15/08/19 18:18:19 INFO InternalParquetRecordReader: block read in memory in 195 ms. row count = 3432120
15/08/19 18:18:19 INFO Executor: Finished task 7.0 in stage 4.0 (TID 303). 2125 bytes result sent to driver
15/08/19 18:18:19 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 324, localhost, ANY, 1694 bytes)
15/08/19 18:18:19 INFO Executor: Running task 0.0 in stage 6.0 (TID 324)
15/08/19 18:18:19 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 303) in 13439 ms on localhost (13/28)
15/08/19 18:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00111-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727582 length: 1727582 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:19 INFO Executor: Finished task 12.0 in stage 4.0 (TID 308). 2125 bytes result sent to driver
15/08/19 18:18:19 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 325, localhost, ANY, 1696 bytes)
15/08/19 18:18:19 INFO Executor: Running task 1.0 in stage 6.0 (TID 325)
15/08/19 18:18:19 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 308) in 13021 ms on localhost (14/28)
15/08/19 18:18:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00160-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726344 length: 1726344 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:19 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:19 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 375000
15/08/19 18:18:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:19 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:19 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 0.0 in stage 6.0 (TID 324). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 326, localhost, ANY, 1695 bytes)
15/08/19 18:18:20 INFO Executor: Running task 2.0 in stage 6.0 (TID 326)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 324) in 457 ms on localhost (1/200)
15/08/19 18:18:20 INFO Executor: Finished task 1.0 in stage 6.0 (TID 325). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00031-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727729 length: 1727729 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 327, localhost, ANY, 1697 bytes)
15/08/19 18:18:20 INFO Executor: Running task 3.0 in stage 6.0 (TID 327)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 325) in 436 ms on localhost (2/200)
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00115-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726152 length: 1726152 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 9.0 in stage 4.0 (TID 305). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 328, localhost, ANY, 1695 bytes)
15/08/19 18:18:20 INFO Executor: Running task 4.0 in stage 6.0 (TID 328)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00120-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727574 length: 1727574 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 305) in 13665 ms on localhost (15/28)
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 3.0 in stage 6.0 (TID 327). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 329, localhost, ANY, 1696 bytes)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 327) in 178 ms on localhost (3/200)
15/08/19 18:18:20 INFO Executor: Running task 5.0 in stage 6.0 (TID 329)
15/08/19 18:18:20 INFO Executor: Finished task 13.0 in stage 4.0 (TID 309). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 330, localhost, ANY, 1696 bytes)
15/08/19 18:18:20 INFO Executor: Running task 6.0 in stage 6.0 (TID 330)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00075-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726238 length: 1726238 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 309) in 13609 ms on localhost (16/28)
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00074-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725338 length: 1725338 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 10.0 in stage 4.0 (TID 306). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 331, localhost, ANY, 1697 bytes)
15/08/19 18:18:20 INFO Executor: Running task 7.0 in stage 6.0 (TID 331)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 306) in 13776 ms on localhost (17/28)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00199-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725930 length: 1725930 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO Executor: Finished task 2.0 in stage 6.0 (TID 326). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 332, localhost, ANY, 1695 bytes)
15/08/19 18:18:20 INFO Executor: Running task 8.0 in stage 6.0 (TID 332)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 326) in 299 ms on localhost (4/200)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00145-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725804 length: 1725804 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 4.0 in stage 6.0 (TID 328). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 333, localhost, ANY, 1696 bytes)
15/08/19 18:18:20 INFO Executor: Running task 9.0 in stage 6.0 (TID 333)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 328) in 306 ms on localhost (5/200)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00026-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726028 length: 1726028 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 5.0 in stage 6.0 (TID 329). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 334, localhost, ANY, 1694 bytes)
15/08/19 18:18:20 INFO Executor: Running task 10.0 in stage 6.0 (TID 334)
15/08/19 18:18:20 INFO Executor: Finished task 6.0 in stage 6.0 (TID 330). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 335, localhost, ANY, 1696 bytes)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 329) in 312 ms on localhost (6/200)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 330) in 297 ms on localhost (7/200)
15/08/19 18:18:20 INFO Executor: Running task 11.0 in stage 6.0 (TID 335)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00164-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1728349 length: 1728349 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00180-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726708 length: 1726708 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 8.0 in stage 6.0 (TID 332). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 336, localhost, ANY, 1694 bytes)
15/08/19 18:18:20 INFO Executor: Running task 12.0 in stage 6.0 (TID 336)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 332) in 243 ms on localhost (8/200)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00155-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726520 length: 1726520 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO Executor: Finished task 7.0 in stage 6.0 (TID 331). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 337, localhost, ANY, 1694 bytes)
15/08/19 18:18:20 INFO Executor: Running task 13.0 in stage 6.0 (TID 337)
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 331) in 283 ms on localhost (9/200)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00058-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725277 length: 1725277 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 15.0 in stage 4.0 (TID 311). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 338, localhost, ANY, 1693 bytes)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 311) in 13782 ms on localhost (18/28)
15/08/19 18:18:20 INFO Executor: Running task 14.0 in stage 6.0 (TID 338)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00034-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1728046 length: 1728046 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO Executor: Finished task 9.0 in stage 6.0 (TID 333). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 339, localhost, ANY, 1697 bytes)
15/08/19 18:18:20 INFO Executor: Running task 15.0 in stage 6.0 (TID 339)
15/08/19 18:18:20 INFO Executor: Finished task 10.0 in stage 6.0 (TID 334). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 340, localhost, ANY, 1695 bytes)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 333) in 231 ms on localhost (10/200)
15/08/19 18:18:20 INFO Executor: Running task 16.0 in stage 6.0 (TID 340)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00156-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726153 length: 1726153 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 375000
15/08/19 18:18:20 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 334) in 166 ms on localhost (11/200)
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00057-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727741 length: 1727741 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:20 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 11.0 in stage 6.0 (TID 335). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 341, localhost, ANY, 1696 bytes)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 335) in 224 ms on localhost (12/200)
15/08/19 18:18:20 INFO Executor: Running task 17.0 in stage 6.0 (TID 341)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00095-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725251 length: 1725251 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 12.0 in stage 6.0 (TID 336). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 342, localhost, ANY, 1695 bytes)
15/08/19 18:18:20 INFO Executor: Running task 18.0 in stage 6.0 (TID 342)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 336) in 217 ms on localhost (13/200)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00125-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726872 length: 1726872 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 13.0 in stage 6.0 (TID 337). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 343, localhost, ANY, 1693 bytes)
15/08/19 18:18:20 INFO Executor: Running task 19.0 in stage 6.0 (TID 343)
15/08/19 18:18:20 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 337) in 236 ms on localhost (14/200)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00003-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726332 length: 1726332 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 14.0 in stage 6.0 (TID 338). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 344, localhost, ANY, 1697 bytes)
15/08/19 18:18:20 INFO Executor: Running task 20.0 in stage 6.0 (TID 344)
15/08/19 18:18:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00036-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726149 length: 1726149 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:20 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 338) in 247 ms on localhost (15/200)
15/08/19 18:18:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:20 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 375000
15/08/19 18:18:20 INFO Executor: Finished task 15.0 in stage 6.0 (TID 339). 2125 bytes result sent to driver
15/08/19 18:18:20 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 345, localhost, ANY, 1695 bytes)
15/08/19 18:18:20 INFO Executor: Running task 21.0 in stage 6.0 (TID 345)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 339) in 273 ms on localhost (16/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00048-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725794 length: 1725794 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO Executor: Finished task 16.0 in stage 6.0 (TID 340). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 346, localhost, ANY, 1696 bytes)
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO Executor: Running task 22.0 in stage 6.0 (TID 346)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 340) in 291 ms on localhost (17/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00101-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726431 length: 1726431 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 17.0 in stage 6.0 (TID 341). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 347, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 341) in 274 ms on localhost (18/200)
15/08/19 18:18:21 INFO Executor: Running task 23.0 in stage 6.0 (TID 347)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00110-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727193 length: 1727193 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO Executor: Finished task 18.0 in stage 6.0 (TID 342). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO TaskSetManager: Starting task 24.0 in stage 6.0 (TID 348, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO Executor: Running task 24.0 in stage 6.0 (TID 348)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 342) in 250 ms on localhost (19/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00174-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727628 length: 1727628 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 19.0 in stage 6.0 (TID 343). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 25.0 in stage 6.0 (TID 349, localhost, ANY, 1696 bytes)
15/08/19 18:18:21 INFO Executor: Running task 25.0 in stage 6.0 (TID 349)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 343) in 254 ms on localhost (20/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00114-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726362 length: 1726362 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO Executor: Finished task 22.0 in stage 6.0 (TID 346). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 26.0 in stage 6.0 (TID 350, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO Executor: Running task 26.0 in stage 6.0 (TID 350)
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 346) in 153 ms on localhost (21/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00140-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727135 length: 1727135 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO Executor: Finished task 20.0 in stage 6.0 (TID 344). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 375000
15/08/19 18:18:21 INFO TaskSetManager: Starting task 27.0 in stage 6.0 (TID 351, localhost, ANY, 1694 bytes)
15/08/19 18:18:21 INFO Executor: Running task 27.0 in stage 6.0 (TID 351)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 344) in 245 ms on localhost (22/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00023-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725482 length: 1725482 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 23.0 in stage 6.0 (TID 347). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 28.0 in stage 6.0 (TID 352, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO Executor: Running task 28.0 in stage 6.0 (TID 352)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00142-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727260 length: 1727260 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 347) in 152 ms on localhost (23/200)
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO Executor: Finished task 21.0 in stage 6.0 (TID 345). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO TaskSetManager: Starting task 29.0 in stage 6.0 (TID 353, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO Executor: Running task 29.0 in stage 6.0 (TID 353)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 345) in 257 ms on localhost (24/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00122-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725521 length: 1725521 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 24.0 in stage 6.0 (TID 348). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 30.0 in stage 6.0 (TID 354, localhost, ANY, 1696 bytes)
15/08/19 18:18:21 INFO Executor: Running task 30.0 in stage 6.0 (TID 354)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 24.0 in stage 6.0 (TID 348) in 246 ms on localhost (25/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00098-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726090 length: 1726090 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 25.0 in stage 6.0 (TID 349). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 31.0 in stage 6.0 (TID 355, localhost, ANY, 1696 bytes)
15/08/19 18:18:21 INFO Executor: Running task 31.0 in stage 6.0 (TID 355)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 25.0 in stage 6.0 (TID 349) in 292 ms on localhost (26/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00139-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727678 length: 1727678 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 26.0 in stage 6.0 (TID 350). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 32.0 in stage 6.0 (TID 356, localhost, ANY, 1697 bytes)
15/08/19 18:18:21 INFO Executor: Running task 32.0 in stage 6.0 (TID 356)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 26.0 in stage 6.0 (TID 350) in 346 ms on localhost (27/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00175-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726413 length: 1726413 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO Executor: Finished task 27.0 in stage 6.0 (TID 351). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO TaskSetManager: Starting task 33.0 in stage 6.0 (TID 357, localhost, ANY, 1693 bytes)
15/08/19 18:18:21 INFO Executor: Running task 33.0 in stage 6.0 (TID 357)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 27.0 in stage 6.0 (TID 351) in 348 ms on localhost (28/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00030-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727841 length: 1727841 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 29.0 in stage 6.0 (TID 353). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO Executor: Finished task 28.0 in stage 6.0 (TID 352). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 34.0 in stage 6.0 (TID 358, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO Executor: Running task 34.0 in stage 6.0 (TID 358)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00152-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726966 length: 1726966 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 INFO TaskSetManager: Starting task 35.0 in stage 6.0 (TID 359, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 29.0 in stage 6.0 (TID 353) in 364 ms on localhost (29/200)
15/08/19 18:18:21 INFO Executor: Running task 35.0 in stage 6.0 (TID 359)
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO TaskSetManager: Finished task 28.0 in stage 6.0 (TID 352) in 391 ms on localhost (30/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00194-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725820 length: 1725820 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 30.0 in stage 6.0 (TID 354). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 36.0 in stage 6.0 (TID 360, localhost, ANY, 1694 bytes)
15/08/19 18:18:21 INFO Executor: Running task 36.0 in stage 6.0 (TID 360)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 30.0 in stage 6.0 (TID 354) in 316 ms on localhost (31/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00011-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727237 length: 1727237 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 31.0 in stage 6.0 (TID 355). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 37.0 in stage 6.0 (TID 361, localhost, ANY, 1694 bytes)
15/08/19 18:18:21 INFO Executor: Running task 37.0 in stage 6.0 (TID 361)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 31.0 in stage 6.0 (TID 355) in 288 ms on localhost (32/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00007-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726725 length: 1726725 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO Executor: Finished task 34.0 in stage 6.0 (TID 358). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO Executor: Finished task 16.0 in stage 4.0 (TID 312). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO TaskSetManager: Starting task 38.0 in stage 6.0 (TID 362, localhost, ANY, 1697 bytes)
15/08/19 18:18:21 INFO TaskSetManager: Starting task 39.0 in stage 6.0 (TID 363, localhost, ANY, 1694 bytes)
15/08/19 18:18:21 INFO Executor: Running task 39.0 in stage 6.0 (TID 363)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 34.0 in stage 6.0 (TID 358) in 148 ms on localhost (33/200)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 312) in 13420 ms on localhost (19/28)
15/08/19 18:18:21 INFO Executor: Running task 38.0 in stage 6.0 (TID 362)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00004-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726054 length: 1726054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00163-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726186 length: 1726186 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO Executor: Finished task 35.0 in stage 6.0 (TID 359). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 40.0 in stage 6.0 (TID 364, localhost, ANY, 1696 bytes)
15/08/19 18:18:21 INFO Executor: Running task 40.0 in stage 6.0 (TID 364)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 35.0 in stage 6.0 (TID 359) in 158 ms on localhost (34/200)
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00078-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726528 length: 1726528 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 33.0 in stage 6.0 (TID 357). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 41.0 in stage 6.0 (TID 365, localhost, ANY, 1693 bytes)
15/08/19 18:18:21 INFO Executor: Running task 41.0 in stage 6.0 (TID 365)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 33.0 in stage 6.0 (TID 357) in 273 ms on localhost (35/200)
15/08/19 18:18:21 INFO Executor: Finished task 32.0 in stage 6.0 (TID 356). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 42.0 in stage 6.0 (TID 366, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO Executor: Running task 42.0 in stage 6.0 (TID 366)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 32.0 in stage 6.0 (TID 356) in 292 ms on localhost (36/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00076-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726499 length: 1726499 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00018-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727840 length: 1727840 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:21 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:21 INFO Executor: Finished task 38.0 in stage 6.0 (TID 362). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 43.0 in stage 6.0 (TID 367, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO Executor: Running task 43.0 in stage 6.0 (TID 367)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 38.0 in stage 6.0 (TID 362) in 151 ms on localhost (37/200)
15/08/19 18:18:21 INFO Executor: Finished task 24.0 in stage 4.0 (TID 320). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00046-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727979 length: 1727979 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO TaskSetManager: Starting task 44.0 in stage 6.0 (TID 368, localhost, ANY, 1695 bytes)
15/08/19 18:18:21 INFO Executor: Running task 44.0 in stage 6.0 (TID 368)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 320) in 3215 ms on localhost (20/28)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00109-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725465 length: 1725465 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO Executor: Finished task 36.0 in stage 6.0 (TID 360). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:21 INFO TaskSetManager: Starting task 45.0 in stage 6.0 (TID 369, localhost, ANY, 1696 bytes)
15/08/19 18:18:21 INFO Executor: Running task 45.0 in stage 6.0 (TID 369)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 36.0 in stage 6.0 (TID 360) in 307 ms on localhost (38/200)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00166-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725112 length: 1725112 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:21 INFO Executor: Finished task 27.0 in stage 4.0 (TID 323). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO Executor: Finished task 37.0 in stage 6.0 (TID 361). 2125 bytes result sent to driver
15/08/19 18:18:21 INFO TaskSetManager: Starting task 46.0 in stage 6.0 (TID 370, localhost, ANY, 1697 bytes)
15/08/19 18:18:21 INFO Executor: Running task 46.0 in stage 6.0 (TID 370)
15/08/19 18:18:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:21 INFO TaskSetManager: Starting task 47.0 in stage 6.0 (TID 371, localhost, ANY, 1694 bytes)
15/08/19 18:18:21 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 323) in 2903 ms on localhost (21/28)
15/08/19 18:18:21 INFO Executor: Running task 47.0 in stage 6.0 (TID 371)
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00117-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725927 length: 1725927 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 INFO TaskSetManager: Finished task 37.0 in stage 6.0 (TID 361) in 291 ms on localhost (39/200)
15/08/19 18:18:21 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 375000
15/08/19 18:18:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00177-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727002 length: 1727002 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:22 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:22 INFO Executor: Finished task 39.0 in stage 6.0 (TID 363). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 48.0 in stage 6.0 (TID 372, localhost, ANY, 1695 bytes)
15/08/19 18:18:22 INFO Executor: Running task 48.0 in stage 6.0 (TID 372)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 39.0 in stage 6.0 (TID 363) in 297 ms on localhost (40/200)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00143-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726258 length: 1726258 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO Executor: Finished task 40.0 in stage 6.0 (TID 364). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 49.0 in stage 6.0 (TID 373, localhost, ANY, 1695 bytes)
15/08/19 18:18:22 INFO Executor: Running task 49.0 in stage 6.0 (TID 373)
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO TaskSetManager: Finished task 40.0 in stage 6.0 (TID 364) in 303 ms on localhost (41/200)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00029-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727444 length: 1727444 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:22 INFO Executor: Finished task 43.0 in stage 6.0 (TID 367). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 50.0 in stage 6.0 (TID 374, localhost, ANY, 1695 bytes)
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO TaskSetManager: Finished task 43.0 in stage 6.0 (TID 367) in 194 ms on localhost (42/200)
15/08/19 18:18:22 INFO Executor: Running task 50.0 in stage 6.0 (TID 374)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00077-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726543 length: 1726543 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:22 INFO Executor: Finished task 42.0 in stage 6.0 (TID 366). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 51.0 in stage 6.0 (TID 375, localhost, ANY, 1695 bytes)
15/08/19 18:18:22 INFO Executor: Finished task 41.0 in stage 6.0 (TID 365). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO Executor: Running task 51.0 in stage 6.0 (TID 375)
15/08/19 18:18:22 INFO TaskSetManager: Starting task 52.0 in stage 6.0 (TID 376, localhost, ANY, 1694 bytes)
15/08/19 18:18:22 INFO Executor: Running task 52.0 in stage 6.0 (TID 376)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 42.0 in stage 6.0 (TID 366) in 304 ms on localhost (43/200)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00133-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726087 length: 1726087 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00065-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726508 length: 1726508 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 INFO TaskSetManager: Finished task 41.0 in stage 6.0 (TID 365) in 310 ms on localhost (44/200)
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO Executor: Finished task 47.0 in stage 6.0 (TID 371). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 53.0 in stage 6.0 (TID 377, localhost, ANY, 1696 bytes)
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 375000
15/08/19 18:18:22 INFO TaskSetManager: Finished task 47.0 in stage 6.0 (TID 371) in 173 ms on localhost (45/200)
15/08/19 18:18:22 INFO Executor: Running task 53.0 in stage 6.0 (TID 377)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00099-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726360 length: 1726360 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 375000
15/08/19 18:18:22 INFO Executor: Finished task 44.0 in stage 6.0 (TID 368). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 54.0 in stage 6.0 (TID 378, localhost, ANY, 1696 bytes)
15/08/19 18:18:22 INFO Executor: Running task 54.0 in stage 6.0 (TID 378)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 44.0 in stage 6.0 (TID 368) in 267 ms on localhost (46/200)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00019-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726483 length: 1726483 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO Executor: Finished task 45.0 in stage 6.0 (TID 369). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO TaskSetManager: Starting task 55.0 in stage 6.0 (TID 379, localhost, ANY, 1694 bytes)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 45.0 in stage 6.0 (TID 369) in 253 ms on localhost (47/200)
15/08/19 18:18:22 INFO Executor: Running task 55.0 in stage 6.0 (TID 379)
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00027-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727034 length: 1727034 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 375000
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:22 INFO Executor: Finished task 46.0 in stage 6.0 (TID 370). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 56.0 in stage 6.0 (TID 380, localhost, ANY, 1695 bytes)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 46.0 in stage 6.0 (TID 370) in 294 ms on localhost (48/200)
15/08/19 18:18:22 INFO Executor: Running task 56.0 in stage 6.0 (TID 380)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00050-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727174 length: 1727174 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:22 INFO Executor: Finished task 54.0 in stage 6.0 (TID 378). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 57.0 in stage 6.0 (TID 381, localhost, ANY, 1697 bytes)
15/08/19 18:18:22 INFO Executor: Running task 57.0 in stage 6.0 (TID 381)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 54.0 in stage 6.0 (TID 378) in 169 ms on localhost (49/200)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00159-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725893 length: 1725893 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO Executor: Finished task 55.0 in stage 6.0 (TID 379). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 58.0 in stage 6.0 (TID 382, localhost, ANY, 1696 bytes)
15/08/19 18:18:22 INFO Executor: Running task 58.0 in stage 6.0 (TID 382)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 55.0 in stage 6.0 (TID 379) in 163 ms on localhost (50/200)
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00167-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726285 length: 1726285 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 375000
15/08/19 18:18:22 INFO Executor: Finished task 48.0 in stage 6.0 (TID 372). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 59.0 in stage 6.0 (TID 383, localhost, ANY, 1696 bytes)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 48.0 in stage 6.0 (TID 372) in 365 ms on localhost (51/200)
15/08/19 18:18:22 INFO Executor: Running task 59.0 in stage 6.0 (TID 383)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00150-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725767 length: 1725767 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO Executor: Finished task 49.0 in stage 6.0 (TID 373). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO TaskSetManager: Starting task 60.0 in stage 6.0 (TID 384, localhost, ANY, 1695 bytes)
15/08/19 18:18:22 INFO Executor: Running task 60.0 in stage 6.0 (TID 384)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 49.0 in stage 6.0 (TID 373) in 363 ms on localhost (52/200)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00096-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727137 length: 1727137 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO Executor: Finished task 50.0 in stage 6.0 (TID 374). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 61.0 in stage 6.0 (TID 385, localhost, ANY, 1694 bytes)
15/08/19 18:18:22 INFO Executor: Running task 61.0 in stage 6.0 (TID 385)
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO Executor: Finished task 52.0 in stage 6.0 (TID 376). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO Executor: Finished task 51.0 in stage 6.0 (TID 375). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 62.0 in stage 6.0 (TID 386, localhost, ANY, 1695 bytes)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00158-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726559 length: 1726559 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 INFO TaskSetManager: Finished task 50.0 in stage 6.0 (TID 374) in 367 ms on localhost (53/200)
15/08/19 18:18:22 INFO Executor: Running task 62.0 in stage 6.0 (TID 386)
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00033-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727194 length: 1727194 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 INFO TaskSetManager: Starting task 63.0 in stage 6.0 (TID 387, localhost, ANY, 1694 bytes)
15/08/19 18:18:22 INFO Executor: Running task 63.0 in stage 6.0 (TID 387)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 52.0 in stage 6.0 (TID 376) in 350 ms on localhost (54/200)
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 375000
15/08/19 18:18:22 INFO TaskSetManager: Finished task 51.0 in stage 6.0 (TID 375) in 354 ms on localhost (55/200)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00024-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726812 length: 1726812 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO Executor: Finished task 57.0 in stage 6.0 (TID 381). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 64.0 in stage 6.0 (TID 388, localhost, ANY, 1694 bytes)
15/08/19 18:18:22 INFO Executor: Running task 64.0 in stage 6.0 (TID 388)
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO TaskSetManager: Finished task 57.0 in stage 6.0 (TID 381) in 142 ms on localhost (56/200)
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00053-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1728037 length: 1728037 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 375000
15/08/19 18:18:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:22 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:22 INFO Executor: Finished task 53.0 in stage 6.0 (TID 377). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO TaskSetManager: Starting task 65.0 in stage 6.0 (TID 389, localhost, ANY, 1696 bytes)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 53.0 in stage 6.0 (TID 377) in 378 ms on localhost (57/200)
15/08/19 18:18:22 INFO Executor: Running task 65.0 in stage 6.0 (TID 389)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00066-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727202 length: 1727202 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:22 INFO Executor: Finished task 56.0 in stage 6.0 (TID 380). 2125 bytes result sent to driver
15/08/19 18:18:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:22 INFO TaskSetManager: Starting task 66.0 in stage 6.0 (TID 390, localhost, ANY, 1697 bytes)
15/08/19 18:18:22 INFO Executor: Running task 66.0 in stage 6.0 (TID 390)
15/08/19 18:18:22 INFO TaskSetManager: Finished task 56.0 in stage 6.0 (TID 380) in 299 ms on localhost (58/200)
15/08/19 18:18:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00173-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725907 length: 1725907 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 58.0 in stage 6.0 (TID 382). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 67.0 in stage 6.0 (TID 391, localhost, ANY, 1694 bytes)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 58.0 in stage 6.0 (TID 382) in 727 ms on localhost (59/200)
15/08/19 18:18:23 INFO Executor: Running task 67.0 in stage 6.0 (TID 391)
15/08/19 18:18:23 INFO Executor: Finished task 63.0 in stage 6.0 (TID 387). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 68.0 in stage 6.0 (TID 392, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00039-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726769 length: 1726769 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 INFO Executor: Running task 68.0 in stage 6.0 (TID 392)
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO TaskSetManager: Finished task 63.0 in stage 6.0 (TID 387) in 639 ms on localhost (60/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00002-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725949 length: 1725949 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 59.0 in stage 6.0 (TID 383). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO Executor: Finished task 60.0 in stage 6.0 (TID 384). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 69.0 in stage 6.0 (TID 393, localhost, ANY, 1696 bytes)
15/08/19 18:18:23 INFO Executor: Running task 69.0 in stage 6.0 (TID 393)
15/08/19 18:18:23 INFO TaskSetManager: Starting task 70.0 in stage 6.0 (TID 394, localhost, ANY, 1694 bytes)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 59.0 in stage 6.0 (TID 383) in 800 ms on localhost (61/200)
15/08/19 18:18:23 INFO Executor: Running task 70.0 in stage 6.0 (TID 394)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 60.0 in stage 6.0 (TID 384) in 787 ms on localhost (62/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00017-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725436 length: 1725436 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00126-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726773 length: 1726773 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO Executor: Finished task 68.0 in stage 6.0 (TID 392). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 71.0 in stage 6.0 (TID 395, localhost, ANY, 1696 bytes)
15/08/19 18:18:23 INFO Executor: Running task 71.0 in stage 6.0 (TID 395)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00169-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726981 length: 1726981 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO TaskSetManager: Finished task 68.0 in stage 6.0 (TID 392) in 167 ms on localhost (63/200)
15/08/19 18:18:23 INFO Executor: Finished task 62.0 in stage 6.0 (TID 386). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO TaskSetManager: Starting task 72.0 in stage 6.0 (TID 396, localhost, ANY, 1696 bytes)
15/08/19 18:18:23 INFO Executor: Running task 72.0 in stage 6.0 (TID 396)
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO TaskSetManager: Finished task 62.0 in stage 6.0 (TID 386) in 813 ms on localhost (64/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00012-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725709 length: 1725709 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO Executor: Finished task 61.0 in stage 6.0 (TID 385). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO Executor: Finished task 64.0 in stage 6.0 (TID 388). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO TaskSetManager: Starting task 73.0 in stage 6.0 (TID 397, localhost, ANY, 1696 bytes)
15/08/19 18:18:23 INFO Executor: Running task 73.0 in stage 6.0 (TID 397)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00063-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725125 length: 1725125 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:23 INFO TaskSetManager: Starting task 74.0 in stage 6.0 (TID 398, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 74.0 in stage 6.0 (TID 398)
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO TaskSetManager: Finished task 61.0 in stage 6.0 (TID 385) in 856 ms on localhost (65/200)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 64.0 in stage 6.0 (TID 388) in 835 ms on localhost (66/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00085-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726850 length: 1726850 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 65.0 in stage 6.0 (TID 389). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 75.0 in stage 6.0 (TID 399, localhost, ANY, 1694 bytes)
15/08/19 18:18:23 INFO Executor: Running task 75.0 in stage 6.0 (TID 399)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 65.0 in stage 6.0 (TID 389) in 839 ms on localhost (67/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00043-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725808 length: 1725808 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO Executor: Finished task 69.0 in stage 6.0 (TID 393). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO Executor: Finished task 66.0 in stage 6.0 (TID 390). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 76.0 in stage 6.0 (TID 400, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 76.0 in stage 6.0 (TID 400)
15/08/19 18:18:23 INFO TaskSetManager: Starting task 77.0 in stage 6.0 (TID 401, localhost, ANY, 1697 bytes)
15/08/19 18:18:23 INFO Executor: Running task 77.0 in stage 6.0 (TID 401)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 69.0 in stage 6.0 (TID 393) in 180 ms on localhost (68/200)
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO TaskSetManager: Finished task 66.0 in stage 6.0 (TID 390) in 821 ms on localhost (69/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00090-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725584 length: 1725584 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00168-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725913 length: 1725913 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO Executor: Finished task 67.0 in stage 6.0 (TID 391). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO TaskSetManager: Starting task 78.0 in stage 6.0 (TID 402, localhost, ANY, 1696 bytes)
15/08/19 18:18:23 INFO Executor: Running task 78.0 in stage 6.0 (TID 402)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00187-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727417 length: 1727417 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 INFO TaskSetManager: Finished task 67.0 in stage 6.0 (TID 391) in 320 ms on localhost (70/200)
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 74.0 in stage 6.0 (TID 398). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 79.0 in stage 6.0 (TID 403, localhost, ANY, 1693 bytes)
15/08/19 18:18:23 INFO Executor: Running task 79.0 in stage 6.0 (TID 403)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 74.0 in stage 6.0 (TID 398) in 149 ms on localhost (71/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00130-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727545 length: 1727545 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 75.0 in stage 6.0 (TID 399). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 80.0 in stage 6.0 (TID 404, localhost, ANY, 1697 bytes)
15/08/19 18:18:23 INFO Executor: Running task 80.0 in stage 6.0 (TID 404)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 75.0 in stage 6.0 (TID 399) in 158 ms on localhost (72/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00041-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726175 length: 1726175 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO Executor: Finished task 71.0 in stage 6.0 (TID 395). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 81.0 in stage 6.0 (TID 405, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 81.0 in stage 6.0 (TID 405)
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:23 INFO TaskSetManager: Finished task 71.0 in stage 6.0 (TID 395) in 313 ms on localhost (73/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00015-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725464 length: 1725464 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 72.0 in stage 6.0 (TID 396). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 82.0 in stage 6.0 (TID 406, localhost, ANY, 1696 bytes)
15/08/19 18:18:23 INFO Executor: Running task 82.0 in stage 6.0 (TID 406)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 72.0 in stage 6.0 (TID 396) in 318 ms on localhost (74/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00198-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725349 length: 1725349 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 70.0 in stage 6.0 (TID 394). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 83.0 in stage 6.0 (TID 407, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 83.0 in stage 6.0 (TID 407)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 70.0 in stage 6.0 (TID 394) in 445 ms on localhost (75/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00137-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726251 length: 1726251 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO Executor: Finished task 73.0 in stage 6.0 (TID 397). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:23 INFO TaskSetManager: Starting task 84.0 in stage 6.0 (TID 408, localhost, ANY, 1693 bytes)
15/08/19 18:18:23 INFO Executor: Running task 84.0 in stage 6.0 (TID 408)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 73.0 in stage 6.0 (TID 397) in 388 ms on localhost (76/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00118-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727084 length: 1727084 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO Executor: Finished task 82.0 in stage 6.0 (TID 406). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:23 INFO TaskSetManager: Starting task 85.0 in stage 6.0 (TID 409, localhost, ANY, 1697 bytes)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 82.0 in stage 6.0 (TID 406) in 138 ms on localhost (77/200)
15/08/19 18:18:23 INFO Executor: Running task 85.0 in stage 6.0 (TID 409)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00144-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726416 length: 1726416 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO Executor: Finished task 77.0 in stage 6.0 (TID 401). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO TaskSetManager: Starting task 86.0 in stage 6.0 (TID 410, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 86.0 in stage 6.0 (TID 410)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 77.0 in stage 6.0 (TID 401) in 354 ms on localhost (78/200)
15/08/19 18:18:23 INFO Executor: Finished task 76.0 in stage 6.0 (TID 400). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00013-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726749 length: 1726749 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 INFO TaskSetManager: Starting task 87.0 in stage 6.0 (TID 411, localhost, ANY, 1696 bytes)
15/08/19 18:18:23 INFO Executor: Running task 87.0 in stage 6.0 (TID 411)
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO TaskSetManager: Finished task 76.0 in stage 6.0 (TID 400) in 363 ms on localhost (79/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00087-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726664 length: 1726664 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO Executor: Finished task 78.0 in stage 6.0 (TID 402). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO TaskSetManager: Starting task 88.0 in stage 6.0 (TID 412, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 88.0 in stage 6.0 (TID 412)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 78.0 in stage 6.0 (TID 402) in 369 ms on localhost (80/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00134-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727114 length: 1727114 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 79.0 in stage 6.0 (TID 403). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 89.0 in stage 6.0 (TID 413, localhost, ANY, 1694 bytes)
15/08/19 18:18:23 INFO Executor: Running task 89.0 in stage 6.0 (TID 413)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 79.0 in stage 6.0 (TID 403) in 359 ms on localhost (81/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00104-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725052 length: 1725052 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO Executor: Finished task 80.0 in stage 6.0 (TID 404). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:23 INFO TaskSetManager: Starting task 90.0 in stage 6.0 (TID 414, localhost, ANY, 1694 bytes)
15/08/19 18:18:23 INFO Executor: Running task 90.0 in stage 6.0 (TID 414)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 80.0 in stage 6.0 (TID 404) in 315 ms on localhost (82/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00182-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727558 length: 1727558 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 81.0 in stage 6.0 (TID 405). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 91.0 in stage 6.0 (TID 415, localhost, ANY, 1696 bytes)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 81.0 in stage 6.0 (TID 405) in 315 ms on localhost (83/200)
15/08/19 18:18:23 INFO Executor: Running task 91.0 in stage 6.0 (TID 415)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00131-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725889 length: 1725889 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 88.0 in stage 6.0 (TID 412). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 92.0 in stage 6.0 (TID 416, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 92.0 in stage 6.0 (TID 416)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 88.0 in stage 6.0 (TID 412) in 133 ms on localhost (84/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00044-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727128 length: 1727128 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO Executor: Finished task 83.0 in stage 6.0 (TID 407). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO TaskSetManager: Starting task 93.0 in stage 6.0 (TID 417, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 93.0 in stage 6.0 (TID 417)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 83.0 in stage 6.0 (TID 407) in 268 ms on localhost (85/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00123-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1728192 length: 1728192 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO Executor: Finished task 89.0 in stage 6.0 (TID 413). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 94.0 in stage 6.0 (TID 418, localhost, ANY, 1694 bytes)
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO Executor: Running task 94.0 in stage 6.0 (TID 418)
15/08/19 18:18:23 INFO TaskSetManager: Finished task 89.0 in stage 6.0 (TID 413) in 133 ms on localhost (86/200)
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00016-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726831 length: 1726831 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO Executor: Finished task 84.0 in stage 6.0 (TID 408). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 95.0 in stage 6.0 (TID 419, localhost, ANY, 1695 bytes)
15/08/19 18:18:23 INFO Executor: Running task 95.0 in stage 6.0 (TID 419)
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO TaskSetManager: Finished task 84.0 in stage 6.0 (TID 408) in 279 ms on localhost (87/200)
15/08/19 18:18:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00010-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725695 length: 1725695 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:23 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:23 INFO Executor: Finished task 91.0 in stage 6.0 (TID 415). 2125 bytes result sent to driver
15/08/19 18:18:23 INFO TaskSetManager: Starting task 96.0 in stage 6.0 (TID 420, localhost, ANY, 1697 bytes)
15/08/19 18:18:24 INFO Executor: Running task 96.0 in stage 6.0 (TID 420)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 91.0 in stage 6.0 (TID 415) in 155 ms on localhost (88/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00190-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726196 length: 1726196 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 86.0 in stage 6.0 (TID 410). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 97.0 in stage 6.0 (TID 421, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 97.0 in stage 6.0 (TID 421)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 86.0 in stage 6.0 (TID 410) in 349 ms on localhost (89/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00068-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726308 length: 1726308 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO Executor: Finished task 85.0 in stage 6.0 (TID 409). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 98.0 in stage 6.0 (TID 422, localhost, ANY, 1695 bytes)
15/08/19 18:18:24 INFO Executor: Finished task 87.0 in stage 6.0 (TID 411). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO Executor: Running task 98.0 in stage 6.0 (TID 422)
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00073-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726888 length: 1726888 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO TaskSetManager: Starting task 99.0 in stage 6.0 (TID 423, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 85.0 in stage 6.0 (TID 409) in 390 ms on localhost (90/200)
15/08/19 18:18:24 INFO Executor: Running task 99.0 in stage 6.0 (TID 423)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 87.0 in stage 6.0 (TID 411) in 360 ms on localhost (91/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00079-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726827 length: 1726827 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO Executor: Finished task 90.0 in stage 6.0 (TID 414). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:24 INFO TaskSetManager: Starting task 100.0 in stage 6.0 (TID 424, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 100.0 in stage 6.0 (TID 424)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 90.0 in stage 6.0 (TID 414) in 289 ms on localhost (92/200)
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00170-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727874 length: 1727874 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 92.0 in stage 6.0 (TID 416). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 101.0 in stage 6.0 (TID 425, localhost, ANY, 1696 bytes)
15/08/19 18:18:24 INFO Executor: Running task 101.0 in stage 6.0 (TID 425)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00146-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725991 length: 1725991 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO TaskSetManager: Finished task 92.0 in stage 6.0 (TID 416) in 305 ms on localhost (93/200)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO Executor: Finished task 97.0 in stage 6.0 (TID 421). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO Executor: Finished task 93.0 in stage 6.0 (TID 417). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 102.0 in stage 6.0 (TID 426, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 102.0 in stage 6.0 (TID 426)
15/08/19 18:18:24 INFO TaskSetManager: Starting task 103.0 in stage 6.0 (TID 427, localhost, ANY, 1696 bytes)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00084-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726322 length: 1726322 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO TaskSetManager: Finished task 97.0 in stage 6.0 (TID 421) in 142 ms on localhost (94/200)
15/08/19 18:18:24 INFO Executor: Running task 103.0 in stage 6.0 (TID 427)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00045-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727207 length: 1727207 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO TaskSetManager: Finished task 93.0 in stage 6.0 (TID 417) in 307 ms on localhost (95/200)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 94.0 in stage 6.0 (TID 418). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 104.0 in stage 6.0 (TID 428, localhost, ANY, 1697 bytes)
15/08/19 18:18:24 INFO Executor: Running task 104.0 in stage 6.0 (TID 428)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 94.0 in stage 6.0 (TID 418) in 333 ms on localhost (96/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00071-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725625 length: 1725625 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 95.0 in stage 6.0 (TID 419). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 105.0 in stage 6.0 (TID 429, localhost, ANY, 1695 bytes)
15/08/19 18:18:24 INFO Executor: Running task 105.0 in stage 6.0 (TID 429)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 95.0 in stage 6.0 (TID 419) in 356 ms on localhost (97/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00059-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1724678 length: 1724678 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 96.0 in stage 6.0 (TID 420). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO Executor: Finished task 102.0 in stage 6.0 (TID 426). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 106.0 in stage 6.0 (TID 430, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 106.0 in stage 6.0 (TID 430)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00072-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727762 length: 1727762 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO TaskSetManager: Starting task 107.0 in stage 6.0 (TID 431, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 96.0 in stage 6.0 (TID 420) in 358 ms on localhost (98/200)
15/08/19 18:18:24 INFO Executor: Running task 107.0 in stage 6.0 (TID 431)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO TaskSetManager: Finished task 102.0 in stage 6.0 (TID 426) in 150 ms on localhost (99/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00083-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727508 length: 1727508 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 98.0 in stage 6.0 (TID 422). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO Executor: Finished task 99.0 in stage 6.0 (TID 423). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 108.0 in stage 6.0 (TID 432, localhost, ANY, 1696 bytes)
15/08/19 18:18:24 INFO Executor: Running task 108.0 in stage 6.0 (TID 432)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00097-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725759 length: 1725759 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO TaskSetManager: Starting task 109.0 in stage 6.0 (TID 433, localhost, ANY, 1697 bytes)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 98.0 in stage 6.0 (TID 422) in 373 ms on localhost (100/200)
15/08/19 18:18:24 INFO Executor: Running task 109.0 in stage 6.0 (TID 433)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00064-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726438 length: 1726438 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO TaskSetManager: Finished task 99.0 in stage 6.0 (TID 423) in 370 ms on localhost (101/200)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO Executor: Finished task 106.0 in stage 6.0 (TID 430). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO Executor: Finished task 100.0 in stage 6.0 (TID 424). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO Executor: Finished task 107.0 in stage 6.0 (TID 431). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 110.0 in stage 6.0 (TID 434, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 110.0 in stage 6.0 (TID 434)
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO TaskSetManager: Starting task 111.0 in stage 6.0 (TID 435, localhost, ANY, 1695 bytes)
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00106-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725450 length: 1725450 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO Executor: Running task 111.0 in stage 6.0 (TID 435)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00060-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727050 length: 1727050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO TaskSetManager: Starting task 112.0 in stage 6.0 (TID 436, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 100.0 in stage 6.0 (TID 424) in 376 ms on localhost (102/200)
15/08/19 18:18:24 INFO Executor: Running task 112.0 in stage 6.0 (TID 436)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 106.0 in stage 6.0 (TID 430) in 141 ms on localhost (103/200)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO TaskSetManager: Finished task 107.0 in stage 6.0 (TID 431) in 139 ms on localhost (104/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00105-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1728342 length: 1728342 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 375000
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 101.0 in stage 6.0 (TID 425). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 113.0 in stage 6.0 (TID 437, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 113.0 in stage 6.0 (TID 437)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 101.0 in stage 6.0 (TID 425) in 386 ms on localhost (105/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00116-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727887 length: 1727887 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO Executor: Finished task 103.0 in stage 6.0 (TID 427). 2125 bytes result sent to driver
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO TaskSetManager: Starting task 114.0 in stage 6.0 (TID 438, localhost, ANY, 1693 bytes)
15/08/19 18:18:24 INFO Executor: Running task 114.0 in stage 6.0 (TID 438)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 103.0 in stage 6.0 (TID 427) in 371 ms on localhost (106/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00001-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725799 length: 1725799 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 108.0 in stage 6.0 (TID 432). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 115.0 in stage 6.0 (TID 439, localhost, ANY, 1696 bytes)
15/08/19 18:18:24 INFO Executor: Running task 115.0 in stage 6.0 (TID 439)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 108.0 in stage 6.0 (TID 432) in 167 ms on localhost (107/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00162-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726023 length: 1726023 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO Executor: Finished task 112.0 in stage 6.0 (TID 436). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO Executor: Finished task 104.0 in stage 6.0 (TID 428). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO Executor: Finished task 110.0 in stage 6.0 (TID 434). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 116.0 in stage 6.0 (TID 440, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 116.0 in stage 6.0 (TID 440)
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00006-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726225 length: 1726225 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO TaskSetManager: Starting task 117.0 in stage 6.0 (TID 441, localhost, ANY, 1696 bytes)
15/08/19 18:18:24 INFO Executor: Running task 117.0 in stage 6.0 (TID 441)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 112.0 in stage 6.0 (TID 436) in 165 ms on localhost (108/200)
15/08/19 18:18:24 INFO TaskSetManager: Starting task 118.0 in stage 6.0 (TID 442, localhost, ANY, 1695 bytes)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00107-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727207 length: 1727207 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO TaskSetManager: Finished task 104.0 in stage 6.0 (TID 428) in 384 ms on localhost (109/200)
15/08/19 18:18:24 INFO Executor: Running task 118.0 in stage 6.0 (TID 442)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO TaskSetManager: Finished task 110.0 in stage 6.0 (TID 434) in 175 ms on localhost (110/200)
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00172-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726233 length: 1726233 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO Executor: Finished task 105.0 in stage 6.0 (TID 429). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO TaskSetManager: Starting task 119.0 in stage 6.0 (TID 443, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 119.0 in stage 6.0 (TID 443)
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO TaskSetManager: Finished task 105.0 in stage 6.0 (TID 429) in 362 ms on localhost (111/200)
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00055-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727030 length: 1727030 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 114.0 in stage 6.0 (TID 438). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 120.0 in stage 6.0 (TID 444, localhost, ANY, 1695 bytes)
15/08/19 18:18:24 INFO Executor: Running task 120.0 in stage 6.0 (TID 444)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 114.0 in stage 6.0 (TID 438) in 141 ms on localhost (112/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00181-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726795 length: 1726795 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 109.0 in stage 6.0 (TID 433). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 121.0 in stage 6.0 (TID 445, localhost, ANY, 1696 bytes)
15/08/19 18:18:24 INFO Executor: Running task 121.0 in stage 6.0 (TID 445)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 109.0 in stage 6.0 (TID 433) in 372 ms on localhost (113/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00153-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725357 length: 1725357 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO Executor: Finished task 111.0 in stage 6.0 (TID 435). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 122.0 in stage 6.0 (TID 446, localhost, ANY, 1694 bytes)
15/08/19 18:18:24 INFO Executor: Running task 122.0 in stage 6.0 (TID 446)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 111.0 in stage 6.0 (TID 435) in 359 ms on localhost (114/200)
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00000-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726478 length: 1726478 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 113.0 in stage 6.0 (TID 437). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 123.0 in stage 6.0 (TID 447, localhost, ANY, 1696 bytes)
15/08/19 18:18:24 INFO Executor: Running task 123.0 in stage 6.0 (TID 447)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 113.0 in stage 6.0 (TID 437) in 337 ms on localhost (115/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00132-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726679 length: 1726679 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:24 INFO Executor: Finished task 115.0 in stage 6.0 (TID 439). 2125 bytes result sent to driver
15/08/19 18:18:24 INFO TaskSetManager: Starting task 124.0 in stage 6.0 (TID 448, localhost, ANY, 1696 bytes)
15/08/19 18:18:24 INFO Executor: Running task 124.0 in stage 6.0 (TID 448)
15/08/19 18:18:24 INFO TaskSetManager: Finished task 115.0 in stage 6.0 (TID 439) in 345 ms on localhost (116/200)
15/08/19 18:18:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00061-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726539 length: 1726539 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:24 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 116.0 in stage 6.0 (TID 440). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 125.0 in stage 6.0 (TID 449, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 125.0 in stage 6.0 (TID 449)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 116.0 in stage 6.0 (TID 440) in 400 ms on localhost (117/200)
15/08/19 18:18:25 INFO Executor: Finished task 118.0 in stage 6.0 (TID 442). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00108-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726716 length: 1726716 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO TaskSetManager: Starting task 126.0 in stage 6.0 (TID 450, localhost, ANY, 1695 bytes)
15/08/19 18:18:25 INFO Executor: Running task 126.0 in stage 6.0 (TID 450)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 118.0 in stage 6.0 (TID 442) in 399 ms on localhost (118/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00119-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725534 length: 1725534 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO Executor: Finished task 117.0 in stage 6.0 (TID 441). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO TaskSetManager: Starting task 127.0 in stage 6.0 (TID 451, localhost, ANY, 1695 bytes)
15/08/19 18:18:25 INFO Executor: Running task 127.0 in stage 6.0 (TID 451)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 117.0 in stage 6.0 (TID 441) in 426 ms on localhost (119/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00049-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725978 length: 1725978 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 119.0 in stage 6.0 (TID 443). 2125 bytes result sent to driver
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO TaskSetManager: Starting task 128.0 in stage 6.0 (TID 452, localhost, ANY, 1697 bytes)
15/08/19 18:18:25 INFO Executor: Running task 128.0 in stage 6.0 (TID 452)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 119.0 in stage 6.0 (TID 443) in 418 ms on localhost (120/200)
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00128-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726702 length: 1726702 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 120.0 in stage 6.0 (TID 444). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 129.0 in stage 6.0 (TID 453, localhost, ANY, 1695 bytes)
15/08/19 18:18:25 INFO Executor: Running task 129.0 in stage 6.0 (TID 453)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 120.0 in stage 6.0 (TID 444) in 402 ms on localhost (121/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00136-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726310 length: 1726310 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 125.0 in stage 6.0 (TID 449). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 130.0 in stage 6.0 (TID 454, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 130.0 in stage 6.0 (TID 454)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00103-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725154 length: 1725154 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO TaskSetManager: Finished task 125.0 in stage 6.0 (TID 449) in 140 ms on localhost (122/200)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO Executor: Finished task 121.0 in stage 6.0 (TID 445). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO Executor: Finished task 122.0 in stage 6.0 (TID 446). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 131.0 in stage 6.0 (TID 455, localhost, ANY, 1697 bytes)
15/08/19 18:18:25 INFO Executor: Running task 131.0 in stage 6.0 (TID 455)
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00129-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726646 length: 1726646 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO TaskSetManager: Starting task 132.0 in stage 6.0 (TID 456, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 121.0 in stage 6.0 (TID 445) in 378 ms on localhost (123/200)
15/08/19 18:18:25 INFO Executor: Running task 132.0 in stage 6.0 (TID 456)
15/08/19 18:18:25 INFO Executor: Finished task 127.0 in stage 6.0 (TID 451). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Finished task 122.0 in stage 6.0 (TID 446) in 366 ms on localhost (124/200)
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00193-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727135 length: 1727135 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO TaskSetManager: Starting task 133.0 in stage 6.0 (TID 457, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO Executor: Running task 133.0 in stage 6.0 (TID 457)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 127.0 in stage 6.0 (TID 451) in 145 ms on localhost (125/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00093-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726940 length: 1726940 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 123.0 in stage 6.0 (TID 447). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:25 INFO TaskSetManager: Starting task 134.0 in stage 6.0 (TID 458, localhost, ANY, 1697 bytes)
15/08/19 18:18:25 INFO Executor: Running task 134.0 in stage 6.0 (TID 458)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 123.0 in stage 6.0 (TID 447) in 334 ms on localhost (126/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00148-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726144 length: 1726144 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 124.0 in stage 6.0 (TID 448). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 135.0 in stage 6.0 (TID 459, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 135.0 in stage 6.0 (TID 459)
15/08/19 18:18:25 INFO Executor: Finished task 130.0 in stage 6.0 (TID 454). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Finished task 124.0 in stage 6.0 (TID 448) in 354 ms on localhost (127/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00141-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726231 length: 1726231 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO TaskSetManager: Starting task 136.0 in stage 6.0 (TID 460, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 136.0 in stage 6.0 (TID 460)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO TaskSetManager: Finished task 130.0 in stage 6.0 (TID 454) in 153 ms on localhost (128/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00092-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726205 length: 1726205 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO Executor: Finished task 131.0 in stage 6.0 (TID 455). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO TaskSetManager: Starting task 137.0 in stage 6.0 (TID 461, localhost, ANY, 1695 bytes)
15/08/19 18:18:25 INFO Executor: Running task 137.0 in stage 6.0 (TID 461)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 131.0 in stage 6.0 (TID 455) in 143 ms on localhost (129/200)
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00009-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726279 length: 1726279 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 126.0 in stage 6.0 (TID 450). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 138.0 in stage 6.0 (TID 462, localhost, ANY, 1694 bytes)
15/08/19 18:18:25 INFO Executor: Running task 138.0 in stage 6.0 (TID 462)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 126.0 in stage 6.0 (TID 450) in 334 ms on localhost (130/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00069-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726766 length: 1726766 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 128.0 in stage 6.0 (TID 452). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 139.0 in stage 6.0 (TID 463, localhost, ANY, 1695 bytes)
15/08/19 18:18:25 INFO Executor: Running task 139.0 in stage 6.0 (TID 463)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 128.0 in stage 6.0 (TID 452) in 348 ms on localhost (131/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00161-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726760 length: 1726760 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO Executor: Finished task 135.0 in stage 6.0 (TID 459). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 375000
15/08/19 18:18:25 INFO TaskSetManager: Starting task 140.0 in stage 6.0 (TID 464, localhost, ANY, 1694 bytes)
15/08/19 18:18:25 INFO Executor: Running task 140.0 in stage 6.0 (TID 464)
15/08/19 18:18:25 INFO Executor: Finished task 129.0 in stage 6.0 (TID 453). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00005-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725835 length: 1725835 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO TaskSetManager: Starting task 141.0 in stage 6.0 (TID 465, localhost, ANY, 1695 bytes)
15/08/19 18:18:25 INFO Executor: Running task 141.0 in stage 6.0 (TID 465)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 135.0 in stage 6.0 (TID 459) in 149 ms on localhost (132/200)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00154-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726999 length: 1726999 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO TaskSetManager: Finished task 129.0 in stage 6.0 (TID 453) in 339 ms on localhost (133/200)
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 138.0 in stage 6.0 (TID 462). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 142.0 in stage 6.0 (TID 466, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 142.0 in stage 6.0 (TID 466)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 138.0 in stage 6.0 (TID 462) in 166 ms on localhost (134/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00032-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726231 length: 1726231 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 139.0 in stage 6.0 (TID 463). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 143.0 in stage 6.0 (TID 467, localhost, ANY, 1697 bytes)
15/08/19 18:18:25 INFO Executor: Running task 143.0 in stage 6.0 (TID 467)
15/08/19 18:18:25 INFO Executor: Finished task 133.0 in stage 6.0 (TID 457). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Finished task 139.0 in stage 6.0 (TID 463) in 165 ms on localhost (135/200)
15/08/19 18:18:25 INFO Executor: Finished task 132.0 in stage 6.0 (TID 456). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 144.0 in stage 6.0 (TID 468, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00184-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726108 length: 1726108 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO Executor: Running task 144.0 in stage 6.0 (TID 468)
15/08/19 18:18:25 INFO TaskSetManager: Starting task 145.0 in stage 6.0 (TID 469, localhost, ANY, 1695 bytes)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 133.0 in stage 6.0 (TID 457) in 388 ms on localhost (136/200)
15/08/19 18:18:25 INFO Executor: Running task 145.0 in stage 6.0 (TID 469)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00157-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726471 length: 1726471 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO TaskSetManager: Finished task 132.0 in stage 6.0 (TID 456) in 399 ms on localhost (137/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00112-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726244 length: 1726244 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 134.0 in stage 6.0 (TID 458). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 146.0 in stage 6.0 (TID 470, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 146.0 in stage 6.0 (TID 470)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00121-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726200 length: 1726200 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO TaskSetManager: Finished task 134.0 in stage 6.0 (TID 458) in 402 ms on localhost (138/200)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 19.0 in stage 4.0 (TID 315). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO Executor: Finished task 136.0 in stage 6.0 (TID 460). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 147.0 in stage 6.0 (TID 471, localhost, ANY, 1694 bytes)
15/08/19 18:18:25 INFO Executor: Running task 147.0 in stage 6.0 (TID 471)
15/08/19 18:18:25 INFO TaskSetManager: Starting task 148.0 in stage 6.0 (TID 472, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 148.0 in stage 6.0 (TID 472)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 315) in 13616 ms on localhost (22/28)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00025-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727484 length: 1727484 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO TaskSetManager: Finished task 136.0 in stage 6.0 (TID 460) in 370 ms on localhost (139/200)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00178-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725719 length: 1725719 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO Executor: Finished task 137.0 in stage 6.0 (TID 461). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO TaskSetManager: Starting task 149.0 in stage 6.0 (TID 473, localhost, ANY, 1697 bytes)
15/08/19 18:18:25 INFO Executor: Running task 149.0 in stage 6.0 (TID 473)
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO TaskSetManager: Finished task 137.0 in stage 6.0 (TID 461) in 379 ms on localhost (140/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00127-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726650 length: 1726650 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 141.0 in stage 6.0 (TID 465). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO Executor: Finished task 140.0 in stage 6.0 (TID 464). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 150.0 in stage 6.0 (TID 474, localhost, ANY, 1695 bytes)
15/08/19 18:18:25 INFO Executor: Running task 150.0 in stage 6.0 (TID 474)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00038-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727448 length: 1727448 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO TaskSetManager: Starting task 151.0 in stage 6.0 (TID 475, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 141.0 in stage 6.0 (TID 465) in 366 ms on localhost (141/200)
15/08/19 18:18:25 INFO Executor: Running task 151.0 in stage 6.0 (TID 475)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO TaskSetManager: Finished task 140.0 in stage 6.0 (TID 464) in 372 ms on localhost (142/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00124-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726384 length: 1726384 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO Executor: Finished task 147.0 in stage 6.0 (TID 471). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO TaskSetManager: Starting task 152.0 in stage 6.0 (TID 476, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 152.0 in stage 6.0 (TID 476)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00185-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725935 length: 1725935 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO TaskSetManager: Finished task 147.0 in stage 6.0 (TID 471) in 159 ms on localhost (143/200)
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 142.0 in stage 6.0 (TID 466). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 153.0 in stage 6.0 (TID 477, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 153.0 in stage 6.0 (TID 477)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 142.0 in stage 6.0 (TID 466) in 392 ms on localhost (144/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00042-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726023 length: 1726023 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO Executor: Finished task 150.0 in stage 6.0 (TID 474). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 154.0 in stage 6.0 (TID 478, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 154.0 in stage 6.0 (TID 478)
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:25 INFO TaskSetManager: Finished task 150.0 in stage 6.0 (TID 474) in 141 ms on localhost (145/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00081-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726359 length: 1726359 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO Executor: Finished task 144.0 in stage 6.0 (TID 468). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO TaskSetManager: Starting task 155.0 in stage 6.0 (TID 479, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 155.0 in stage 6.0 (TID 479)
15/08/19 18:18:25 INFO Executor: Finished task 145.0 in stage 6.0 (TID 469). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Finished task 144.0 in stage 6.0 (TID 468) in 379 ms on localhost (146/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00186-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727207 length: 1727207 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO TaskSetManager: Starting task 156.0 in stage 6.0 (TID 480, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 156.0 in stage 6.0 (TID 480)
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO TaskSetManager: Finished task 145.0 in stage 6.0 (TID 469) in 383 ms on localhost (147/200)
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00021-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726018 length: 1726018 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO Executor: Finished task 143.0 in stage 6.0 (TID 467). 2125 bytes result sent to driver
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO TaskSetManager: Starting task 157.0 in stage 6.0 (TID 481, localhost, ANY, 1697 bytes)
15/08/19 18:18:25 INFO Executor: Running task 157.0 in stage 6.0 (TID 481)
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO TaskSetManager: Finished task 143.0 in stage 6.0 (TID 467) in 407 ms on localhost (148/200)
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00149-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726431 length: 1726431 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:25 INFO Executor: Finished task 146.0 in stage 6.0 (TID 470). 2125 bytes result sent to driver
15/08/19 18:18:25 INFO TaskSetManager: Starting task 158.0 in stage 6.0 (TID 482, localhost, ANY, 1696 bytes)
15/08/19 18:18:25 INFO Executor: Running task 158.0 in stage 6.0 (TID 482)
15/08/19 18:18:25 INFO TaskSetManager: Finished task 146.0 in stage 6.0 (TID 470) in 374 ms on localhost (149/200)
15/08/19 18:18:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00056-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726638 length: 1726638 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:25 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO Executor: Finished task 148.0 in stage 6.0 (TID 472). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO TaskSetManager: Starting task 159.0 in stage 6.0 (TID 483, localhost, ANY, 1694 bytes)
15/08/19 18:18:26 INFO Executor: Running task 159.0 in stage 6.0 (TID 483)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 148.0 in stage 6.0 (TID 472) in 360 ms on localhost (150/200)
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00052-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726318 length: 1726318 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO Executor: Finished task 149.0 in stage 6.0 (TID 473). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO TaskSetManager: Starting task 160.0 in stage 6.0 (TID 484, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Running task 160.0 in stage 6.0 (TID 484)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 149.0 in stage 6.0 (TID 473) in 360 ms on localhost (151/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00028-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726650 length: 1726650 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO Executor: Finished task 153.0 in stage 6.0 (TID 477). 2125 bytes result sent to driver
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO TaskSetManager: Starting task 161.0 in stage 6.0 (TID 485, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Running task 161.0 in stage 6.0 (TID 485)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 153.0 in stage 6.0 (TID 477) in 158 ms on localhost (152/200)
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00082-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726399 length: 1726399 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:26 INFO Executor: Finished task 151.0 in stage 6.0 (TID 475). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO TaskSetManager: Starting task 162.0 in stage 6.0 (TID 486, localhost, ANY, 1695 bytes)
15/08/19 18:18:26 INFO Executor: Running task 162.0 in stage 6.0 (TID 486)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 151.0 in stage 6.0 (TID 475) in 289 ms on localhost (153/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00094-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727142 length: 1727142 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO Executor: Finished task 156.0 in stage 6.0 (TID 480). 2125 bytes result sent to driver
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO TaskSetManager: Starting task 163.0 in stage 6.0 (TID 487, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Running task 163.0 in stage 6.0 (TID 487)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 156.0 in stage 6.0 (TID 480) in 146 ms on localhost (154/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00176-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725437 length: 1725437 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO Executor: Finished task 152.0 in stage 6.0 (TID 476). 2125 bytes result sent to driver
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO TaskSetManager: Starting task 164.0 in stage 6.0 (TID 488, localhost, ANY, 1694 bytes)
15/08/19 18:18:26 INFO Executor: Running task 164.0 in stage 6.0 (TID 488)
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO TaskSetManager: Finished task 152.0 in stage 6.0 (TID 476) in 293 ms on localhost (155/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00102-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726254 length: 1726254 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:26 INFO Executor: Finished task 164.0 in stage 6.0 (TID 488). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO TaskSetManager: Starting task 165.0 in stage 6.0 (TID 489, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Running task 165.0 in stage 6.0 (TID 489)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 164.0 in stage 6.0 (TID 488) in 141 ms on localhost (156/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00196-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1728183 length: 1728183 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO Executor: Finished task 154.0 in stage 6.0 (TID 478). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO TaskSetManager: Starting task 166.0 in stage 6.0 (TID 490, localhost, ANY, 1694 bytes)
15/08/19 18:18:26 INFO Executor: Running task 166.0 in stage 6.0 (TID 490)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 154.0 in stage 6.0 (TID 478) in 341 ms on localhost (157/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00138-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725744 length: 1725744 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:26 INFO Executor: Finished task 155.0 in stage 6.0 (TID 479). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO TaskSetManager: Starting task 167.0 in stage 6.0 (TID 491, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Running task 167.0 in stage 6.0 (TID 491)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 155.0 in stage 6.0 (TID 479) in 380 ms on localhost (158/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00070-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726620 length: 1726620 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:26 INFO Executor: Finished task 157.0 in stage 6.0 (TID 481). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO TaskSetManager: Starting task 168.0 in stage 6.0 (TID 492, localhost, ANY, 1697 bytes)
15/08/19 18:18:26 INFO Executor: Running task 168.0 in stage 6.0 (TID 492)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 157.0 in stage 6.0 (TID 481) in 414 ms on localhost (159/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00147-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726276 length: 1726276 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO Executor: Finished task 158.0 in stage 6.0 (TID 482). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:26 INFO TaskSetManager: Starting task 169.0 in stage 6.0 (TID 493, localhost, ANY, 1695 bytes)
15/08/19 18:18:26 INFO Executor: Running task 169.0 in stage 6.0 (TID 493)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 158.0 in stage 6.0 (TID 482) in 433 ms on localhost (160/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00088-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726968 length: 1726968 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO Executor: Finished task 159.0 in stage 6.0 (TID 483). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO Executor: Finished task 160.0 in stage 6.0 (TID 484). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO TaskSetManager: Starting task 170.0 in stage 6.0 (TID 494, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Finished task 161.0 in stage 6.0 (TID 485). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO Executor: Running task 170.0 in stage 6.0 (TID 494)
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00183-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726706 length: 1726706 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO TaskSetManager: Starting task 171.0 in stage 6.0 (TID 495, localhost, ANY, 1693 bytes)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 159.0 in stage 6.0 (TID 483) in 431 ms on localhost (161/200)
15/08/19 18:18:26 INFO Executor: Running task 171.0 in stage 6.0 (TID 495)
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO TaskSetManager: Finished task 160.0 in stage 6.0 (TID 484) in 414 ms on localhost (162/200)
15/08/19 18:18:26 INFO Executor: Finished task 167.0 in stage 6.0 (TID 491). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00020-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726315 length: 1726315 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO TaskSetManager: Starting task 172.0 in stage 6.0 (TID 496, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 161.0 in stage 6.0 (TID 485) in 418 ms on localhost (163/200)
15/08/19 18:18:26 INFO Executor: Running task 172.0 in stage 6.0 (TID 496)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00195-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726492 length: 1726492 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO TaskSetManager: Starting task 173.0 in stage 6.0 (TID 497, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Running task 173.0 in stage 6.0 (TID 497)
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 375000
15/08/19 18:18:26 INFO TaskSetManager: Finished task 167.0 in stage 6.0 (TID 491) in 164 ms on localhost (164/200)
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00151-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726533 length: 1726533 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO Executor: Finished task 162.0 in stage 6.0 (TID 486). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO TaskSetManager: Starting task 174.0 in stage 6.0 (TID 498, localhost, ANY, 1695 bytes)
15/08/19 18:18:26 INFO Executor: Running task 174.0 in stage 6.0 (TID 498)
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO TaskSetManager: Finished task 162.0 in stage 6.0 (TID 486) in 401 ms on localhost (165/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00062-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725729 length: 1725729 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO Executor: Finished task 163.0 in stage 6.0 (TID 487). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 375000
15/08/19 18:18:26 INFO TaskSetManager: Starting task 175.0 in stage 6.0 (TID 499, localhost, ANY, 1693 bytes)
15/08/19 18:18:26 INFO Executor: Running task 175.0 in stage 6.0 (TID 499)
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO TaskSetManager: Finished task 163.0 in stage 6.0 (TID 487) in 416 ms on localhost (166/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00113-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727581 length: 1727581 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:26 INFO Executor: Finished task 165.0 in stage 6.0 (TID 489). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO TaskSetManager: Starting task 176.0 in stage 6.0 (TID 500, localhost, ANY, 1694 bytes)
15/08/19 18:18:26 INFO Executor: Running task 176.0 in stage 6.0 (TID 500)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 165.0 in stage 6.0 (TID 489) in 349 ms on localhost (167/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00188-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726819 length: 1726819 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO Executor: Finished task 166.0 in stage 6.0 (TID 490). 2125 bytes result sent to driver
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO TaskSetManager: Starting task 177.0 in stage 6.0 (TID 501, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Running task 177.0 in stage 6.0 (TID 501)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 166.0 in stage 6.0 (TID 490) in 328 ms on localhost (168/200)
15/08/19 18:18:26 INFO Executor: Finished task 171.0 in stage 6.0 (TID 495). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00086-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726896 length: 1726896 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO TaskSetManager: Starting task 178.0 in stage 6.0 (TID 502, localhost, ANY, 1694 bytes)
15/08/19 18:18:26 INFO Executor: Running task 178.0 in stage 6.0 (TID 502)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 171.0 in stage 6.0 (TID 495) in 151 ms on localhost (169/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00022-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726585 length: 1726585 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO Executor: Finished task 172.0 in stage 6.0 (TID 496). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO TaskSetManager: Starting task 179.0 in stage 6.0 (TID 503, localhost, ANY, 1696 bytes)
15/08/19 18:18:26 INFO Executor: Running task 179.0 in stage 6.0 (TID 503)
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO TaskSetManager: Finished task 172.0 in stage 6.0 (TID 496) in 156 ms on localhost (170/200)
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00035-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726529 length: 1726529 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 375000
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:26 INFO Executor: Finished task 18.0 in stage 4.0 (TID 314). 2125 bytes result sent to driver
15/08/19 18:18:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:26 INFO TaskSetManager: Starting task 180.0 in stage 6.0 (TID 504, localhost, ANY, 1694 bytes)
15/08/19 18:18:26 INFO Executor: Running task 180.0 in stage 6.0 (TID 504)
15/08/19 18:18:26 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 314) in 14842 ms on localhost (23/28)
15/08/19 18:18:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00089-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726767 length: 1726767 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:27 INFO Executor: Finished task 168.0 in stage 6.0 (TID 492). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO Executor: Finished task 178.0 in stage 6.0 (TID 502). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO TaskSetManager: Starting task 181.0 in stage 6.0 (TID 505, localhost, ANY, 1694 bytes)
15/08/19 18:18:27 INFO Executor: Running task 181.0 in stage 6.0 (TID 505)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00040-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725032 length: 1725032 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO TaskSetManager: Starting task 182.0 in stage 6.0 (TID 506, localhost, ANY, 1696 bytes)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 168.0 in stage 6.0 (TID 492) in 728 ms on localhost (171/200)
15/08/19 18:18:27 INFO Executor: Running task 182.0 in stage 6.0 (TID 506)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 178.0 in stage 6.0 (TID 502) in 512 ms on localhost (172/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00080-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726427 length: 1726427 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:27 INFO Executor: Finished task 180.0 in stage 6.0 (TID 504). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO Executor: Finished task 169.0 in stage 6.0 (TID 493). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO Executor: Finished task 170.0 in stage 6.0 (TID 494). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO TaskSetManager: Starting task 183.0 in stage 6.0 (TID 507, localhost, ANY, 1695 bytes)
15/08/19 18:18:27 INFO Executor: Running task 183.0 in stage 6.0 (TID 507)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00192-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727048 length: 1727048 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO TaskSetManager: Starting task 184.0 in stage 6.0 (TID 508, localhost, ANY, 1696 bytes)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 180.0 in stage 6.0 (TID 504) in 520 ms on localhost (173/200)
15/08/19 18:18:27 INFO Executor: Running task 184.0 in stage 6.0 (TID 508)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00067-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727460 length: 1727460 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO TaskSetManager: Starting task 185.0 in stage 6.0 (TID 509, localhost, ANY, 1695 bytes)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 169.0 in stage 6.0 (TID 493) in 756 ms on localhost (174/200)
15/08/19 18:18:27 INFO Executor: Running task 185.0 in stage 6.0 (TID 509)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 170.0 in stage 6.0 (TID 494) in 727 ms on localhost (175/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00100-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726600 length: 1726600 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 375000
15/08/19 18:18:27 INFO Executor: Finished task 174.0 in stage 6.0 (TID 498). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO TaskSetManager: Starting task 186.0 in stage 6.0 (TID 510, localhost, ANY, 1696 bytes)
15/08/19 18:18:27 INFO Executor: Running task 186.0 in stage 6.0 (TID 510)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 174.0 in stage 6.0 (TID 498) in 743 ms on localhost (176/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00051-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725893 length: 1725893 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO Executor: Finished task 173.0 in stage 6.0 (TID 497). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO TaskSetManager: Starting task 187.0 in stage 6.0 (TID 511, localhost, ANY, 1694 bytes)
15/08/19 18:18:27 INFO Executor: Running task 187.0 in stage 6.0 (TID 511)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 173.0 in stage 6.0 (TID 497) in 789 ms on localhost (177/200)
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00008-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726869 length: 1726869 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO Executor: Finished task 175.0 in stage 6.0 (TID 499). 2125 bytes result sent to driver
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:27 INFO TaskSetManager: Starting task 188.0 in stage 6.0 (TID 512, localhost, ANY, 1697 bytes)
15/08/19 18:18:27 INFO Executor: Running task 188.0 in stage 6.0 (TID 512)
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO TaskSetManager: Finished task 175.0 in stage 6.0 (TID 499) in 780 ms on localhost (178/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00135-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725657 length: 1725657 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:27 INFO Executor: Finished task 179.0 in stage 6.0 (TID 503). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO Executor: Finished task 176.0 in stage 6.0 (TID 500). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO Executor: Finished task 177.0 in stage 6.0 (TID 501). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO TaskSetManager: Starting task 189.0 in stage 6.0 (TID 513, localhost, ANY, 1697 bytes)
15/08/19 18:18:27 INFO Executor: Running task 189.0 in stage 6.0 (TID 513)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00091-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726437 length: 1726437 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO TaskSetManager: Starting task 190.0 in stage 6.0 (TID 514, localhost, ANY, 1695 bytes)
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO TaskSetManager: Finished task 179.0 in stage 6.0 (TID 503) in 785 ms on localhost (179/200)
15/08/19 18:18:27 INFO Executor: Running task 190.0 in stage 6.0 (TID 514)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00197-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727501 length: 1727501 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO Executor: Finished task 188.0 in stage 6.0 (TID 512). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO TaskSetManager: Starting task 191.0 in stage 6.0 (TID 515, localhost, ANY, 1695 bytes)
15/08/19 18:18:27 INFO Executor: Running task 191.0 in stage 6.0 (TID 515)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 176.0 in stage 6.0 (TID 500) in 828 ms on localhost (180/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00179-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1728082 length: 1728082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO TaskSetManager: Starting task 192.0 in stage 6.0 (TID 516, localhost, ANY, 1694 bytes)
15/08/19 18:18:27 INFO Executor: Running task 192.0 in stage 6.0 (TID 516)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 177.0 in stage 6.0 (TID 501) in 833 ms on localhost (181/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00165-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1727080 length: 1727080 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:27 INFO TaskSetManager: Finished task 188.0 in stage 6.0 (TID 512) in 157 ms on localhost (182/200)
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 375000
15/08/19 18:18:27 INFO Executor: Finished task 181.0 in stage 6.0 (TID 505). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO Executor: Finished task 182.0 in stage 6.0 (TID 506). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO TaskSetManager: Starting task 193.0 in stage 6.0 (TID 517, localhost, ANY, 1696 bytes)
15/08/19 18:18:27 INFO Executor: Running task 193.0 in stage 6.0 (TID 517)
15/08/19 18:18:27 INFO TaskSetManager: Starting task 194.0 in stage 6.0 (TID 518, localhost, ANY, 1695 bytes)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00191-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726641 length: 1726641 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO Executor: Running task 194.0 in stage 6.0 (TID 518)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 181.0 in stage 6.0 (TID 505) in 428 ms on localhost (183/200)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 182.0 in stage 6.0 (TID 506) in 427 ms on localhost (184/200)
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00171-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726301 length: 1726301 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 375000
15/08/19 18:18:27 INFO Executor: Finished task 183.0 in stage 6.0 (TID 507). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO TaskSetManager: Starting task 195.0 in stage 6.0 (TID 519, localhost, ANY, 1696 bytes)
15/08/19 18:18:27 INFO Executor: Running task 195.0 in stage 6.0 (TID 519)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 183.0 in stage 6.0 (TID 507) in 438 ms on localhost (185/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00037-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725109 length: 1725109 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO Executor: Finished task 184.0 in stage 6.0 (TID 508). 2125 bytes result sent to driver
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO TaskSetManager: Starting task 196.0 in stage 6.0 (TID 520, localhost, ANY, 1695 bytes)
15/08/19 18:18:27 INFO Executor: Running task 196.0 in stage 6.0 (TID 520)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 184.0 in stage 6.0 (TID 508) in 439 ms on localhost (186/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00014-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726313 length: 1726313 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 375000
15/08/19 18:18:27 INFO Executor: Finished task 185.0 in stage 6.0 (TID 509). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 375000
15/08/19 18:18:27 INFO TaskSetManager: Starting task 197.0 in stage 6.0 (TID 521, localhost, ANY, 1697 bytes)
15/08/19 18:18:27 INFO Executor: Running task 197.0 in stage 6.0 (TID 521)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 185.0 in stage 6.0 (TID 509) in 458 ms on localhost (187/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00189-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726701 length: 1726701 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO Executor: Finished task 186.0 in stage 6.0 (TID 510). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 375000
15/08/19 18:18:27 INFO TaskSetManager: Starting task 198.0 in stage 6.0 (TID 522, localhost, ANY, 1696 bytes)
15/08/19 18:18:27 INFO Executor: Running task 198.0 in stage 6.0 (TID 522)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 186.0 in stage 6.0 (TID 510) in 427 ms on localhost (188/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00047-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1725968 length: 1725968 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO Executor: Finished task 187.0 in stage 6.0 (TID 511). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO TaskSetManager: Starting task 199.0 in stage 6.0 (TID 523, localhost, ANY, 1696 bytes)
15/08/19 18:18:27 INFO Executor: Running task 199.0 in stage 6.0 (TID 523)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 187.0 in stage 6.0 (TID 511) in 430 ms on localhost (189/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00054-e9fe8ad2-f27c-46b0-b5c0-528fb5e03b3f.gz.parquet start: 0 end: 1726360 length: 1726360 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 375000
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 375000 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 375000
15/08/19 18:18:27 INFO Executor: Finished task 195.0 in stage 6.0 (TID 519). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 524, localhost, ANY, 1757 bytes)
15/08/19 18:18:27 INFO Executor: Running task 0.0 in stage 7.0 (TID 524)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 195.0 in stage 6.0 (TID 519) in 144 ms on localhost (190/200)
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500100
15/08/19 18:18:27 INFO Executor: Finished task 189.0 in stage 6.0 (TID 513). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO Executor: Finished task 190.0 in stage 6.0 (TID 514). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 525, localhost, ANY, 1770 bytes)
15/08/19 18:18:27 INFO Executor: Running task 1.0 in stage 7.0 (TID 525)
15/08/19 18:18:27 INFO Executor: Finished task 191.0 in stage 6.0 (TID 515). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 134217728 end: 258365400 length: 124147672 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 526, localhost, ANY, 1757 bytes)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 189.0 in stage 6.0 (TID 513) in 442 ms on localhost (191/200)
15/08/19 18:18:27 INFO Executor: Running task 2.0 in stage 7.0 (TID 526)
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO Executor: Finished task 192.0 in stage 6.0 (TID 516). 2125 bytes result sent to driver
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 527, localhost, ANY, 1770 bytes)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 190.0 in stage 6.0 (TID 514) in 447 ms on localhost (192/200)
15/08/19 18:18:27 INFO Executor: Running task 3.0 in stage 7.0 (TID 527)
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609665 records.
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 134217728 end: 260663202 length: 126445474 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 528, localhost, ANY, 1755 bytes)
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501536 records.
15/08/19 18:18:27 INFO Executor: Running task 4.0 in stage 7.0 (TID 528)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 191.0 in stage 6.0 (TID 515) in 447 ms on localhost (193/200)
15/08/19 18:18:27 INFO TaskSetManager: Finished task 192.0 in stage 6.0 (TID 516) in 440 ms on localhost (194/200)
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3662666 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501210 records.
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 3503026
15/08/19 18:18:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3501536
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 3500100
15/08/19 18:18:27 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3501210
15/08/19 18:18:27 INFO Executor: Finished task 193.0 in stage 6.0 (TID 517). 2125 bytes result sent to driver
15/08/19 18:18:28 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 529, localhost, ANY, 1771 bytes)
15/08/19 18:18:28 INFO Executor: Running task 5.0 in stage 7.0 (TID 529)
15/08/19 18:18:28 INFO Executor: Finished task 194.0 in stage 6.0 (TID 518). 2125 bytes result sent to driver
15/08/19 18:18:28 INFO TaskSetManager: Finished task 193.0 in stage 6.0 (TID 517) in 521 ms on localhost (195/200)
15/08/19 18:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 134217728 end: 262730160 length: 128512432 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:28 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 530, localhost, ANY, 1758 bytes)
15/08/19 18:18:28 INFO Executor: Running task 6.0 in stage 7.0 (TID 530)
15/08/19 18:18:28 INFO TaskSetManager: Finished task 194.0 in stage 6.0 (TID 518) in 518 ms on localhost (196/200)
15/08/19 18:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:28 INFO Executor: Finished task 196.0 in stage 6.0 (TID 520). 2125 bytes result sent to driver
15/08/19 18:18:28 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 531, localhost, ANY, 1769 bytes)
15/08/19 18:18:28 INFO Executor: Running task 7.0 in stage 7.0 (TID 531)
15/08/19 18:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502843 records.
15/08/19 18:18:28 INFO TaskSetManager: Finished task 196.0 in stage 6.0 (TID 520) in 482 ms on localhost (197/200)
15/08/19 18:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 134217728 end: 258497939 length: 124280211 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3726777 records.
15/08/19 18:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:28 INFO Executor: Finished task 197.0 in stage 6.0 (TID 521). 2125 bytes result sent to driver
15/08/19 18:18:28 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 532, localhost, ANY, 1758 bytes)
15/08/19 18:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3606822 records.
15/08/19 18:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:28 INFO TaskSetManager: Finished task 197.0 in stage 6.0 (TID 521) in 488 ms on localhost (198/200)
15/08/19 18:18:28 INFO Executor: Running task 8.0 in stage 7.0 (TID 532)
15/08/19 18:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:28 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 3502843
15/08/19 18:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:18:28 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 3503264
15/08/19 18:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:28 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 3500949
15/08/19 18:18:28 INFO Executor: Finished task 199.0 in stage 6.0 (TID 523). 2125 bytes result sent to driver
15/08/19 18:18:28 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 533, localhost, ANY, 1769 bytes)
15/08/19 18:18:28 INFO Executor: Running task 9.0 in stage 7.0 (TID 533)
15/08/19 18:18:28 INFO TaskSetManager: Finished task 199.0 in stage 6.0 (TID 523) in 545 ms on localhost (199/200)
15/08/19 18:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 134217728 end: 259054175 length: 124836447 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:28 INFO Executor: Finished task 198.0 in stage 6.0 (TID 522). 2125 bytes result sent to driver
15/08/19 18:18:28 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 534, localhost, ANY, 1758 bytes)
15/08/19 18:18:28 INFO Executor: Running task 10.0 in stage 7.0 (TID 534)
15/08/19 18:18:28 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 3500100
15/08/19 18:18:28 INFO TaskSetManager: Finished task 198.0 in stage 6.0 (TID 522) in 593 ms on localhost (200/200)
15/08/19 18:18:28 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/19 18:18:28 INFO DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:423) finished in 23.675 s
15/08/19 18:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:28 INFO DAGScheduler: looking for newly runnable stages
15/08/19 18:18:28 INFO DAGScheduler: running: Set(ShuffleMapStage 7, ShuffleMapStage 4)
15/08/19 18:18:28 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/19 18:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609919 records.
15/08/19 18:18:28 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@6e4aa580
15/08/19 18:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:28 INFO DAGScheduler: failed: Set()
15/08/19 18:18:28 INFO StatsReportListener: task runtime:(count: 223, mean: 1355.497758, stdev: 3345.256831, max: 14842.000000, min: 133.000000)
15/08/19 18:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:28 INFO StatsReportListener: 	133.0 ms	143.0 ms	153.0 ms	254.0 ms	359.0 ms	440.0 ms	2.4 s	13.4 s	14.8 s
15/08/19 18:18:28 INFO StatsReportListener: shuffle bytes written:(count: 223, mean: 6434665.878924, stdev: 20961961.486039, max: 85005161.000000, min: 0.000000)
15/08/19 18:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:28 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	3.1 KB	3.2 KB	3.2 KB	25.3 MB	80.5 MB	81.1 MB
15/08/19 18:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:28 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List(ShuffleMapStage 4)
15/08/19 18:18:28 INFO StatsReportListener: task result size:(count: 223, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/19 18:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:28 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/19 18:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:18:28 INFO StatsReportListener: executor (non-fetch) time pct: (count: 223, mean: 93.182451, stdev: 4.553558, max: 99.882431, min: 75.609756)
15/08/19 18:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:28 INFO StatsReportListener: 	76 %	85 %	87 %	91 %	94 %	96 %	99 %	100 %	100 %
15/08/19 18:18:28 INFO StatsReportListener: other time pct: (count: 223, mean: 6.817549, stdev: 4.553558, max: 24.390244, min: 0.117569)
15/08/19 18:18:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:28 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 4 %	 6 %	 9 %	13 %	15 %	24 %
15/08/19 18:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:28 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 7)
15/08/19 18:18:28 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3500100
15/08/19 18:18:28 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 3500100
15/08/19 18:18:28 INFO Executor: Finished task 21.0 in stage 4.0 (TID 317). 2125 bytes result sent to driver
15/08/19 18:18:28 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 535, localhost, ANY, 1770 bytes)
15/08/19 18:18:28 INFO Executor: Running task 11.0 in stage 7.0 (TID 535)
15/08/19 18:18:28 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 317) in 15315 ms on localhost (24/28)
15/08/19 18:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 134217728 end: 261069615 length: 126851887 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3663748 records.
15/08/19 18:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:28 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 3500741
15/08/19 18:18:28 INFO Executor: Finished task 22.0 in stage 4.0 (TID 318). 2125 bytes result sent to driver
15/08/19 18:18:28 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 536, localhost, ANY, 1757 bytes)
15/08/19 18:18:28 INFO Executor: Running task 12.0 in stage 7.0 (TID 536)
15/08/19 18:18:28 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 318) in 14280 ms on localhost (25/28)
15/08/19 18:18:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:18:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:29 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 3500100
15/08/19 18:18:29 INFO Executor: Finished task 23.0 in stage 4.0 (TID 319). 2125 bytes result sent to driver
15/08/19 18:18:29 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 537, localhost, ANY, 1771 bytes)
15/08/19 18:18:29 INFO Executor: Running task 13.0 in stage 7.0 (TID 537)
15/08/19 18:18:29 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 319) in 10890 ms on localhost (26/28)
15/08/19 18:18:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 134217728 end: 258787350 length: 124569622 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609688 records.
15/08/19 18:18:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:29 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 3501123
15/08/19 18:18:29 INFO Executor: Finished task 25.0 in stage 4.0 (TID 321). 2125 bytes result sent to driver
15/08/19 18:18:29 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 538, localhost, ANY, 1757 bytes)
15/08/19 18:18:29 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 321) in 10480 ms on localhost (27/28)
15/08/19 18:18:29 INFO Executor: Running task 14.0 in stage 7.0 (TID 538)
15/08/19 18:18:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501133 records.
15/08/19 18:18:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:29 INFO Executor: Finished task 26.0 in stage 4.0 (TID 322). 2125 bytes result sent to driver
15/08/19 18:18:29 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 539, localhost, ANY, 1771 bytes)
15/08/19 18:18:29 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 322) in 10353 ms on localhost (28/28)
15/08/19 18:18:29 INFO Executor: Running task 15.0 in stage 7.0 (TID 539)
15/08/19 18:18:29 INFO DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:423) finished in 25.033 s
15/08/19 18:18:29 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/19 18:18:29 INFO DAGScheduler: looking for newly runnable stages
15/08/19 18:18:29 INFO DAGScheduler: running: Set(ShuffleMapStage 7)
15/08/19 18:18:29 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/19 18:18:29 INFO DAGScheduler: failed: Set()
15/08/19 18:18:29 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3cb9d255
15/08/19 18:18:29 INFO StatsReportListener: task runtime:(count: 5, mean: 12263.600000, stdev: 2102.156474, max: 15315.000000, min: 10353.000000)
15/08/19 18:18:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:29 INFO StatsReportListener: 	10.4 s	10.4 s	10.4 s	10.5 s	10.9 s	14.3 s	15.3 s	15.3 s	15.3 s
15/08/19 18:18:29 INFO StatsReportListener: shuffle bytes written:(count: 5, mean: 84582491.000000, stdev: 839989.801037, max: 85012024.000000, min: 82902559.000000)
15/08/19 18:18:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:29 INFO StatsReportListener: 	79.1 MB	79.1 MB	79.1 MB	81.1 MB	81.1 MB	81.1 MB	81.1 MB	81.1 MB	81.1 MB
15/08/19 18:18:29 INFO StatsReportListener: task result size:(count: 5, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/19 18:18:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:29 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/19 18:18:29 INFO StatsReportListener: executor (non-fetch) time pct: (count: 5, mean: 99.860411, stdev: 0.019576, max: 99.880624, min: 99.826137)
15/08/19 18:18:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:29 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List()
15/08/19 18:18:29 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/19 18:18:29 INFO StatsReportListener: other time pct: (count: 5, mean: 0.139589, stdev: 0.019576, max: 0.173863, min: 0.119376)
15/08/19 18:18:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:18:29 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/19 18:18:29 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 3501133
15/08/19 18:18:29 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 7)
15/08/19 18:18:29 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[33] at processCmd at CliDriver.java:423), which is now runnable
15/08/19 18:18:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 134217728 end: 260413554 length: 126195826 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:18:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:18:29 INFO MemoryStore: ensureFreeSpace(9704) called with curMem=1429891, maxMem=22226833244
15/08/19 18:18:29 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.5 KB, free 20.7 GB)
15/08/19 18:18:29 INFO MemoryStore: ensureFreeSpace(5001) called with curMem=1439595, maxMem=22226833244
15/08/19 18:18:29 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.9 KB, free 20.7 GB)
15/08/19 18:18:29 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:36176 (size: 4.9 KB, free: 20.7 GB)
15/08/19 18:18:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3663081 records.
15/08/19 18:18:29 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874
15/08/19 18:18:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:18:29 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[33] at processCmd at CliDriver.java:423)
15/08/19 18:18:29 INFO TaskSchedulerImpl: Adding task set 5.0 with 200 tasks
15/08/19 18:18:29 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 3502953
15/08/19 18:18:31 INFO InternalParquetRecordReader: Assembled and processed 3503026 records from 2 columns in 3789 ms: 924.5252 rec/ms, 1849.0504 cell/ms
15/08/19 18:18:31 INFO InternalParquetRecordReader: time spent so far 0% reading (34 ms) and 99% processing (3789 ms)
15/08/19 18:18:31 INFO InternalParquetRecordReader: at row 3503026. reading next block
15/08/19 18:18:31 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 106639
15/08/19 18:18:31 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 3827 ms: 914.5806 rec/ms, 1829.1613 cell/ms
15/08/19 18:18:31 INFO InternalParquetRecordReader: time spent so far 1% reading (54 ms) and 98% processing (3827 ms)
15/08/19 18:18:31 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:18:31 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 162566
15/08/19 18:18:31 INFO Executor: Finished task 2.0 in stage 7.0 (TID 526). 2125 bytes result sent to driver
15/08/19 18:18:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 540, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:31 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 526) in 4114 ms on localhost (1/85)
15/08/19 18:18:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 540)
15/08/19 18:18:31 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:31 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:31 INFO InternalParquetRecordReader: Assembled and processed 3503264 records from 2 columns in 3821 ms: 916.8448 rec/ms, 1833.6896 cell/ms
15/08/19 18:18:31 INFO InternalParquetRecordReader: time spent so far 1% reading (64 ms) and 98% processing (3821 ms)
15/08/19 18:18:31 INFO InternalParquetRecordReader: at row 3503264. reading next block
15/08/19 18:18:31 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 223513
15/08/19 18:18:32 INFO Executor: Finished task 1.0 in stage 7.0 (TID 525). 2125 bytes result sent to driver
15/08/19 18:18:32 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 541, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:32 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 525) in 4238 ms on localhost (2/85)
15/08/19 18:18:32 INFO Executor: Running task 1.0 in stage 5.0 (TID 541)
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO Executor: Finished task 3.0 in stage 7.0 (TID 527). 2125 bytes result sent to driver
15/08/19 18:18:32 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 542, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:32 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 527) in 4304 ms on localhost (3/85)
15/08/19 18:18:32 INFO Executor: Running task 2.0 in stage 5.0 (TID 542)
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO Executor: Finished task 6.0 in stage 7.0 (TID 530). 2125 bytes result sent to driver
15/08/19 18:18:32 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 543, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:32 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 530) in 4198 ms on localhost (4/85)
15/08/19 18:18:32 INFO Executor: Running task 3.0 in stage 5.0 (TID 543)
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO InternalParquetRecordReader: Assembled and processed 3500741 records from 2 columns in 3654 ms: 958.0572 rec/ms, 1916.1144 cell/ms
15/08/19 18:18:32 INFO InternalParquetRecordReader: time spent so far 1% reading (48 ms) and 98% processing (3654 ms)
15/08/19 18:18:32 INFO InternalParquetRecordReader: at row 3500741. reading next block
15/08/19 18:18:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 163007
15/08/19 18:18:32 INFO Executor: Finished task 10.0 in stage 7.0 (TID 534). 2125 bytes result sent to driver
15/08/19 18:18:32 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 544, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:32 INFO Executor: Running task 4.0 in stage 5.0 (TID 544)
15/08/19 18:18:32 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 534) in 4057 ms on localhost (5/85)
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO InternalParquetRecordReader: Assembled and processed 3500949 records from 2 columns in 4164 ms: 840.76587 rec/ms, 1681.5317 cell/ms
15/08/19 18:18:32 INFO InternalParquetRecordReader: time spent so far 1% reading (58 ms) and 98% processing (4164 ms)
15/08/19 18:18:32 INFO InternalParquetRecordReader: at row 3500949. reading next block
15/08/19 18:18:32 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 105873
15/08/19 18:18:32 INFO Executor: Finished task 8.0 in stage 7.0 (TID 532). 2125 bytes result sent to driver
15/08/19 18:18:32 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 545, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:32 INFO Executor: Running task 5.0 in stage 5.0 (TID 545)
15/08/19 18:18:32 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 532) in 4604 ms on localhost (6/85)
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4481 ms: 781.09796 rec/ms, 1562.1959 cell/ms
15/08/19 18:18:32 INFO InternalParquetRecordReader: time spent so far 1% reading (65 ms) and 98% processing (4481 ms)
15/08/19 18:18:32 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:18:32 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 109819
15/08/19 18:18:32 INFO InternalParquetRecordReader: Assembled and processed 3502953 records from 2 columns in 3330 ms: 1051.9379 rec/ms, 2103.8757 cell/ms
15/08/19 18:18:32 INFO InternalParquetRecordReader: time spent so far 2% reading (86 ms) and 97% processing (3330 ms)
15/08/19 18:18:32 INFO InternalParquetRecordReader: at row 3502953. reading next block
15/08/19 18:18:32 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 160128
15/08/19 18:18:32 INFO InternalParquetRecordReader: Assembled and processed 3501123 records from 2 columns in 3578 ms: 978.514 rec/ms, 1957.028 cell/ms
15/08/19 18:18:32 INFO InternalParquetRecordReader: time spent so far 0% reading (24 ms) and 99% processing (3578 ms)
15/08/19 18:18:32 INFO InternalParquetRecordReader: at row 3501123. reading next block
15/08/19 18:18:32 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 108565
15/08/19 18:18:32 INFO Executor: Finished task 0.0 in stage 7.0 (TID 524). 2125 bytes result sent to driver
15/08/19 18:18:32 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 546, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:32 INFO Executor: Running task 6.0 in stage 5.0 (TID 546)
15/08/19 18:18:32 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 524) in 5264 ms on localhost (7/85)
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO Executor: Finished task 5.0 in stage 7.0 (TID 529). 2125 bytes result sent to driver
15/08/19 18:18:32 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 547, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:32 INFO Executor: Running task 7.0 in stage 5.0 (TID 547)
15/08/19 18:18:32 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 529) in 4974 ms on localhost (8/85)
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO Executor: Finished task 4.0 in stage 7.0 (TID 528). 2125 bytes result sent to driver
15/08/19 18:18:33 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 548, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:33 INFO Executor: Running task 8.0 in stage 5.0 (TID 548)
15/08/19 18:18:33 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 528) in 5329 ms on localhost (9/85)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:33 INFO Executor: Finished task 11.0 in stage 7.0 (TID 535). 2125 bytes result sent to driver
15/08/19 18:18:33 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 549, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:33 INFO Executor: Running task 9.0 in stage 5.0 (TID 549)
15/08/19 18:18:33 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 535) in 4662 ms on localhost (10/85)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO Executor: Finished task 7.0 in stage 7.0 (TID 531). 2125 bytes result sent to driver
15/08/19 18:18:33 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 550, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:33 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 531) in 5160 ms on localhost (11/85)
15/08/19 18:18:33 INFO Executor: Running task 10.0 in stage 5.0 (TID 550)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO Executor: Finished task 12.0 in stage 7.0 (TID 536). 2125 bytes result sent to driver
15/08/19 18:18:33 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 551, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:33 INFO Executor: Running task 11.0 in stage 5.0 (TID 551)
15/08/19 18:18:33 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 536) in 4320 ms on localhost (12/85)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:33 INFO Executor: Finished task 13.0 in stage 7.0 (TID 537). 2125 bytes result sent to driver
15/08/19 18:18:33 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 552, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:33 INFO Executor: Running task 12.0 in stage 5.0 (TID 552)
15/08/19 18:18:33 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 537) in 4188 ms on localhost (13/85)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO Executor: Finished task 9.0 in stage 7.0 (TID 533). 2125 bytes result sent to driver
15/08/19 18:18:33 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 553, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:33 INFO Executor: Running task 13.0 in stage 5.0 (TID 553)
15/08/19 18:18:33 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 533) in 5312 ms on localhost (14/85)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO Executor: Finished task 15.0 in stage 7.0 (TID 539). 2125 bytes result sent to driver
15/08/19 18:18:33 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 554, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:33 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 539) in 4148 ms on localhost (15/85)
15/08/19 18:18:33 INFO Executor: Running task 14.0 in stage 5.0 (TID 554)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:33 INFO Executor: Finished task 14.0 in stage 7.0 (TID 538). 2125 bytes result sent to driver
15/08/19 18:18:33 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 555, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:33 INFO Executor: Running task 15.0 in stage 5.0 (TID 555)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 538) in 4232 ms on localhost (16/85)
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO Executor: Finished task 0.0 in stage 5.0 (TID 540). 1219 bytes result sent to driver
15/08/19 18:18:34 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 556, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:34 INFO Executor: Running task 16.0 in stage 5.0 (TID 556)
15/08/19 18:18:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 540) in 2564 ms on localhost (1/200)
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO Executor: Finished task 1.0 in stage 5.0 (TID 541). 1219 bytes result sent to driver
15/08/19 18:18:34 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 557, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:34 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 541) in 2514 ms on localhost (2/200)
15/08/19 18:18:34 INFO Executor: Running task 17.0 in stage 5.0 (TID 557)
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO Executor: Finished task 2.0 in stage 5.0 (TID 542). 1219 bytes result sent to driver
15/08/19 18:18:34 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 558, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:34 INFO Executor: Running task 18.0 in stage 5.0 (TID 558)
15/08/19 18:18:34 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 542) in 2663 ms on localhost (3/200)
15/08/19 18:18:34 INFO Executor: Finished task 3.0 in stage 5.0 (TID 543). 1219 bytes result sent to driver
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:34 INFO TaskSetManager: Starting task 19.0 in stage 5.0 (TID 559, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO Executor: Running task 19.0 in stage 5.0 (TID 559)
15/08/19 18:18:34 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 543) in 2581 ms on localhost (4/200)
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO Executor: Finished task 4.0 in stage 5.0 (TID 544). 1219 bytes result sent to driver
15/08/19 18:18:34 INFO TaskSetManager: Starting task 20.0 in stage 5.0 (TID 560, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:34 INFO Executor: Running task 20.0 in stage 5.0 (TID 560)
15/08/19 18:18:34 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 544) in 2712 ms on localhost (5/200)
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:34 INFO Executor: Finished task 5.0 in stage 5.0 (TID 545). 1219 bytes result sent to driver
15/08/19 18:18:34 INFO TaskSetManager: Starting task 21.0 in stage 5.0 (TID 561, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:34 INFO Executor: Running task 21.0 in stage 5.0 (TID 561)
15/08/19 18:18:34 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 545) in 2327 ms on localhost (6/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO Executor: Finished task 7.0 in stage 5.0 (TID 547). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO TaskSetManager: Starting task 22.0 in stage 5.0 (TID 562, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 547) in 2041 ms on localhost (7/200)
15/08/19 18:18:35 INFO Executor: Running task 22.0 in stage 5.0 (TID 562)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO Executor: Finished task 6.0 in stage 5.0 (TID 546). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO TaskSetManager: Starting task 23.0 in stage 5.0 (TID 563, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO Executor: Running task 23.0 in stage 5.0 (TID 563)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 546) in 2123 ms on localhost (8/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO Executor: Finished task 10.0 in stage 5.0 (TID 550). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO Executor: Finished task 9.0 in stage 5.0 (TID 549). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO TaskSetManager: Starting task 24.0 in stage 5.0 (TID 564, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO TaskSetManager: Starting task 25.0 in stage 5.0 (TID 565, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO Executor: Running task 24.0 in stage 5.0 (TID 564)
15/08/19 18:18:35 INFO Executor: Running task 25.0 in stage 5.0 (TID 565)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 550) in 2009 ms on localhost (9/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 549) in 2051 ms on localhost (10/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO Executor: Finished task 11.0 in stage 5.0 (TID 551). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO TaskSetManager: Starting task 26.0 in stage 5.0 (TID 566, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO Executor: Running task 26.0 in stage 5.0 (TID 566)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 551) in 2048 ms on localhost (11/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO Executor: Finished task 14.0 in stage 5.0 (TID 554). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO TaskSetManager: Starting task 27.0 in stage 5.0 (TID 567, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO Executor: Running task 27.0 in stage 5.0 (TID 567)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 554) in 2044 ms on localhost (12/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:35 INFO Executor: Finished task 13.0 in stage 5.0 (TID 553). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO TaskSetManager: Starting task 28.0 in stage 5.0 (TID 568, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO Executor: Running task 28.0 in stage 5.0 (TID 568)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 553) in 2151 ms on localhost (13/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO Executor: Finished task 12.0 in stage 5.0 (TID 552). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO TaskSetManager: Starting task 29.0 in stage 5.0 (TID 569, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO Executor: Running task 29.0 in stage 5.0 (TID 569)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 552) in 2190 ms on localhost (14/200)
15/08/19 18:18:35 INFO Executor: Finished task 15.0 in stage 5.0 (TID 555). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:35 INFO TaskSetManager: Starting task 30.0 in stage 5.0 (TID 570, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO Executor: Running task 30.0 in stage 5.0 (TID 570)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 555) in 2105 ms on localhost (15/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO Executor: Finished task 8.0 in stage 5.0 (TID 548). 1219 bytes result sent to driver
15/08/19 18:18:35 INFO TaskSetManager: Starting task 31.0 in stage 5.0 (TID 571, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:35 INFO Executor: Running task 31.0 in stage 5.0 (TID 571)
15/08/19 18:18:35 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 548) in 2538 ms on localhost (16/200)
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:36 INFO Executor: Finished task 17.0 in stage 5.0 (TID 557). 1219 bytes result sent to driver
15/08/19 18:18:36 INFO TaskSetManager: Starting task 32.0 in stage 5.0 (TID 572, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:36 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 557) in 1771 ms on localhost (17/200)
15/08/19 18:18:36 INFO Executor: Running task 32.0 in stage 5.0 (TID 572)
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO Executor: Finished task 16.0 in stage 5.0 (TID 556). 1219 bytes result sent to driver
15/08/19 18:18:36 INFO TaskSetManager: Starting task 33.0 in stage 5.0 (TID 573, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:36 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 556) in 1904 ms on localhost (18/200)
15/08/19 18:18:36 INFO Executor: Running task 33.0 in stage 5.0 (TID 573)
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO Executor: Finished task 18.0 in stage 5.0 (TID 558). 1219 bytes result sent to driver
15/08/19 18:18:36 INFO Executor: Finished task 19.0 in stage 5.0 (TID 559). 1219 bytes result sent to driver
15/08/19 18:18:36 INFO TaskSetManager: Starting task 34.0 in stage 5.0 (TID 574, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:36 INFO TaskSetManager: Starting task 35.0 in stage 5.0 (TID 575, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:36 INFO Executor: Running task 35.0 in stage 5.0 (TID 575)
15/08/19 18:18:36 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 558) in 1996 ms on localhost (19/200)
15/08/19 18:18:36 INFO TaskSetManager: Finished task 19.0 in stage 5.0 (TID 559) in 1989 ms on localhost (20/200)
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO Executor: Running task 34.0 in stage 5.0 (TID 574)
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO Executor: Finished task 21.0 in stage 5.0 (TID 561). 1219 bytes result sent to driver
15/08/19 18:18:36 INFO TaskSetManager: Starting task 36.0 in stage 5.0 (TID 576, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:36 INFO Executor: Running task 36.0 in stage 5.0 (TID 576)
15/08/19 18:18:36 INFO TaskSetManager: Finished task 21.0 in stage 5.0 (TID 561) in 1873 ms on localhost (21/200)
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 20.0 in stage 5.0 (TID 560). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 37.0 in stage 5.0 (TID 577, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO Executor: Running task 37.0 in stage 5.0 (TID 577)
15/08/19 18:18:37 INFO TaskSetManager: Finished task 20.0 in stage 5.0 (TID 560) in 2039 ms on localhost (22/200)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 23.0 in stage 5.0 (TID 563). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 38.0 in stage 5.0 (TID 578, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO Executor: Running task 38.0 in stage 5.0 (TID 578)
15/08/19 18:18:37 INFO TaskSetManager: Finished task 23.0 in stage 5.0 (TID 563) in 1976 ms on localhost (23/200)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 22.0 in stage 5.0 (TID 562). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 39.0 in stage 5.0 (TID 579, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO Executor: Running task 39.0 in stage 5.0 (TID 579)
15/08/19 18:18:37 INFO TaskSetManager: Finished task 22.0 in stage 5.0 (TID 562) in 2117 ms on localhost (24/200)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 25.0 in stage 5.0 (TID 565). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 40.0 in stage 5.0 (TID 580, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO Executor: Running task 40.0 in stage 5.0 (TID 580)
15/08/19 18:18:37 INFO TaskSetManager: Finished task 25.0 in stage 5.0 (TID 565) in 2069 ms on localhost (25/200)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 26.0 in stage 5.0 (TID 566). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 41.0 in stage 5.0 (TID 581, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO Executor: Running task 41.0 in stage 5.0 (TID 581)
15/08/19 18:18:37 INFO TaskSetManager: Finished task 26.0 in stage 5.0 (TID 566) in 2144 ms on localhost (26/200)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 24.0 in stage 5.0 (TID 564). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 42.0 in stage 5.0 (TID 582, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO TaskSetManager: Finished task 24.0 in stage 5.0 (TID 564) in 2251 ms on localhost (27/200)
15/08/19 18:18:37 INFO Executor: Running task 42.0 in stage 5.0 (TID 582)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 27.0 in stage 5.0 (TID 567). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 43.0 in stage 5.0 (TID 583, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO Executor: Running task 43.0 in stage 5.0 (TID 583)
15/08/19 18:18:37 INFO TaskSetManager: Finished task 27.0 in stage 5.0 (TID 567) in 1942 ms on localhost (28/200)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 29.0 in stage 5.0 (TID 569). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 44.0 in stage 5.0 (TID 584, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO Executor: Running task 44.0 in stage 5.0 (TID 584)
15/08/19 18:18:37 INFO TaskSetManager: Finished task 29.0 in stage 5.0 (TID 569) in 2128 ms on localhost (29/200)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO Executor: Finished task 31.0 in stage 5.0 (TID 571). 1219 bytes result sent to driver
15/08/19 18:18:37 INFO TaskSetManager: Starting task 45.0 in stage 5.0 (TID 585, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:37 INFO Executor: Running task 45.0 in stage 5.0 (TID 585)
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:37 INFO TaskSetManager: Finished task 31.0 in stage 5.0 (TID 571) in 2228 ms on localhost (30/200)
15/08/19 18:18:38 INFO Executor: Finished task 30.0 in stage 5.0 (TID 570). 1219 bytes result sent to driver
15/08/19 18:18:38 INFO TaskSetManager: Starting task 46.0 in stage 5.0 (TID 586, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:38 INFO Executor: Running task 46.0 in stage 5.0 (TID 586)
15/08/19 18:18:38 INFO TaskSetManager: Finished task 30.0 in stage 5.0 (TID 570) in 2425 ms on localhost (31/200)
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:38 INFO Executor: Finished task 28.0 in stage 5.0 (TID 568). 1219 bytes result sent to driver
15/08/19 18:18:38 INFO TaskSetManager: Starting task 47.0 in stage 5.0 (TID 587, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:38 INFO Executor: Running task 47.0 in stage 5.0 (TID 587)
15/08/19 18:18:38 INFO TaskSetManager: Finished task 28.0 in stage 5.0 (TID 568) in 2535 ms on localhost (32/200)
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:38 INFO Executor: Finished task 32.0 in stage 5.0 (TID 572). 1219 bytes result sent to driver
15/08/19 18:18:38 INFO TaskSetManager: Starting task 48.0 in stage 5.0 (TID 588, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:38 INFO Executor: Running task 48.0 in stage 5.0 (TID 588)
15/08/19 18:18:38 INFO TaskSetManager: Finished task 32.0 in stage 5.0 (TID 572) in 2085 ms on localhost (33/200)
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:39 INFO Executor: Finished task 35.0 in stage 5.0 (TID 575). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO Executor: Finished task 33.0 in stage 5.0 (TID 573). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 49.0 in stage 5.0 (TID 589, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO TaskSetManager: Starting task 50.0 in stage 5.0 (TID 590, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO Executor: Running task 49.0 in stage 5.0 (TID 589)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 35.0 in stage 5.0 (TID 575) in 2346 ms on localhost (34/200)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 33.0 in stage 5.0 (TID 573) in 2748 ms on localhost (35/200)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Running task 50.0 in stage 5.0 (TID 590)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Finished task 36.0 in stage 5.0 (TID 576). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 51.0 in stage 5.0 (TID 591, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO Executor: Running task 51.0 in stage 5.0 (TID 591)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 36.0 in stage 5.0 (TID 576) in 2337 ms on localhost (36/200)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Finished task 38.0 in stage 5.0 (TID 578). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 52.0 in stage 5.0 (TID 592, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO Executor: Running task 52.0 in stage 5.0 (TID 592)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 38.0 in stage 5.0 (TID 578) in 2364 ms on localhost (37/200)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Finished task 40.0 in stage 5.0 (TID 580). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 53.0 in stage 5.0 (TID 593, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO Executor: Running task 53.0 in stage 5.0 (TID 593)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 40.0 in stage 5.0 (TID 580) in 2271 ms on localhost (38/200)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Finished task 37.0 in stage 5.0 (TID 577). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 54.0 in stage 5.0 (TID 594, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO Executor: Running task 54.0 in stage 5.0 (TID 594)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 37.0 in stage 5.0 (TID 577) in 2556 ms on localhost (39/200)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Finished task 34.0 in stage 5.0 (TID 574). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 55.0 in stage 5.0 (TID 595, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 34.0 in stage 5.0 (TID 574) in 2832 ms on localhost (40/200)
15/08/19 18:18:39 INFO Executor: Running task 55.0 in stage 5.0 (TID 595)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Finished task 41.0 in stage 5.0 (TID 581). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 56.0 in stage 5.0 (TID 596, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 41.0 in stage 5.0 (TID 581) in 2279 ms on localhost (41/200)
15/08/19 18:18:39 INFO Executor: Running task 56.0 in stage 5.0 (TID 596)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Finished task 42.0 in stage 5.0 (TID 582). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 57.0 in stage 5.0 (TID 597, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 42.0 in stage 5.0 (TID 582) in 2443 ms on localhost (42/200)
15/08/19 18:18:39 INFO Executor: Running task 57.0 in stage 5.0 (TID 597)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO Executor: Finished task 39.0 in stage 5.0 (TID 579). 1219 bytes result sent to driver
15/08/19 18:18:39 INFO TaskSetManager: Starting task 58.0 in stage 5.0 (TID 598, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:39 INFO Executor: Running task 58.0 in stage 5.0 (TID 598)
15/08/19 18:18:39 INFO TaskSetManager: Finished task 39.0 in stage 5.0 (TID 579) in 2800 ms on localhost (43/200)
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO Executor: Finished task 43.0 in stage 5.0 (TID 583). 1219 bytes result sent to driver
15/08/19 18:18:40 INFO TaskSetManager: Starting task 59.0 in stage 5.0 (TID 599, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:40 INFO TaskSetManager: Finished task 43.0 in stage 5.0 (TID 583) in 2499 ms on localhost (44/200)
15/08/19 18:18:40 INFO Executor: Running task 59.0 in stage 5.0 (TID 599)
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO Executor: Finished task 45.0 in stage 5.0 (TID 585). 1219 bytes result sent to driver
15/08/19 18:18:40 INFO TaskSetManager: Starting task 60.0 in stage 5.0 (TID 600, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:40 INFO Executor: Running task 60.0 in stage 5.0 (TID 600)
15/08/19 18:18:40 INFO TaskSetManager: Finished task 45.0 in stage 5.0 (TID 585) in 2246 ms on localhost (45/200)
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO Executor: Finished task 44.0 in stage 5.0 (TID 584). 1219 bytes result sent to driver
15/08/19 18:18:40 INFO TaskSetManager: Starting task 61.0 in stage 5.0 (TID 601, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:40 INFO TaskSetManager: Finished task 44.0 in stage 5.0 (TID 584) in 2381 ms on localhost (46/200)
15/08/19 18:18:40 INFO Executor: Running task 61.0 in stage 5.0 (TID 601)
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO Executor: Finished task 46.0 in stage 5.0 (TID 586). 1219 bytes result sent to driver
15/08/19 18:18:40 INFO TaskSetManager: Starting task 62.0 in stage 5.0 (TID 602, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:40 INFO Executor: Running task 62.0 in stage 5.0 (TID 602)
15/08/19 18:18:40 INFO TaskSetManager: Finished task 46.0 in stage 5.0 (TID 586) in 2107 ms on localhost (47/200)
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO Executor: Finished task 47.0 in stage 5.0 (TID 587). 1219 bytes result sent to driver
15/08/19 18:18:40 INFO TaskSetManager: Starting task 63.0 in stage 5.0 (TID 603, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:40 INFO TaskSetManager: Finished task 47.0 in stage 5.0 (TID 587) in 2257 ms on localhost (48/200)
15/08/19 18:18:40 INFO Executor: Running task 63.0 in stage 5.0 (TID 603)
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:40 INFO Executor: Finished task 48.0 in stage 5.0 (TID 588). 1219 bytes result sent to driver
15/08/19 18:18:40 INFO TaskSetManager: Starting task 64.0 in stage 5.0 (TID 604, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:40 INFO Executor: Running task 64.0 in stage 5.0 (TID 604)
15/08/19 18:18:40 INFO TaskSetManager: Finished task 48.0 in stage 5.0 (TID 588) in 2206 ms on localhost (49/200)
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:41 INFO Executor: Finished task 50.0 in stage 5.0 (TID 590). 1219 bytes result sent to driver
15/08/19 18:18:41 INFO TaskSetManager: Starting task 65.0 in stage 5.0 (TID 605, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:41 INFO Executor: Running task 65.0 in stage 5.0 (TID 605)
15/08/19 18:18:41 INFO TaskSetManager: Finished task 50.0 in stage 5.0 (TID 590) in 2143 ms on localhost (50/200)
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:41 INFO Executor: Finished task 53.0 in stage 5.0 (TID 593). 1219 bytes result sent to driver
15/08/19 18:18:41 INFO TaskSetManager: Starting task 66.0 in stage 5.0 (TID 606, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:41 INFO Executor: Running task 66.0 in stage 5.0 (TID 606)
15/08/19 18:18:41 INFO TaskSetManager: Finished task 53.0 in stage 5.0 (TID 593) in 1783 ms on localhost (51/200)
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:41 INFO Executor: Finished task 51.0 in stage 5.0 (TID 591). 1219 bytes result sent to driver
15/08/19 18:18:41 INFO TaskSetManager: Starting task 67.0 in stage 5.0 (TID 607, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:41 INFO Executor: Running task 67.0 in stage 5.0 (TID 607)
15/08/19 18:18:41 INFO TaskSetManager: Finished task 51.0 in stage 5.0 (TID 591) in 2636 ms on localhost (52/200)
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:41 INFO Executor: Finished task 52.0 in stage 5.0 (TID 592). 1219 bytes result sent to driver
15/08/19 18:18:41 INFO TaskSetManager: Starting task 68.0 in stage 5.0 (TID 608, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:41 INFO Executor: Running task 68.0 in stage 5.0 (TID 608)
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:41 INFO TaskSetManager: Finished task 52.0 in stage 5.0 (TID 592) in 2457 ms on localhost (53/200)
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:41 INFO Executor: Finished task 49.0 in stage 5.0 (TID 589). 1219 bytes result sent to driver
15/08/19 18:18:41 INFO TaskSetManager: Starting task 69.0 in stage 5.0 (TID 609, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:41 INFO Executor: Running task 69.0 in stage 5.0 (TID 609)
15/08/19 18:18:41 INFO TaskSetManager: Finished task 49.0 in stage 5.0 (TID 589) in 2821 ms on localhost (54/200)
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO Executor: Finished task 55.0 in stage 5.0 (TID 595). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 70.0 in stage 5.0 (TID 610, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO Executor: Running task 70.0 in stage 5.0 (TID 610)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 55.0 in stage 5.0 (TID 595) in 2482 ms on localhost (55/200)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO Executor: Finished task 54.0 in stage 5.0 (TID 594). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO Executor: Finished task 56.0 in stage 5.0 (TID 596). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 71.0 in stage 5.0 (TID 611, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO TaskSetManager: Starting task 72.0 in stage 5.0 (TID 612, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 54.0 in stage 5.0 (TID 594) in 2530 ms on localhost (56/200)
15/08/19 18:18:42 INFO Executor: Running task 72.0 in stage 5.0 (TID 612)
15/08/19 18:18:42 INFO Executor: Running task 71.0 in stage 5.0 (TID 611)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 56.0 in stage 5.0 (TID 596) in 2419 ms on localhost (57/200)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:42 INFO Executor: Finished task 61.0 in stage 5.0 (TID 601). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 73.0 in stage 5.0 (TID 613, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO Executor: Running task 73.0 in stage 5.0 (TID 613)
15/08/19 18:18:42 INFO Executor: Finished task 58.0 in stage 5.0 (TID 598). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 74.0 in stage 5.0 (TID 614, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO Executor: Running task 74.0 in stage 5.0 (TID 614)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO TaskSetManager: Finished task 58.0 in stage 5.0 (TID 598) in 2202 ms on localhost (58/200)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 61.0 in stage 5.0 (TID 601) in 1957 ms on localhost (59/200)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO Executor: Finished task 57.0 in stage 5.0 (TID 597). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 75.0 in stage 5.0 (TID 615, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO Executor: Running task 75.0 in stage 5.0 (TID 615)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 57.0 in stage 5.0 (TID 597) in 2432 ms on localhost (60/200)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO Executor: Finished task 60.0 in stage 5.0 (TID 600). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 76.0 in stage 5.0 (TID 616, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO Executor: Running task 76.0 in stage 5.0 (TID 616)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 60.0 in stage 5.0 (TID 600) in 2226 ms on localhost (61/200)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO Executor: Finished task 62.0 in stage 5.0 (TID 602). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO Executor: Finished task 59.0 in stage 5.0 (TID 599). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 77.0 in stage 5.0 (TID 617, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO TaskSetManager: Starting task 78.0 in stage 5.0 (TID 618, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO Executor: Running task 78.0 in stage 5.0 (TID 618)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 59.0 in stage 5.0 (TID 599) in 2382 ms on localhost (62/200)
15/08/19 18:18:42 INFO Executor: Running task 77.0 in stage 5.0 (TID 617)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 62.0 in stage 5.0 (TID 602) in 2209 ms on localhost (63/200)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO Executor: Finished task 63.0 in stage 5.0 (TID 603). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 79.0 in stage 5.0 (TID 619, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO Executor: Running task 79.0 in stage 5.0 (TID 619)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 63.0 in stage 5.0 (TID 603) in 2056 ms on localhost (64/200)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO Executor: Finished task 64.0 in stage 5.0 (TID 604). 1219 bytes result sent to driver
15/08/19 18:18:42 INFO TaskSetManager: Starting task 80.0 in stage 5.0 (TID 620, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:42 INFO TaskSetManager: Finished task 64.0 in stage 5.0 (TID 604) in 2259 ms on localhost (65/200)
15/08/19 18:18:42 INFO Executor: Running task 80.0 in stage 5.0 (TID 620)
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:43 INFO Executor: Finished task 66.0 in stage 5.0 (TID 606). 1219 bytes result sent to driver
15/08/19 18:18:43 INFO TaskSetManager: Starting task 81.0 in stage 5.0 (TID 621, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:43 INFO Executor: Running task 81.0 in stage 5.0 (TID 621)
15/08/19 18:18:43 INFO TaskSetManager: Finished task 66.0 in stage 5.0 (TID 606) in 2165 ms on localhost (66/200)
15/08/19 18:18:43 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:43 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:43 INFO Executor: Finished task 65.0 in stage 5.0 (TID 605). 1219 bytes result sent to driver
15/08/19 18:18:43 INFO TaskSetManager: Starting task 82.0 in stage 5.0 (TID 622, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:43 INFO Executor: Running task 82.0 in stage 5.0 (TID 622)
15/08/19 18:18:43 INFO TaskSetManager: Finished task 65.0 in stage 5.0 (TID 605) in 2304 ms on localhost (67/200)
15/08/19 18:18:43 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:43 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO Executor: Finished task 68.0 in stage 5.0 (TID 608). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 83.0 in stage 5.0 (TID 623, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 83.0 in stage 5.0 (TID 623)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 68.0 in stage 5.0 (TID 608) in 2180 ms on localhost (68/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO Executor: Finished task 71.0 in stage 5.0 (TID 611). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 84.0 in stage 5.0 (TID 624, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 84.0 in stage 5.0 (TID 624)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 71.0 in stage 5.0 (TID 611) in 2058 ms on localhost (69/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO Executor: Finished task 67.0 in stage 5.0 (TID 607). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 85.0 in stage 5.0 (TID 625, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 85.0 in stage 5.0 (TID 625)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 67.0 in stage 5.0 (TID 607) in 2505 ms on localhost (70/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO Executor: Finished task 72.0 in stage 5.0 (TID 612). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 86.0 in stage 5.0 (TID 626, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 86.0 in stage 5.0 (TID 626)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 72.0 in stage 5.0 (TID 612) in 2358 ms on localhost (71/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO Executor: Finished task 73.0 in stage 5.0 (TID 613). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 87.0 in stage 5.0 (TID 627, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 87.0 in stage 5.0 (TID 627)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 73.0 in stage 5.0 (TID 613) in 2491 ms on localhost (72/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO Executor: Finished task 74.0 in stage 5.0 (TID 614). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 88.0 in stage 5.0 (TID 628, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 88.0 in stage 5.0 (TID 628)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO TaskSetManager: Finished task 74.0 in stage 5.0 (TID 614) in 2510 ms on localhost (73/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO Executor: Finished task 75.0 in stage 5.0 (TID 615). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 89.0 in stage 5.0 (TID 629, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 89.0 in stage 5.0 (TID 629)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 75.0 in stage 5.0 (TID 615) in 2414 ms on localhost (74/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO Executor: Finished task 70.0 in stage 5.0 (TID 610). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 90.0 in stage 5.0 (TID 630, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 70.0 in stage 5.0 (TID 610) in 2702 ms on localhost (75/200)
15/08/19 18:18:44 INFO Executor: Running task 90.0 in stage 5.0 (TID 630)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO Executor: Finished task 69.0 in stage 5.0 (TID 609). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 91.0 in stage 5.0 (TID 631, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 91.0 in stage 5.0 (TID 631)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 69.0 in stage 5.0 (TID 609) in 2889 ms on localhost (76/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO Executor: Finished task 78.0 in stage 5.0 (TID 618). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 92.0 in stage 5.0 (TID 632, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 78.0 in stage 5.0 (TID 618) in 2428 ms on localhost (77/200)
15/08/19 18:18:44 INFO Executor: Running task 92.0 in stage 5.0 (TID 632)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO Executor: Finished task 76.0 in stage 5.0 (TID 616). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 93.0 in stage 5.0 (TID 633, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 93.0 in stage 5.0 (TID 633)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 76.0 in stage 5.0 (TID 616) in 2521 ms on localhost (78/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO Executor: Finished task 79.0 in stage 5.0 (TID 619). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 94.0 in stage 5.0 (TID 634, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 94.0 in stage 5.0 (TID 634)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO TaskSetManager: Finished task 79.0 in stage 5.0 (TID 619) in 2420 ms on localhost (79/200)
15/08/19 18:18:44 INFO Executor: Finished task 80.0 in stage 5.0 (TID 620). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Starting task 95.0 in stage 5.0 (TID 635, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 95.0 in stage 5.0 (TID 635)
15/08/19 18:18:44 INFO Executor: Finished task 77.0 in stage 5.0 (TID 617). 1219 bytes result sent to driver
15/08/19 18:18:44 INFO TaskSetManager: Finished task 80.0 in stage 5.0 (TID 620) in 2066 ms on localhost (80/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO TaskSetManager: Starting task 96.0 in stage 5.0 (TID 636, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:44 INFO Executor: Running task 96.0 in stage 5.0 (TID 636)
15/08/19 18:18:44 INFO TaskSetManager: Finished task 77.0 in stage 5.0 (TID 617) in 2533 ms on localhost (81/200)
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:45 INFO Executor: Finished task 81.0 in stage 5.0 (TID 621). 1219 bytes result sent to driver
15/08/19 18:18:45 INFO Executor: Finished task 82.0 in stage 5.0 (TID 622). 1219 bytes result sent to driver
15/08/19 18:18:45 INFO TaskSetManager: Starting task 97.0 in stage 5.0 (TID 637, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:45 INFO Executor: Running task 97.0 in stage 5.0 (TID 637)
15/08/19 18:18:45 INFO TaskSetManager: Starting task 98.0 in stage 5.0 (TID 638, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:45 INFO Executor: Running task 98.0 in stage 5.0 (TID 638)
15/08/19 18:18:45 INFO TaskSetManager: Finished task 81.0 in stage 5.0 (TID 621) in 2280 ms on localhost (82/200)
15/08/19 18:18:45 INFO TaskSetManager: Finished task 82.0 in stage 5.0 (TID 622) in 2207 ms on localhost (83/200)
15/08/19 18:18:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:45 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:45 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:45 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO Executor: Finished task 85.0 in stage 5.0 (TID 625). 1219 bytes result sent to driver
15/08/19 18:18:46 INFO TaskSetManager: Starting task 99.0 in stage 5.0 (TID 639, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:46 INFO Executor: Running task 99.0 in stage 5.0 (TID 639)
15/08/19 18:18:46 INFO TaskSetManager: Finished task 85.0 in stage 5.0 (TID 625) in 1729 ms on localhost (84/200)
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO Executor: Finished task 84.0 in stage 5.0 (TID 624). 1219 bytes result sent to driver
15/08/19 18:18:46 INFO TaskSetManager: Starting task 100.0 in stage 5.0 (TID 640, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:46 INFO Executor: Running task 100.0 in stage 5.0 (TID 640)
15/08/19 18:18:46 INFO TaskSetManager: Finished task 84.0 in stage 5.0 (TID 624) in 1968 ms on localhost (85/200)
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO Executor: Finished task 83.0 in stage 5.0 (TID 623). 1219 bytes result sent to driver
15/08/19 18:18:46 INFO TaskSetManager: Starting task 101.0 in stage 5.0 (TID 641, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:46 INFO Executor: Running task 101.0 in stage 5.0 (TID 641)
15/08/19 18:18:46 INFO TaskSetManager: Finished task 83.0 in stage 5.0 (TID 623) in 2336 ms on localhost (86/200)
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO Executor: Finished task 88.0 in stage 5.0 (TID 628). 1219 bytes result sent to driver
15/08/19 18:18:46 INFO TaskSetManager: Starting task 102.0 in stage 5.0 (TID 642, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:46 INFO Executor: Running task 102.0 in stage 5.0 (TID 642)
15/08/19 18:18:46 INFO TaskSetManager: Finished task 88.0 in stage 5.0 (TID 628) in 2022 ms on localhost (87/200)
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO Executor: Finished task 86.0 in stage 5.0 (TID 626). 1219 bytes result sent to driver
15/08/19 18:18:46 INFO Executor: Finished task 89.0 in stage 5.0 (TID 629). 1219 bytes result sent to driver
15/08/19 18:18:46 INFO TaskSetManager: Starting task 103.0 in stage 5.0 (TID 643, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:46 INFO Executor: Running task 103.0 in stage 5.0 (TID 643)
15/08/19 18:18:46 INFO TaskSetManager: Starting task 104.0 in stage 5.0 (TID 644, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:46 INFO Executor: Running task 104.0 in stage 5.0 (TID 644)
15/08/19 18:18:46 INFO TaskSetManager: Finished task 86.0 in stage 5.0 (TID 626) in 2276 ms on localhost (88/200)
15/08/19 18:18:46 INFO TaskSetManager: Finished task 89.0 in stage 5.0 (TID 629) in 1984 ms on localhost (89/200)
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO Executor: Finished task 95.0 in stage 5.0 (TID 635). 1219 bytes result sent to driver
15/08/19 18:18:46 INFO TaskSetManager: Starting task 105.0 in stage 5.0 (TID 645, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:46 INFO Executor: Running task 105.0 in stage 5.0 (TID 645)
15/08/19 18:18:46 INFO TaskSetManager: Finished task 95.0 in stage 5.0 (TID 635) in 1939 ms on localhost (90/200)
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO Executor: Finished task 87.0 in stage 5.0 (TID 627). 1219 bytes result sent to driver
15/08/19 18:18:47 INFO TaskSetManager: Starting task 106.0 in stage 5.0 (TID 646, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:47 INFO Executor: Running task 106.0 in stage 5.0 (TID 646)
15/08/19 18:18:47 INFO TaskSetManager: Finished task 87.0 in stage 5.0 (TID 627) in 2534 ms on localhost (91/200)
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO Executor: Finished task 90.0 in stage 5.0 (TID 630). 1219 bytes result sent to driver
15/08/19 18:18:47 INFO TaskSetManager: Starting task 107.0 in stage 5.0 (TID 647, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:47 INFO Executor: Running task 107.0 in stage 5.0 (TID 647)
15/08/19 18:18:47 INFO TaskSetManager: Finished task 90.0 in stage 5.0 (TID 630) in 2384 ms on localhost (92/200)
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO Executor: Finished task 93.0 in stage 5.0 (TID 633). 1219 bytes result sent to driver
15/08/19 18:18:47 INFO TaskSetManager: Starting task 108.0 in stage 5.0 (TID 648, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:47 INFO Executor: Running task 108.0 in stage 5.0 (TID 648)
15/08/19 18:18:47 INFO TaskSetManager: Finished task 93.0 in stage 5.0 (TID 633) in 2456 ms on localhost (93/200)
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO Executor: Finished task 96.0 in stage 5.0 (TID 636). 1219 bytes result sent to driver
15/08/19 18:18:47 INFO TaskSetManager: Starting task 109.0 in stage 5.0 (TID 649, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:47 INFO Executor: Running task 109.0 in stage 5.0 (TID 649)
15/08/19 18:18:47 INFO TaskSetManager: Finished task 96.0 in stage 5.0 (TID 636) in 2562 ms on localhost (94/200)
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO Executor: Finished task 94.0 in stage 5.0 (TID 634). 1219 bytes result sent to driver
15/08/19 18:18:47 INFO TaskSetManager: Starting task 110.0 in stage 5.0 (TID 650, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:47 INFO Executor: Running task 110.0 in stage 5.0 (TID 650)
15/08/19 18:18:47 INFO TaskSetManager: Finished task 94.0 in stage 5.0 (TID 634) in 2637 ms on localhost (95/200)
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO Executor: Finished task 91.0 in stage 5.0 (TID 631). 1219 bytes result sent to driver
15/08/19 18:18:47 INFO TaskSetManager: Starting task 111.0 in stage 5.0 (TID 651, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:47 INFO Executor: Running task 111.0 in stage 5.0 (TID 651)
15/08/19 18:18:47 INFO TaskSetManager: Finished task 91.0 in stage 5.0 (TID 631) in 2726 ms on localhost (96/200)
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO Executor: Finished task 92.0 in stage 5.0 (TID 632). 1219 bytes result sent to driver
15/08/19 18:18:47 INFO TaskSetManager: Starting task 112.0 in stage 5.0 (TID 652, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:47 INFO TaskSetManager: Finished task 92.0 in stage 5.0 (TID 632) in 2732 ms on localhost (97/200)
15/08/19 18:18:47 INFO Executor: Running task 112.0 in stage 5.0 (TID 652)
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:47 INFO Executor: Finished task 98.0 in stage 5.0 (TID 638). 1219 bytes result sent to driver
15/08/19 18:18:47 INFO TaskSetManager: Starting task 113.0 in stage 5.0 (TID 653, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:47 INFO Executor: Running task 113.0 in stage 5.0 (TID 653)
15/08/19 18:18:47 INFO TaskSetManager: Finished task 98.0 in stage 5.0 (TID 638) in 2218 ms on localhost (98/200)
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:48 INFO Executor: Finished task 99.0 in stage 5.0 (TID 639). 1219 bytes result sent to driver
15/08/19 18:18:48 INFO Executor: Finished task 97.0 in stage 5.0 (TID 637). 1219 bytes result sent to driver
15/08/19 18:18:48 INFO TaskSetManager: Starting task 114.0 in stage 5.0 (TID 654, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:48 INFO TaskSetManager: Starting task 115.0 in stage 5.0 (TID 655, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:48 INFO TaskSetManager: Finished task 97.0 in stage 5.0 (TID 637) in 2627 ms on localhost (99/200)
15/08/19 18:18:48 INFO Executor: Running task 114.0 in stage 5.0 (TID 654)
15/08/19 18:18:48 INFO TaskSetManager: Finished task 99.0 in stage 5.0 (TID 639) in 2323 ms on localhost (100/200)
15/08/19 18:18:48 INFO Executor: Running task 115.0 in stage 5.0 (TID 655)
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:48 INFO Executor: Finished task 100.0 in stage 5.0 (TID 640). 1219 bytes result sent to driver
15/08/19 18:18:48 INFO TaskSetManager: Starting task 116.0 in stage 5.0 (TID 656, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:48 INFO Executor: Running task 116.0 in stage 5.0 (TID 656)
15/08/19 18:18:48 INFO TaskSetManager: Finished task 100.0 in stage 5.0 (TID 640) in 2346 ms on localhost (101/200)
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:48 INFO Executor: Finished task 101.0 in stage 5.0 (TID 641). 1219 bytes result sent to driver
15/08/19 18:18:48 INFO TaskSetManager: Starting task 117.0 in stage 5.0 (TID 657, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:48 INFO Executor: Running task 117.0 in stage 5.0 (TID 657)
15/08/19 18:18:48 INFO TaskSetManager: Finished task 101.0 in stage 5.0 (TID 641) in 2244 ms on localhost (102/200)
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:48 INFO Executor: Finished task 102.0 in stage 5.0 (TID 642). 1219 bytes result sent to driver
15/08/19 18:18:48 INFO TaskSetManager: Starting task 118.0 in stage 5.0 (TID 658, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:48 INFO Executor: Running task 118.0 in stage 5.0 (TID 658)
15/08/19 18:18:48 INFO TaskSetManager: Finished task 102.0 in stage 5.0 (TID 642) in 2145 ms on localhost (103/200)
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 103.0 in stage 5.0 (TID 643). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 119.0 in stage 5.0 (TID 659, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 119.0 in stage 5.0 (TID 659)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 103.0 in stage 5.0 (TID 643) in 2350 ms on localhost (104/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 105.0 in stage 5.0 (TID 645). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 120.0 in stage 5.0 (TID 660, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 120.0 in stage 5.0 (TID 660)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 105.0 in stage 5.0 (TID 645) in 2246 ms on localhost (105/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 104.0 in stage 5.0 (TID 644). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 121.0 in stage 5.0 (TID 661, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 121.0 in stage 5.0 (TID 661)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 104.0 in stage 5.0 (TID 644) in 2478 ms on localhost (106/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 107.0 in stage 5.0 (TID 647). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 122.0 in stage 5.0 (TID 662, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 122.0 in stage 5.0 (TID 662)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 107.0 in stage 5.0 (TID 647) in 2073 ms on localhost (107/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 108.0 in stage 5.0 (TID 648). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 123.0 in stage 5.0 (TID 663, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 123.0 in stage 5.0 (TID 663)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 108.0 in stage 5.0 (TID 648) in 2112 ms on localhost (108/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 106.0 in stage 5.0 (TID 646). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 124.0 in stage 5.0 (TID 664, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 124.0 in stage 5.0 (TID 664)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 106.0 in stage 5.0 (TID 646) in 2333 ms on localhost (109/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 109.0 in stage 5.0 (TID 649). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 125.0 in stage 5.0 (TID 665, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 125.0 in stage 5.0 (TID 665)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 109.0 in stage 5.0 (TID 649) in 2060 ms on localhost (110/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 112.0 in stage 5.0 (TID 652). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 126.0 in stage 5.0 (TID 666, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 126.0 in stage 5.0 (TID 666)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 112.0 in stage 5.0 (TID 652) in 2012 ms on localhost (111/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 110.0 in stage 5.0 (TID 650). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 127.0 in stage 5.0 (TID 667, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 127.0 in stage 5.0 (TID 667)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 110.0 in stage 5.0 (TID 650) in 2124 ms on localhost (112/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO Executor: Finished task 111.0 in stage 5.0 (TID 651). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 128.0 in stage 5.0 (TID 668, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 128.0 in stage 5.0 (TID 668)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 111.0 in stage 5.0 (TID 651) in 2128 ms on localhost (113/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:49 INFO Executor: Finished task 113.0 in stage 5.0 (TID 653). 1219 bytes result sent to driver
15/08/19 18:18:49 INFO TaskSetManager: Starting task 129.0 in stage 5.0 (TID 669, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:49 INFO Executor: Running task 129.0 in stage 5.0 (TID 669)
15/08/19 18:18:49 INFO TaskSetManager: Finished task 113.0 in stage 5.0 (TID 653) in 1943 ms on localhost (114/200)
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO Executor: Finished task 114.0 in stage 5.0 (TID 654). 1219 bytes result sent to driver
15/08/19 18:18:50 INFO TaskSetManager: Starting task 130.0 in stage 5.0 (TID 670, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:50 INFO TaskSetManager: Finished task 114.0 in stage 5.0 (TID 654) in 1970 ms on localhost (115/200)
15/08/19 18:18:50 INFO Executor: Running task 130.0 in stage 5.0 (TID 670)
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO Executor: Finished task 115.0 in stage 5.0 (TID 655). 1219 bytes result sent to driver
15/08/19 18:18:50 INFO TaskSetManager: Starting task 131.0 in stage 5.0 (TID 671, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:50 INFO TaskSetManager: Finished task 115.0 in stage 5.0 (TID 655) in 1978 ms on localhost (116/200)
15/08/19 18:18:50 INFO Executor: Running task 131.0 in stage 5.0 (TID 671)
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:50 INFO Executor: Finished task 116.0 in stage 5.0 (TID 656). 1219 bytes result sent to driver
15/08/19 18:18:50 INFO TaskSetManager: Starting task 132.0 in stage 5.0 (TID 672, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:50 INFO Executor: Running task 132.0 in stage 5.0 (TID 672)
15/08/19 18:18:50 INFO TaskSetManager: Finished task 116.0 in stage 5.0 (TID 656) in 1938 ms on localhost (117/200)
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO Executor: Finished task 117.0 in stage 5.0 (TID 657). 1219 bytes result sent to driver
15/08/19 18:18:50 INFO TaskSetManager: Starting task 133.0 in stage 5.0 (TID 673, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:50 INFO Executor: Running task 133.0 in stage 5.0 (TID 673)
15/08/19 18:18:50 INFO TaskSetManager: Finished task 117.0 in stage 5.0 (TID 657) in 1804 ms on localhost (118/200)
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO Executor: Finished task 118.0 in stage 5.0 (TID 658). 1219 bytes result sent to driver
15/08/19 18:18:50 INFO TaskSetManager: Starting task 134.0 in stage 5.0 (TID 674, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:50 INFO Executor: Running task 134.0 in stage 5.0 (TID 674)
15/08/19 18:18:50 INFO TaskSetManager: Finished task 118.0 in stage 5.0 (TID 658) in 1838 ms on localhost (119/200)
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:51 INFO Executor: Finished task 120.0 in stage 5.0 (TID 660). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 135.0 in stage 5.0 (TID 675, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 135.0 in stage 5.0 (TID 675)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 120.0 in stage 5.0 (TID 660) in 1942 ms on localhost (120/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 119.0 in stage 5.0 (TID 659). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 136.0 in stage 5.0 (TID 676, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 136.0 in stage 5.0 (TID 676)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 119.0 in stage 5.0 (TID 659) in 2025 ms on localhost (121/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 121.0 in stage 5.0 (TID 661). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 137.0 in stage 5.0 (TID 677, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 137.0 in stage 5.0 (TID 677)
15/08/19 18:18:51 INFO Executor: Finished task 122.0 in stage 5.0 (TID 662). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 138.0 in stage 5.0 (TID 678, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 138.0 in stage 5.0 (TID 678)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 121.0 in stage 5.0 (TID 661) in 1930 ms on localhost (122/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO TaskSetManager: Finished task 122.0 in stage 5.0 (TID 662) in 1896 ms on localhost (123/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 123.0 in stage 5.0 (TID 663). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 139.0 in stage 5.0 (TID 679, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 139.0 in stage 5.0 (TID 679)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 123.0 in stage 5.0 (TID 663) in 1877 ms on localhost (124/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 125.0 in stage 5.0 (TID 665). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 140.0 in stage 5.0 (TID 680, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 140.0 in stage 5.0 (TID 680)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 125.0 in stage 5.0 (TID 665) in 1882 ms on localhost (125/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 124.0 in stage 5.0 (TID 664). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 141.0 in stage 5.0 (TID 681, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 141.0 in stage 5.0 (TID 681)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 124.0 in stage 5.0 (TID 664) in 2012 ms on localhost (126/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 126.0 in stage 5.0 (TID 666). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 142.0 in stage 5.0 (TID 682, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 142.0 in stage 5.0 (TID 682)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 126.0 in stage 5.0 (TID 666) in 1983 ms on localhost (127/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 128.0 in stage 5.0 (TID 668). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 143.0 in stage 5.0 (TID 683, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 143.0 in stage 5.0 (TID 683)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 128.0 in stage 5.0 (TID 668) in 1918 ms on localhost (128/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 127.0 in stage 5.0 (TID 667). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 144.0 in stage 5.0 (TID 684, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 144.0 in stage 5.0 (TID 684)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 127.0 in stage 5.0 (TID 667) in 1937 ms on localhost (129/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:51 INFO Executor: Finished task 129.0 in stage 5.0 (TID 669). 1219 bytes result sent to driver
15/08/19 18:18:51 INFO TaskSetManager: Starting task 145.0 in stage 5.0 (TID 685, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:51 INFO Executor: Running task 145.0 in stage 5.0 (TID 685)
15/08/19 18:18:51 INFO TaskSetManager: Finished task 129.0 in stage 5.0 (TID 669) in 1771 ms on localhost (130/200)
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:52 INFO Executor: Finished task 131.0 in stage 5.0 (TID 671). 1219 bytes result sent to driver
15/08/19 18:18:52 INFO TaskSetManager: Starting task 146.0 in stage 5.0 (TID 686, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:52 INFO Executor: Running task 146.0 in stage 5.0 (TID 686)
15/08/19 18:18:52 INFO TaskSetManager: Finished task 131.0 in stage 5.0 (TID 671) in 1853 ms on localhost (131/200)
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:52 INFO Executor: Finished task 130.0 in stage 5.0 (TID 670). 1219 bytes result sent to driver
15/08/19 18:18:52 INFO TaskSetManager: Starting task 147.0 in stage 5.0 (TID 687, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:52 INFO Executor: Running task 147.0 in stage 5.0 (TID 687)
15/08/19 18:18:52 INFO Executor: Finished task 133.0 in stage 5.0 (TID 673). 1219 bytes result sent to driver
15/08/19 18:18:52 INFO TaskSetManager: Starting task 148.0 in stage 5.0 (TID 688, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:52 INFO Executor: Running task 148.0 in stage 5.0 (TID 688)
15/08/19 18:18:52 INFO TaskSetManager: Finished task 130.0 in stage 5.0 (TID 670) in 1871 ms on localhost (132/200)
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:52 INFO TaskSetManager: Finished task 133.0 in stage 5.0 (TID 673) in 1809 ms on localhost (133/200)
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:52 INFO Executor: Finished task 132.0 in stage 5.0 (TID 672). 1219 bytes result sent to driver
15/08/19 18:18:52 INFO TaskSetManager: Starting task 149.0 in stage 5.0 (TID 689, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:52 INFO Executor: Running task 149.0 in stage 5.0 (TID 689)
15/08/19 18:18:52 INFO TaskSetManager: Finished task 132.0 in stage 5.0 (TID 672) in 2225 ms on localhost (134/200)
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:52 INFO Executor: Finished task 134.0 in stage 5.0 (TID 674). 1219 bytes result sent to driver
15/08/19 18:18:52 INFO TaskSetManager: Starting task 150.0 in stage 5.0 (TID 690, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:52 INFO Executor: Running task 150.0 in stage 5.0 (TID 690)
15/08/19 18:18:52 INFO TaskSetManager: Finished task 134.0 in stage 5.0 (TID 674) in 2202 ms on localhost (135/200)
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:53 INFO Executor: Finished task 138.0 in stage 5.0 (TID 678). 1219 bytes result sent to driver
15/08/19 18:18:53 INFO TaskSetManager: Starting task 151.0 in stage 5.0 (TID 691, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:53 INFO Executor: Running task 151.0 in stage 5.0 (TID 691)
15/08/19 18:18:53 INFO TaskSetManager: Finished task 138.0 in stage 5.0 (TID 678) in 2389 ms on localhost (136/200)
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:53 INFO Executor: Finished task 136.0 in stage 5.0 (TID 676). 1219 bytes result sent to driver
15/08/19 18:18:53 INFO TaskSetManager: Starting task 152.0 in stage 5.0 (TID 692, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:53 INFO TaskSetManager: Finished task 136.0 in stage 5.0 (TID 676) in 2442 ms on localhost (137/200)
15/08/19 18:18:53 INFO Executor: Running task 152.0 in stage 5.0 (TID 692)
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:53 INFO Executor: Finished task 137.0 in stage 5.0 (TID 677). 1219 bytes result sent to driver
15/08/19 18:18:53 INFO TaskSetManager: Starting task 153.0 in stage 5.0 (TID 693, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:53 INFO Executor: Running task 153.0 in stage 5.0 (TID 693)
15/08/19 18:18:53 INFO TaskSetManager: Finished task 137.0 in stage 5.0 (TID 677) in 2428 ms on localhost (138/200)
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:53 INFO Executor: Finished task 135.0 in stage 5.0 (TID 675). 1219 bytes result sent to driver
15/08/19 18:18:53 INFO TaskSetManager: Starting task 154.0 in stage 5.0 (TID 694, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:53 INFO Executor: Running task 154.0 in stage 5.0 (TID 694)
15/08/19 18:18:53 INFO TaskSetManager: Finished task 135.0 in stage 5.0 (TID 675) in 2548 ms on localhost (139/200)
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:53 INFO Executor: Finished task 139.0 in stage 5.0 (TID 679). 1219 bytes result sent to driver
15/08/19 18:18:53 INFO TaskSetManager: Starting task 155.0 in stage 5.0 (TID 695, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:53 INFO Executor: Running task 155.0 in stage 5.0 (TID 695)
15/08/19 18:18:53 INFO TaskSetManager: Finished task 139.0 in stage 5.0 (TID 679) in 2599 ms on localhost (140/200)
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:54 INFO Executor: Finished task 141.0 in stage 5.0 (TID 681). 1219 bytes result sent to driver
15/08/19 18:18:54 INFO TaskSetManager: Starting task 156.0 in stage 5.0 (TID 696, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:54 INFO Executor: Running task 156.0 in stage 5.0 (TID 696)
15/08/19 18:18:54 INFO TaskSetManager: Finished task 141.0 in stage 5.0 (TID 681) in 2714 ms on localhost (141/200)
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:54 INFO Executor: Finished task 142.0 in stage 5.0 (TID 682). 1219 bytes result sent to driver
15/08/19 18:18:54 INFO TaskSetManager: Starting task 157.0 in stage 5.0 (TID 697, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:54 INFO Executor: Finished task 140.0 in stage 5.0 (TID 680). 1219 bytes result sent to driver
15/08/19 18:18:54 INFO TaskSetManager: Starting task 158.0 in stage 5.0 (TID 698, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:54 INFO TaskSetManager: Finished task 142.0 in stage 5.0 (TID 682) in 2643 ms on localhost (142/200)
15/08/19 18:18:54 INFO TaskSetManager: Finished task 140.0 in stage 5.0 (TID 680) in 2761 ms on localhost (143/200)
15/08/19 18:18:54 INFO Executor: Running task 158.0 in stage 5.0 (TID 698)
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:54 INFO Executor: Running task 157.0 in stage 5.0 (TID 697)
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:54 INFO Executor: Finished task 143.0 in stage 5.0 (TID 683). 1219 bytes result sent to driver
15/08/19 18:18:54 INFO TaskSetManager: Starting task 159.0 in stage 5.0 (TID 699, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:54 INFO Executor: Running task 159.0 in stage 5.0 (TID 699)
15/08/19 18:18:54 INFO TaskSetManager: Finished task 143.0 in stage 5.0 (TID 683) in 2693 ms on localhost (144/200)
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:54 INFO Executor: Finished task 144.0 in stage 5.0 (TID 684). 1219 bytes result sent to driver
15/08/19 18:18:54 INFO TaskSetManager: Starting task 160.0 in stage 5.0 (TID 700, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:54 INFO Executor: Running task 160.0 in stage 5.0 (TID 700)
15/08/19 18:18:54 INFO TaskSetManager: Finished task 144.0 in stage 5.0 (TID 684) in 3053 ms on localhost (145/200)
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:54 INFO Executor: Finished task 145.0 in stage 5.0 (TID 685). 1219 bytes result sent to driver
15/08/19 18:18:54 INFO TaskSetManager: Starting task 161.0 in stage 5.0 (TID 701, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:54 INFO Executor: Running task 161.0 in stage 5.0 (TID 701)
15/08/19 18:18:54 INFO TaskSetManager: Finished task 145.0 in stage 5.0 (TID 685) in 2983 ms on localhost (146/200)
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO Executor: Finished task 149.0 in stage 5.0 (TID 689). 1219 bytes result sent to driver
15/08/19 18:18:55 INFO TaskSetManager: Starting task 162.0 in stage 5.0 (TID 702, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:55 INFO Executor: Running task 162.0 in stage 5.0 (TID 702)
15/08/19 18:18:55 INFO TaskSetManager: Finished task 149.0 in stage 5.0 (TID 689) in 2770 ms on localhost (147/200)
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO Executor: Finished task 148.0 in stage 5.0 (TID 688). 1219 bytes result sent to driver
15/08/19 18:18:55 INFO TaskSetManager: Starting task 163.0 in stage 5.0 (TID 703, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:55 INFO Executor: Running task 163.0 in stage 5.0 (TID 703)
15/08/19 18:18:55 INFO TaskSetManager: Finished task 148.0 in stage 5.0 (TID 688) in 3416 ms on localhost (148/200)
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO Executor: Finished task 146.0 in stage 5.0 (TID 686). 1219 bytes result sent to driver
15/08/19 18:18:55 INFO TaskSetManager: Starting task 164.0 in stage 5.0 (TID 704, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:55 INFO Executor: Running task 164.0 in stage 5.0 (TID 704)
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO Executor: Finished task 150.0 in stage 5.0 (TID 690). 1219 bytes result sent to driver
15/08/19 18:18:55 INFO TaskSetManager: Finished task 146.0 in stage 5.0 (TID 686) in 3442 ms on localhost (149/200)
15/08/19 18:18:55 INFO TaskSetManager: Starting task 165.0 in stage 5.0 (TID 705, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:55 INFO Executor: Running task 165.0 in stage 5.0 (TID 705)
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:55 INFO TaskSetManager: Finished task 150.0 in stage 5.0 (TID 690) in 2869 ms on localhost (150/200)
15/08/19 18:18:55 INFO Executor: Finished task 147.0 in stage 5.0 (TID 687). 1219 bytes result sent to driver
15/08/19 18:18:55 INFO TaskSetManager: Starting task 166.0 in stage 5.0 (TID 706, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:55 INFO TaskSetManager: Finished task 147.0 in stage 5.0 (TID 687) in 3479 ms on localhost (151/200)
15/08/19 18:18:55 INFO Executor: Running task 166.0 in stage 5.0 (TID 706)
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO Executor: Finished task 152.0 in stage 5.0 (TID 692). 1219 bytes result sent to driver
15/08/19 18:18:55 INFO TaskSetManager: Starting task 167.0 in stage 5.0 (TID 707, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:55 INFO TaskSetManager: Finished task 152.0 in stage 5.0 (TID 692) in 2297 ms on localhost (152/200)
15/08/19 18:18:55 INFO Executor: Running task 167.0 in stage 5.0 (TID 707)
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO Executor: Finished task 153.0 in stage 5.0 (TID 693). 1219 bytes result sent to driver
15/08/19 18:18:55 INFO TaskSetManager: Starting task 168.0 in stage 5.0 (TID 708, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:55 INFO Executor: Running task 168.0 in stage 5.0 (TID 708)
15/08/19 18:18:55 INFO TaskSetManager: Finished task 153.0 in stage 5.0 (TID 693) in 2434 ms on localhost (153/200)
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO Executor: Finished task 151.0 in stage 5.0 (TID 691). 1219 bytes result sent to driver
15/08/19 18:18:56 INFO TaskSetManager: Starting task 169.0 in stage 5.0 (TID 709, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:56 INFO Executor: Finished task 154.0 in stage 5.0 (TID 694). 1219 bytes result sent to driver
15/08/19 18:18:56 INFO Executor: Running task 169.0 in stage 5.0 (TID 709)
15/08/19 18:18:56 INFO TaskSetManager: Starting task 170.0 in stage 5.0 (TID 710, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:56 INFO Executor: Running task 170.0 in stage 5.0 (TID 710)
15/08/19 18:18:56 INFO TaskSetManager: Finished task 154.0 in stage 5.0 (TID 694) in 2469 ms on localhost (154/200)
15/08/19 18:18:56 INFO TaskSetManager: Finished task 151.0 in stage 5.0 (TID 691) in 2548 ms on localhost (155/200)
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO Executor: Finished task 155.0 in stage 5.0 (TID 695). 1219 bytes result sent to driver
15/08/19 18:18:56 INFO TaskSetManager: Starting task 171.0 in stage 5.0 (TID 711, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:56 INFO Executor: Running task 171.0 in stage 5.0 (TID 711)
15/08/19 18:18:56 INFO TaskSetManager: Finished task 155.0 in stage 5.0 (TID 695) in 2225 ms on localhost (156/200)
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO Executor: Finished task 156.0 in stage 5.0 (TID 696). 1219 bytes result sent to driver
15/08/19 18:18:56 INFO TaskSetManager: Starting task 172.0 in stage 5.0 (TID 712, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:56 INFO Executor: Running task 172.0 in stage 5.0 (TID 712)
15/08/19 18:18:56 INFO TaskSetManager: Finished task 156.0 in stage 5.0 (TID 696) in 2252 ms on localhost (157/200)
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO Executor: Finished task 157.0 in stage 5.0 (TID 697). 1219 bytes result sent to driver
15/08/19 18:18:56 INFO TaskSetManager: Starting task 173.0 in stage 5.0 (TID 713, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:56 INFO Executor: Running task 173.0 in stage 5.0 (TID 713)
15/08/19 18:18:56 INFO TaskSetManager: Finished task 157.0 in stage 5.0 (TID 697) in 2480 ms on localhost (158/200)
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO Executor: Finished task 159.0 in stage 5.0 (TID 699). 1219 bytes result sent to driver
15/08/19 18:18:57 INFO TaskSetManager: Starting task 174.0 in stage 5.0 (TID 714, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:57 INFO Executor: Running task 174.0 in stage 5.0 (TID 714)
15/08/19 18:18:57 INFO TaskSetManager: Finished task 159.0 in stage 5.0 (TID 699) in 2847 ms on localhost (159/200)
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:57 INFO Executor: Finished task 158.0 in stage 5.0 (TID 698). 1219 bytes result sent to driver
15/08/19 18:18:57 INFO TaskSetManager: Starting task 175.0 in stage 5.0 (TID 715, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:57 INFO Executor: Running task 175.0 in stage 5.0 (TID 715)
15/08/19 18:18:57 INFO Executor: Finished task 160.0 in stage 5.0 (TID 700). 1219 bytes result sent to driver
15/08/19 18:18:57 INFO Executor: Finished task 161.0 in stage 5.0 (TID 701). 1219 bytes result sent to driver
15/08/19 18:18:57 INFO TaskSetManager: Starting task 176.0 in stage 5.0 (TID 716, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:57 INFO TaskSetManager: Finished task 158.0 in stage 5.0 (TID 698) in 2929 ms on localhost (160/200)
15/08/19 18:18:57 INFO Executor: Running task 176.0 in stage 5.0 (TID 716)
15/08/19 18:18:57 INFO TaskSetManager: Starting task 177.0 in stage 5.0 (TID 717, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:57 INFO Executor: Running task 177.0 in stage 5.0 (TID 717)
15/08/19 18:18:57 INFO TaskSetManager: Finished task 160.0 in stage 5.0 (TID 700) in 2490 ms on localhost (161/200)
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO TaskSetManager: Finished task 161.0 in stage 5.0 (TID 701) in 2463 ms on localhost (162/200)
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:57 INFO Executor: Finished task 162.0 in stage 5.0 (TID 702). 1219 bytes result sent to driver
15/08/19 18:18:57 INFO TaskSetManager: Starting task 178.0 in stage 5.0 (TID 718, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:57 INFO Executor: Running task 178.0 in stage 5.0 (TID 718)
15/08/19 18:18:57 INFO TaskSetManager: Finished task 162.0 in stage 5.0 (TID 702) in 2140 ms on localhost (163/200)
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO Executor: Finished task 166.0 in stage 5.0 (TID 706). 1219 bytes result sent to driver
15/08/19 18:18:57 INFO TaskSetManager: Starting task 179.0 in stage 5.0 (TID 719, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:57 INFO Executor: Running task 179.0 in stage 5.0 (TID 719)
15/08/19 18:18:57 INFO TaskSetManager: Finished task 166.0 in stage 5.0 (TID 706) in 2117 ms on localhost (164/200)
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO Executor: Finished task 163.0 in stage 5.0 (TID 703). 1219 bytes result sent to driver
15/08/19 18:18:57 INFO TaskSetManager: Starting task 180.0 in stage 5.0 (TID 720, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:57 INFO Executor: Running task 180.0 in stage 5.0 (TID 720)
15/08/19 18:18:57 INFO TaskSetManager: Finished task 163.0 in stage 5.0 (TID 703) in 2317 ms on localhost (165/200)
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO Executor: Finished task 167.0 in stage 5.0 (TID 707). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO TaskSetManager: Starting task 181.0 in stage 5.0 (TID 721, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 181.0 in stage 5.0 (TID 721)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 167.0 in stage 5.0 (TID 707) in 2456 ms on localhost (166/200)
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:18:58 INFO Executor: Finished task 168.0 in stage 5.0 (TID 708). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO TaskSetManager: Starting task 182.0 in stage 5.0 (TID 722, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 182.0 in stage 5.0 (TID 722)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 168.0 in stage 5.0 (TID 708) in 2540 ms on localhost (167/200)
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO Executor: Finished task 164.0 in stage 5.0 (TID 704). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO Executor: Finished task 165.0 in stage 5.0 (TID 705). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO TaskSetManager: Starting task 183.0 in stage 5.0 (TID 723, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 183.0 in stage 5.0 (TID 723)
15/08/19 18:18:58 INFO TaskSetManager: Starting task 184.0 in stage 5.0 (TID 724, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 184.0 in stage 5.0 (TID 724)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 164.0 in stage 5.0 (TID 704) in 2887 ms on localhost (168/200)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 165.0 in stage 5.0 (TID 705) in 2878 ms on localhost (169/200)
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO Executor: Finished task 171.0 in stage 5.0 (TID 711). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO TaskSetManager: Starting task 185.0 in stage 5.0 (TID 725, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 185.0 in stage 5.0 (TID 725)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 171.0 in stage 5.0 (TID 711) in 2468 ms on localhost (170/200)
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO Executor: Finished task 170.0 in stage 5.0 (TID 710). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO TaskSetManager: Starting task 186.0 in stage 5.0 (TID 726, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 186.0 in stage 5.0 (TID 726)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 170.0 in stage 5.0 (TID 710) in 2560 ms on localhost (171/200)
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO Executor: Finished task 169.0 in stage 5.0 (TID 709). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO TaskSetManager: Starting task 187.0 in stage 5.0 (TID 727, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 187.0 in stage 5.0 (TID 727)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 169.0 in stage 5.0 (TID 709) in 2585 ms on localhost (172/200)
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO Executor: Finished task 172.0 in stage 5.0 (TID 712). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO TaskSetManager: Starting task 188.0 in stage 5.0 (TID 728, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 188.0 in stage 5.0 (TID 728)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 172.0 in stage 5.0 (TID 712) in 2302 ms on localhost (173/200)
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:58 INFO Executor: Finished task 173.0 in stage 5.0 (TID 713). 1219 bytes result sent to driver
15/08/19 18:18:58 INFO TaskSetManager: Starting task 189.0 in stage 5.0 (TID 729, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:58 INFO Executor: Running task 189.0 in stage 5.0 (TID 729)
15/08/19 18:18:58 INFO TaskSetManager: Finished task 173.0 in stage 5.0 (TID 713) in 2265 ms on localhost (174/200)
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO Executor: Finished task 175.0 in stage 5.0 (TID 715). 1219 bytes result sent to driver
15/08/19 18:18:59 INFO TaskSetManager: Starting task 190.0 in stage 5.0 (TID 730, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:59 INFO Executor: Running task 190.0 in stage 5.0 (TID 730)
15/08/19 18:18:59 INFO TaskSetManager: Finished task 175.0 in stage 5.0 (TID 715) in 2150 ms on localhost (175/200)
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO Executor: Finished task 177.0 in stage 5.0 (TID 717). 1219 bytes result sent to driver
15/08/19 18:18:59 INFO TaskSetManager: Starting task 191.0 in stage 5.0 (TID 731, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:59 INFO Executor: Running task 191.0 in stage 5.0 (TID 731)
15/08/19 18:18:59 INFO TaskSetManager: Finished task 177.0 in stage 5.0 (TID 717) in 2213 ms on localhost (176/200)
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO Executor: Finished task 174.0 in stage 5.0 (TID 714). 1219 bytes result sent to driver
15/08/19 18:18:59 INFO TaskSetManager: Starting task 192.0 in stage 5.0 (TID 732, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:59 INFO Executor: Running task 192.0 in stage 5.0 (TID 732)
15/08/19 18:18:59 INFO TaskSetManager: Finished task 174.0 in stage 5.0 (TID 714) in 2245 ms on localhost (177/200)
15/08/19 18:18:59 INFO Executor: Finished task 176.0 in stage 5.0 (TID 716). 1219 bytes result sent to driver
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO TaskSetManager: Starting task 193.0 in stage 5.0 (TID 733, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:59 INFO Executor: Running task 193.0 in stage 5.0 (TID 733)
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:59 INFO TaskSetManager: Finished task 176.0 in stage 5.0 (TID 716) in 2234 ms on localhost (178/200)
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO Executor: Finished task 178.0 in stage 5.0 (TID 718). 1219 bytes result sent to driver
15/08/19 18:18:59 INFO TaskSetManager: Starting task 194.0 in stage 5.0 (TID 734, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:59 INFO Executor: Running task 194.0 in stage 5.0 (TID 734)
15/08/19 18:18:59 INFO TaskSetManager: Finished task 178.0 in stage 5.0 (TID 718) in 2012 ms on localhost (179/200)
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO Executor: Finished task 179.0 in stage 5.0 (TID 719). 1219 bytes result sent to driver
15/08/19 18:18:59 INFO TaskSetManager: Starting task 195.0 in stage 5.0 (TID 735, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:59 INFO Executor: Running task 195.0 in stage 5.0 (TID 735)
15/08/19 18:18:59 INFO TaskSetManager: Finished task 179.0 in stage 5.0 (TID 719) in 1981 ms on localhost (180/200)
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:18:59 INFO Executor: Finished task 181.0 in stage 5.0 (TID 721). 1219 bytes result sent to driver
15/08/19 18:18:59 INFO TaskSetManager: Starting task 196.0 in stage 5.0 (TID 736, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:18:59 INFO Executor: Running task 196.0 in stage 5.0 (TID 736)
15/08/19 18:18:59 INFO TaskSetManager: Finished task 181.0 in stage 5.0 (TID 721) in 1597 ms on localhost (181/200)
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:18:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:00 INFO Executor: Finished task 180.0 in stage 5.0 (TID 720). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 197.0 in stage 5.0 (TID 737, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:19:00 INFO Executor: Running task 197.0 in stage 5.0 (TID 737)
15/08/19 18:19:00 INFO TaskSetManager: Finished task 180.0 in stage 5.0 (TID 720) in 2178 ms on localhost (182/200)
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:00 INFO Executor: Finished task 182.0 in stage 5.0 (TID 722). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 198.0 in stage 5.0 (TID 738, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:19:00 INFO Executor: Running task 198.0 in stage 5.0 (TID 738)
15/08/19 18:19:00 INFO TaskSetManager: Finished task 182.0 in stage 5.0 (TID 722) in 1619 ms on localhost (183/200)
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:00 INFO Executor: Finished task 184.0 in stage 5.0 (TID 724). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 199.0 in stage 5.0 (TID 739, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/19 18:19:00 INFO Executor: Running task 199.0 in stage 5.0 (TID 739)
15/08/19 18:19:00 INFO TaskSetManager: Finished task 184.0 in stage 5.0 (TID 724) in 1834 ms on localhost (184/200)
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Getting 28 non-empty blocks out of 28 blocks
15/08/19 18:19:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:00 INFO Executor: Finished task 183.0 in stage 5.0 (TID 723). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 740, localhost, ANY, 1758 bytes)
15/08/19 18:19:00 INFO TaskSetManager: Finished task 183.0 in stage 5.0 (TID 723) in 1913 ms on localhost (185/200)
15/08/19 18:19:00 INFO Executor: Running task 16.0 in stage 7.0 (TID 740)
15/08/19 18:19:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:00 INFO Executor: Finished task 185.0 in stage 5.0 (TID 725). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 741, localhost, ANY, 1769 bytes)
15/08/19 18:19:00 INFO Executor: Running task 17.0 in stage 7.0 (TID 741)
15/08/19 18:19:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 134217728 end: 258769491 length: 124551763 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:00 INFO TaskSetManager: Finished task 185.0 in stage 5.0 (TID 725) in 1919 ms on localhost (186/200)
15/08/19 18:19:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609845 records.
15/08/19 18:19:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:00 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500974
15/08/19 18:19:00 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 3500100
15/08/19 18:19:00 INFO Executor: Finished task 187.0 in stage 5.0 (TID 727). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 742, localhost, ANY, 1757 bytes)
15/08/19 18:19:00 INFO Executor: Finished task 186.0 in stage 5.0 (TID 726). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 743, localhost, ANY, 1770 bytes)
15/08/19 18:19:00 INFO TaskSetManager: Finished task 187.0 in stage 5.0 (TID 727) in 1993 ms on localhost (187/200)
15/08/19 18:19:00 INFO Executor: Running task 19.0 in stage 7.0 (TID 743)
15/08/19 18:19:00 INFO TaskSetManager: Finished task 186.0 in stage 5.0 (TID 726) in 2016 ms on localhost (188/200)
15/08/19 18:19:00 INFO Executor: Running task 18.0 in stage 7.0 (TID 742)
15/08/19 18:19:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 134217728 end: 258780135 length: 124562407 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:00 INFO Executor: Finished task 188.0 in stage 5.0 (TID 728). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 744, localhost, ANY, 1757 bytes)
15/08/19 18:19:00 INFO Executor: Running task 20.0 in stage 7.0 (TID 744)
15/08/19 18:19:00 INFO TaskSetManager: Finished task 188.0 in stage 5.0 (TID 728) in 1939 ms on localhost (189/200)
15/08/19 18:19:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501014 records.
15/08/19 18:19:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608767 records.
15/08/19 18:19:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:00 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3501014
15/08/19 18:19:00 INFO InternalParquetRecordReader: block read in memory in 107 ms. row count = 3500100
15/08/19 18:19:00 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500100
15/08/19 18:19:00 INFO Executor: Finished task 189.0 in stage 5.0 (TID 729). 1219 bytes result sent to driver
15/08/19 18:19:00 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 745, localhost, ANY, 1770 bytes)
15/08/19 18:19:00 INFO Executor: Running task 21.0 in stage 7.0 (TID 745)
15/08/19 18:19:00 INFO TaskSetManager: Finished task 189.0 in stage 5.0 (TID 729) in 1892 ms on localhost (190/200)
15/08/19 18:19:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 134217728 end: 258792739 length: 124575011 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609476 records.
15/08/19 18:19:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:00 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 3501078
15/08/19 18:19:01 INFO Executor: Finished task 190.0 in stage 5.0 (TID 730). 1219 bytes result sent to driver
15/08/19 18:19:01 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 746, localhost, ANY, 1757 bytes)
15/08/19 18:19:01 INFO TaskSetManager: Finished task 190.0 in stage 5.0 (TID 730) in 2142 ms on localhost (191/200)
15/08/19 18:19:01 INFO Executor: Running task 22.0 in stage 7.0 (TID 746)
15/08/19 18:19:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:01 INFO Executor: Finished task 193.0 in stage 5.0 (TID 733). 1219 bytes result sent to driver
15/08/19 18:19:01 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 747, localhost, ANY, 1769 bytes)
15/08/19 18:19:01 INFO Executor: Running task 23.0 in stage 7.0 (TID 747)
15/08/19 18:19:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503194 records.
15/08/19 18:19:01 INFO TaskSetManager: Finished task 193.0 in stage 5.0 (TID 733) in 2138 ms on localhost (192/200)
15/08/19 18:19:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 134217728 end: 260267299 length: 126049571 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3660722 records.
15/08/19 18:19:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:01 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 3503194
15/08/19 18:19:01 INFO InternalParquetRecordReader: block read in memory in 47 ms. row count = 3500100
15/08/19 18:19:01 INFO Executor: Finished task 191.0 in stage 5.0 (TID 731). 1219 bytes result sent to driver
15/08/19 18:19:01 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 748, localhost, ANY, 1758 bytes)
15/08/19 18:19:01 INFO Executor: Running task 24.0 in stage 7.0 (TID 748)
15/08/19 18:19:01 INFO TaskSetManager: Finished task 191.0 in stage 5.0 (TID 731) in 2346 ms on localhost (193/200)
15/08/19 18:19:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500966 records.
15/08/19 18:19:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:01 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 3500966
15/08/19 18:19:01 INFO Executor: Finished task 194.0 in stage 5.0 (TID 734). 1219 bytes result sent to driver
15/08/19 18:19:01 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 749, localhost, ANY, 1771 bytes)
15/08/19 18:19:01 INFO Executor: Running task 25.0 in stage 7.0 (TID 749)
15/08/19 18:19:01 INFO TaskSetManager: Finished task 194.0 in stage 5.0 (TID 734) in 2412 ms on localhost (194/200)
15/08/19 18:19:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 134217728 end: 258509806 length: 124292078 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608800 records.
15/08/19 18:19:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:01 INFO Executor: Finished task 192.0 in stage 5.0 (TID 732). 1219 bytes result sent to driver
15/08/19 18:19:01 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 750, localhost, ANY, 1758 bytes)
15/08/19 18:19:01 INFO Executor: Running task 26.0 in stage 7.0 (TID 750)
15/08/19 18:19:02 INFO TaskSetManager: Finished task 192.0 in stage 5.0 (TID 732) in 2661 ms on localhost (195/200)
15/08/19 18:19:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501036 records.
15/08/19 18:19:02 INFO InternalParquetRecordReader: block read in memory in 113 ms. row count = 3502755
15/08/19 18:19:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:02 INFO InternalParquetRecordReader: block read in memory in 135 ms. row count = 3501036
15/08/19 18:19:02 INFO Executor: Finished task 195.0 in stage 5.0 (TID 735). 1219 bytes result sent to driver
15/08/19 18:19:02 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 751, localhost, ANY, 1770 bytes)
15/08/19 18:19:02 INFO Executor: Running task 27.0 in stage 7.0 (TID 751)
15/08/19 18:19:02 INFO TaskSetManager: Finished task 195.0 in stage 5.0 (TID 735) in 2495 ms on localhost (196/200)
15/08/19 18:19:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 134217728 end: 259215293 length: 124997565 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609239 records.
15/08/19 18:19:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:02 INFO InternalParquetRecordReader: block read in memory in 159 ms. row count = 3500100
15/08/19 18:19:02 INFO Executor: Finished task 198.0 in stage 5.0 (TID 738). 1219 bytes result sent to driver
15/08/19 18:19:02 INFO Executor: Finished task 196.0 in stage 5.0 (TID 736). 1219 bytes result sent to driver
15/08/19 18:19:02 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 752, localhost, ANY, 1758 bytes)
15/08/19 18:19:02 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 753, localhost, ANY, 1770 bytes)
15/08/19 18:19:02 INFO Executor: Running task 29.0 in stage 7.0 (TID 753)
15/08/19 18:19:02 INFO Executor: Running task 28.0 in stage 7.0 (TID 752)
15/08/19 18:19:02 INFO TaskSetManager: Finished task 198.0 in stage 5.0 (TID 738) in 2636 ms on localhost (197/200)
15/08/19 18:19:02 INFO TaskSetManager: Finished task 196.0 in stage 5.0 (TID 736) in 2900 ms on localhost (198/200)
15/08/19 18:19:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 134217728 end: 258903039 length: 124685311 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609008 records.
15/08/19 18:19:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500807 records.
15/08/19 18:19:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:03 INFO InternalParquetRecordReader: block read in memory in 411 ms. row count = 3501316
15/08/19 18:19:03 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500807
15/08/19 18:19:03 INFO Executor: Finished task 197.0 in stage 5.0 (TID 737). 1219 bytes result sent to driver
15/08/19 18:19:03 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 754, localhost, ANY, 1757 bytes)
15/08/19 18:19:03 INFO TaskSetManager: Finished task 197.0 in stage 5.0 (TID 737) in 3205 ms on localhost (199/200)
15/08/19 18:19:03 INFO Executor: Running task 30.0 in stage 7.0 (TID 754)
15/08/19 18:19:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503166 records.
15/08/19 18:19:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:03 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 3503166
15/08/19 18:19:03 INFO Executor: Finished task 199.0 in stage 5.0 (TID 739). 1219 bytes result sent to driver
15/08/19 18:19:03 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 755, localhost, ANY, 1770 bytes)
15/08/19 18:19:03 INFO Executor: Running task 31.0 in stage 7.0 (TID 755)
15/08/19 18:19:03 INFO TaskSetManager: Finished task 199.0 in stage 5.0 (TID 739) in 3147 ms on localhost (200/200)
15/08/19 18:19:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 134217728 end: 259962410 length: 125744682 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:03 INFO DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:423) finished in 33.988 s
15/08/19 18:19:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/19 18:19:03 INFO DAGScheduler: looking for newly runnable stages
15/08/19 18:19:03 INFO DAGScheduler: running: Set(ShuffleMapStage 7)
15/08/19 18:19:03 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/19 18:19:03 INFO DAGScheduler: failed: Set()
15/08/19 18:19:03 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@459472fa
15/08/19 18:19:03 INFO StatsReportListener: task runtime:(count: 216, mean: 2475.171296, stdev: 687.678955, max: 5329.000000, min: 1597.000000)
15/08/19 18:19:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:03 INFO StatsReportListener: 	1.6 s	1.9 s	1.9 s	2.1 s	2.3 s	2.6 s	3.1 s	4.2 s	5.3 s
15/08/19 18:19:03 INFO StatsReportListener: shuffle bytes written:(count: 216, mean: 12765801.800926, stdev: 2332708.714166, max: 21905569.000000, min: 12044547.000000)
15/08/19 18:19:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:03 INFO StatsReportListener: 	11.5 MB	11.5 MB	11.5 MB	11.5 MB	11.5 MB	11.6 MB	11.6 MB	19.6 MB	20.9 MB
15/08/19 18:19:03 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.125000, stdev: 0.457575, max: 3.000000, min: 0.000000)
15/08/19 18:19:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:03 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	3.0 ms
15/08/19 18:19:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:03 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/19 18:19:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:03 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/19 18:19:03 INFO StatsReportListener: task result size:(count: 216, mean: 1286.111111, stdev: 237.273609, max: 2125.000000, min: 1219.000000)
15/08/19 18:19:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:03 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	2.1 KB	2.1 KB
15/08/19 18:19:03 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 7)
15/08/19 18:19:03 INFO StatsReportListener: executor (non-fetch) time pct: (count: 216, mean: 99.279521, stdev: 0.313681, max: 99.761223, min: 97.883065)
15/08/19 18:19:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:03 INFO StatsReportListener: 	98 %	99 %	99 %	99 %	99 %	99 %	100 %	100 %	100 %
15/08/19 18:19:03 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.005487, stdev: 0.019907, max: 0.123533, min: 0.000000)
15/08/19 18:19:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:03 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/19 18:19:03 INFO StatsReportListener: other time pct: (count: 216, mean: 0.715399, stdev: 0.313368, max: 2.116935, min: 0.238777)
15/08/19 18:19:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:03 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 1 %	 1 %	 1 %	 1 %	 1 %	 2 %
15/08/19 18:19:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3660809 records.
15/08/19 18:19:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:03 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 3502900
15/08/19 18:19:04 INFO InternalParquetRecordReader: Assembled and processed 3500974 records from 2 columns in 4307 ms: 812.85675 rec/ms, 1625.7135 cell/ms
15/08/19 18:19:04 INFO InternalParquetRecordReader: time spent so far 0% reading (39 ms) and 99% processing (4307 ms)
15/08/19 18:19:04 INFO InternalParquetRecordReader: at row 3500974. reading next block
15/08/19 18:19:04 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 108871
15/08/19 18:19:05 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4220 ms: 829.4076 rec/ms, 1658.8152 cell/ms
15/08/19 18:19:05 INFO InternalParquetRecordReader: time spent so far 2% reading (107 ms) and 97% processing (4220 ms)
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 108667
15/08/19 18:19:05 INFO InternalParquetRecordReader: Assembled and processed 3501078 records from 2 columns in 4225 ms: 828.65753 rec/ms, 1657.3151 cell/ms
15/08/19 18:19:05 INFO InternalParquetRecordReader: time spent so far 1% reading (73 ms) and 98% processing (4225 ms)
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 3501078. reading next block
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 108398
15/08/19 18:19:05 INFO Executor: Finished task 16.0 in stage 7.0 (TID 740). 2125 bytes result sent to driver
15/08/19 18:19:05 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 756, localhost, ANY, 1757 bytes)
15/08/19 18:19:05 INFO Executor: Running task 32.0 in stage 7.0 (TID 756)
15/08/19 18:19:05 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 740) in 4916 ms on localhost (17/85)
15/08/19 18:19:05 INFO Executor: Finished task 17.0 in stage 7.0 (TID 741). 2125 bytes result sent to driver
15/08/19 18:19:05 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 757, localhost, ANY, 1771 bytes)
15/08/19 18:19:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:05 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 741) in 4854 ms on localhost (18/85)
15/08/19 18:19:05 INFO Executor: Running task 33.0 in stage 7.0 (TID 757)
15/08/19 18:19:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 134217728 end: 260302148 length: 126084420 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3664018 records.
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:05 INFO Executor: Finished task 19.0 in stage 7.0 (TID 743). 2125 bytes result sent to driver
15/08/19 18:19:05 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 758, localhost, ANY, 1757 bytes)
15/08/19 18:19:05 INFO Executor: Running task 34.0 in stage 7.0 (TID 758)
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:05 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 743) in 4775 ms on localhost (19/85)
15/08/19 18:19:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 3500100
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 3503024
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 3500100
15/08/19 18:19:05 INFO Executor: Finished task 21.0 in stage 7.0 (TID 745). 2125 bytes result sent to driver
15/08/19 18:19:05 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 759, localhost, ANY, 1773 bytes)
15/08/19 18:19:05 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 745) in 4716 ms on localhost (20/85)
15/08/19 18:19:05 INFO Executor: Running task 35.0 in stage 7.0 (TID 759)
15/08/19 18:19:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 134217728 end: 261087966 length: 126870238 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3663964 records.
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 3501325
15/08/19 18:19:05 INFO Executor: Finished task 18.0 in stage 7.0 (TID 742). 2125 bytes result sent to driver
15/08/19 18:19:05 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 760, localhost, ANY, 1758 bytes)
15/08/19 18:19:05 INFO Executor: Running task 36.0 in stage 7.0 (TID 760)
15/08/19 18:19:05 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 742) in 5022 ms on localhost (21/85)
15/08/19 18:19:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503694 records.
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 3503694
15/08/19 18:19:05 INFO Executor: Finished task 20.0 in stage 7.0 (TID 744). 2125 bytes result sent to driver
15/08/19 18:19:05 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 761, localhost, ANY, 1771 bytes)
15/08/19 18:19:05 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 744) in 5042 ms on localhost (22/85)
15/08/19 18:19:05 INFO Executor: Running task 37.0 in stage 7.0 (TID 761)
15/08/19 18:19:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 134217728 end: 258061335 length: 123843607 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3606669 records.
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 3503512
15/08/19 18:19:05 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4185 ms: 836.3441 rec/ms, 1672.6882 cell/ms
15/08/19 18:19:05 INFO InternalParquetRecordReader: time spent so far 1% reading (47 ms) and 98% processing (4185 ms)
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 160622
15/08/19 18:19:05 INFO Executor: Finished task 22.0 in stage 7.0 (TID 746). 2125 bytes result sent to driver
15/08/19 18:19:05 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 762, localhost, ANY, 1757 bytes)
15/08/19 18:19:05 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 746) in 4433 ms on localhost (23/85)
15/08/19 18:19:05 INFO Executor: Running task 38.0 in stage 7.0 (TID 762)
15/08/19 18:19:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501619 records.
15/08/19 18:19:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:05 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 3501619
15/08/19 18:19:06 INFO InternalParquetRecordReader: Assembled and processed 3502755 records from 2 columns in 4092 ms: 856.00073 rec/ms, 1712.0015 cell/ms
15/08/19 18:19:06 INFO InternalParquetRecordReader: time spent so far 2% reading (113 ms) and 97% processing (4092 ms)
15/08/19 18:19:06 INFO InternalParquetRecordReader: at row 3502755. reading next block
15/08/19 18:19:06 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 106045
15/08/19 18:19:06 INFO Executor: Finished task 23.0 in stage 7.0 (TID 747). 2125 bytes result sent to driver
15/08/19 18:19:06 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 763, localhost, ANY, 1770 bytes)
15/08/19 18:19:06 INFO Executor: Running task 39.0 in stage 7.0 (TID 763)
15/08/19 18:19:06 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 747) in 4935 ms on localhost (24/85)
15/08/19 18:19:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 134217728 end: 258877467 length: 124659739 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608467 records.
15/08/19 18:19:06 INFO Executor: Finished task 24.0 in stage 7.0 (TID 748). 2125 bytes result sent to driver
15/08/19 18:19:06 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 764, localhost, ANY, 1757 bytes)
15/08/19 18:19:06 INFO Executor: Running task 40.0 in stage 7.0 (TID 764)
15/08/19 18:19:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:06 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 748) in 4766 ms on localhost (25/85)
15/08/19 18:19:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:06 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 3501092
15/08/19 18:19:06 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 3500100
15/08/19 18:19:06 INFO Executor: Finished task 25.0 in stage 7.0 (TID 749). 2125 bytes result sent to driver
15/08/19 18:19:06 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 765, localhost, ANY, 1771 bytes)
15/08/19 18:19:06 INFO Executor: Running task 41.0 in stage 7.0 (TID 765)
15/08/19 18:19:06 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 749) in 4623 ms on localhost (26/85)
15/08/19 18:19:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 134217728 end: 258786694 length: 124568966 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609766 records.
15/08/19 18:19:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:06 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 3500891
15/08/19 18:19:06 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4104 ms: 852.8509 rec/ms, 1705.7018 cell/ms
15/08/19 18:19:06 INFO InternalParquetRecordReader: time spent so far 3% reading (159 ms) and 96% processing (4104 ms)
15/08/19 18:19:06 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:06 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 109139
15/08/19 18:19:06 INFO Executor: Finished task 26.0 in stage 7.0 (TID 750). 2125 bytes result sent to driver
15/08/19 18:19:06 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 766, localhost, ANY, 1757 bytes)
15/08/19 18:19:06 INFO Executor: Running task 42.0 in stage 7.0 (TID 766)
15/08/19 18:19:06 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 750) in 4852 ms on localhost (27/85)
15/08/19 18:19:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501324 records.
15/08/19 18:19:06 INFO InternalParquetRecordReader: Assembled and processed 3501316 records from 2 columns in 3618 ms: 967.749 rec/ms, 1935.498 cell/ms
15/08/19 18:19:06 INFO InternalParquetRecordReader: time spent so far 10% reading (411 ms) and 89% processing (3618 ms)
15/08/19 18:19:06 INFO InternalParquetRecordReader: at row 3501316. reading next block
15/08/19 18:19:06 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 107692
15/08/19 18:19:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:06 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 3501324
15/08/19 18:19:06 INFO Executor: Finished task 27.0 in stage 7.0 (TID 751). 2125 bytes result sent to driver
15/08/19 18:19:06 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 767, localhost, ANY, 1768 bytes)
15/08/19 18:19:06 INFO Executor: Running task 43.0 in stage 7.0 (TID 767)
15/08/19 18:19:07 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 751) in 4733 ms on localhost (28/85)
15/08/19 18:19:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 134217728 end: 260666224 length: 126448496 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3662650 records.
15/08/19 18:19:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:07 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 3500100
15/08/19 18:19:07 INFO Executor: Finished task 28.0 in stage 7.0 (TID 752). 2125 bytes result sent to driver
15/08/19 18:19:07 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 768, localhost, ANY, 1758 bytes)
15/08/19 18:19:07 INFO Executor: Running task 44.0 in stage 7.0 (TID 768)
15/08/19 18:19:07 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 752) in 4328 ms on localhost (29/85)
15/08/19 18:19:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:07 INFO InternalParquetRecordReader: Assembled and processed 3502900 records from 2 columns in 3510 ms: 997.97723 rec/ms, 1995.9545 cell/ms
15/08/19 18:19:07 INFO InternalParquetRecordReader: time spent so far 0% reading (33 ms) and 99% processing (3510 ms)
15/08/19 18:19:07 INFO InternalParquetRecordReader: at row 3502900. reading next block
15/08/19 18:19:07 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 157909
15/08/19 18:19:07 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 3500100
15/08/19 18:19:07 INFO Executor: Finished task 30.0 in stage 7.0 (TID 754). 2125 bytes result sent to driver
15/08/19 18:19:07 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 769, localhost, ANY, 1764 bytes)
15/08/19 18:19:07 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 754) in 3898 ms on localhost (30/85)
15/08/19 18:19:07 INFO Executor: Running task 45.0 in stage 7.0 (TID 769)
15/08/19 18:19:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 4159434 records.
15/08/19 18:19:07 INFO Executor: Finished task 29.0 in stage 7.0 (TID 753). 2125 bytes result sent to driver
15/08/19 18:19:07 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 770, localhost, ANY, 1770 bytes)
15/08/19 18:19:07 INFO Executor: Running task 46.0 in stage 7.0 (TID 770)
15/08/19 18:19:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:07 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 753) in 4500 ms on localhost (31/85)
15/08/19 18:19:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 268435456 end: 279508675 length: 11073219 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 0 records.
15/08/19 18:19:07 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 3501026
15/08/19 18:19:07 INFO Executor: Finished task 46.0 in stage 7.0 (TID 770). 2125 bytes result sent to driver
15/08/19 18:19:07 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 771, localhost, ANY, 1757 bytes)
15/08/19 18:19:07 INFO Executor: Running task 47.0 in stage 7.0 (TID 771)
15/08/19 18:19:07 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 770) in 52 ms on localhost (32/85)
15/08/19 18:19:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501151 records.
15/08/19 18:19:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:07 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 3501151
15/08/19 18:19:07 INFO Executor: Finished task 31.0 in stage 7.0 (TID 755). 2125 bytes result sent to driver
15/08/19 18:19:07 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 772, localhost, ANY, 1770 bytes)
15/08/19 18:19:07 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 755) in 3973 ms on localhost (33/85)
15/08/19 18:19:07 INFO Executor: Running task 48.0 in stage 7.0 (TID 772)
15/08/19 18:19:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 134217728 end: 258866877 length: 124649149 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608451 records.
15/08/19 18:19:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:07 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 3501003
15/08/19 18:19:08 INFO InternalParquetRecordReader: Assembled and processed 3503024 records from 2 columns in 3395 ms: 1031.8186 rec/ms, 2063.6372 cell/ms
15/08/19 18:19:08 INFO InternalParquetRecordReader: time spent so far 0% reading (34 ms) and 99% processing (3395 ms)
15/08/19 18:19:08 INFO InternalParquetRecordReader: at row 3503024. reading next block
15/08/19 18:19:08 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 160994
15/08/19 18:19:09 INFO InternalParquetRecordReader: Assembled and processed 3501325 records from 2 columns in 3479 ms: 1006.41705 rec/ms, 2012.8341 cell/ms
15/08/19 18:19:09 INFO InternalParquetRecordReader: time spent so far 1% reading (37 ms) and 98% processing (3479 ms)
15/08/19 18:19:09 INFO InternalParquetRecordReader: at row 3501325. reading next block
15/08/19 18:19:09 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 162639
15/08/19 18:19:09 INFO Executor: Finished task 34.0 in stage 7.0 (TID 758). 2125 bytes result sent to driver
15/08/19 18:19:09 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 773, localhost, ANY, 1758 bytes)
15/08/19 18:19:09 INFO Executor: Running task 49.0 in stage 7.0 (TID 773)
15/08/19 18:19:09 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 758) in 4245 ms on localhost (34/85)
15/08/19 18:19:09 INFO InternalParquetRecordReader: Assembled and processed 3503512 records from 2 columns in 3901 ms: 898.10614 rec/ms, 1796.2123 cell/ms
15/08/19 18:19:09 INFO InternalParquetRecordReader: time spent so far 1% reading (46 ms) and 98% processing (3901 ms)
15/08/19 18:19:09 INFO InternalParquetRecordReader: at row 3503512. reading next block
15/08/19 18:19:09 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 103157
15/08/19 18:19:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500783 records.
15/08/19 18:19:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:09 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500783
15/08/19 18:19:10 INFO Executor: Finished task 32.0 in stage 7.0 (TID 756). 2125 bytes result sent to driver
15/08/19 18:19:10 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 774, localhost, ANY, 1771 bytes)
15/08/19 18:19:10 INFO Executor: Running task 50.0 in stage 7.0 (TID 774)
15/08/19 18:19:10 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 756) in 4887 ms on localhost (35/85)
15/08/19 18:19:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 134217728 end: 258790728 length: 124573000 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608955 records.
15/08/19 18:19:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:10 INFO Executor: Finished task 33.0 in stage 7.0 (TID 757). 2125 bytes result sent to driver
15/08/19 18:19:10 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 775, localhost, ANY, 1757 bytes)
15/08/19 18:19:10 INFO Executor: Running task 51.0 in stage 7.0 (TID 775)
15/08/19 18:19:10 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 757) in 4982 ms on localhost (36/85)
15/08/19 18:19:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501135 records.
15/08/19 18:19:10 INFO InternalParquetRecordReader: Assembled and processed 3501092 records from 2 columns in 3875 ms: 903.5076 rec/ms, 1807.0153 cell/ms
15/08/19 18:19:10 INFO InternalParquetRecordReader: time spent so far 0% reading (29 ms) and 99% processing (3875 ms)
15/08/19 18:19:10 INFO InternalParquetRecordReader: at row 3501092. reading next block
15/08/19 18:19:10 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 107375
15/08/19 18:19:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:10 INFO InternalParquetRecordReader: block read in memory in 108 ms. row count = 3500100
15/08/19 18:19:10 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 3501135
15/08/19 18:19:10 INFO Executor: Finished task 35.0 in stage 7.0 (TID 759). 2125 bytes result sent to driver
15/08/19 18:19:10 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 776, localhost, ANY, 1772 bytes)
15/08/19 18:19:10 INFO Executor: Running task 52.0 in stage 7.0 (TID 776)
15/08/19 18:19:10 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 759) in 5051 ms on localhost (37/85)
15/08/19 18:19:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 134217728 end: 260690571 length: 126472843 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3662957 records.
15/08/19 18:19:10 INFO Executor: Finished task 36.0 in stage 7.0 (TID 760). 2125 bytes result sent to driver
15/08/19 18:19:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:10 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 777, localhost, ANY, 1758 bytes)
15/08/19 18:19:10 INFO Executor: Running task 53.0 in stage 7.0 (TID 777)
15/08/19 18:19:10 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 760) in 5012 ms on localhost (38/85)
15/08/19 18:19:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501088 records.
15/08/19 18:19:10 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 3500100
15/08/19 18:19:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:10 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3501088
15/08/19 18:19:10 INFO Executor: Finished task 37.0 in stage 7.0 (TID 761). 2125 bytes result sent to driver
15/08/19 18:19:10 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 778, localhost, ANY, 1768 bytes)
15/08/19 18:19:10 INFO Executor: Running task 54.0 in stage 7.0 (TID 778)
15/08/19 18:19:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 134217728 end: 258899835 length: 124682107 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:10 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 761) in 5120 ms on localhost (39/85)
15/08/19 18:19:10 INFO Executor: Finished task 38.0 in stage 7.0 (TID 762). 2125 bytes result sent to driver
15/08/19 18:19:10 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 779, localhost, ANY, 1756 bytes)
15/08/19 18:19:10 INFO Executor: Running task 55.0 in stage 7.0 (TID 779)
15/08/19 18:19:10 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 762) in 4981 ms on localhost (40/85)
15/08/19 18:19:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608995 records.
15/08/19 18:19:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500719 records.
15/08/19 18:19:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:10 INFO InternalParquetRecordReader: block read in memory in 52 ms. row count = 3501407
15/08/19 18:19:10 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 3500719
15/08/19 18:19:10 INFO Executor: Finished task 39.0 in stage 7.0 (TID 763). 2125 bytes result sent to driver
15/08/19 18:19:10 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 780, localhost, ANY, 1769 bytes)
15/08/19 18:19:10 INFO Executor: Running task 56.0 in stage 7.0 (TID 780)
15/08/19 18:19:11 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 763) in 4588 ms on localhost (41/85)
15/08/19 18:19:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 134217728 end: 261449270 length: 127231542 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3684271 records.
15/08/19 18:19:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:11 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 3500100
15/08/19 18:19:11 INFO Executor: Finished task 40.0 in stage 7.0 (TID 764). 2125 bytes result sent to driver
15/08/19 18:19:11 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 781, localhost, ANY, 1756 bytes)
15/08/19 18:19:11 INFO Executor: Running task 57.0 in stage 7.0 (TID 781)
15/08/19 18:19:11 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 764) in 4939 ms on localhost (42/85)
15/08/19 18:19:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501051 records.
15/08/19 18:19:11 INFO Executor: Finished task 42.0 in stage 7.0 (TID 766). 2125 bytes result sent to driver
15/08/19 18:19:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:11 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 782, localhost, ANY, 1772 bytes)
15/08/19 18:19:11 INFO Executor: Running task 58.0 in stage 7.0 (TID 782)
15/08/19 18:19:11 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4437 ms: 788.8438 rec/ms, 1577.6876 cell/ms
15/08/19 18:19:11 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 766) in 4668 ms on localhost (43/85)
15/08/19 18:19:11 INFO InternalParquetRecordReader: time spent so far 0% reading (41 ms) and 99% processing (4437 ms)
15/08/19 18:19:11 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 134217728 end: 260804804 length: 126587076 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:11 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 162550
15/08/19 18:19:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3663150 records.
15/08/19 18:19:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:11 INFO InternalParquetRecordReader: Assembled and processed 3500891 records from 2 columns in 4990 ms: 701.58136 rec/ms, 1403.1627 cell/ms
15/08/19 18:19:11 INFO InternalParquetRecordReader: time spent so far 0% reading (27 ms) and 99% processing (4990 ms)
15/08/19 18:19:11 INFO InternalParquetRecordReader: at row 3500891. reading next block
15/08/19 18:19:11 INFO InternalParquetRecordReader: block read in memory in 92 ms. row count = 3501051
15/08/19 18:19:11 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 108875
15/08/19 18:19:11 INFO InternalParquetRecordReader: block read in memory in 158 ms. row count = 3501200
15/08/19 18:19:11 INFO InternalParquetRecordReader: Assembled and processed 3501026 records from 2 columns in 4611 ms: 759.2769 rec/ms, 1518.5538 cell/ms
15/08/19 18:19:11 INFO InternalParquetRecordReader: time spent so far 0% reading (42 ms) and 99% processing (4611 ms)
15/08/19 18:19:11 INFO InternalParquetRecordReader: at row 3501026. reading next block
15/08/19 18:19:11 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 658408
15/08/19 18:19:12 INFO Executor: Finished task 43.0 in stage 7.0 (TID 767). 2125 bytes result sent to driver
15/08/19 18:19:12 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 783, localhost, ANY, 1758 bytes)
15/08/19 18:19:12 INFO Executor: Running task 59.0 in stage 7.0 (TID 783)
15/08/19 18:19:12 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 767) in 5175 ms on localhost (44/85)
15/08/19 18:19:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500780 records.
15/08/19 18:19:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:12 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 3500780
15/08/19 18:19:12 INFO Executor: Finished task 41.0 in stage 7.0 (TID 765). 2125 bytes result sent to driver
15/08/19 18:19:12 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 784, localhost, ANY, 1769 bytes)
15/08/19 18:19:12 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 765) in 5816 ms on localhost (45/85)
15/08/19 18:19:12 INFO Executor: Running task 60.0 in stage 7.0 (TID 784)
15/08/19 18:19:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 134217728 end: 258898835 length: 124681107 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609136 records.
15/08/19 18:19:12 INFO InternalParquetRecordReader: Assembled and processed 3501003 records from 2 columns in 4833 ms: 724.3954 rec/ms, 1448.7908 cell/ms
15/08/19 18:19:12 INFO InternalParquetRecordReader: time spent so far 0% reading (30 ms) and 99% processing (4833 ms)
15/08/19 18:19:12 INFO InternalParquetRecordReader: at row 3501003. reading next block
15/08/19 18:19:12 INFO Executor: Finished task 44.0 in stage 7.0 (TID 768). 2125 bytes result sent to driver
15/08/19 18:19:12 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 107448
15/08/19 18:19:12 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 785, localhost, ANY, 1757 bytes)
15/08/19 18:19:12 INFO Executor: Running task 61.0 in stage 7.0 (TID 785)
15/08/19 18:19:12 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 768) in 5324 ms on localhost (46/85)
15/08/19 18:19:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:12 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 3501367
15/08/19 18:19:12 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 3500100
15/08/19 18:19:12 INFO Executor: Finished task 47.0 in stage 7.0 (TID 771). 2125 bytes result sent to driver
15/08/19 18:19:12 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 786, localhost, ANY, 1770 bytes)
15/08/19 18:19:12 INFO Executor: Running task 62.0 in stage 7.0 (TID 786)
15/08/19 18:19:12 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 771) in 5251 ms on localhost (47/85)
15/08/19 18:19:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 134217728 end: 258817871 length: 124600143 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3610161 records.
15/08/19 18:19:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:12 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 3501357
15/08/19 18:19:12 INFO Executor: Finished task 48.0 in stage 7.0 (TID 772). 2125 bytes result sent to driver
15/08/19 18:19:12 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 787, localhost, ANY, 1758 bytes)
15/08/19 18:19:12 INFO Executor: Running task 63.0 in stage 7.0 (TID 787)
15/08/19 18:19:12 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 772) in 5303 ms on localhost (48/85)
15/08/19 18:19:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501165 records.
15/08/19 18:19:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:12 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 3501165
15/08/19 18:19:12 INFO Executor: Finished task 45.0 in stage 7.0 (TID 769). 2125 bytes result sent to driver
15/08/19 18:19:12 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 788, localhost, ANY, 1771 bytes)
15/08/19 18:19:12 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 769) in 5760 ms on localhost (49/85)
15/08/19 18:19:12 INFO Executor: Running task 64.0 in stage 7.0 (TID 788)
15/08/19 18:19:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 134217728 end: 258510998 length: 124293270 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608780 records.
15/08/19 18:19:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:13 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 3502929
15/08/19 18:19:14 INFO Executor: Finished task 49.0 in stage 7.0 (TID 773). 2125 bytes result sent to driver
15/08/19 18:19:14 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 789, localhost, ANY, 1757 bytes)
15/08/19 18:19:14 INFO Executor: Running task 65.0 in stage 7.0 (TID 789)
15/08/19 18:19:14 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 773) in 5065 ms on localhost (50/85)
15/08/19 18:19:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502743 records.
15/08/19 18:19:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:14 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 3502743
15/08/19 18:19:14 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4569 ms: 766.05383 rec/ms, 1532.1077 cell/ms
15/08/19 18:19:14 INFO InternalParquetRecordReader: time spent so far 2% reading (108 ms) and 97% processing (4569 ms)
15/08/19 18:19:14 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:15 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 108855
15/08/19 18:19:15 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4428 ms: 790.44714 rec/ms, 1580.8943 cell/ms
15/08/19 18:19:15 INFO InternalParquetRecordReader: time spent so far 0% reading (43 ms) and 99% processing (4428 ms)
15/08/19 18:19:15 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:15 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 162857
15/08/19 18:19:15 INFO Executor: Finished task 53.0 in stage 7.0 (TID 777). 2125 bytes result sent to driver
15/08/19 18:19:15 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 790, localhost, ANY, 1770 bytes)
15/08/19 18:19:15 INFO Executor: Running task 66.0 in stage 7.0 (TID 790)
15/08/19 18:19:15 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 777) in 4670 ms on localhost (51/85)
15/08/19 18:19:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 134217728 end: 258077510 length: 123859782 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3607229 records.
15/08/19 18:19:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:15 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 3503385
15/08/19 18:19:15 INFO Executor: Finished task 51.0 in stage 7.0 (TID 775). 2125 bytes result sent to driver
15/08/19 18:19:15 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 791, localhost, ANY, 1758 bytes)
15/08/19 18:19:15 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 775) in 5112 ms on localhost (52/85)
15/08/19 18:19:15 INFO Executor: Running task 67.0 in stage 7.0 (TID 791)
15/08/19 18:19:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:15 INFO Executor: Finished task 50.0 in stage 7.0 (TID 774). 2125 bytes result sent to driver
15/08/19 18:19:15 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 792, localhost, ANY, 1772 bytes)
15/08/19 18:19:15 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 774) in 5234 ms on localhost (53/85)
15/08/19 18:19:15 INFO Executor: Running task 68.0 in stage 7.0 (TID 792)
15/08/19 18:19:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 134217728 end: 258658247 length: 124440519 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:15 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4383 ms: 798.5626 rec/ms, 1597.1252 cell/ms
15/08/19 18:19:15 INFO InternalParquetRecordReader: time spent so far 0% reading (44 ms) and 99% processing (4383 ms)
15/08/19 18:19:15 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609673 records.
15/08/19 18:19:15 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 184171
15/08/19 18:19:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:15 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 3500100
15/08/19 18:19:15 INFO Executor: Finished task 52.0 in stage 7.0 (TID 776). 2125 bytes result sent to driver
15/08/19 18:19:15 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 3500100
15/08/19 18:19:15 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 793, localhost, ANY, 1758 bytes)
15/08/19 18:19:15 INFO Executor: Running task 69.0 in stage 7.0 (TID 793)
15/08/19 18:19:15 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 776) in 5012 ms on localhost (54/85)
15/08/19 18:19:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500967 records.
15/08/19 18:19:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:15 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 3500967
15/08/19 18:19:15 INFO InternalParquetRecordReader: Assembled and processed 3501200 records from 2 columns in 4084 ms: 857.29675 rec/ms, 1714.5935 cell/ms
15/08/19 18:19:15 INFO InternalParquetRecordReader: time spent so far 3% reading (158 ms) and 96% processing (4084 ms)
15/08/19 18:19:15 INFO InternalParquetRecordReader: at row 3501200. reading next block
15/08/19 18:19:15 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 161950
15/08/19 18:19:16 INFO Executor: Finished task 56.0 in stage 7.0 (TID 780). 2125 bytes result sent to driver
15/08/19 18:19:16 INFO Executor: Finished task 55.0 in stage 7.0 (TID 779). 2125 bytes result sent to driver
15/08/19 18:19:16 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 794, localhost, ANY, 1768 bytes)
15/08/19 18:19:16 INFO Executor: Running task 70.0 in stage 7.0 (TID 794)
15/08/19 18:19:16 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 795, localhost, ANY, 1757 bytes)
15/08/19 18:19:16 INFO Executor: Running task 71.0 in stage 7.0 (TID 795)
15/08/19 18:19:16 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 780) in 5045 ms on localhost (55/85)
15/08/19 18:19:16 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 779) in 5229 ms on localhost (56/85)
15/08/19 18:19:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 134217728 end: 258890765 length: 124673037 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608672 records.
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 3500860
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 3500100
15/08/19 18:19:16 INFO InternalParquetRecordReader: Assembled and processed 3501407 records from 2 columns in 5202 ms: 673.0886 rec/ms, 1346.1772 cell/ms
15/08/19 18:19:16 INFO InternalParquetRecordReader: time spent so far 0% reading (52 ms) and 99% processing (5202 ms)
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 3501407. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 107588
15/08/19 18:19:16 INFO Executor: Finished task 58.0 in stage 7.0 (TID 782). 2125 bytes result sent to driver
15/08/19 18:19:16 INFO Executor: Finished task 57.0 in stage 7.0 (TID 781). 2125 bytes result sent to driver
15/08/19 18:19:16 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 796, localhost, ANY, 1768 bytes)
15/08/19 18:19:16 INFO Executor: Running task 72.0 in stage 7.0 (TID 796)
15/08/19 18:19:16 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 797, localhost, ANY, 1758 bytes)
15/08/19 18:19:16 INFO Executor: Running task 73.0 in stage 7.0 (TID 797)
15/08/19 18:19:16 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 782) in 4765 ms on localhost (57/85)
15/08/19 18:19:16 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 781) in 4908 ms on localhost (58/85)
15/08/19 18:19:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 134217728 end: 258681625 length: 124463897 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609901 records.
15/08/19 18:19:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501368 records.
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 3501368
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 3500100
15/08/19 18:19:16 INFO Executor: Finished task 54.0 in stage 7.0 (TID 778). 2125 bytes result sent to driver
15/08/19 18:19:16 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 798, localhost, ANY, 1771 bytes)
15/08/19 18:19:16 INFO Executor: Running task 74.0 in stage 7.0 (TID 798)
15/08/19 18:19:16 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 778) in 5649 ms on localhost (59/85)
15/08/19 18:19:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 134217728 end: 258792347 length: 124574619 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608782 records.
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 3500100
15/08/19 18:19:16 INFO InternalParquetRecordReader: Assembled and processed 3501367 records from 2 columns in 4140 ms: 845.74084 rec/ms, 1691.4817 cell/ms
15/08/19 18:19:16 INFO InternalParquetRecordReader: time spent so far 0% reading (40 ms) and 99% processing (4140 ms)
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 3501367. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 107769
15/08/19 18:19:16 INFO Executor: Finished task 59.0 in stage 7.0 (TID 783). 2125 bytes result sent to driver
15/08/19 18:19:16 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 799, localhost, ANY, 1757 bytes)
15/08/19 18:19:16 INFO Executor: Running task 75.0 in stage 7.0 (TID 799)
15/08/19 18:19:16 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 783) in 4624 ms on localhost (60/85)
15/08/19 18:19:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:16 INFO InternalParquetRecordReader: Assembled and processed 3501357 records from 2 columns in 4179 ms: 837.84564 rec/ms, 1675.6913 cell/ms
15/08/19 18:19:16 INFO InternalParquetRecordReader: time spent so far 0% reading (35 ms) and 99% processing (4179 ms)
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 3501357. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 108804
15/08/19 18:19:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501297 records.
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 3501297
15/08/19 18:19:16 INFO InternalParquetRecordReader: Assembled and processed 3502929 records from 2 columns in 3868 ms: 905.6176 rec/ms, 1811.2352 cell/ms
15/08/19 18:19:16 INFO InternalParquetRecordReader: time spent so far 1% reading (40 ms) and 98% processing (3868 ms)
15/08/19 18:19:16 INFO InternalParquetRecordReader: at row 3502929. reading next block
15/08/19 18:19:16 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 105851
15/08/19 18:19:16 INFO Executor: Finished task 60.0 in stage 7.0 (TID 784). 2125 bytes result sent to driver
15/08/19 18:19:16 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 800, localhost, ANY, 1769 bytes)
15/08/19 18:19:16 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 784) in 4607 ms on localhost (61/85)
15/08/19 18:19:16 INFO Executor: Running task 76.0 in stage 7.0 (TID 800)
15/08/19 18:19:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 134217728 end: 258482455 length: 124264727 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3608268 records.
15/08/19 18:19:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:17 INFO InternalParquetRecordReader: block read in memory in 71 ms. row count = 3502399
15/08/19 18:19:17 INFO Executor: Finished task 63.0 in stage 7.0 (TID 787). 2125 bytes result sent to driver
15/08/19 18:19:17 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 801, localhost, ANY, 1758 bytes)
15/08/19 18:19:17 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 787) in 4462 ms on localhost (62/85)
15/08/19 18:19:17 INFO Executor: Running task 77.0 in stage 7.0 (TID 801)
15/08/19 18:19:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501079 records.
15/08/19 18:19:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:17 INFO InternalParquetRecordReader: block read in memory in 87 ms. row count = 3501079
15/08/19 18:19:17 INFO Executor: Finished task 62.0 in stage 7.0 (TID 786). 2125 bytes result sent to driver
15/08/19 18:19:17 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 802, localhost, ANY, 1771 bytes)
15/08/19 18:19:17 INFO Executor: Running task 78.0 in stage 7.0 (TID 802)
15/08/19 18:19:17 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 786) in 4943 ms on localhost (63/85)
15/08/19 18:19:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 134217728 end: 260809124 length: 126591396 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3663023 records.
15/08/19 18:19:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:17 INFO Executor: Finished task 64.0 in stage 7.0 (TID 788). 2125 bytes result sent to driver
15/08/19 18:19:17 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 803, localhost, ANY, 1758 bytes)
15/08/19 18:19:17 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 788) in 4629 ms on localhost (64/85)
15/08/19 18:19:17 INFO Executor: Running task 79.0 in stage 7.0 (TID 803)
15/08/19 18:19:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:17 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 3501178
15/08/19 18:19:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503367 records.
15/08/19 18:19:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:17 INFO InternalParquetRecordReader: block read in memory in 74 ms. row count = 3503367
15/08/19 18:19:17 INFO Executor: Finished task 61.0 in stage 7.0 (TID 785). 2125 bytes result sent to driver
15/08/19 18:19:17 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 804, localhost, ANY, 1770 bytes)
15/08/19 18:19:17 INFO Executor: Running task 80.0 in stage 7.0 (TID 804)
15/08/19 18:19:17 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 785) in 5511 ms on localhost (65/85)
15/08/19 18:19:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 134217728 end: 259851958 length: 125634230 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3658058 records.
15/08/19 18:19:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:18 INFO InternalParquetRecordReader: block read in memory in 334 ms. row count = 3503415
15/08/19 18:19:19 INFO Executor: Finished task 65.0 in stage 7.0 (TID 789). 2125 bytes result sent to driver
15/08/19 18:19:19 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 805, localhost, ANY, 1757 bytes)
15/08/19 18:19:19 INFO Executor: Running task 81.0 in stage 7.0 (TID 805)
15/08/19 18:19:19 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 789) in 4764 ms on localhost (66/85)
15/08/19 18:19:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/19 18:19:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:19 INFO InternalParquetRecordReader: block read in memory in 127 ms. row count = 3500100
15/08/19 18:19:19 INFO InternalParquetRecordReader: Assembled and processed 3503385 records from 2 columns in 4387 ms: 798.5833 rec/ms, 1597.1666 cell/ms
15/08/19 18:19:19 INFO InternalParquetRecordReader: time spent so far 1% reading (45 ms) and 98% processing (4387 ms)
15/08/19 18:19:19 INFO InternalParquetRecordReader: at row 3503385. reading next block
15/08/19 18:19:19 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 103844
15/08/19 18:19:19 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4320 ms: 810.2083 rec/ms, 1620.4166 cell/ms
15/08/19 18:19:19 INFO InternalParquetRecordReader: time spent so far 0% reading (40 ms) and 99% processing (4320 ms)
15/08/19 18:19:19 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:19 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 109573
15/08/19 18:19:20 INFO Executor: Finished task 68.0 in stage 7.0 (TID 792). 2125 bytes result sent to driver
15/08/19 18:19:20 INFO Executor: Finished task 66.0 in stage 7.0 (TID 790). 2125 bytes result sent to driver
15/08/19 18:19:20 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 806, localhost, ANY, 1768 bytes)
15/08/19 18:19:20 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 807, localhost, ANY, 1758 bytes)
15/08/19 18:19:20 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 792) in 5104 ms on localhost (67/85)
15/08/19 18:19:20 INFO Executor: Running task 83.0 in stage 7.0 (TID 807)
15/08/19 18:19:20 INFO Executor: Running task 82.0 in stage 7.0 (TID 806)
15/08/19 18:19:20 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 790) in 5260 ms on localhost (68/85)
15/08/19 18:19:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 134217728 end: 259121277 length: 124903549 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609750 records.
15/08/19 18:19:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500752 records.
15/08/19 18:19:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:20 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 3500752
15/08/19 18:19:20 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 3500100
15/08/19 18:19:20 INFO Executor: Finished task 69.0 in stage 7.0 (TID 793). 2125 bytes result sent to driver
15/08/19 18:19:20 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 808, localhost, ANY, 1769 bytes)
15/08/19 18:19:20 INFO Executor: Running task 84.0 in stage 7.0 (TID 808)
15/08/19 18:19:20 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 793) in 5358 ms on localhost (69/85)
15/08/19 18:19:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 134217728 end: 258781787 length: 124564059 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/19 18:19:20 INFO Executor: Finished task 67.0 in stage 7.0 (TID 791). 2125 bytes result sent to driver
15/08/19 18:19:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/19 18:19:20 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 791) in 5513 ms on localhost (70/85)
15/08/19 18:19:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3609212 records.
15/08/19 18:19:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/19 18:19:21 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 3500100
15/08/19 18:19:21 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4699 ms: 744.8606 rec/ms, 1489.7212 cell/ms
15/08/19 18:19:21 INFO InternalParquetRecordReader: time spent so far 0% reading (39 ms) and 99% processing (4699 ms)
15/08/19 18:19:21 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 109801
15/08/19 18:19:21 INFO InternalParquetRecordReader: Assembled and processed 3500860 records from 2 columns in 5195 ms: 673.89026 rec/ms, 1347.7805 cell/ms
15/08/19 18:19:21 INFO InternalParquetRecordReader: time spent so far 0% reading (32 ms) and 99% processing (5195 ms)
15/08/19 18:19:21 INFO InternalParquetRecordReader: at row 3500860. reading next block
15/08/19 18:19:21 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 107812
15/08/19 18:19:21 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4818 ms: 726.46326 rec/ms, 1452.9265 cell/ms
15/08/19 18:19:21 INFO InternalParquetRecordReader: time spent so far 0% reading (31 ms) and 99% processing (4818 ms)
15/08/19 18:19:21 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:21 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 108682
15/08/19 18:19:21 INFO Executor: Finished task 72.0 in stage 7.0 (TID 796). 2125 bytes result sent to driver
15/08/19 18:19:21 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 796) in 5083 ms on localhost (71/85)
15/08/19 18:19:21 INFO Executor: Finished task 71.0 in stage 7.0 (TID 795). 2125 bytes result sent to driver
15/08/19 18:19:21 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 795) in 5396 ms on localhost (72/85)
15/08/19 18:19:21 INFO Executor: Finished task 73.0 in stage 7.0 (TID 797). 2125 bytes result sent to driver
15/08/19 18:19:21 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 797) in 5444 ms on localhost (73/85)
15/08/19 18:19:21 INFO Executor: Finished task 70.0 in stage 7.0 (TID 794). 2125 bytes result sent to driver
15/08/19 18:19:21 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 794) in 5720 ms on localhost (74/85)
15/08/19 18:19:21 INFO Executor: Finished task 74.0 in stage 7.0 (TID 798). 2125 bytes result sent to driver
15/08/19 18:19:21 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 798) in 5325 ms on localhost (75/85)
15/08/19 18:19:22 INFO InternalParquetRecordReader: Assembled and processed 3502399 records from 2 columns in 4933 ms: 709.9937 rec/ms, 1419.9874 cell/ms
15/08/19 18:19:22 INFO InternalParquetRecordReader: time spent so far 1% reading (71 ms) and 98% processing (4933 ms)
15/08/19 18:19:22 INFO InternalParquetRecordReader: at row 3502399. reading next block
15/08/19 18:19:22 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 105869
15/08/19 18:19:22 INFO Executor: Finished task 79.0 in stage 7.0 (TID 803). 2125 bytes result sent to driver
15/08/19 18:19:22 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 803) in 4611 ms on localhost (76/85)
15/08/19 18:19:22 INFO Executor: Finished task 75.0 in stage 7.0 (TID 799). 2125 bytes result sent to driver
15/08/19 18:19:22 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 799) in 5429 ms on localhost (77/85)
15/08/19 18:19:22 INFO InternalParquetRecordReader: Assembled and processed 3501178 records from 2 columns in 4643 ms: 754.07666 rec/ms, 1508.1533 cell/ms
15/08/19 18:19:22 INFO InternalParquetRecordReader: time spent so far 1% reading (81 ms) and 98% processing (4643 ms)
15/08/19 18:19:22 INFO InternalParquetRecordReader: at row 3501178. reading next block
15/08/19 18:19:22 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 161845
15/08/19 18:19:22 INFO Executor: Finished task 76.0 in stage 7.0 (TID 800). 2125 bytes result sent to driver
15/08/19 18:19:22 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 800) in 5389 ms on localhost (78/85)
15/08/19 18:19:22 INFO Executor: Finished task 77.0 in stage 7.0 (TID 801). 2125 bytes result sent to driver
15/08/19 18:19:22 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 801) in 5269 ms on localhost (79/85)
15/08/19 18:19:22 INFO Executor: Finished task 78.0 in stage 7.0 (TID 802). 2125 bytes result sent to driver
15/08/19 18:19:22 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 802) in 5187 ms on localhost (80/85)
15/08/19 18:19:23 INFO InternalParquetRecordReader: Assembled and processed 3503415 records from 2 columns in 4738 ms: 739.4291 rec/ms, 1478.8582 cell/ms
15/08/19 18:19:23 INFO InternalParquetRecordReader: time spent so far 6% reading (334 ms) and 93% processing (4738 ms)
15/08/19 18:19:23 INFO InternalParquetRecordReader: at row 3503415. reading next block
15/08/19 18:19:23 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 154643
15/08/19 18:19:23 INFO Executor: Finished task 80.0 in stage 7.0 (TID 804). 2125 bytes result sent to driver
15/08/19 18:19:23 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 804) in 5510 ms on localhost (81/85)
15/08/19 18:19:23 INFO Executor: Finished task 81.0 in stage 7.0 (TID 805). 2125 bytes result sent to driver
15/08/19 18:19:23 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 805) in 4161 ms on localhost (82/85)
15/08/19 18:19:24 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 3395 ms: 1030.9573 rec/ms, 2061.9146 cell/ms
15/08/19 18:19:24 INFO InternalParquetRecordReader: time spent so far 1% reading (44 ms) and 98% processing (3395 ms)
15/08/19 18:19:24 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:24 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 109650
15/08/19 18:19:24 INFO Executor: Finished task 83.0 in stage 7.0 (TID 807). 2125 bytes result sent to driver
15/08/19 18:19:24 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 807) in 3697 ms on localhost (83/85)
15/08/19 18:19:24 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 3273 ms: 1069.3859 rec/ms, 2138.7717 cell/ms
15/08/19 18:19:24 INFO InternalParquetRecordReader: time spent so far 1% reading (38 ms) and 98% processing (3273 ms)
15/08/19 18:19:24 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/19 18:19:24 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 109112
15/08/19 18:19:24 INFO Executor: Finished task 82.0 in stage 7.0 (TID 806). 2125 bytes result sent to driver
15/08/19 18:19:24 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 806) in 3789 ms on localhost (84/85)
15/08/19 18:19:24 INFO Executor: Finished task 84.0 in stage 7.0 (TID 808). 2125 bytes result sent to driver
15/08/19 18:19:24 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 808) in 3630 ms on localhost (85/85)
15/08/19 18:19:24 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/19 18:19:24 INFO DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:423) finished in 79.926 s
15/08/19 18:19:24 INFO DAGScheduler: looking for newly runnable stages
15/08/19 18:19:24 INFO DAGScheduler: running: Set()
15/08/19 18:19:24 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/19 18:19:24 INFO DAGScheduler: failed: Set()
15/08/19 18:19:24 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@267a6ecb
15/08/19 18:19:24 INFO StatsReportListener: task runtime:(count: 69, mean: 4849.623188, stdev: 750.900563, max: 5816.000000, min: 52.000000)
15/08/19 18:19:24 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:24 INFO StatsReportListener: 	52.0 ms	3.8 s	4.2 s	4.6 s	5.0 s	5.3 s	5.5 s	5.6 s	5.8 s
15/08/19 18:19:24 INFO StatsReportListener: shuffle bytes written:(count: 69, mean: 20686229.420290, stdev: 2570534.951075, max: 24442972.000000, min: 0.000000)
15/08/19 18:19:24 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:24 INFO StatsReportListener: 	0.0 B	19.6 MB	19.6 MB	19.6 MB	19.7 MB	20.2 MB	20.5 MB	20.6 MB	23.3 MB
15/08/19 18:19:24 INFO StatsReportListener: task result size:(count: 69, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/19 18:19:24 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:24 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/19 18:19:24 INFO StatsReportListener: executor (non-fetch) time pct: (count: 69, mean: 99.202488, stdev: 2.948466, max: 99.833210, min: 75.000000)
15/08/19 18:19:24 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:24 INFO StatsReportListener: 	75 %	99 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %
15/08/19 18:19:24 INFO StatsReportListener: other time pct: (count: 69, mean: 0.797512, stdev: 2.948466, max: 25.000000, min: 0.166790)
15/08/19 18:19:24 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:19:24 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 1 %	25 %
15/08/19 18:19:24 INFO DAGScheduler: Missing parents for ResultStage 8: List()
15/08/19 18:19:24 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423), which is now runnable
15/08/19 18:19:24 INFO MemoryStore: ensureFreeSpace(16720) called with curMem=1444596, maxMem=22226833244
15/08/19 18:19:24 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.3 KB, free 20.7 GB)
15/08/19 18:19:24 INFO MemoryStore: ensureFreeSpace(7736) called with curMem=1461316, maxMem=22226833244
15/08/19 18:19:24 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.6 KB, free 20.7 GB)
15/08/19 18:19:24 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:36176 (size: 7.6 KB, free: 20.7 GB)
15/08/19 18:19:24 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874
15/08/19 18:19:24 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 8 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423)
15/08/19 18:19:24 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/19 18:19:24 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 809, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 810, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 811, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 812, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 813, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 814, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 815, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 816, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 817, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 818, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 819, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 820, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 821, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 822, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 823, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 824, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:24 INFO Executor: Running task 1.0 in stage 8.0 (TID 810)
15/08/19 18:19:24 INFO Executor: Running task 8.0 in stage 8.0 (TID 817)
15/08/19 18:19:24 INFO Executor: Running task 0.0 in stage 8.0 (TID 809)
15/08/19 18:19:24 INFO Executor: Running task 6.0 in stage 8.0 (TID 815)
15/08/19 18:19:24 INFO Executor: Running task 4.0 in stage 8.0 (TID 813)
15/08/19 18:19:24 INFO Executor: Running task 2.0 in stage 8.0 (TID 811)
15/08/19 18:19:24 INFO Executor: Running task 9.0 in stage 8.0 (TID 818)
15/08/19 18:19:24 INFO Executor: Running task 5.0 in stage 8.0 (TID 814)
15/08/19 18:19:24 INFO Executor: Running task 3.0 in stage 8.0 (TID 812)
15/08/19 18:19:24 INFO Executor: Running task 7.0 in stage 8.0 (TID 816)
15/08/19 18:19:24 INFO Executor: Running task 14.0 in stage 8.0 (TID 823)
15/08/19 18:19:24 INFO Executor: Running task 11.0 in stage 8.0 (TID 820)
15/08/19 18:19:24 INFO Executor: Running task 13.0 in stage 8.0 (TID 822)
15/08/19 18:19:24 INFO Executor: Running task 12.0 in stage 8.0 (TID 821)
15/08/19 18:19:24 INFO Executor: Running task 10.0 in stage 8.0 (TID 819)
15/08/19 18:19:24 INFO Executor: Running task 15.0 in stage 8.0 (TID 824)
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:31 INFO Executor: Finished task 2.0 in stage 8.0 (TID 811). 1426 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 10.0 in stage 8.0 (TID 819). 1737 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 0.0 in stage 8.0 (TID 809). 1737 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 14.0 in stage 8.0 (TID 823). 1668 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 5.0 in stage 8.0 (TID 814). 1738 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 13.0 in stage 8.0 (TID 822). 1669 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 3.0 in stage 8.0 (TID 812). 1668 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 15.0 in stage 8.0 (TID 824). 1602 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 4.0 in stage 8.0 (TID 813). 1602 bytes result sent to driver
15/08/19 18:19:31 INFO Executor: Finished task 6.0 in stage 8.0 (TID 815). 1667 bytes result sent to driver
15/08/19 18:19:31 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 825, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 16.0 in stage 8.0 (TID 825)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 826, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 17.0 in stage 8.0 (TID 826)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 827, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 18.0 in stage 8.0 (TID 827)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 828, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 829, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 19.0 in stage 8.0 (TID 828)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 830, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 831, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 832, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 20.0 in stage 8.0 (TID 829)
15/08/19 18:19:31 INFO Executor: Running task 23.0 in stage 8.0 (TID 832)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 833, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 24.0 in stage 8.0 (TID 833)
15/08/19 18:19:31 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 834, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 25.0 in stage 8.0 (TID 834)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO Executor: Running task 21.0 in stage 8.0 (TID 830)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO Executor: Running task 22.0 in stage 8.0 (TID 831)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 823) in 6893 ms on localhost (1/200)
15/08/19 18:19:31 INFO Executor: Finished task 8.0 in stage 8.0 (TID 817). 1803 bytes result sent to driver
15/08/19 18:19:31 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 835, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 819) in 6896 ms on localhost (2/200)
15/08/19 18:19:31 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 811) in 6901 ms on localhost (3/200)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 809) in 6903 ms on localhost (4/200)
15/08/19 18:19:31 INFO Executor: Running task 26.0 in stage 8.0 (TID 835)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:19:31 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 822) in 6921 ms on localhost (5/200)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 824) in 6929 ms on localhost (6/200)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:19:31 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 812) in 6937 ms on localhost (7/200)
15/08/19 18:19:31 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 814) in 6938 ms on localhost (8/200)
15/08/19 18:19:31 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 813) in 6941 ms on localhost (9/200)
15/08/19 18:19:31 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 817) in 6954 ms on localhost (10/200)
15/08/19 18:19:31 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 815) in 6955 ms on localhost (11/200)
15/08/19 18:19:31 INFO Executor: Finished task 12.0 in stage 8.0 (TID 821). 1669 bytes result sent to driver
15/08/19 18:19:31 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 836, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 27.0 in stage 8.0 (TID 836)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 821) in 6965 ms on localhost (12/200)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:31 INFO Executor: Finished task 9.0 in stage 8.0 (TID 818). 1602 bytes result sent to driver
15/08/19 18:19:31 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 837, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 28.0 in stage 8.0 (TID 837)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:31 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 818) in 7209 ms on localhost (13/200)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO Executor: Finished task 11.0 in stage 8.0 (TID 820). 1601 bytes result sent to driver
15/08/19 18:19:31 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 838, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 29.0 in stage 8.0 (TID 838)
15/08/19 18:19:31 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 820) in 7230 ms on localhost (14/200)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:19:31 INFO Executor: Finished task 7.0 in stage 8.0 (TID 816). 1669 bytes result sent to driver
15/08/19 18:19:31 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 839, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO Executor: Running task 30.0 in stage 8.0 (TID 839)
15/08/19 18:19:31 INFO Executor: Finished task 1.0 in stage 8.0 (TID 810). 1426 bytes result sent to driver
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 840, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO Executor: Running task 31.0 in stage 8.0 (TID 840)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 816) in 7260 ms on localhost (15/200)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 810) in 7265 ms on localhost (16/200)
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:36 INFO Executor: Finished task 27.0 in stage 8.0 (TID 836). 1426 bytes result sent to driver
15/08/19 18:19:36 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 841, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:36 INFO Executor: Running task 32.0 in stage 8.0 (TID 841)
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:36 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 836) in 5360 ms on localhost (17/200)
15/08/19 18:19:36 INFO Executor: Finished task 23.0 in stage 8.0 (TID 832). 1669 bytes result sent to driver
15/08/19 18:19:36 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 842, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:36 INFO Executor: Running task 33.0 in stage 8.0 (TID 842)
15/08/19 18:19:36 INFO Executor: Finished task 22.0 in stage 8.0 (TID 831). 1426 bytes result sent to driver
15/08/19 18:19:36 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 843, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:36 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 832) in 5490 ms on localhost (18/200)
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:36 INFO Executor: Finished task 20.0 in stage 8.0 (TID 829). 1426 bytes result sent to driver
15/08/19 18:19:36 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 844, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO Executor: Running task 35.0 in stage 8.0 (TID 844)
15/08/19 18:19:37 INFO Executor: Running task 34.0 in stage 8.0 (TID 843)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 831) in 5507 ms on localhost (19/200)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 829) in 5538 ms on localhost (20/200)
15/08/19 18:19:37 INFO Executor: Finished task 19.0 in stage 8.0 (TID 828). 1426 bytes result sent to driver
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 845, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO Executor: Running task 36.0 in stage 8.0 (TID 845)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO Executor: Finished task 21.0 in stage 8.0 (TID 830). 1426 bytes result sent to driver
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 828) in 5567 ms on localhost (21/200)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/19 18:19:37 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 846, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO Executor: Running task 37.0 in stage 8.0 (TID 846)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 830) in 5601 ms on localhost (22/200)
15/08/19 18:19:37 INFO Executor: Finished task 25.0 in stage 8.0 (TID 834). 1426 bytes result sent to driver
15/08/19 18:19:37 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 847, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO Executor: Running task 38.0 in stage 8.0 (TID 847)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:19:37 INFO Executor: Finished task 16.0 in stage 8.0 (TID 825). 1668 bytes result sent to driver
15/08/19 18:19:37 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 848, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO Executor: Running task 39.0 in stage 8.0 (TID 848)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO Executor: Finished task 24.0 in stage 8.0 (TID 833). 1426 bytes result sent to driver
15/08/19 18:19:37 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 849, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 834) in 5640 ms on localhost (23/200)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO Executor: Running task 40.0 in stage 8.0 (TID 849)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 833) in 5660 ms on localhost (24/200)
15/08/19 18:19:37 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 825) in 5669 ms on localhost (25/200)
15/08/19 18:19:37 INFO Executor: Finished task 26.0 in stage 8.0 (TID 835). 1736 bytes result sent to driver
15/08/19 18:19:37 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 850, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO Executor: Running task 41.0 in stage 8.0 (TID 850)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 835) in 5659 ms on localhost (26/200)
15/08/19 18:19:37 INFO Executor: Finished task 31.0 in stage 8.0 (TID 840). 1601 bytes result sent to driver
15/08/19 18:19:37 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 851, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO Executor: Running task 42.0 in stage 8.0 (TID 851)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 840) in 5413 ms on localhost (27/200)
15/08/19 18:19:37 INFO Executor: Finished task 28.0 in stage 8.0 (TID 837). 1601 bytes result sent to driver
15/08/19 18:19:37 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 852, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO Executor: Running task 43.0 in stage 8.0 (TID 852)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 837) in 5982 ms on localhost (28/200)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO Executor: Finished task 18.0 in stage 8.0 (TID 827). 1670 bytes result sent to driver
15/08/19 18:19:37 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 853, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO Executor: Running task 44.0 in stage 8.0 (TID 853)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 827) in 6349 ms on localhost (29/200)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO Executor: Finished task 30.0 in stage 8.0 (TID 839). 1601 bytes result sent to driver
15/08/19 18:19:37 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 854, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:37 INFO Executor: Running task 45.0 in stage 8.0 (TID 854)
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:37 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 839) in 6125 ms on localhost (30/200)
15/08/19 18:19:38 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:39 INFO Executor: Finished task 17.0 in stage 8.0 (TID 826). 1426 bytes result sent to driver
15/08/19 18:19:39 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 855, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:39 INFO Executor: Running task 46.0 in stage 8.0 (TID 855)
15/08/19 18:19:39 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 826) in 7579 ms on localhost (31/200)
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:39 INFO Executor: Finished task 29.0 in stage 8.0 (TID 838). 1805 bytes result sent to driver
15/08/19 18:19:39 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 856, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:39 INFO Executor: Running task 47.0 in stage 8.0 (TID 856)
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:39 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 838) in 7304 ms on localhost (32/200)
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:42 INFO Executor: Finished task 33.0 in stage 8.0 (TID 842). 1736 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 857, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 48.0 in stage 8.0 (TID 857)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/19 18:19:42 INFO Executor: Finished task 34.0 in stage 8.0 (TID 843). 1670 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 842) in 5142 ms on localhost (33/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 858, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Finished task 36.0 in stage 8.0 (TID 845). 1803 bytes result sent to driver
15/08/19 18:19:42 INFO Executor: Running task 49.0 in stage 8.0 (TID 858)
15/08/19 18:19:42 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 859, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:42 INFO Executor: Running task 50.0 in stage 8.0 (TID 859)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 845) in 5094 ms on localhost (34/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 843) in 5166 ms on localhost (35/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
15/08/19 18:19:42 INFO Executor: Finished task 40.0 in stage 8.0 (TID 849). 1600 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 860, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 51.0 in stage 8.0 (TID 860)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 849) in 5054 ms on localhost (36/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:42 INFO Executor: Finished task 42.0 in stage 8.0 (TID 851). 1426 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 861, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 52.0 in stage 8.0 (TID 861)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 851) in 4972 ms on localhost (37/200)
15/08/19 18:19:42 INFO Executor: Finished task 37.0 in stage 8.0 (TID 846). 1426 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 862, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 53.0 in stage 8.0 (TID 862)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 846) in 5186 ms on localhost (38/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:42 INFO Executor: Finished task 38.0 in stage 8.0 (TID 847). 1426 bytes result sent to driver
15/08/19 18:19:42 INFO Executor: Finished task 35.0 in stage 8.0 (TID 844). 1669 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 863, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 54.0 in stage 8.0 (TID 863)
15/08/19 18:19:42 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 864, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 55.0 in stage 8.0 (TID 864)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 847) in 5196 ms on localhost (39/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 844) in 5316 ms on localhost (40/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO Executor: Finished task 32.0 in stage 8.0 (TID 841). 1602 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 865, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 56.0 in stage 8.0 (TID 865)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 841) in 5460 ms on localhost (41/200)
15/08/19 18:19:42 INFO Executor: Finished task 41.0 in stage 8.0 (TID 850). 1668 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 866, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 57.0 in stage 8.0 (TID 866)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 850) in 5635 ms on localhost (42/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO Executor: Finished task 43.0 in stage 8.0 (TID 852). 1426 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 867, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 58.0 in stage 8.0 (TID 867)
15/08/19 18:19:42 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 852) in 5085 ms on localhost (43/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO Executor: Finished task 47.0 in stage 8.0 (TID 856). 1670 bytes result sent to driver
15/08/19 18:19:42 INFO Executor: Finished task 46.0 in stage 8.0 (TID 855). 1426 bytes result sent to driver
15/08/19 18:19:42 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 868, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 869, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:42 INFO Executor: Running task 60.0 in stage 8.0 (TID 869)
15/08/19 18:19:42 INFO Executor: Running task 59.0 in stage 8.0 (TID 868)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 856) in 3775 ms on localhost (44/200)
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:42 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 855) in 3867 ms on localhost (45/200)
15/08/19 18:19:43 INFO Executor: Finished task 45.0 in stage 8.0 (TID 854). 1602 bytes result sent to driver
15/08/19 18:19:43 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 870, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:43 INFO Executor: Running task 61.0 in stage 8.0 (TID 870)
15/08/19 18:19:43 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 854) in 5040 ms on localhost (46/200)
15/08/19 18:19:43 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:43 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:43 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:44 INFO Executor: Finished task 39.0 in stage 8.0 (TID 848). 1736 bytes result sent to driver
15/08/19 18:19:44 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 871, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:44 INFO Executor: Running task 62.0 in stage 8.0 (TID 871)
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:44 INFO Executor: Finished task 44.0 in stage 8.0 (TID 853). 1668 bytes result sent to driver
15/08/19 18:19:44 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 848) in 7203 ms on localhost (47/200)
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/19 18:19:44 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 872, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:44 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 853) in 6527 ms on localhost (48/200)
15/08/19 18:19:44 INFO Executor: Running task 63.0 in stage 8.0 (TID 872)
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:19:45 INFO Executor: Finished task 52.0 in stage 8.0 (TID 861). 1736 bytes result sent to driver
15/08/19 18:19:45 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 873, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:45 INFO Executor: Running task 64.0 in stage 8.0 (TID 873)
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:45 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 861) in 2985 ms on localhost (49/200)
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:45 INFO Executor: Finished task 48.0 in stage 8.0 (TID 857). 1669 bytes result sent to driver
15/08/19 18:19:45 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 874, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:45 INFO Executor: Running task 65.0 in stage 8.0 (TID 874)
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:45 INFO Executor: Finished task 53.0 in stage 8.0 (TID 862). 1805 bytes result sent to driver
15/08/19 18:19:45 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 875, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:45 INFO Executor: Running task 66.0 in stage 8.0 (TID 875)
15/08/19 18:19:45 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 857) in 3118 ms on localhost (50/200)
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 862) in 2981 ms on localhost (51/200)
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:46 INFO Executor: Finished task 50.0 in stage 8.0 (TID 859). 1602 bytes result sent to driver
15/08/19 18:19:46 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 876, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:46 INFO Executor: Running task 67.0 in stage 8.0 (TID 876)
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:46 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 859) in 4472 ms on localhost (52/200)
15/08/19 18:19:46 INFO Executor: Finished task 49.0 in stage 8.0 (TID 858). 1426 bytes result sent to driver
15/08/19 18:19:46 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 877, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:46 INFO Executor: Running task 68.0 in stage 8.0 (TID 877)
15/08/19 18:19:46 INFO Executor: Finished task 54.0 in stage 8.0 (TID 863). 1669 bytes result sent to driver
15/08/19 18:19:46 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 878, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 858) in 4533 ms on localhost (53/200)
15/08/19 18:19:46 INFO Executor: Running task 69.0 in stage 8.0 (TID 878)
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 863) in 4389 ms on localhost (54/200)
15/08/19 18:19:46 INFO Executor: Finished task 56.0 in stage 8.0 (TID 865). 1736 bytes result sent to driver
15/08/19 18:19:46 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 879, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:46 INFO Executor: Running task 70.0 in stage 8.0 (TID 879)
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 865) in 4359 ms on localhost (55/200)
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO Executor: Finished task 51.0 in stage 8.0 (TID 860). 1668 bytes result sent to driver
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/19 18:19:46 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 880, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:46 INFO Executor: Running task 71.0 in stage 8.0 (TID 880)
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:46 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 860) in 4608 ms on localhost (56/200)
15/08/19 18:19:47 INFO Executor: Finished task 57.0 in stage 8.0 (TID 866). 1602 bytes result sent to driver
15/08/19 18:19:47 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 881, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:47 INFO Executor: Running task 72.0 in stage 8.0 (TID 881)
15/08/19 18:19:47 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 866) in 4227 ms on localhost (57/200)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO Executor: Finished task 58.0 in stage 8.0 (TID 867). 1600 bytes result sent to driver
15/08/19 18:19:47 INFO Executor: Finished task 55.0 in stage 8.0 (TID 864). 1669 bytes result sent to driver
15/08/19 18:19:47 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 882, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:47 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 883, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:47 INFO Executor: Running task 73.0 in stage 8.0 (TID 882)
15/08/19 18:19:47 INFO Executor: Running task 74.0 in stage 8.0 (TID 883)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:19:47 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 864) in 4923 ms on localhost (58/200)
15/08/19 18:19:47 INFO Executor: Finished task 59.0 in stage 8.0 (TID 868). 1737 bytes result sent to driver
15/08/19 18:19:47 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 867) in 4371 ms on localhost (59/200)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 884, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:47 INFO Executor: Running task 75.0 in stage 8.0 (TID 884)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 868) in 4322 ms on localhost (60/200)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:47 INFO Executor: Finished task 62.0 in stage 8.0 (TID 871). 1601 bytes result sent to driver
15/08/19 18:19:47 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 885, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:47 INFO Executor: Running task 76.0 in stage 8.0 (TID 885)
15/08/19 18:19:47 INFO Executor: Finished task 60.0 in stage 8.0 (TID 869). 1669 bytes result sent to driver
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:47 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 886, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO Executor: Running task 77.0 in stage 8.0 (TID 886)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:47 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 871) in 3013 ms on localhost (61/200)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 869) in 4440 ms on localhost (62/200)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:47 INFO Executor: Finished task 63.0 in stage 8.0 (TID 872). 1667 bytes result sent to driver
15/08/19 18:19:47 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 887, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:47 INFO Executor: Running task 78.0 in stage 8.0 (TID 887)
15/08/19 18:19:47 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 872) in 3232 ms on localhost (63/200)
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:48 INFO Executor: Finished task 61.0 in stage 8.0 (TID 870). 1872 bytes result sent to driver
15/08/19 18:19:48 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 888, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:48 INFO Executor: Running task 79.0 in stage 8.0 (TID 888)
15/08/19 18:19:48 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:48 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:48 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:48 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 870) in 5536 ms on localhost (64/200)
15/08/19 18:19:49 INFO Executor: Finished task 65.0 in stage 8.0 (TID 874). 1668 bytes result sent to driver
15/08/19 18:19:49 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 889, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:49 INFO Executor: Running task 80.0 in stage 8.0 (TID 889)
15/08/19 18:19:49 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/19 18:19:49 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:49 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 874) in 4096 ms on localhost (65/200)
15/08/19 18:19:49 INFO Executor: Finished task 64.0 in stage 8.0 (TID 873). 1735 bytes result sent to driver
15/08/19 18:19:50 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 890, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:50 INFO Executor: Running task 81.0 in stage 8.0 (TID 890)
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:50 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 873) in 5659 ms on localhost (66/200)
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:50 INFO Executor: Finished task 69.0 in stage 8.0 (TID 878). 1426 bytes result sent to driver
15/08/19 18:19:50 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 891, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:50 INFO Executor: Finished task 71.0 in stage 8.0 (TID 880). 1805 bytes result sent to driver
15/08/19 18:19:50 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 892, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:50 INFO Executor: Running task 83.0 in stage 8.0 (TID 892)
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:50 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 878) in 4216 ms on localhost (67/200)
15/08/19 18:19:50 INFO Executor: Running task 82.0 in stage 8.0 (TID 891)
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:50 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 880) in 4156 ms on localhost (68/200)
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO Executor: Finished task 68.0 in stage 8.0 (TID 877). 1426 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 893, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 877) in 4379 ms on localhost (69/200)
15/08/19 18:19:51 INFO Executor: Running task 84.0 in stage 8.0 (TID 893)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO Executor: Finished task 66.0 in stage 8.0 (TID 875). 1602 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 894, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 85.0 in stage 8.0 (TID 894)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 875) in 5947 ms on localhost (70/200)
15/08/19 18:19:51 INFO Executor: Finished task 67.0 in stage 8.0 (TID 876). 1668 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 895, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 86.0 in stage 8.0 (TID 895)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 876) in 4688 ms on localhost (71/200)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO Executor: Finished task 70.0 in stage 8.0 (TID 879). 1426 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 896, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 87.0 in stage 8.0 (TID 896)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 879) in 4585 ms on localhost (72/200)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO Executor: Finished task 75.0 in stage 8.0 (TID 884). 1737 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 897, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 88.0 in stage 8.0 (TID 897)
15/08/19 18:19:51 INFO Executor: Finished task 76.0 in stage 8.0 (TID 885). 1669 bytes result sent to driver
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:51 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 898, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 89.0 in stage 8.0 (TID 898)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 885) in 3983 ms on localhost (73/200)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 884) in 4101 ms on localhost (74/200)
15/08/19 18:19:51 INFO Executor: Finished task 73.0 in stage 8.0 (TID 882). 1804 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 899, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 90.0 in stage 8.0 (TID 899)
15/08/19 18:19:51 INFO Executor: Finished task 74.0 in stage 8.0 (TID 883). 1737 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 900, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 91.0 in stage 8.0 (TID 900)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 882) in 4143 ms on localhost (75/200)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 883) in 4144 ms on localhost (76/200)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO Executor: Finished task 72.0 in stage 8.0 (TID 881). 1426 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 901, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 92.0 in stage 8.0 (TID 901)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 881) in 4457 ms on localhost (77/200)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO Executor: Finished task 77.0 in stage 8.0 (TID 886). 1736 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 902, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 93.0 in stage 8.0 (TID 902)
15/08/19 18:19:51 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 886) in 4149 ms on localhost (78/200)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO Executor: Finished task 78.0 in stage 8.0 (TID 887). 1601 bytes result sent to driver
15/08/19 18:19:51 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 903, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:51 INFO Executor: Running task 94.0 in stage 8.0 (TID 903)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:51 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 887) in 4116 ms on localhost (79/200)
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:52 INFO Executor: Finished task 79.0 in stage 8.0 (TID 888). 1801 bytes result sent to driver
15/08/19 18:19:52 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 904, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:52 INFO Executor: Running task 95.0 in stage 8.0 (TID 904)
15/08/19 18:19:52 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:52 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 888) in 3930 ms on localhost (80/200)
15/08/19 18:19:52 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:53 INFO Executor: Finished task 80.0 in stage 8.0 (TID 889). 1670 bytes result sent to driver
15/08/19 18:19:53 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 905, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:53 INFO Executor: Running task 96.0 in stage 8.0 (TID 905)
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:53 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 889) in 3829 ms on localhost (81/200)
15/08/19 18:19:53 INFO Executor: Finished task 83.0 in stage 8.0 (TID 892). 1426 bytes result sent to driver
15/08/19 18:19:53 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 906, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:53 INFO Executor: Running task 97.0 in stage 8.0 (TID 906)
15/08/19 18:19:53 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 892) in 2427 ms on localhost (82/200)
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:53 INFO Executor: Finished task 82.0 in stage 8.0 (TID 891). 1736 bytes result sent to driver
15/08/19 18:19:53 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 907, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:53 INFO Executor: Running task 98.0 in stage 8.0 (TID 907)
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:53 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 891) in 2498 ms on localhost (83/200)
15/08/19 18:19:55 INFO Executor: Finished task 84.0 in stage 8.0 (TID 893). 1426 bytes result sent to driver
15/08/19 18:19:55 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 908, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:55 INFO Executor: Running task 99.0 in stage 8.0 (TID 908)
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:55 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 893) in 4815 ms on localhost (84/200)
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:55 INFO Executor: Finished task 86.0 in stage 8.0 (TID 895). 1669 bytes result sent to driver
15/08/19 18:19:55 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 909, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:55 INFO Executor: Running task 100.0 in stage 8.0 (TID 909)
15/08/19 18:19:55 INFO Executor: Finished task 85.0 in stage 8.0 (TID 894). 1426 bytes result sent to driver
15/08/19 18:19:55 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 910, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:55 INFO Executor: Running task 101.0 in stage 8.0 (TID 910)
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:55 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 895) in 4703 ms on localhost (85/200)
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:55 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 894) in 4744 ms on localhost (86/200)
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:55 INFO Executor: Finished task 87.0 in stage 8.0 (TID 896). 1601 bytes result sent to driver
15/08/19 18:19:55 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 911, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:55 INFO Executor: Running task 102.0 in stage 8.0 (TID 911)
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:55 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 896) in 4693 ms on localhost (87/200)
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO Executor: Finished task 81.0 in stage 8.0 (TID 890). 1668 bytes result sent to driver
15/08/19 18:19:56 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 912, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:56 INFO Executor: Running task 103.0 in stage 8.0 (TID 912)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO Executor: Finished task 90.0 in stage 8.0 (TID 899). 1804 bytes result sent to driver
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 913, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:56 INFO Executor: Running task 104.0 in stage 8.0 (TID 913)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 899) in 4733 ms on localhost (88/200)
15/08/19 18:19:56 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 890) in 5238 ms on localhost (89/200)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO Executor: Finished task 89.0 in stage 8.0 (TID 898). 1426 bytes result sent to driver
15/08/19 18:19:56 INFO Executor: Finished task 91.0 in stage 8.0 (TID 900). 1941 bytes result sent to driver
15/08/19 18:19:56 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 914, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:56 INFO Executor: Running task 105.0 in stage 8.0 (TID 914)
15/08/19 18:19:56 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 915, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:56 INFO Executor: Running task 106.0 in stage 8.0 (TID 915)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 900) in 4795 ms on localhost (90/200)
15/08/19 18:19:56 INFO Executor: Finished task 88.0 in stage 8.0 (TID 897). 1735 bytes result sent to driver
15/08/19 18:19:56 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 916, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:56 INFO Executor: Running task 107.0 in stage 8.0 (TID 916)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:56 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 898) in 4826 ms on localhost (91/200)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:56 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 897) in 4845 ms on localhost (92/200)
15/08/19 18:19:56 INFO Executor: Finished task 93.0 in stage 8.0 (TID 902). 1736 bytes result sent to driver
15/08/19 18:19:56 INFO Executor: Finished task 92.0 in stage 8.0 (TID 901). 1602 bytes result sent to driver
15/08/19 18:19:56 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 917, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:56 INFO Executor: Running task 108.0 in stage 8.0 (TID 917)
15/08/19 18:19:56 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 918, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:56 INFO Executor: Running task 109.0 in stage 8.0 (TID 918)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 901) in 4935 ms on localhost (93/200)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:56 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 902) in 4921 ms on localhost (94/200)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:56 INFO Executor: Finished task 94.0 in stage 8.0 (TID 903). 1670 bytes result sent to driver
15/08/19 18:19:56 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 919, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:19:56 INFO Executor: Running task 110.0 in stage 8.0 (TID 919)
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:19:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:19:59 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 903) in 8316 ms on localhost (95/200)
15/08/19 18:20:00 INFO Executor: Finished task 95.0 in stage 8.0 (TID 904). 1805 bytes result sent to driver
15/08/19 18:20:00 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 920, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:00 INFO Executor: Running task 111.0 in stage 8.0 (TID 920)
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:00 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 904) in 7664 ms on localhost (96/200)
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:00 INFO Executor: Finished task 96.0 in stage 8.0 (TID 905). 1670 bytes result sent to driver
15/08/19 18:20:00 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 921, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:00 INFO Executor: Finished task 97.0 in stage 8.0 (TID 906). 1426 bytes result sent to driver
15/08/19 18:20:00 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 922, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:00 INFO Executor: Running task 113.0 in stage 8.0 (TID 922)
15/08/19 18:20:00 INFO Executor: Running task 112.0 in stage 8.0 (TID 921)
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:00 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 906) in 7607 ms on localhost (97/200)
15/08/19 18:20:00 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 905) in 7827 ms on localhost (98/200)
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:00 INFO Executor: Finished task 98.0 in stage 8.0 (TID 907). 1668 bytes result sent to driver
15/08/19 18:20:00 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 923, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:00 INFO Executor: Running task 114.0 in stage 8.0 (TID 923)
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:20:00 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 907) in 7599 ms on localhost (99/200)
15/08/19 18:20:04 INFO Executor: Finished task 99.0 in stage 8.0 (TID 908). 1601 bytes result sent to driver
15/08/19 18:20:04 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 924, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:04 INFO Executor: Finished task 100.0 in stage 8.0 (TID 909). 1669 bytes result sent to driver
15/08/19 18:20:04 INFO Executor: Running task 115.0 in stage 8.0 (TID 924)
15/08/19 18:20:04 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 925, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO Executor: Running task 116.0 in stage 8.0 (TID 925)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 909) in 8686 ms on localhost (100/200)
15/08/19 18:20:04 INFO Executor: Finished task 102.0 in stage 8.0 (TID 911). 1426 bytes result sent to driver
15/08/19 18:20:04 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 926, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO Executor: Running task 117.0 in stage 8.0 (TID 926)
15/08/19 18:20:04 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 908) in 8752 ms on localhost (101/200)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 911) in 8650 ms on localhost (102/200)
15/08/19 18:20:04 INFO Executor: Finished task 101.0 in stage 8.0 (TID 910). 1426 bytes result sent to driver
15/08/19 18:20:04 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 927, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:04 INFO Executor: Running task 118.0 in stage 8.0 (TID 927)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 910) in 8743 ms on localhost (103/200)
15/08/19 18:20:04 INFO Executor: Finished task 103.0 in stage 8.0 (TID 912). 1426 bytes result sent to driver
15/08/19 18:20:04 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 928, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:04 INFO Executor: Running task 119.0 in stage 8.0 (TID 928)
15/08/19 18:20:04 INFO Executor: Finished task 104.0 in stage 8.0 (TID 913). 1426 bytes result sent to driver
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 929, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:04 INFO Executor: Running task 120.0 in stage 8.0 (TID 929)
15/08/19 18:20:04 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 912) in 8726 ms on localhost (104/200)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 913) in 8720 ms on localhost (105/200)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO Executor: Finished task 105.0 in stage 8.0 (TID 914). 1426 bytes result sent to driver
15/08/19 18:20:04 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 930, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:04 INFO Executor: Running task 121.0 in stage 8.0 (TID 930)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 914) in 8772 ms on localhost (106/200)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO Executor: Finished task 107.0 in stage 8.0 (TID 916). 1669 bytes result sent to driver
15/08/19 18:20:04 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 931, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:04 INFO Executor: Running task 122.0 in stage 8.0 (TID 931)
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:04 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 916) in 8799 ms on localhost (107/200)
15/08/19 18:20:05 INFO Executor: Finished task 109.0 in stage 8.0 (TID 918). 1601 bytes result sent to driver
15/08/19 18:20:05 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 932, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:05 INFO Executor: Running task 123.0 in stage 8.0 (TID 932)
15/08/19 18:20:05 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 918) in 8785 ms on localhost (108/200)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO Executor: Finished task 106.0 in stage 8.0 (TID 915). 1426 bytes result sent to driver
15/08/19 18:20:05 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 933, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:05 INFO Executor: Running task 124.0 in stage 8.0 (TID 933)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 915) in 9198 ms on localhost (109/200)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO Executor: Finished task 110.0 in stage 8.0 (TID 919). 1426 bytes result sent to driver
15/08/19 18:20:05 INFO Executor: Finished task 111.0 in stage 8.0 (TID 920). 1601 bytes result sent to driver
15/08/19 18:20:05 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 934, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:05 INFO Executor: Running task 125.0 in stage 8.0 (TID 934)
15/08/19 18:20:05 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 935, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:05 INFO Executor: Running task 126.0 in stage 8.0 (TID 935)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 919) in 8899 ms on localhost (110/200)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 920) in 5537 ms on localhost (111/200)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO Executor: Finished task 108.0 in stage 8.0 (TID 917). 1426 bytes result sent to driver
15/08/19 18:20:05 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 936, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO Executor: Running task 127.0 in stage 8.0 (TID 936)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/19 18:20:05 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 917) in 9219 ms on localhost (112/200)
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:09 INFO Executor: Finished task 113.0 in stage 8.0 (TID 922). 1600 bytes result sent to driver
15/08/19 18:20:09 INFO Executor: Finished task 112.0 in stage 8.0 (TID 921). 1736 bytes result sent to driver
15/08/19 18:20:09 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 937, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:09 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 938, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:09 INFO Executor: Running task 129.0 in stage 8.0 (TID 938)
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:09 INFO Executor: Running task 128.0 in stage 8.0 (TID 937)
15/08/19 18:20:09 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 922) in 8676 ms on localhost (113/200)
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:09 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 921) in 8685 ms on localhost (114/200)
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:09 INFO Executor: Finished task 114.0 in stage 8.0 (TID 923). 1738 bytes result sent to driver
15/08/19 18:20:09 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 939, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:09 INFO Executor: Running task 130.0 in stage 8.0 (TID 939)
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:09 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 923) in 8707 ms on localhost (115/200)
15/08/19 18:20:09 INFO Executor: Finished task 117.0 in stage 8.0 (TID 926). 1602 bytes result sent to driver
15/08/19 18:20:09 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 940, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:09 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 926) in 5336 ms on localhost (116/200)
15/08/19 18:20:09 INFO Executor: Running task 131.0 in stage 8.0 (TID 940)
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:10 INFO Executor: Finished task 115.0 in stage 8.0 (TID 924). 1426 bytes result sent to driver
15/08/19 18:20:10 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 941, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:10 INFO Executor: Running task 132.0 in stage 8.0 (TID 941)
15/08/19 18:20:10 INFO Executor: Finished task 116.0 in stage 8.0 (TID 925). 1668 bytes result sent to driver
15/08/19 18:20:10 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 942, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:10 INFO Executor: Running task 133.0 in stage 8.0 (TID 942)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:10 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 924) in 5563 ms on localhost (117/200)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:10 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 925) in 5562 ms on localhost (118/200)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO Executor: Finished task 118.0 in stage 8.0 (TID 927). 1668 bytes result sent to driver
15/08/19 18:20:10 INFO Executor: Finished task 119.0 in stage 8.0 (TID 928). 1670 bytes result sent to driver
15/08/19 18:20:10 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 943, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:10 INFO Executor: Running task 134.0 in stage 8.0 (TID 943)
15/08/19 18:20:10 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 944, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:10 INFO Executor: Running task 135.0 in stage 8.0 (TID 944)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 927) in 5522 ms on localhost (119/200)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:10 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 928) in 5414 ms on localhost (120/200)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO Executor: Finished task 120.0 in stage 8.0 (TID 929). 1602 bytes result sent to driver
15/08/19 18:20:10 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 945, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:10 INFO Executor: Running task 136.0 in stage 8.0 (TID 945)
15/08/19 18:20:10 INFO Executor: Finished task 122.0 in stage 8.0 (TID 931). 1669 bytes result sent to driver
15/08/19 18:20:10 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 946, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:10 INFO Executor: Running task 137.0 in stage 8.0 (TID 946)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:20:10 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 931) in 5308 ms on localhost (121/200)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/19 18:20:10 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 929) in 5452 ms on localhost (122/200)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO Executor: Finished task 121.0 in stage 8.0 (TID 930). 1601 bytes result sent to driver
15/08/19 18:20:10 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 947, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:10 INFO Executor: Running task 138.0 in stage 8.0 (TID 947)
15/08/19 18:20:10 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 930) in 5483 ms on localhost (123/200)
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:12 INFO Executor: Finished task 123.0 in stage 8.0 (TID 932). 1737 bytes result sent to driver
15/08/19 18:20:12 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 948, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:12 INFO Executor: Running task 139.0 in stage 8.0 (TID 948)
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:12 INFO Executor: Finished task 124.0 in stage 8.0 (TID 933). 1805 bytes result sent to driver
15/08/19 18:20:12 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 949, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:12 INFO Executor: Running task 140.0 in stage 8.0 (TID 949)
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:12 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 932) in 7802 ms on localhost (124/200)
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:12 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 933) in 7682 ms on localhost (125/200)
15/08/19 18:20:13 INFO Executor: Finished task 126.0 in stage 8.0 (TID 935). 1602 bytes result sent to driver
15/08/19 18:20:13 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 950, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:13 INFO Executor: Running task 141.0 in stage 8.0 (TID 950)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 935) in 7511 ms on localhost (126/200)
15/08/19 18:20:13 INFO Executor: Finished task 125.0 in stage 8.0 (TID 934). 1670 bytes result sent to driver
15/08/19 18:20:13 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 951, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:13 INFO Executor: Running task 142.0 in stage 8.0 (TID 951)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 934) in 7540 ms on localhost (127/200)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:13 INFO Executor: Finished task 127.0 in stage 8.0 (TID 936). 1602 bytes result sent to driver
15/08/19 18:20:13 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 952, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:13 INFO Executor: Running task 143.0 in stage 8.0 (TID 952)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 936) in 7767 ms on localhost (128/200)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO Executor: Finished task 130.0 in stage 8.0 (TID 939). 1601 bytes result sent to driver
15/08/19 18:20:13 INFO Executor: Finished task 129.0 in stage 8.0 (TID 938). 1426 bytes result sent to driver
15/08/19 18:20:13 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 953, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:13 INFO Executor: Running task 144.0 in stage 8.0 (TID 953)
15/08/19 18:20:13 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 954, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:13 INFO Executor: Running task 145.0 in stage 8.0 (TID 954)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:13 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 939) in 4279 ms on localhost (129/200)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 938) in 4314 ms on localhost (130/200)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO Executor: Finished task 128.0 in stage 8.0 (TID 937). 1601 bytes result sent to driver
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 955, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:13 INFO Executor: Running task 146.0 in stage 8.0 (TID 955)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:13 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 937) in 4345 ms on localhost (131/200)
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:16 INFO Executor: Finished task 131.0 in stage 8.0 (TID 940). 1426 bytes result sent to driver
15/08/19 18:20:16 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 956, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:16 INFO Executor: Running task 147.0 in stage 8.0 (TID 956)
15/08/19 18:20:16 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 940) in 6927 ms on localhost (132/200)
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:16 INFO Executor: Finished task 132.0 in stage 8.0 (TID 941). 1602 bytes result sent to driver
15/08/19 18:20:16 INFO Executor: Finished task 135.0 in stage 8.0 (TID 944). 1426 bytes result sent to driver
15/08/19 18:20:16 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 957, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:16 INFO Executor: Running task 148.0 in stage 8.0 (TID 957)
15/08/19 18:20:16 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 958, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:16 INFO Executor: Running task 149.0 in stage 8.0 (TID 958)
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:16 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 941) in 6894 ms on localhost (133/200)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO Executor: Finished task 134.0 in stage 8.0 (TID 943). 1426 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 944) in 6883 ms on localhost (134/200)
15/08/19 18:20:17 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 959, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 150.0 in stage 8.0 (TID 959)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 943) in 6902 ms on localhost (135/200)
15/08/19 18:20:17 INFO Executor: Finished task 138.0 in stage 8.0 (TID 947). 1602 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 960, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 151.0 in stage 8.0 (TID 960)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 947) in 6864 ms on localhost (136/200)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO Executor: Finished task 133.0 in stage 8.0 (TID 942). 1602 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 961, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 152.0 in stage 8.0 (TID 961)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 942) in 7189 ms on localhost (137/200)
15/08/19 18:20:17 INFO Executor: Finished task 136.0 in stage 8.0 (TID 945). 1735 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 962, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 153.0 in stage 8.0 (TID 962)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 945) in 7125 ms on localhost (138/200)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO Executor: Finished task 137.0 in stage 8.0 (TID 946). 1737 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 963, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 154.0 in stage 8.0 (TID 963)
15/08/19 18:20:17 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 946) in 7198 ms on localhost (139/200)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO Executor: Finished task 140.0 in stage 8.0 (TID 949). 1600 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 964, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 155.0 in stage 8.0 (TID 964)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 949) in 4588 ms on localhost (140/200)
15/08/19 18:20:17 INFO Executor: Finished task 141.0 in stage 8.0 (TID 950). 1426 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 965, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 156.0 in stage 8.0 (TID 965)
15/08/19 18:20:17 INFO Executor: Finished task 139.0 in stage 8.0 (TID 948). 1602 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 966, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 157.0 in stage 8.0 (TID 966)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 948) in 4686 ms on localhost (141/200)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 950) in 4570 ms on localhost (142/200)
15/08/19 18:20:17 INFO Executor: Finished task 142.0 in stage 8.0 (TID 951). 1668 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 967, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 158.0 in stage 8.0 (TID 967)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 951) in 4637 ms on localhost (143/200)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO Executor: Finished task 143.0 in stage 8.0 (TID 952). 1602 bytes result sent to driver
15/08/19 18:20:17 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 968, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:17 INFO Executor: Running task 159.0 in stage 8.0 (TID 968)
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:17 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 952) in 4549 ms on localhost (144/200)
15/08/19 18:20:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/19 18:20:20 INFO Executor: Finished task 145.0 in stage 8.0 (TID 954). 1735 bytes result sent to driver
15/08/19 18:20:20 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 969, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:20 INFO Executor: Running task 160.0 in stage 8.0 (TID 969)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:21 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 954) in 7156 ms on localhost (145/200)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO Executor: Finished task 144.0 in stage 8.0 (TID 953). 1669 bytes result sent to driver
15/08/19 18:20:21 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 970, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:21 INFO Executor: Running task 161.0 in stage 8.0 (TID 970)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 953) in 7225 ms on localhost (146/200)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:21 INFO Executor: Finished task 146.0 in stage 8.0 (TID 955). 1601 bytes result sent to driver
15/08/19 18:20:21 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 971, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:21 INFO Executor: Running task 162.0 in stage 8.0 (TID 971)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 955) in 7255 ms on localhost (147/200)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO Executor: Finished task 147.0 in stage 8.0 (TID 956). 1601 bytes result sent to driver
15/08/19 18:20:21 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 972, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:21 INFO Executor: Running task 163.0 in stage 8.0 (TID 972)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 956) in 4581 ms on localhost (148/200)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO Executor: Finished task 148.0 in stage 8.0 (TID 957). 1874 bytes result sent to driver
15/08/19 18:20:21 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 973, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:21 INFO Executor: Running task 164.0 in stage 8.0 (TID 973)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 957) in 4473 ms on localhost (149/200)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO Executor: Finished task 150.0 in stage 8.0 (TID 959). 1736 bytes result sent to driver
15/08/19 18:20:21 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 974, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:21 INFO Executor: Running task 165.0 in stage 8.0 (TID 974)
15/08/19 18:20:21 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 959) in 4503 ms on localhost (150/200)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO Executor: Finished task 149.0 in stage 8.0 (TID 958). 1735 bytes result sent to driver
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:21 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 975, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:21 INFO Executor: Running task 166.0 in stage 8.0 (TID 975)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 958) in 4537 ms on localhost (151/200)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO Executor: Finished task 151.0 in stage 8.0 (TID 960). 1669 bytes result sent to driver
15/08/19 18:20:21 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 976, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:21 INFO Executor: Running task 167.0 in stage 8.0 (TID 976)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:21 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 960) in 4483 ms on localhost (152/200)
15/08/19 18:20:21 INFO Executor: Finished task 153.0 in stage 8.0 (TID 962). 1601 bytes result sent to driver
15/08/19 18:20:21 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 977, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:21 INFO Executor: Running task 168.0 in stage 8.0 (TID 977)
15/08/19 18:20:21 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 962) in 4495 ms on localhost (153/200)
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:23 INFO Executor: Finished task 154.0 in stage 8.0 (TID 963). 1670 bytes result sent to driver
15/08/19 18:20:23 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 978, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:23 INFO Executor: Running task 169.0 in stage 8.0 (TID 978)
15/08/19 18:20:23 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:23 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 963) in 6598 ms on localhost (154/200)
15/08/19 18:20:23 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO Executor: Finished task 155.0 in stage 8.0 (TID 964). 1601 bytes result sent to driver
15/08/19 18:20:24 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 979, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:24 INFO Executor: Running task 170.0 in stage 8.0 (TID 979)
15/08/19 18:20:24 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 964) in 6522 ms on localhost (155/200)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO Executor: Finished task 157.0 in stage 8.0 (TID 966). 1602 bytes result sent to driver
15/08/19 18:20:24 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 980, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:24 INFO Executor: Running task 171.0 in stage 8.0 (TID 980)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 966) in 6620 ms on localhost (156/200)
15/08/19 18:20:24 INFO Executor: Finished task 158.0 in stage 8.0 (TID 967). 1872 bytes result sent to driver
15/08/19 18:20:24 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 981, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:24 INFO Executor: Running task 172.0 in stage 8.0 (TID 981)
15/08/19 18:20:24 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 967) in 6589 ms on localhost (157/200)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO Executor: Finished task 152.0 in stage 8.0 (TID 961). 1426 bytes result sent to driver
15/08/19 18:20:24 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 982, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:24 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 961) in 7049 ms on localhost (158/200)
15/08/19 18:20:24 INFO Executor: Running task 173.0 in stage 8.0 (TID 982)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO Executor: Finished task 156.0 in stage 8.0 (TID 965). 1669 bytes result sent to driver
15/08/19 18:20:24 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 983, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:24 INFO Executor: Running task 174.0 in stage 8.0 (TID 983)
15/08/19 18:20:24 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 965) in 6804 ms on localhost (159/200)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO Executor: Finished task 159.0 in stage 8.0 (TID 968). 1426 bytes result sent to driver
15/08/19 18:20:24 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 984, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:24 INFO Executor: Running task 175.0 in stage 8.0 (TID 984)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:24 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 968) in 6754 ms on localhost (160/200)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:24 INFO Executor: Finished task 161.0 in stage 8.0 (TID 970). 1426 bytes result sent to driver
15/08/19 18:20:24 INFO Executor: Finished task 162.0 in stage 8.0 (TID 971). 1670 bytes result sent to driver
15/08/19 18:20:24 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 985, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:24 INFO Executor: Running task 176.0 in stage 8.0 (TID 985)
15/08/19 18:20:24 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 986, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:24 INFO Executor: Running task 177.0 in stage 8.0 (TID 986)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 971) in 3857 ms on localhost (161/200)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:24 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 970) in 3908 ms on localhost (162/200)
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:25 INFO Executor: Finished task 160.0 in stage 8.0 (TID 969). 1602 bytes result sent to driver
15/08/19 18:20:25 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 987, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:25 INFO Executor: Running task 178.0 in stage 8.0 (TID 987)
15/08/19 18:20:25 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:25 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:25 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 969) in 4285 ms on localhost (163/200)
15/08/19 18:20:28 INFO Executor: Finished task 164.0 in stage 8.0 (TID 973). 1426 bytes result sent to driver
15/08/19 18:20:28 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 988, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:28 INFO Executor: Running task 179.0 in stage 8.0 (TID 988)
15/08/19 18:20:28 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 973) in 7168 ms on localhost (164/200)
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:28 INFO Executor: Finished task 163.0 in stage 8.0 (TID 972). 1426 bytes result sent to driver
15/08/19 18:20:28 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 989, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:28 INFO Executor: Running task 180.0 in stage 8.0 (TID 989)
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:28 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 972) in 7294 ms on localhost (165/200)
15/08/19 18:20:28 INFO Executor: Finished task 166.0 in stage 8.0 (TID 975). 1601 bytes result sent to driver
15/08/19 18:20:28 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 990, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:28 INFO Executor: Running task 181.0 in stage 8.0 (TID 990)
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:28 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 975) in 7279 ms on localhost (166/200)
15/08/19 18:20:28 INFO Executor: Finished task 165.0 in stage 8.0 (TID 974). 1669 bytes result sent to driver
15/08/19 18:20:28 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 991, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:28 INFO Executor: Running task 182.0 in stage 8.0 (TID 991)
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:28 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 974) in 7363 ms on localhost (167/200)
15/08/19 18:20:29 INFO Executor: Finished task 167.0 in stage 8.0 (TID 976). 1602 bytes result sent to driver
15/08/19 18:20:29 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 992, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:29 INFO Executor: Running task 183.0 in stage 8.0 (TID 992)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 976) in 7360 ms on localhost (168/200)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO Executor: Finished task 168.0 in stage 8.0 (TID 977). 1668 bytes result sent to driver
15/08/19 18:20:29 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 993, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:29 INFO Executor: Running task 184.0 in stage 8.0 (TID 993)
15/08/19 18:20:29 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 977) in 7407 ms on localhost (169/200)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO Executor: Finished task 170.0 in stage 8.0 (TID 979). 1668 bytes result sent to driver
15/08/19 18:20:29 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 994, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:29 INFO Executor: Running task 185.0 in stage 8.0 (TID 994)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:29 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 979) in 5441 ms on localhost (170/200)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:29 INFO Executor: Finished task 171.0 in stage 8.0 (TID 980). 1668 bytes result sent to driver
15/08/19 18:20:29 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 995, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:29 INFO Executor: Running task 186.0 in stage 8.0 (TID 995)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 980) in 5451 ms on localhost (171/200)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO Executor: Finished task 172.0 in stage 8.0 (TID 981). 1668 bytes result sent to driver
15/08/19 18:20:29 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 996, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:29 INFO Executor: Running task 187.0 in stage 8.0 (TID 996)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 981) in 5398 ms on localhost (172/200)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO Executor: Finished task 169.0 in stage 8.0 (TID 978). 1736 bytes result sent to driver
15/08/19 18:20:29 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 997, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:29 INFO Executor: Running task 188.0 in stage 8.0 (TID 997)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 978) in 5883 ms on localhost (173/200)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO Executor: Finished task 173.0 in stage 8.0 (TID 982). 1602 bytes result sent to driver
15/08/19 18:20:29 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 998, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:29 INFO Executor: Running task 189.0 in stage 8.0 (TID 998)
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:29 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 982) in 5647 ms on localhost (174/200)
15/08/19 18:20:30 INFO Executor: Finished task 175.0 in stage 8.0 (TID 984). 1601 bytes result sent to driver
15/08/19 18:20:30 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 999, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:30 INFO Executor: Running task 190.0 in stage 8.0 (TID 999)
15/08/19 18:20:30 INFO Executor: Finished task 174.0 in stage 8.0 (TID 983). 1669 bytes result sent to driver
15/08/19 18:20:30 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 1000, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:30 INFO Executor: Running task 191.0 in stage 8.0 (TID 1000)
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:30 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 983) in 5755 ms on localhost (175/200)
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:30 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 984) in 5529 ms on localhost (176/200)
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:33 INFO Executor: Finished task 176.0 in stage 8.0 (TID 985). 1602 bytes result sent to driver
15/08/19 18:20:33 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 1001, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:33 INFO Executor: Running task 192.0 in stage 8.0 (TID 1001)
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:33 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 985) in 8338 ms on localhost (177/200)
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:33 INFO Executor: Finished task 177.0 in stage 8.0 (TID 986). 1426 bytes result sent to driver
15/08/19 18:20:33 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 1002, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:33 INFO Executor: Running task 193.0 in stage 8.0 (TID 1002)
15/08/19 18:20:33 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 986) in 8565 ms on localhost (178/200)
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:33 INFO Executor: Finished task 181.0 in stage 8.0 (TID 990). 1668 bytes result sent to driver
15/08/19 18:20:33 INFO Executor: Finished task 182.0 in stage 8.0 (TID 991). 1802 bytes result sent to driver
15/08/19 18:20:33 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 1003, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:33 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 1004, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:33 INFO Executor: Running task 195.0 in stage 8.0 (TID 1004)
15/08/19 18:20:33 INFO Executor: Running task 194.0 in stage 8.0 (TID 1003)
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:34 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 990) in 5237 ms on localhost (179/200)
15/08/19 18:20:34 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 991) in 5172 ms on localhost (180/200)
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO Executor: Finished task 179.0 in stage 8.0 (TID 988). 1670 bytes result sent to driver
15/08/19 18:20:34 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 1005, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:34 INFO Executor: Running task 196.0 in stage 8.0 (TID 1005)
15/08/19 18:20:34 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 988) in 5430 ms on localhost (181/200)
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO Executor: Finished task 178.0 in stage 8.0 (TID 987). 1805 bytes result sent to driver
15/08/19 18:20:34 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 1006, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:34 INFO Executor: Running task 197.0 in stage 8.0 (TID 1006)
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:34 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 987) in 8982 ms on localhost (182/200)
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/19 18:20:34 INFO Executor: Finished task 185.0 in stage 8.0 (TID 994). 1602 bytes result sent to driver
15/08/19 18:20:34 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 1007, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:34 INFO Executor: Running task 198.0 in stage 8.0 (TID 1007)
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 994) in 4962 ms on localhost (183/200)
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO Executor: Finished task 180.0 in stage 8.0 (TID 989). 1669 bytes result sent to driver
15/08/19 18:20:34 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 1008, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/19 18:20:34 INFO Executor: Running task 199.0 in stage 8.0 (TID 1008)
15/08/19 18:20:34 INFO Executor: Finished task 184.0 in stage 8.0 (TID 993). 1601 bytes result sent to driver
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 84 non-empty blocks out of 85 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 989) in 5819 ms on localhost (184/200)
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 152 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/19 18:20:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/19 18:20:34 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 993) in 5300 ms on localhost (185/200)
15/08/19 18:20:34 INFO Executor: Finished task 183.0 in stage 8.0 (TID 992). 1600 bytes result sent to driver
15/08/19 18:20:34 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 992) in 5568 ms on localhost (186/200)
15/08/19 18:20:34 INFO Executor: Finished task 186.0 in stage 8.0 (TID 995). 1602 bytes result sent to driver
15/08/19 18:20:34 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 995) in 5008 ms on localhost (187/200)
15/08/19 18:20:34 INFO Executor: Finished task 187.0 in stage 8.0 (TID 996). 1602 bytes result sent to driver
15/08/19 18:20:34 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 996) in 5114 ms on localhost (188/200)
15/08/19 18:20:34 INFO Executor: Finished task 188.0 in stage 8.0 (TID 997). 1601 bytes result sent to driver
15/08/19 18:20:34 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 997) in 5085 ms on localhost (189/200)
15/08/19 18:20:36 INFO Executor: Finished task 189.0 in stage 8.0 (TID 998). 1668 bytes result sent to driver
15/08/19 18:20:36 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 998) in 6885 ms on localhost (190/200)
15/08/19 18:20:37 INFO Executor: Finished task 191.0 in stage 8.0 (TID 1000). 1601 bytes result sent to driver
15/08/19 18:20:37 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 1000) in 6954 ms on localhost (191/200)
15/08/19 18:20:37 INFO Executor: Finished task 190.0 in stage 8.0 (TID 999). 1803 bytes result sent to driver
15/08/19 18:20:37 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 999) in 7013 ms on localhost (192/200)
15/08/19 18:20:37 INFO Executor: Finished task 193.0 in stage 8.0 (TID 1002). 1601 bytes result sent to driver
15/08/19 18:20:37 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 1002) in 3778 ms on localhost (193/200)
15/08/19 18:20:37 INFO Executor: Finished task 192.0 in stage 8.0 (TID 1001). 1601 bytes result sent to driver
15/08/19 18:20:37 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 1001) in 4098 ms on localhost (194/200)
15/08/19 18:20:37 INFO Executor: Finished task 195.0 in stage 8.0 (TID 1004). 1426 bytes result sent to driver
15/08/19 18:20:37 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 1004) in 3622 ms on localhost (195/200)
15/08/19 18:20:37 INFO Executor: Finished task 194.0 in stage 8.0 (TID 1003). 1426 bytes result sent to driver
15/08/19 18:20:37 INFO Executor: Finished task 197.0 in stage 8.0 (TID 1006). 1667 bytes result sent to driver
15/08/19 18:20:37 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 1003) in 3914 ms on localhost (196/200)
15/08/19 18:20:37 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 1006) in 3700 ms on localhost (197/200)
15/08/19 18:20:37 INFO Executor: Finished task 196.0 in stage 8.0 (TID 1005). 1602 bytes result sent to driver
15/08/19 18:20:37 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 1005) in 3927 ms on localhost (198/200)
15/08/19 18:20:38 INFO Executor: Finished task 199.0 in stage 8.0 (TID 1008). 1803 bytes result sent to driver
15/08/19 18:20:38 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 1008) in 3564 ms on localhost (199/200)
15/08/19 18:20:38 INFO Executor: Finished task 198.0 in stage 8.0 (TID 1007). 1426 bytes result sent to driver
15/08/19 18:20:38 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 1007) in 3757 ms on localhost (200/200)
15/08/19 18:20:38 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/19 18:20:38 INFO DAGScheduler: ResultStage 8 (processCmd at CliDriver.java:423) finished in 73.527 s
15/08/19 18:20:38 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3a09866b
15/08/19 18:20:38 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:423, took 154.428768 s
15/08/19 18:20:38 INFO StatsReportListener: task runtime:(count: 200, mean: 5797.190000, stdev: 1553.389099, max: 9219.000000, min: 2427.000000)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	2.4 s	3.8 s	4.0 s	4.6 s	5.5 s	7.0 s	7.8 s	8.7 s	9.2 s
15/08/19 18:20:38 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.940000, stdev: 2.392990, max: 24.000000, min: 0.000000)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	3.0 ms	24.0 ms
15/08/19 18:20:38 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/19 18:20:38 INFO StatsReportListener: task result size:(count: 200, mean: 1613.935000, stdev: 123.486561, max: 1941.000000, min: 1426.000000)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	1426.0 B	1426.0 B	1426.0 B	1600.0 B	1602.0 B	1670.0 B	1738.0 B	1805.0 B	1941.0 B
15/08/19 18:20:38 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 98.952809, stdev: 3.261847, max: 99.772002, min: 60.209235)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	60 %	98 %	99 %	99 %	99 %	100 %	100 %	100 %	100 %
15/08/19 18:20:38 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.017525, stdev: 0.042258, max: 0.425909, min: 0.000000)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/19 18:20:38 INFO StatsReportListener: other time pct: (count: 200, mean: 1.029667, stdev: 3.262787, max: 39.790765, min: 0.227998)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 1 %	 2 %	40 %
15/08/19 18:20:38 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/19 18:20:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:20:38 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/19 18:20:38 INFO DAGScheduler: Got job 3 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/19 18:20:38 INFO DAGScheduler: Final stage: ResultStage 9(processCmd at CliDriver.java:423)
15/08/19 18:20:38 INFO DAGScheduler: Parents of final stage: List()
15/08/19 18:20:38 INFO DAGScheduler: Missing parents: List()
15/08/19 18:20:38 INFO DAGScheduler: Submitting ResultStage 9 (ParallelCollectionRDD[45] at processCmd at CliDriver.java:423), which has no missing parents
15/08/19 18:20:38 INFO MemoryStore: ensureFreeSpace(71392) called with curMem=1469052, maxMem=22226833244
15/08/19 18:20:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 69.7 KB, free 20.7 GB)
15/08/19 18:20:38 INFO MemoryStore: ensureFreeSpace(26152) called with curMem=1540444, maxMem=22226833244
15/08/19 18:20:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.5 KB, free 20.7 GB)
15/08/19 18:20:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:36176 (size: 25.5 KB, free: 20.7 GB)
15/08/19 18:20:38 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874
15/08/19 18:20:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (ParallelCollectionRDD[45] at processCmd at CliDriver.java:423)
15/08/19 18:20:38 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/08/19 18:20:38 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 1009, localhost, PROCESS_LOCAL, 8196 bytes)
15/08/19 18:20:38 INFO Executor: Running task 0.0 in stage 9.0 (TID 1009)
15/08/19 18:20:38 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/19 18:20:38 INFO CodecConfig: Compression: GZIP
15/08/19 18:20:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/19 18:20:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/19 18:20:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/19 18:20:38 INFO ParquetOutputFormat: Dictionary is on
15/08/19 18:20:38 INFO ParquetOutputFormat: Validation is off
15/08/19 18:20:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/19 18:20:38 INFO CodecPool: Got brand-new compressor [.gz]
15/08/19 18:20:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 40,692,448
15/08/19 18:20:38 INFO ColumnChunkPageWriteStore: written 610B for [c_name] BINARY: 100 values, 2,207B raw, 546B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:20:38 INFO ColumnChunkPageWriteStore: written 453B for [c_custkey] INT32: 100 values, 407B raw, 417B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:20:38 INFO ColumnChunkPageWriteStore: written 466B for [o_orderkey] INT32: 100 values, 407B raw, 430B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:20:38 INFO ColumnChunkPageWriteStore: written 432B for [o_orderdate] BINARY: 100 values, 1,407B raw, 384B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:20:38 INFO ColumnChunkPageWriteStore: written 623B for [o_totalprice] DOUBLE: 100 values, 807B raw, 579B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN]
15/08/19 18:20:38 INFO ColumnChunkPageWriteStore: written 141B for [sum_quantity] DOUBLE: 100 values, 74B raw, 97B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 17 entries, 136B raw, 17B comp}
15/08/19 18:20:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508191820_0009_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_large_volume_customer_par/_temporary/0/task_201508191820_0009_m_000000
15/08/19 18:20:38 INFO SparkHadoopMapRedUtil: attempt_201508191820_0009_m_000000_0: Committed
15/08/19 18:20:38 INFO Executor: Finished task 0.0 in stage 9.0 (TID 1009). 577 bytes result sent to driver
15/08/19 18:20:38 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 1009) in 654 ms on localhost (1/1)
15/08/19 18:20:38 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/19 18:20:38 INFO DAGScheduler: ResultStage 9 (processCmd at CliDriver.java:423) finished in 0.655 s
15/08/19 18:20:38 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4dfdf4bc
15/08/19 18:20:38 INFO DAGScheduler: Job 3 finished: processCmd at CliDriver.java:423, took 0.697776 s
15/08/19 18:20:38 INFO StatsReportListener: task runtime:(count: 1, mean: 654.000000, stdev: 0.000000, max: 654.000000, min: 654.000000)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	654.0 ms	654.0 ms	654.0 ms	654.0 ms	654.0 ms	654.0 ms	654.0 ms	654.0 ms	654.0 ms
15/08/19 18:20:38 INFO StatsReportListener: task result size:(count: 1, mean: 577.000000, stdev: 0.000000, max: 577.000000, min: 577.000000)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B
15/08/19 18:20:38 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 91.131498, stdev: 0.000000, max: 91.131498, min: 91.131498)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	91 %	91 %	91 %	91 %	91 %	91 %	91 %	91 %	91 %
15/08/19 18:20:38 INFO StatsReportListener: other time pct: (count: 1, mean: 8.868502, stdev: 0.000000, max: 8.868502, min: 8.868502)
15/08/19 18:20:38 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:38 INFO StatsReportListener: 	 9 %	 9 %	 9 %	 9 %	 9 %	 9 %	 9 %	 9 %	 9 %
15/08/19 18:20:38 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:20:39 INFO DefaultWriterContainer: Job job_201508191820_0000 committed.
15/08/19 18:20:39 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/19 18:20:39 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_large_volume_customer_par/_common_metadata
15/08/19 18:20:39 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/19 18:20:39 INFO DAGScheduler: Got job 4 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/19 18:20:39 INFO DAGScheduler: Final stage: ResultStage 10(processCmd at CliDriver.java:423)
15/08/19 18:20:39 INFO DAGScheduler: Parents of final stage: List()
15/08/19 18:20:39 INFO DAGScheduler: Missing parents: List()
15/08/19 18:20:39 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423), which has no missing parents
15/08/19 18:20:39 INFO MemoryStore: ensureFreeSpace(3104) called with curMem=1566596, maxMem=22226833244
15/08/19 18:20:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.0 KB, free 20.7 GB)
15/08/19 18:20:39 INFO MemoryStore: ensureFreeSpace(1803) called with curMem=1569700, maxMem=22226833244
15/08/19 18:20:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1803.0 B, free 20.7 GB)
15/08/19 18:20:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:36176 (size: 1803.0 B, free: 20.7 GB)
15/08/19 18:20:39 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874
15/08/19 18:20:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423)
15/08/19 18:20:39 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
15/08/19 18:20:39 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 1010, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/19 18:20:39 INFO Executor: Running task 0.0 in stage 10.0 (TID 1010)
15/08/19 18:20:39 INFO Executor: Finished task 0.0 in stage 10.0 (TID 1010). 606 bytes result sent to driver
15/08/19 18:20:39 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 1010) in 11 ms on localhost (1/1)
15/08/19 18:20:39 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/19 18:20:39 INFO DAGScheduler: ResultStage 10 (processCmd at CliDriver.java:423) finished in 0.011 s
15/08/19 18:20:39 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@e8236b1
15/08/19 18:20:39 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:423, took 0.029405 s
15/08/19 18:20:39 INFO StatsReportListener: task runtime:(count: 1, mean: 11.000000, stdev: 0.000000, max: 11.000000, min: 11.000000)
15/08/19 18:20:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:39 INFO StatsReportListener: 	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms	11.0 ms
Time taken: 156.52 seconds
15/08/19 18:20:39 INFO CliDriver: Time taken: 156.52 seconds
15/08/19 18:20:39 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/19 18:20:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:39 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/19 18:20:39 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/19 18:20:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:39 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/19 18:20:39 INFO StatsReportListener: other time pct: (count: 1, mean: 100.000000, stdev: 0.000000, max: 100.000000, min: 100.000000)
15/08/19 18:20:39 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/19 18:20:39 INFO StatsReportListener: 	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/19 18:20:39 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/19 18:20:39 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/19 18:20:39 INFO DAGScheduler: Stopping DAGScheduler
15/08/19 18:20:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/19 18:20:39 INFO Utils: path = /tmp/spark-5bca4007-eaeb-411d-91ad-eee4fafc7e75/blockmgr-81695391-cdd1-4610-bfcd-5ca0a755c1a3, already present as root for deletion.
15/08/19 18:20:39 INFO MemoryStore: MemoryStore cleared
15/08/19 18:20:39 INFO BlockManager: BlockManager stopped
15/08/19 18:20:39 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/19 18:20:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/19 18:20:39 INFO SparkContext: Successfully stopped SparkContext
15/08/19 18:20:39 INFO Utils: Shutdown hook called
15/08/19 18:20:39 INFO Utils: Deleting directory /tmp/spark-5bca4007-eaeb-411d-91ad-eee4fafc7e75
15/08/19 18:20:39 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/19 18:20:39 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/19 18:20:39 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/08/19 18:20:41 INFO Utils: Deleting directory /tmp/spark-8ba1a280-9451-4e7f-9b09-d1181fc2731c
15/08/19 18:20:41 INFO Utils: Deleting directory /tmp/spark-1971d902-0bcd-4e6d-b33d-cbeafa2cb824
