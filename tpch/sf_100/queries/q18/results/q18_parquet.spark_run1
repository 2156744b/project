 -- the query
insert into table q18_tmp_par
select 
  l_orderkey, sum(l_quantity) as t_sum_quantity
from 
  lineitem_par
group by l_orderkey;

insert into table q18_large_volume_customer_par
select 
  c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice,sum(l_quantity)
from 
  customer_par c join orders_par o 
  on 
    c.c_custkey = o.o_custkey
  join q18_tmp_par t 
  on 
    o.o_orderkey = t.l_orderkey and t.t_sum_quantity > 315
  join lineitem_par l 
  on 
    o.o_orderkey = l.l_orderkey
group by c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice
order by o_totalprice desc,o_orderdate
limit 100;
15/08/21 08:42:07 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 08:42:07 INFO metastore: Connected to metastore.
15/08/21 08:42:08 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/21 08:42:08 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:08 INFO SparkContext: Running Spark version 1.4.1
15/08/21 08:42:08 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:08 INFO SecurityManager: Changing view acls to: hive
15/08/21 08:42:08 INFO SecurityManager: Changing modify acls to: hive
15/08/21 08:42:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/21 08:42:09 INFO Slf4jLogger: Slf4jLogger started
15/08/21 08:42:09 INFO Remoting: Starting remoting
15/08/21 08:42:09 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:42914]
15/08/21 08:42:09 INFO Utils: Successfully started service 'sparkDriver' on port 42914.
15/08/21 08:42:09 INFO SparkEnv: Registering MapOutputTracker
15/08/21 08:42:09 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:09 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:09 INFO SparkEnv: Registering BlockManagerMaster
15/08/21 08:42:09 INFO DiskBlockManager: Created local directory at /tmp/spark-0c40987a-aced-4d32-8eb2-2707f5ab193e/blockmgr-0d41c176-1df1-4415-8082-ee73248c575d
15/08/21 08:42:09 INFO MemoryStore: MemoryStore started with capacity 20.7 GB
15/08/21 08:42:09 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:09 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0c40987a-aced-4d32-8eb2-2707f5ab193e/httpd-50775da1-8672-4c4e-8db6-44339a32428f
15/08/21 08:42:09 INFO HttpServer: Starting HTTP Server
15/08/21 08:42:09 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 08:42:09 INFO AbstractConnector: Started SocketConnector@0.0.0.0:46150
15/08/21 08:42:09 INFO Utils: Successfully started service 'HTTP file server' on port 46150.
15/08/21 08:42:09 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/21 08:42:10 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/21 08:42:10 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/21 08:42:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/21 08:42:10 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/21 08:42:10 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:10 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:10 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:10 INFO Executor: Starting executor ID driver on host localhost
15/08/21 08:42:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51693.
15/08/21 08:42:10 INFO NettyBlockTransferService: Server created on 51693
15/08/21 08:42:10 INFO BlockManagerMaster: Trying to register BlockManager
15/08/21 08:42:10 INFO BlockManagerMasterEndpoint: Registering block manager localhost:51693 with 20.7 GB RAM, BlockManagerId(driver, localhost, 51693)
15/08/21 08:42:10 INFO BlockManagerMaster: Registered BlockManager
15/08/21 08:42:10 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/21 08:42:10 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/21 08:42:10 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/21 08:42:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/21 08:42:12 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/21 08:42:12 INFO metastore: Connected to metastore.
15/08/21 08:42:12 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/21 08:42:12 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/21 08:42:13 INFO ParseDriver: Parsing command: -- the query
insert into table q18_tmp_par
select 
  l_orderkey, sum(l_quantity) as t_sum_quantity
from 
  lineitem_par
group by l_orderkey
15/08/21 08:42:13 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/21 08:42:16 INFO MemoryStore: ensureFreeSpace(326528) called with curMem=0, maxMem=22226833244
15/08/21 08:42:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.9 KB, free 20.7 GB)
15/08/21 08:42:16 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=326528, maxMem=22226833244
15/08/21 08:42:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 08:42:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:51693 (size: 22.3 KB, free: 20.7 GB)
15/08/21 08:42:16 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/21 08:42:16 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 08:42:16 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/21 08:42:16 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/21 08:42:16 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/21 08:42:16 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/21 08:42:16 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/21 08:42:17 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 08:42:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:42:17 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 08:42:17 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/21 08:42:17 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/21 08:42:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 08:42:17 INFO DAGScheduler: Registering RDD 3 (processCmd at CliDriver.java:423)
15/08/21 08:42:17 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 08:42:17 INFO DAGScheduler: Final stage: ResultStage 1(processCmd at CliDriver.java:423)
15/08/21 08:42:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15/08/21 08:42:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15/08/21 08:42:17 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 08:42:17 INFO MemoryStore: ensureFreeSpace(9208) called with curMem=349321, maxMem=22226833244
15/08/21 08:42:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 20.7 GB)
15/08/21 08:42:17 INFO MemoryStore: ensureFreeSpace(4552) called with curMem=358529, maxMem=22226833244
15/08/21 08:42:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 20.7 GB)
15/08/21 08:42:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:51693 (size: 4.4 KB, free: 20.7 GB)
15/08/21 08:42:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/08/21 08:42:17 INFO DAGScheduler: Submitting 170 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423)
15/08/21 08:42:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 170 tasks
15/08/21 08:42:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1758 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1770 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, ANY, 1757 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, ANY, 1769 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, ANY, 1757 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, ANY, 1771 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, ANY, 1758 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, ANY, 1773 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, ANY, 1758 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, ANY, 1770 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, localhost, ANY, 1757 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, localhost, ANY, 1767 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, localhost, ANY, 1757 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, localhost, ANY, 1772 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, localhost, ANY, 1758 bytes)
15/08/21 08:42:17 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, localhost, ANY, 1770 bytes)
15/08/21 08:42:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/21 08:42:18 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
15/08/21 08:42:18 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/21 08:42:18 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
15/08/21 08:42:18 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/08/21 08:42:18 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
15/08/21 08:42:18 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/08/21 08:42:18 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
15/08/21 08:42:18 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
15/08/21 08:42:18 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
15/08/21 08:42:18 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
15/08/21 08:42:18 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
15/08/21 08:42:18 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
15/08/21 08:42:18 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
15/08/21 08:42:18 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)
15/08/21 08:42:18 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 134217728 end: 257171772 length: 122954044 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 134217728 end: 257329816 length: 123112088 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 134217728 end: 259746640 length: 125528912 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 134217728 end: 257463677 length: 123245949 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 134217728 end: 257420451 length: 123202723 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 134217728 end: 257477064 length: 123259336 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 134217728 end: 257425918 length: 123208190 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 134217728 end: 257568535 length: 123350807 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
21-Aug-2015 08:42:14 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 08:42:14 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 08:42:14 INFO: parquet.hadoop.ParquetFileReader: reading another 85 footers
21-Aug-2015 08:42:14 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572933 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501583 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572843 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573045 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572784 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573983 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574066 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501150 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501187 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573892 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627997 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 470 ms. row count = 3501150
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 522 ms. row count = 3502678
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 528 ms. row count = 3500100
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 546 ms. row count = 3500780
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 547 ms. row count = 15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:19 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/21 08:42:50 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2125 bytes result sent to driver
15/08/21 08:42:50 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, localhost, ANY, 1755 bytes)
15/08/21 08:42:50 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)
15/08/21 08:42:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:50 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 32350 ms on localhost (1/170)
15/08/21 08:42:51 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2125 bytes result sent to driver
15/08/21 08:42:51 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, localhost, ANY, 1770 bytes)
15/08/21 08:42:51 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)
15/08/21 08:42:51 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 33727 ms on localhost (2/170)
15/08/21 08:42:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 134217728 end: 261799262 length: 127581534 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:51 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 2125 bytes result sent to driver
15/08/21 08:42:51 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, localhost, ANY, 1758 bytes)
15/08/21 08:42:51 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)
15/08/21 08:42:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:51 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 33770 ms on localhost (3/170)
15/08/21 08:42:54 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 2125 bytes result sent to driver
15/08/21 08:42:54 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, localhost, ANY, 1769 bytes)
15/08/21 08:42:54 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)
15/08/21 08:42:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 134217728 end: 257435404 length: 123217676 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:54 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 36649 ms on localhost (4/170)
15/08/21 08:42:55 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 2125 bytes result sent to driver
15/08/21 08:42:55 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20, localhost, ANY, 1758 bytes)
15/08/21 08:42:55 INFO Executor: Running task 20.0 in stage 0.0 (TID 20)
15/08/21 08:42:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:55 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 37283 ms on localhost (5/170)
15/08/21 08:42:55 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2125 bytes result sent to driver
15/08/21 08:42:55 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21, localhost, ANY, 1772 bytes)
15/08/21 08:42:55 INFO Executor: Running task 21.0 in stage 0.0 (TID 21)
15/08/21 08:42:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 134217728 end: 257347114 length: 123129386 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:42:55 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 37834 ms on localhost (6/170)
15/08/21 08:42:56 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 2125 bytes result sent to driver
15/08/21 08:42:56 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22, localhost, ANY, 1758 bytes)
15/08/21 08:42:56 INFO Executor: Running task 22.0 in stage 0.0 (TID 22)
15/08/21 08:42:56 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 38162 ms on localhost (7/170)
15/08/21 08:42:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
3500100
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 552 ms. row count = 3500100
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 571 ms. row count = 3501187
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 653 ms. row count = 3500100
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 713 ms. row count = 3501191
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 727 ms. row count = 3500100
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 849 ms. row count = 3503008
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 851 ms. row count = 3500100
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 861 ms. row count = 3500939
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 882 ms. row count = 3500100
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 918 ms. row count = 3501583
21-Aug-2015 08:42:19 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 939 ms. row count = 3500100
21-Aug-2015 08:42:40 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502678 records from 2 columns in 21086 ms: 166.1139 rec/ms, 332.2278 cell/ms
21-Aug-2015 08:42:40 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 2% reading (522 ms) and 97% processing (21086 ms)
21-Aug-2015 08:42:40 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502678. reading next block
21-Aug-2015 08:42:40 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 70255
21-Aug-2015 08:42:50 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:50 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501183 records.
21-Aug-2015 08:42:50 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:50 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 41 ms. row count = 3501183
21-Aug-2015 08:42:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3689969 records.
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 96 ms. row count = 3500100
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 178 ms. row count = 3501351
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32285 ms: 108.412575 rec/ms, 216.82515 cell/ms
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 1% reading (552 ms) and 98% processing (32285 ms)
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 22 ms. row count = 72684
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32391 ms: 108.05779 rec/ms, 216.11559 cell/ms
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 2% reading (851 ms) and 97% processing (32391 ms)
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:42:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 21 ms. row count = 127897
21-Aug-2015 08:42:52 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32921 ms: 106.31815 rec/ms, 212.6363 cell/ms
21-Aug-2015 08:42:52 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 1% reading (547 ms) and 98% processing (32921 ms)
21-Aug-2015 08:42:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:42:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 24 ms. row count = 73883
21-Aug-2015 08:42:52 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500939 records from 2 columns in 33083 ms: 105.8229 rec/ms, 211.6458 cell/ms
21-Aug-2015 08:42:52 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 2% reading (861 ms) and 97% processing (33083 ms)
21-Aug-2015 08:42:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500939. reading next block
21-Aug-2015 08:42:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 20 ms. row count = 71904
21-Aug-2015 08:42:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 33478 ms: 104.636116 rec/ms, 209.27223 cell/ms
21-Aug-2015 08:42:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 2% reading (849 ms) and 97% processing (33478 ms)
21-Aug-2015 08:42:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503008. reading next block
21-Aug-2015 08:42:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 70884
21-Aug-2015 08:42:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573923 records.
21-Aug-2015 08:42:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 124 ms. row count = 3501239
21-Aug-2015 08:42:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:42:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 119 ms. row count = 3500100
21-Aug-2015 08:42:55 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:55 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
21-Aug-2015 08:42:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 182 ms. row count = 3500100
21-Aug-2015 08:42:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but 15/08/21 08:42:56 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 2125 bytes result sent to driver
15/08/21 08:42:56 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23, localhost, ANY, 1769 bytes)
15/08/21 08:42:56 INFO Executor: Running task 23.0 in stage 0.0 (TID 23)
15/08/21 08:42:56 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 38412 ms on localhost (8/170)
15/08/21 08:42:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 134217728 end: 257844810 length: 123627082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:02 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2125 bytes result sent to driver
15/08/21 08:43:02 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24, localhost, ANY, 1758 bytes)
15/08/21 08:43:02 INFO Executor: Running task 24.0 in stage 0.0 (TID 24)
15/08/21 08:43:02 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 44876 ms on localhost (9/170)
15/08/21 08:43:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:02 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2125 bytes result sent to driver
15/08/21 08:43:02 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25, localhost, ANY, 1768 bytes)
15/08/21 08:43:02 INFO Executor: Running task 25.0 in stage 0.0 (TID 25)
15/08/21 08:43:02 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 45112 ms on localhost (10/170)
15/08/21 08:43:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 134217728 end: 257827380 length: 123609652 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:03 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 2125 bytes result sent to driver
15/08/21 08:43:03 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26, localhost, ANY, 1758 bytes)
15/08/21 08:43:03 INFO Executor: Running task 26.0 in stage 0.0 (TID 26)
15/08/21 08:43:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:03 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 45350 ms on localhost (11/170)
15/08/21 08:43:03 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 2125 bytes result sent to driver
15/08/21 08:43:03 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27, localhost, ANY, 1770 bytes)
15/08/21 08:43:03 INFO Executor: Running task 27.0 in stage 0.0 (TID 27)
15/08/21 08:43:03 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 45699 ms on localhost (12/170)
15/08/21 08:43:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 134217728 end: 257458294 length: 123240566 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:03 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 2125 bytes result sent to driver
15/08/21 08:43:03 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28, localhost, ANY, 1758 bytes)
15/08/21 08:43:03 INFO Executor: Running task 28.0 in stage 0.0 (TID 28)
15/08/21 08:43:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:03 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 45912 ms on localhost (13/170)
15/08/21 08:43:13 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 2125 bytes result sent to driver
15/08/21 08:43:13 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29, localhost, ANY, 1770 bytes)
15/08/21 08:43:13 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 23601 ms on localhost (14/170)
15/08/21 08:43:13 INFO Executor: Running task 29.0 in stage 0.0 (TID 29)
15/08/21 08:43:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2125 bytes result sent to driver
15/08/21 08:43:13 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30, localhost, ANY, 1758 bytes)
15/08/21 08:43:13 INFO Executor: Running task 30.0 in stage 0.0 (TID 30)
15/08/21 08:43:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 134217728 end: 257369456 length: 123151728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 55965 ms on localhost (15/170)
15/08/21 08:43:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:14 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2125 bytes result sent to driver
15/08/21 08:43:14 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31, localhost, ANY, 1773 bytes)
15/08/21 08:43:14 INFO Executor: Running task 31.0 in stage 0.0 (TID 31)
15/08/21 08:43:14 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 56513 ms on localhost (16/170)
15/08/21 08:43:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 134217728 end: 257340601 length: 123122873 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:20 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 2125 bytes result sent to driver
15/08/21 08:43:20 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32, localhost, ANY, 1758 bytes)
15/08/21 08:43:20 INFO Executor: Running task 32.0 in stage 0.0 (TID 32)
15/08/21 08:43:20 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 28463 ms on localhost (17/170)
15/08/21 08:43:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:20 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 2125 bytes result sent to driver
15/08/21 08:43:20 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33, localhost, ANY, 1768 bytes)
15/08/21 08:43:20 INFO Executor: Running task 33.0 in stage 0.0 (TID 33)
15/08/21 08:43:20 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 62228 ms on localhost (18/170)
15/08/21 08:43:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 134217728 end: 259459156 length: 125241428 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501423 records.
21-Aug-2015 08:42:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 153 ms. row count = 3501423
21-Aug-2015 08:42:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:42:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573035 records.
21-Aug-2015 08:42:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:42:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 102 ms. row count = 3500100
21-Aug-2015 08:43:02 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:02 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:43:02 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:02 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 90 ms. row count = 3500100
21-Aug-2015 08:43:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573921 records.
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 43579 ms: 80.31621 rec/ms, 160.63242 cell/ms
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 1% reading (528 ms) and 98% processing (43579 ms)
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14 ms. row count = 72945
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 116 ms. row count = 3501305
21-Aug-2015 08:43:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501667 records.
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 104 ms. row count = 3501667
21-Aug-2015 08:43:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572747 records.
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 147 ms. row count = 3500100
21-Aug-2015 08:43:03 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:03 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 105 ms. row count = 3500100
21-Aug-2015 08:43:04 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500780 records from 2 columns in 44762 ms: 78.20875 rec/ms, 156.4175 cell/ms
21-Aug-2015 08:43:04 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 1% reading (546 ms) and 98% processing (44762 ms)
21-Aug-2015 08:43:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500780. reading next block
21-Aug-2015 08:43:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 73286
21-Aug-2015 08:43:13 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:13 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501351 records from 2 columns in 22008 ms: 159.09447 rec/ms, 318.18893 cell/ms
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (178 ms) and 99% processing (22008 ms)
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501351. reading next block
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 23 ms. row count = 188618
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:13 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 82 ms. row count = 3500100
21-Aug-2015 08:43:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 162 ms. row count = 3500100
21-Aug-2015 08:43:14 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:14 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574247 records.
21-Aug-2015 08:43:14 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 108 ms. row count = 3500100
21-Aug-2015 08:43:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:20 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:43:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:20 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627517 records.
21-Aug-2015 08:43:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 139 ms. 15/08/21 08:43:30 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 2125 bytes result sent to driver
15/08/21 08:43:30 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34, localhost, ANY, 1758 bytes)
15/08/21 08:43:30 INFO Executor: Running task 34.0 in stage 0.0 (TID 34)
15/08/21 08:43:30 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 38636 ms on localhost (19/170)
15/08/21 08:43:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:37 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 2125 bytes result sent to driver
15/08/21 08:43:37 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35, localhost, ANY, 1769 bytes)
15/08/21 08:43:37 INFO Executor: Running task 35.0 in stage 0.0 (TID 35)
15/08/21 08:43:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 134217728 end: 257580347 length: 123362619 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:37 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 42510 ms on localhost (20/170)
15/08/21 08:43:38 INFO Executor: Finished task 20.0 in stage 0.0 (TID 20). 2125 bytes result sent to driver
15/08/21 08:43:38 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36, localhost, ANY, 1757 bytes)
15/08/21 08:43:38 INFO Executor: Running task 36.0 in stage 0.0 (TID 36)
15/08/21 08:43:38 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 42962 ms on localhost (21/170)
15/08/21 08:43:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:38 INFO Executor: Finished task 21.0 in stage 0.0 (TID 21). 2125 bytes result sent to driver
15/08/21 08:43:38 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37, localhost, ANY, 1770 bytes)
15/08/21 08:43:38 INFO Executor: Running task 37.0 in stage 0.0 (TID 37)
15/08/21 08:43:38 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 42461 ms on localhost (22/170)
15/08/21 08:43:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 134217728 end: 257857124 length: 123639396 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:38 INFO Executor: Finished task 25.0 in stage 0.0 (TID 25). 2125 bytes result sent to driver
15/08/21 08:43:38 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38, localhost, ANY, 1757 bytes)
15/08/21 08:43:38 INFO Executor: Running task 38.0 in stage 0.0 (TID 38)
15/08/21 08:43:38 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 35370 ms on localhost (23/170)
15/08/21 08:43:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:38 INFO Executor: Finished task 22.0 in stage 0.0 (TID 22). 2125 bytes result sent to driver
15/08/21 08:43:38 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39, localhost, ANY, 1770 bytes)
15/08/21 08:43:38 INFO Executor: Running task 39.0 in stage 0.0 (TID 39)
15/08/21 08:43:38 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 42585 ms on localhost (24/170)
15/08/21 08:43:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 134217728 end: 259199417 length: 124981689 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:58 INFO Executor: Finished task 24.0 in stage 0.0 (TID 24). 2125 bytes result sent to driver
15/08/21 08:43:58 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, localhost, ANY, 1758 bytes)
15/08/21 08:43:58 INFO Executor: Running task 40.0 in stage 0.0 (TID 40)
15/08/21 08:43:58 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 55414 ms on localhost (25/170)
15/08/21 08:43:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:58 INFO Executor: Finished task 23.0 in stage 0.0 (TID 23). 2125 bytes result sent to driver
15/08/21 08:43:58 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41, localhost, ANY, 1769 bytes)
15/08/21 08:43:58 INFO Executor: Running task 41.0 in stage 0.0 (TID 41)
15/08/21 08:43:58 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 62126 ms on localhost (26/170)
15/08/21 08:43:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 134217728 end: 257564901 length: 123347173 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:58 INFO Executor: Finished task 27.0 in stage 0.0 (TID 27). 2125 bytes result sent to driver
15/08/21 08:43:58 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42, localhost, ANY, 1757 bytes)
15/08/21 08:43:58 INFO Executor: Running task 42.0 in stage 0.0 (TID 42)
15/08/21 08:43:58 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 55321 ms on localhost (27/170)
15/08/21 08:43:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
row count = 3500100
21-Aug-2015 08:43:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 154 ms. row count = 3500968
21-Aug-2015 08:43:24 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501239 records from 2 columns in 30240 ms: 115.781715 rec/ms, 231.56343 cell/ms
21-Aug-2015 08:43:24 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (124 ms) and 99% processing (30240 ms)
21-Aug-2015 08:43:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501239. reading next block
21-Aug-2015 08:43:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 29 ms. row count = 72684
21-Aug-2015 08:43:25 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 29956 ms: 116.84137 rec/ms, 233.68274 cell/ms
21-Aug-2015 08:43:25 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (182 ms) and 99% processing (29956 ms)
21-Aug-2015 08:43:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4211 ms. row count = 74238
21-Aug-2015 08:43:30 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501180 records.
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 103 ms. row count = 3501180
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 33936 ms: 103.13826 rec/ms, 206.27652 cell/ms
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (102 ms) and 99% processing (33936 ms)
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 36 ms. row count = 72935
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501305 records from 2 columns in 27385 ms: 127.85485 rec/ms, 255.7097 cell/ms
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (116 ms) and 99% processing (27385 ms)
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501305. reading next block
21-Aug-2015 08:43:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 17 ms. row count = 72616
21-Aug-2015 08:43:31 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 27516 ms: 127.202354 rec/ms, 254.40471 cell/ms
21-Aug-2015 08:43:31 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (147 ms) and 99% processing (27516 ms)
21-Aug-2015 08:43:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:43:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 15 ms. row count = 72647
21-Aug-2015 08:43:37 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:37 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573027 records.
21-Aug-2015 08:43:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 178 ms. row count = 3500932
21-Aug-2015 08:43:38 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:38 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574216 records.
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:38 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 180 ms. row count = 3500100
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 153 ms. row count = 3501165
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 24428 ms: 143.2823 rec/ms, 286.5646 cell/ms
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (82 ms) and 99% processing (24428 ms)
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 31 ms. row count = 74054
21-Aug-2015 08:43:38 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 91 ms. row count = 3501121
21-Aug-2015 08:43:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626725 records.
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 115 ms. row count = 3502670
21-Aug-2015 08:43:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501407 records.
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 121 ms. row count = 3501407
21-Aug-2015 08:43:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572791 records.
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 103 ms. row count = 3500841
21-Aug-2015 08:43:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-20115/08/21 08:43:59 INFO Executor: Finished task 26.0 in stage 0.0 (TID 26). 2125 bytes result sent to driver
15/08/21 08:43:59 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43, localhost, ANY, 1769 bytes)
15/08/21 08:43:59 INFO Executor: Running task 43.0 in stage 0.0 (TID 43)
15/08/21 08:43:59 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 55792 ms on localhost (28/170)
15/08/21 08:43:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 134217728 end: 257460000 length: 123242272 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:59 INFO Executor: Finished task 28.0 in stage 0.0 (TID 28). 2125 bytes result sent to driver
15/08/21 08:43:59 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44, localhost, ANY, 1758 bytes)
15/08/21 08:43:59 INFO Executor: Running task 44.0 in stage 0.0 (TID 44)
15/08/21 08:43:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:43:59 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 55482 ms on localhost (29/170)
15/08/21 08:44:04 INFO Executor: Finished task 29.0 in stage 0.0 (TID 29). 2125 bytes result sent to driver
15/08/21 08:44:04 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45, localhost, ANY, 1768 bytes)
15/08/21 08:44:04 INFO Executor: Running task 45.0 in stage 0.0 (TID 45)
15/08/21 08:44:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 134217728 end: 257842743 length: 123625015 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:04 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 51086 ms on localhost (30/170)
15/08/21 08:44:04 INFO Executor: Finished task 30.0 in stage 0.0 (TID 30). 2125 bytes result sent to driver
15/08/21 08:44:04 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46, localhost, ANY, 1758 bytes)
15/08/21 08:44:04 INFO Executor: Running task 46.0 in stage 0.0 (TID 46)
15/08/21 08:44:04 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 51169 ms on localhost (31/170)
15/08/21 08:44:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:11 INFO Executor: Finished task 32.0 in stage 0.0 (TID 32). 2125 bytes result sent to driver
15/08/21 08:44:11 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47, localhost, ANY, 1770 bytes)
15/08/21 08:44:11 INFO Executor: Running task 47.0 in stage 0.0 (TID 47)
15/08/21 08:44:11 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 51386 ms on localhost (32/170)
15/08/21 08:44:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 134217728 end: 257576472 length: 123358744 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:12 INFO Executor: Finished task 31.0 in stage 0.0 (TID 31). 2125 bytes result sent to driver
15/08/21 08:44:12 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48, localhost, ANY, 1758 bytes)
15/08/21 08:44:12 INFO Executor: Running task 48.0 in stage 0.0 (TID 48)
15/08/21 08:44:12 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 57809 ms on localhost (33/170)
15/08/21 08:44:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:12 INFO Executor: Finished task 33.0 in stage 0.0 (TID 33). 2125 bytes result sent to driver
15/08/21 08:44:12 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49, localhost, ANY, 1770 bytes)
15/08/21 08:44:12 INFO Executor: Running task 49.0 in stage 0.0 (TID 49)
15/08/21 08:44:12 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 52287 ms on localhost (34/170)
15/08/21 08:44:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 134217728 end: 256733155 length: 122515427 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:15 INFO Executor: Finished task 34.0 in stage 0.0 (TID 34). 2125 bytes result sent to driver
15/08/21 08:44:15 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50, localhost, ANY, 1757 bytes)
15/08/21 08:44:15 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 45567 ms on localhost (35/170)
15/08/21 08:44:15 INFO Executor: Running task 50.0 in stage 0.0 (TID 50)
15/08/21 08:44:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
5 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501143 records.
21-Aug-2015 08:43:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 123 ms. row count = 3501143
21-Aug-2015 08:43:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:59 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573047 records.
21-Aug-2015 08:43:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 140 ms. row count = 3500100
21-Aug-2015 08:43:59 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:43:59 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501260 records.
21-Aug-2015 08:43:59 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:43:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 156 ms. row count = 3501260
21-Aug-2015 08:44:01 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 47304 ms: 73.99163 rec/ms, 147.98326 cell/ms
21-Aug-2015 08:44:01 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (108 ms) and 99% processing (47304 ms)
21-Aug-2015 08:44:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:44:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 55 ms. row count = 74147
21-Aug-2015 08:44:04 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500968 records from 2 columns in 41822 ms: 83.71116 rec/ms, 167.42232 cell/ms
21-Aug-2015 08:44:04 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (154 ms) and 99% processing (41822 ms)
21-Aug-2015 08:44:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500968. reading next block
21-Aug-2015 08:44:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 126549
21-Aug-2015 08:44:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573207 records.
21-Aug-2015 08:44:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501217 records.
21-Aug-2015 08:44:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 179 ms. row count = 3500100
21-Aug-2015 08:44:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 224 ms. row count = 3501217
21-Aug-2015 08:44:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573258 records.
21-Aug-2015 08:44:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 93 ms. row count = 3501508
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500932 records from 2 columns in 34834 ms: 100.5033 rec/ms, 201.0066 cell/ms
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (178 ms) and 99% processing (34834 ms)
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500932. reading next block
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 27 ms. row count = 72095
21-Aug-2015 08:44:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503050 records.
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 135 ms. row count = 3503050
21-Aug-2015 08:44:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3570986 records.
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 115 ms. row count = 3503008
21-Aug-2015 08:44:15 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501200 records.
21-Aug-2015 08:44:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 121 ms. row count = 3501200
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501165 records from 2 columns in 37692 ms: 92.88881 rec/ms, 185.77762 cell/ms
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (153 ms) and 99% processing (37692 ms)
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501165. reading next block
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 73051
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3502670 records from 2 columns in 18306 ms: 191.34 rec/ms, 382.68 cell/ms
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (115 ms) and 99% processing (18306 ms)
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3502670. reading next block
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 124055
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500841 records from 2 columns in 18226 ms: 192.0795 rec/ms, 384.159 cell/ms
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (103 ms) and 99% processing (18226 ms)
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500841. reading next block
21-Aug-2015 08:44:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 33 ms. row count = 71950
21-Aug-2015 08:44:20 INFO: parquet.h15/08/21 08:44:21 INFO Executor: Finished task 35.0 in stage 0.0 (TID 35). 2125 bytes result sent to driver
15/08/21 08:44:21 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51, localhost, ANY, 1768 bytes)
15/08/21 08:44:21 INFO Executor: Running task 51.0 in stage 0.0 (TID 51)
15/08/21 08:44:21 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 44076 ms on localhost (36/170)
15/08/21 08:44:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 134217728 end: 257568635 length: 123350907 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:21 INFO Executor: Finished task 38.0 in stage 0.0 (TID 38). 2125 bytes result sent to driver
15/08/21 08:44:21 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52, localhost, ANY, 1757 bytes)
15/08/21 08:44:21 INFO Executor: Running task 52.0 in stage 0.0 (TID 52)
15/08/21 08:44:21 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 43267 ms on localhost (37/170)
15/08/21 08:44:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:24 INFO Executor: Finished task 40.0 in stage 0.0 (TID 40). 2125 bytes result sent to driver
15/08/21 08:44:24 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53, localhost, ANY, 1768 bytes)
15/08/21 08:44:24 INFO Executor: Running task 53.0 in stage 0.0 (TID 53)
15/08/21 08:44:24 INFO Executor: Finished task 36.0 in stage 0.0 (TID 36). 2125 bytes result sent to driver
15/08/21 08:44:24 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54, localhost, ANY, 1757 bytes)
15/08/21 08:44:24 INFO Executor: Running task 54.0 in stage 0.0 (TID 54)
15/08/21 08:44:24 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 46337 ms on localhost (38/170)
15/08/21 08:44:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 134217728 end: 257584480 length: 123366752 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:24 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 26311 ms on localhost (39/170)
15/08/21 08:44:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:24 INFO Executor: Finished task 37.0 in stage 0.0 (TID 37). 2125 bytes result sent to driver
15/08/21 08:44:24 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55, localhost, ANY, 1769 bytes)
15/08/21 08:44:24 INFO Executor: Running task 55.0 in stage 0.0 (TID 55)
15/08/21 08:44:24 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 46358 ms on localhost (40/170)
15/08/21 08:44:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 134217728 end: 259050527 length: 124832799 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:25 INFO Executor: Finished task 39.0 in stage 0.0 (TID 39). 2125 bytes result sent to driver
15/08/21 08:44:25 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56, localhost, ANY, 1758 bytes)
15/08/21 08:44:25 INFO Executor: Running task 56.0 in stage 0.0 (TID 56)
15/08/21 08:44:25 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 46495 ms on localhost (41/170)
15/08/21 08:44:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:25 INFO Executor: Finished task 44.0 in stage 0.0 (TID 44). 2125 bytes result sent to driver
15/08/21 08:44:25 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57, localhost, ANY, 1772 bytes)
15/08/21 08:44:25 INFO Executor: Running task 57.0 in stage 0.0 (TID 57)
15/08/21 08:44:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 134217728 end: 257461890 length: 123244162 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:25 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 25931 ms on localhost (42/170)
15/08/21 08:44:25 INFO Executor: Finished task 41.0 in stage 0.0 (TID 41). 2125 bytes result sent to driver
15/08/21 08:44:25 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58, localhost, ANY, 1758 bytes)
15/08/21 08:44:25 INFO Executor: Running task 58.0 in stage 0.0 (TID 58)
15/08/21 08:44:25 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 26928 ms on localhost (43/170)
15/08/21 08:44:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:25 INFO Executor: Finished task 42.0 in stage 0.0 (TID 42). 2125 bytes result sent to driver
15/08/21 08:44:25 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59, localhost, ANY, 1773 bytes)
15/08/21 08:44:25 INFO Executor: Running task 59.0 in stage 0.0 (TID 59)
15/08/21 08:44:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 134217728 end: 257416343 length: 123198615 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:25 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 26535 ms on localhost (44/170)
15/08/21 08:44:28 INFO Executor: Finished task 43.0 in stage 0.0 (TID 43). 2125 bytes result sent to driver
15/08/21 08:44:28 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60, localhost, ANY, 1758 bytes)
15/08/21 08:44:28 INFO Executor: Running task 60.0 in stage 0.0 (TID 60)
15/08/21 08:44:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:28 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 29747 ms on localhost (45/170)
15/08/21 08:44:32 INFO Executor: Finished task 46.0 in stage 0.0 (TID 46). 2125 bytes result sent to driver
15/08/21 08:44:32 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61, localhost, ANY, 1770 bytes)
15/08/21 08:44:32 INFO Executor: Running task 61.0 in stage 0.0 (TID 61)
15/08/21 08:44:32 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 27786 ms on localhost (46/170)
15/08/21 08:44:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 134217728 end: 257883468 length: 123665740 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:32 INFO Executor: Finished task 45.0 in stage 0.0 (TID 45). 2125 bytes result sent to driver
15/08/21 08:44:32 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62, localhost, ANY, 1757 bytes)
15/08/21 08:44:32 INFO Executor: Running task 62.0 in stage 0.0 (TID 62)
15/08/21 08:44:32 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 27933 ms on localhost (47/170)
15/08/21 08:44:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
adoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 20841 ms: 167.943 rec/ms, 335.886 cell/ms
21-Aug-2015 08:44:20 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (140 ms) and 99% processing (20841 ms)
21-Aug-2015 08:44:20 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:44:20 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 35 ms. row count = 72947
21-Aug-2015 08:44:21 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:21 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573168 records.
21-Aug-2015 08:44:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 118 ms. row count = 3501341
21-Aug-2015 08:44:21 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:21 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500823 records.
21-Aug-2015 08:44:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 124 ms. row count = 3500823
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 19134 ms: 182.92567 rec/ms, 365.85135 cell/ms
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (179 ms) and 99% processing (19134 ms)
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 23 ms. row count = 73107
21-Aug-2015 08:44:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503144 records.
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573143 records.
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:24 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3624571 records.
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 125 ms. row count = 3503144
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 95 ms. row count = 3500100
21-Aug-2015 08:44:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 163 ms. row count = 3501035
21-Aug-2015 08:44:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573988 records.
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 147 ms. row count = 3500100
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 146 ms. row count = 3500824
21-Aug-2015 08:44:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:25 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574223 records.
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 96 ms. row count = 3500100
21-Aug-2015 08:44:25 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 117 ms. row count = 3501614
21-Aug-2015 08:44:28 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501508 records from 2 columns in 16969 ms: 206.34734 rec/ms, 412.69467 cell/ms
21-Aug-2015 08:44:28 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (93 ms) and 99% processing (16969 ms)
21-Aug-2015 08:44:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501508. reading next block
21-Aug-2015 08:44:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 64 ms. row count = 71750
21-Aug-2015 08:44:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
21-Aug-2015 08:44:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 236 ms. row count = 3501191
21-Aug-2015 08:44:32 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 19612 ms: 178.61554 rec/ms, 357.23108 cell/ms
21-Aug-2015 08:44:32 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (115 ms) and 99% processing (19612 ms)
21-Aug-2015 08:44:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503008. reading next block
21-Aug-2015 08:44:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 34 ms. row count = 67978
21-Aug-2015 08:44:32 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:32 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of 15/08/21 08:44:53 INFO Executor: Finished task 47.0 in stage 0.0 (TID 47). 2125 bytes result sent to driver
15/08/21 08:44:53 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63, localhost, ANY, 1772 bytes)
15/08/21 08:44:53 INFO Executor: Running task 63.0 in stage 0.0 (TID 63)
15/08/21 08:44:53 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 41629 ms on localhost (48/170)
15/08/21 08:44:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 134217728 end: 257592803 length: 123375075 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:53 INFO Executor: Finished task 49.0 in stage 0.0 (TID 49). 2125 bytes result sent to driver
15/08/21 08:44:53 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64, localhost, ANY, 1758 bytes)
15/08/21 08:44:53 INFO Executor: Running task 64.0 in stage 0.0 (TID 64)
15/08/21 08:44:53 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 40897 ms on localhost (49/170)
15/08/21 08:44:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:53 INFO Executor: Finished task 50.0 in stage 0.0 (TID 50). 2125 bytes result sent to driver
15/08/21 08:44:53 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65, localhost, ANY, 1770 bytes)
15/08/21 08:44:53 INFO Executor: Running task 65.0 in stage 0.0 (TID 65)
15/08/21 08:44:53 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 38092 ms on localhost (50/170)
15/08/21 08:44:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 134217728 end: 258145407 length: 123927679 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:44:54 INFO Executor: Finished task 48.0 in stage 0.0 (TID 48). 2125 bytes result sent to driver
15/08/21 08:44:54 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66, localhost, ANY, 1757 bytes)
15/08/21 08:44:54 INFO Executor: Running task 66.0 in stage 0.0 (TID 66)
15/08/21 08:44:54 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 42042 ms on localhost (51/170)
15/08/21 08:44:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:01 INFO Executor: Finished task 51.0 in stage 0.0 (TID 51). 2125 bytes result sent to driver
15/08/21 08:45:01 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67, localhost, ANY, 1769 bytes)
15/08/21 08:45:01 INFO Executor: Running task 67.0 in stage 0.0 (TID 67)
15/08/21 08:45:01 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 40251 ms on localhost (52/170)
15/08/21 08:45:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 134217728 end: 259344407 length: 125126679 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:04 INFO Executor: Finished task 52.0 in stage 0.0 (TID 52). 2125 bytes result sent to driver
15/08/21 08:45:04 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68, localhost, ANY, 1758 bytes)
15/08/21 08:45:04 INFO Executor: Running task 68.0 in stage 0.0 (TID 68)
15/08/21 08:45:04 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 43148 ms on localhost (53/170)
15/08/21 08:45:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:04 INFO Executor: Finished task 54.0 in stage 0.0 (TID 54). 2125 bytes result sent to driver
15/08/21 08:45:04 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69, localhost, ANY, 1770 bytes)
15/08/21 08:45:04 INFO Executor: Running task 69.0 in stage 0.0 (TID 69)
15/08/21 08:45:04 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 40320 ms on localhost (54/170)
15/08/21 08:45:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 134217728 end: 257768338 length: 123550610 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:04 INFO Executor: Finished task 53.0 in stage 0.0 (TID 53). 2125 bytes result sent to driver
15/08/21 08:45:04 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70, localhost, ANY, 1758 bytes)
15/08/21 08:45:04 INFO Executor: Running task 70.0 in stage 0.0 (TID 70)
15/08/21 08:45:04 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 40450 ms on localhost (55/170)
15/08/21 08:45:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:32 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573362 records.
21-Aug-2015 08:44:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:32 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501059 records.
21-Aug-2015 08:44:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 205 ms. row count = 3500100
21-Aug-2015 08:44:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 148 ms. row count = 3501059
21-Aug-2015 08:44:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573291 records.
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 96 ms. row count = 3501221
21-Aug-2015 08:44:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 122 ms. row count = 3500100
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501341 records from 2 columns in 32258 ms: 108.54179 rec/ms, 217.08357 cell/ms
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (118 ms) and 99% processing (32258 ms)
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501341. reading next block
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14 ms. row count = 71827
21-Aug-2015 08:44:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574053 records.
21-Aug-2015 08:44:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 121 ms. row count = 3500100
21-Aug-2015 08:44:54 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:44:54 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:44:54 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:44:54 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 201 ms. row count = 3500100
21-Aug-2015 08:44:56 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 32338 ms: 108.23489 rec/ms, 216.46979 cell/ms
21-Aug-2015 08:44:56 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (95 ms) and 99% processing (32338 ms)
21-Aug-2015 08:44:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:44:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 124471
21-Aug-2015 08:44:57 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501035 records from 2 columns in 32384 ms: 108.11002 rec/ms, 216.22005 cell/ms
21-Aug-2015 08:44:57 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (163 ms) and 99% processing (32384 ms)
21-Aug-2015 08:44:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501035. reading next block
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2996 ms. row count = 72108
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501614 records from 2 columns in 34595 ms: 101.21735 rec/ms, 202.4347 cell/ms
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (117 ms) and 99% processing (34595 ms)
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501614. reading next block
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 72609
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500824 records from 2 columns in 34916 ms: 100.264175 rec/ms, 200.52835 cell/ms
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (146 ms) and 99% processing (34916 ms)
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500824. reading next block
21-Aug-2015 08:45:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 19 ms. row count = 73164
21-Aug-2015 08:45:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627630 records.
21-Aug-2015 08:45:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 109 ms. row count = 3500100
21-Aug-2015 08:45:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 31737 ms: 110.28452 rec/ms, 220.56905 cell/ms
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (205 ms) and 99% processing (31737 ms)
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574075 records.
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 18 ms. row count = 73262
21-Aug-2015 08:45:04 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502727 records.
21-Aug-2015 08:45:04 INFO: parquet.15/08/21 08:45:05 INFO Executor: Finished task 58.0 in stage 0.0 (TID 58). 2125 bytes result sent to driver
15/08/21 08:45:05 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71, localhost, ANY, 1771 bytes)
15/08/21 08:45:05 INFO Executor: Running task 71.0 in stage 0.0 (TID 71)
15/08/21 08:45:05 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 39809 ms on localhost (56/170)
15/08/21 08:45:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 134217728 end: 257102581 length: 122884853 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:05 INFO Executor: Finished task 57.0 in stage 0.0 (TID 57). 2125 bytes result sent to driver
15/08/21 08:45:05 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72, localhost, ANY, 1758 bytes)
15/08/21 08:45:05 INFO Executor: Running task 72.0 in stage 0.0 (TID 72)
15/08/21 08:45:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:05 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 40372 ms on localhost (57/170)
15/08/21 08:45:05 INFO Executor: Finished task 55.0 in stage 0.0 (TID 55). 2125 bytes result sent to driver
15/08/21 08:45:05 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73, localhost, ANY, 1772 bytes)
15/08/21 08:45:05 INFO Executor: Running task 73.0 in stage 0.0 (TID 73)
15/08/21 08:45:05 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 41039 ms on localhost (58/170)
15/08/21 08:45:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 134217728 end: 257873629 length: 123655901 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:08 INFO Executor: Finished task 59.0 in stage 0.0 (TID 59). 2125 bytes result sent to driver
15/08/21 08:45:08 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74, localhost, ANY, 1757 bytes)
15/08/21 08:45:08 INFO Executor: Running task 74.0 in stage 0.0 (TID 74)
15/08/21 08:45:08 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 42918 ms on localhost (59/170)
15/08/21 08:45:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:08 INFO Executor: Finished task 56.0 in stage 0.0 (TID 56). 2125 bytes result sent to driver
15/08/21 08:45:08 INFO Executor: Finished task 60.0 in stage 0.0 (TID 60). 2125 bytes result sent to driver
15/08/21 08:45:08 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75, localhost, ANY, 1767 bytes)
15/08/21 08:45:08 INFO Executor: Running task 75.0 in stage 0.0 (TID 75)
15/08/21 08:45:08 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76, localhost, ANY, 1757 bytes)
15/08/21 08:45:08 INFO Executor: Running task 76.0 in stage 0.0 (TID 76)
15/08/21 08:45:08 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 43654 ms on localhost (60/170)
15/08/21 08:45:08 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 40006 ms on localhost (61/170)
15/08/21 08:45:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 134217728 end: 259470450 length: 125252722 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:12 INFO Executor: Finished task 61.0 in stage 0.0 (TID 61). 2125 bytes result sent to driver
15/08/21 08:45:12 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77, localhost, ANY, 1771 bytes)
15/08/21 08:45:12 INFO Executor: Running task 77.0 in stage 0.0 (TID 77)
15/08/21 08:45:12 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 39444 ms on localhost (62/170)
15/08/21 08:45:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 134217728 end: 259842205 length: 125624477 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:15 INFO Executor: Finished task 62.0 in stage 0.0 (TID 62). 2125 bytes result sent to driver
15/08/21 08:45:15 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78, localhost, ANY, 1758 bytes)
15/08/21 08:45:15 INFO Executor: Running task 78.0 in stage 0.0 (TID 78)
15/08/21 08:45:15 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 42334 ms on localhost (63/170)
15/08/21 08:45:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:18 INFO Executor: Finished task 64.0 in stage 0.0 (TID 64). 2125 bytes result sent to driver
15/08/21 08:45:18 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79, localhost, ANY, 1773 bytes)
15/08/21 08:45:18 INFO Executor: Running task 79.0 in stage 0.0 (TID 79)
15/08/21 08:45:18 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 25030 ms on localhost (64/170)
15/08/21 08:45:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 134217728 end: 257071082 length: 122853354 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:18 INFO Executor: Finished task 63.0 in stage 0.0 (TID 63). 2125 bytes result sent to driver
15/08/21 08:45:18 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80, localhost, ANY, 1758 bytes)
15/08/21 08:45:18 INFO Executor: Running task 80.0 in stage 0.0 (TID 80)
15/08/21 08:45:18 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 25549 ms on localhost (65/170)
15/08/21 08:45:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:18 INFO Executor: Finished task 65.0 in stage 0.0 (TID 65). 2125 bytes result sent to driver
15/08/21 08:45:18 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81, localhost, ANY, 1770 bytes)
15/08/21 08:45:18 INFO Executor: Running task 81.0 in stage 0.0 (TID 81)
15/08/21 08:45:18 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 24896 ms on localhost (66/170)
15/08/21 08:45:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 134217728 end: 257075001 length: 122857273 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:19 INFO Executor: Finished task 66.0 in stage 0.0 (TID 66). 2125 bytes result sent to driver
15/08/21 08:45:19 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82, localhost, ANY, 1758 bytes)
15/08/21 08:45:19 INFO Executor: Running task 82.0 in stage 0.0 (TID 82)
15/08/21 08:45:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:19 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 24856 ms on localhost (67/170)
hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:04 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 178 ms. row count = 3500100
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 3502727
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 226 ms. row count = 3500100
21-Aug-2015 08:45:05 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571405 records.
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 134 ms. row count = 3500100
21-Aug-2015 08:45:05 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:05 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501229 records.
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572739 records.
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2494 ms. row count = 3500100
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2579 ms. row count = 3501229
21-Aug-2015 08:45:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 163 ms. row count = 3500100
21-Aug-2015 08:45:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627768 records.
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500100
21-Aug-2015 08:45:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 127 ms. row count = 3501462
21-Aug-2015 08:45:11 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 17600 ms: 198.86932 rec/ms, 397.73865 cell/ms
21-Aug-2015 08:45:11 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (121 ms) and 99% processing (17600 ms)
21-Aug-2015 08:45:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:45:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 23 ms. row count = 73953
21-Aug-2015 08:45:11 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501221 records from 2 columns in 18424 ms: 190.03587 rec/ms, 380.07175 cell/ms
21-Aug-2015 08:45:11 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (96 ms) and 99% processing (18424 ms)
21-Aug-2015 08:45:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501221. reading next block
21-Aug-2015 08:45:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 72070
21-Aug-2015 08:45:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627712 records.
21-Aug-2015 08:45:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 83 ms. row count = 3501302
21-Aug-2015 08:45:15 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503221 records.
21-Aug-2015 08:45:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 64 ms. row count = 3503221
21-Aug-2015 08:45:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572089 records.
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 129 ms. row count = 3500100
21-Aug-2015 08:45:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 87 ms. row count = 3500100
21-Aug-2015 08:45:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574581 records.
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 179 ms. row count = 3503231
21-Aug-2015 08:45:19 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:19 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:19 INFO: par15/08/21 08:45:33 INFO Executor: Finished task 68.0 in stage 0.0 (TID 68). 2125 bytes result sent to driver
15/08/21 08:45:33 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83, localhost, ANY, 1770 bytes)
15/08/21 08:45:33 INFO Executor: Finished task 69.0 in stage 0.0 (TID 69). 2125 bytes result sent to driver
15/08/21 08:45:33 INFO Executor: Running task 83.0 in stage 0.0 (TID 83)
15/08/21 08:45:33 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84, localhost, ANY, 1757 bytes)
15/08/21 08:45:33 INFO Executor: Running task 84.0 in stage 0.0 (TID 84)
15/08/21 08:45:33 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 28523 ms on localhost (68/170)
15/08/21 08:45:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 134217728 end: 257427527 length: 123209799 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:33 INFO Executor: Finished task 67.0 in stage 0.0 (TID 67). 2125 bytes result sent to driver
15/08/21 08:45:33 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 28578 ms on localhost (69/170)
15/08/21 08:45:33 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85, localhost, ANY, 1771 bytes)
15/08/21 08:45:33 INFO Executor: Running task 85.0 in stage 0.0 (TID 85)
15/08/21 08:45:33 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 31987 ms on localhost (70/170)
15/08/21 08:45:33 INFO Executor: Finished task 70.0 in stage 0.0 (TID 70). 2125 bytes result sent to driver
15/08/21 08:45:33 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86, localhost, ANY, 1757 bytes)
15/08/21 08:45:33 INFO Executor: Running task 86.0 in stage 0.0 (TID 86)
15/08/21 08:45:33 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 28478 ms on localhost (71/170)
15/08/21 08:45:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 134217728 end: 257349334 length: 123131606 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:33 INFO Executor: Finished task 71.0 in stage 0.0 (TID 71). 2125 bytes result sent to driver
15/08/21 08:45:33 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87, localhost, ANY, 1770 bytes)
15/08/21 08:45:33 INFO Executor: Running task 87.0 in stage 0.0 (TID 87)
15/08/21 08:45:33 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 28565 ms on localhost (72/170)
15/08/21 08:45:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 134217728 end: 257709471 length: 123491743 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:34 INFO Executor: Finished task 74.0 in stage 0.0 (TID 74). 2125 bytes result sent to driver
15/08/21 08:45:34 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88, localhost, ANY, 1757 bytes)
15/08/21 08:45:34 INFO Executor: Running task 88.0 in stage 0.0 (TID 88)
15/08/21 08:45:34 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 26062 ms on localhost (73/170)
15/08/21 08:45:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:34 INFO Executor: Finished task 73.0 in stage 0.0 (TID 73). 2125 bytes result sent to driver
15/08/21 08:45:34 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89, localhost, ANY, 1769 bytes)
15/08/21 08:45:34 INFO Executor: Running task 89.0 in stage 0.0 (TID 89)
15/08/21 08:45:34 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 29120 ms on localhost (74/170)
15/08/21 08:45:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 134217728 end: 259741355 length: 125523627 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:34 INFO Executor: Finished task 72.0 in stage 0.0 (TID 72). 2125 bytes result sent to driver
15/08/21 08:45:34 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90, localhost, ANY, 1758 bytes)
15/08/21 08:45:34 INFO Executor: Running task 90.0 in stage 0.0 (TID 90)
15/08/21 08:45:34 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 29421 ms on localhost (75/170)
15/08/21 08:45:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:51 INFO Executor: Finished task 76.0 in stage 0.0 (TID 76). 2125 bytes result sent to driver
15/08/21 08:45:51 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91, localhost, ANY, 1773 bytes)
15/08/21 08:45:51 INFO Executor: Running task 91.0 in stage 0.0 (TID 91)
15/08/21 08:45:51 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 42702 ms on localhost (76/170)
15/08/21 08:45:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 134217728 end: 257333395 length: 123115667 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
quet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3402 ms. row count = 3500100
21-Aug-2015 08:45:22 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 18855 ms: 185.63246 rec/ms, 371.26492 cell/ms
21-Aug-2015 08:45:22 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (109 ms) and 99% processing (18855 ms)
21-Aug-2015 08:45:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:45:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 36 ms. row count = 127530
21-Aug-2015 08:45:23 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 18377 ms: 190.4609 rec/ms, 380.9218 cell/ms
21-Aug-2015 08:45:23 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 1% reading (226 ms) and 98% processing (18377 ms)
21-Aug-2015 08:45:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:45:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 16 ms. row count = 73975
21-Aug-2015 08:45:27 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 19453 ms: 179.92598 rec/ms, 359.85196 cell/ms
21-Aug-2015 08:45:27 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 11% reading (2494 ms) and 88% processing (19453 ms)
21-Aug-2015 08:45:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:45:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 72639
21-Aug-2015 08:45:27 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22292 ms: 157.01149 rec/ms, 314.02298 cell/ms
21-Aug-2015 08:45:27 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (134 ms) and 99% processing (22292 ms)
21-Aug-2015 08:45:27 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:45:27 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 71305
21-Aug-2015 08:45:32 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501462 records from 2 columns in 23246 ms: 150.62643 rec/ms, 301.25287 cell/ms
21-Aug-2015 08:45:32 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (127 ms) and 99% processing (23246 ms)
21-Aug-2015 08:45:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501462. reading next block
21-Aug-2015 08:45:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 24 ms. row count = 126306
21-Aug-2015 08:45:32 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501302 records from 2 columns in 20217 ms: 173.18604 rec/ms, 346.37207 cell/ms
21-Aug-2015 08:45:32 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (83 ms) and 99% processing (20217 ms)
21-Aug-2015 08:45:32 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501302. reading next block
21-Aug-2015 08:45:32 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 18 ms. row count = 126410
21-Aug-2015 08:45:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574316 records.
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574256 records.
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 219 ms. row count = 3503274
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 214 ms. row count = 3500100
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 214 ms. row count = 3500100
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 242 ms. row count = 3500100
21-Aug-2015 08:45:33 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 140 ms. row count = 3500100
21-Aug-2015 08:45:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 159 ms. row count = 3500100
21-Aug-2015 08:45:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627697 records.
21-Aug-2015 08:45:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 111 ms. row count = 3500100
21-Aug-2015 08:45:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 16514 ms. row count = 3500100
21-Aug-2015 08:45:51 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache15/08/21 08:45:52 INFO Executor: Finished task 75.0 in stage 0.0 (TID 75). 2125 bytes result sent to driver
15/08/21 08:45:52 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92, localhost, ANY, 1758 bytes)
15/08/21 08:45:52 INFO Executor: Running task 92.0 in stage 0.0 (TID 92)
15/08/21 08:45:52 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 43343 ms on localhost (77/170)
15/08/21 08:45:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:52 INFO Executor: Finished task 77.0 in stage 0.0 (TID 77). 2125 bytes result sent to driver
15/08/21 08:45:52 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93, localhost, ANY, 1773 bytes)
15/08/21 08:45:52 INFO Executor: Running task 93.0 in stage 0.0 (TID 93)
15/08/21 08:45:52 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 40657 ms on localhost (78/170)
15/08/21 08:45:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 134217728 end: 257330329 length: 123112601 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:53 INFO Executor: Finished task 78.0 in stage 0.0 (TID 78). 2125 bytes result sent to driver
15/08/21 08:45:53 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94, localhost, ANY, 1758 bytes)
15/08/21 08:45:53 INFO Executor: Running task 94.0 in stage 0.0 (TID 94)
15/08/21 08:45:53 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 38226 ms on localhost (79/170)
15/08/21 08:45:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:56 INFO Executor: Finished task 82.0 in stage 0.0 (TID 82). 2125 bytes result sent to driver
15/08/21 08:45:56 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95, localhost, ANY, 1768 bytes)
15/08/21 08:45:56 INFO Executor: Running task 95.0 in stage 0.0 (TID 95)
15/08/21 08:45:56 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 37394 ms on localhost (80/170)
15/08/21 08:45:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 134217728 end: 257756022 length: 123538294 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:45:56 INFO Executor: Finished task 80.0 in stage 0.0 (TID 80). 2125 bytes result sent to driver
15/08/21 08:45:56 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96, localhost, ANY, 1757 bytes)
15/08/21 08:45:56 INFO Executor: Running task 96.0 in stage 0.0 (TID 96)
15/08/21 08:45:56 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 37875 ms on localhost (81/170)
15/08/21 08:45:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:00 INFO Executor: Finished task 79.0 in stage 0.0 (TID 79). 2125 bytes result sent to driver
15/08/21 08:46:00 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97, localhost, ANY, 1771 bytes)
15/08/21 08:46:00 INFO Executor: Running task 97.0 in stage 0.0 (TID 97)
15/08/21 08:46:00 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 41700 ms on localhost (82/170)
15/08/21 08:46:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 134217728 end: 257832393 length: 123614665 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:00 INFO Executor: Finished task 81.0 in stage 0.0 (TID 81). 2125 bytes result sent to driver
15/08/21 08:46:00 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98, localhost, ANY, 1758 bytes)
15/08/21 08:46:00 INFO Executor: Running task 98.0 in stage 0.0 (TID 98)
15/08/21 08:46:00 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 42029 ms on localhost (83/170)
15/08/21 08:46:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:51 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573967 records.
21-Aug-2015 08:45:51 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:51 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 122 ms. row count = 3500100
21-Aug-2015 08:45:52 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 195 ms. row count = 3500100
21-Aug-2015 08:45:52 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574008 records.
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 34370 ms: 101.83591 rec/ms, 203.67181 cell/ms
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (129 ms) and 99% processing (34370 ms)
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 94 ms. row count = 3500100
21-Aug-2015 08:45:52 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 49 ms. row count = 71989
21-Aug-2015 08:45:53 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:53 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:53 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503231 records from 2 columns in 34378 ms: 101.90328 rec/ms, 203.80656 cell/ms
21-Aug-2015 08:45:53 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (179 ms) and 99% processing (34378 ms)
21-Aug-2015 08:45:53 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503231. reading next block
21-Aug-2015 08:45:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 71 ms. row count = 71350
21-Aug-2015 08:45:53 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 130 ms. row count = 3500100
21-Aug-2015 08:45:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574308 records.
21-Aug-2015 08:45:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:45:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:45:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:45:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 86 ms. row count = 3500100
21-Aug-2015 08:45:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 82 ms. row count = 3500100
21-Aug-2015 08:46:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573690 records.
21-Aug-2015 08:46:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 105 ms. row count = 3500612
21-Aug-2015 08:46:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501124 records.
21-Aug-2015 08:46:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 80 ms. row count = 3501124
21-Aug-2015 08:46:05 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 31690 ms: 110.44809 rec/ms, 220.89618 cell/ms
21-Aug-2015 08:46:05 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (242 ms) and 99% processing (31690 ms)
21-Aug-2015 08:46:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 17 ms. row count = 74156
21-Aug-2015 08:46:05 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 31555 ms: 110.920616 rec/ms, 221.84123 cell/ms
21-Aug-2015 08:46:05 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (140 ms) and 99% processing (31555 ms)
21-Aug-2015 08:46:05 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:05 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 7 ms. row count = 74054
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503274 records from 2 columns in 36553 ms: 95.84094 rec/ms, 191.68188 cell/ms
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (219 ms) and 99% processing (36553 ms)
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503274. reading next block
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 23 ms. row count = 71042
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 35503 ms: 98.58604 rec/ms, 197.17207 cell/ms
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (111 ms) and 99% processing (35503 ms)
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 13 ms. row count = 127597
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 19175 ms: 182.53455 rec/ms, 365.0691 cell/ms
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (122 ms) and 99% processing (19175 ms)
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:10 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 27 ms. row count = 73867
21-A15/08/21 08:46:11 INFO Executor: Finished task 87.0 in stage 0.0 (TID 87). 2125 bytes result sent to driver
15/08/21 08:46:11 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99, localhost, ANY, 1771 bytes)
15/08/21 08:46:11 INFO Executor: Running task 99.0 in stage 0.0 (TID 99)
15/08/21 08:46:11 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 38187 ms on localhost (84/170)
15/08/21 08:46:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 134217728 end: 257486081 length: 123268353 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:12 INFO Executor: Finished task 85.0 in stage 0.0 (TID 85). 2125 bytes result sent to driver
15/08/21 08:46:12 INFO TaskSetManager: Starting task 100.0 in stage 0.0 (TID 100, localhost, ANY, 1757 bytes)
15/08/21 08:46:12 INFO Executor: Running task 100.0 in stage 0.0 (TID 100)
15/08/21 08:46:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:12 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 38729 ms on localhost (85/170)
15/08/21 08:46:12 INFO Executor: Finished task 84.0 in stage 0.0 (TID 84). 2125 bytes result sent to driver
15/08/21 08:46:12 INFO TaskSetManager: Starting task 101.0 in stage 0.0 (TID 101, localhost, ANY, 1769 bytes)
15/08/21 08:46:12 INFO Executor: Running task 101.0 in stage 0.0 (TID 101)
15/08/21 08:46:12 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 38809 ms on localhost (86/170)
15/08/21 08:46:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 134217728 end: 257368738 length: 123151010 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:12 INFO Executor: Finished task 86.0 in stage 0.0 (TID 86). 2125 bytes result sent to driver
15/08/21 08:46:12 INFO TaskSetManager: Starting task 102.0 in stage 0.0 (TID 102, localhost, ANY, 1758 bytes)
15/08/21 08:46:12 INFO Executor: Running task 102.0 in stage 0.0 (TID 102)
15/08/21 08:46:12 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 38872 ms on localhost (87/170)
15/08/21 08:46:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:12 INFO Executor: Finished task 88.0 in stage 0.0 (TID 88). 2125 bytes result sent to driver
15/08/21 08:46:12 INFO TaskSetManager: Starting task 103.0 in stage 0.0 (TID 103, localhost, ANY, 1770 bytes)
15/08/21 08:46:12 INFO Executor: Running task 103.0 in stage 0.0 (TID 103)
15/08/21 08:46:12 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 38247 ms on localhost (88/170)
15/08/21 08:46:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 134217728 end: 257317174 length: 123099446 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:16 INFO Executor: Finished task 83.0 in stage 0.0 (TID 83). 2125 bytes result sent to driver
15/08/21 08:46:16 INFO TaskSetManager: Starting task 104.0 in stage 0.0 (TID 104, localhost, ANY, 1758 bytes)
15/08/21 08:46:16 INFO Executor: Running task 104.0 in stage 0.0 (TID 104)
15/08/21 08:46:16 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 43612 ms on localhost (89/170)
15/08/21 08:46:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:17 INFO Executor: Finished task 89.0 in stage 0.0 (TID 89). 2125 bytes result sent to driver
15/08/21 08:46:17 INFO TaskSetManager: Starting task 105.0 in stage 0.0 (TID 105, localhost, ANY, 1769 bytes)
15/08/21 08:46:17 INFO Executor: Running task 105.0 in stage 0.0 (TID 105)
15/08/21 08:46:17 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 42505 ms on localhost (90/170)
15/08/21 08:46:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 134217728 end: 257566042 length: 123348314 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:17 INFO Executor: Finished task 90.0 in stage 0.0 (TID 90). 2125 bytes result sent to driver
15/08/21 08:46:17 INFO TaskSetManager: Starting task 106.0 in stage 0.0 (TID 106, localhost, ANY, 1758 bytes)
15/08/21 08:46:17 INFO Executor: Running task 106.0 in stage 0.0 (TID 106)
15/08/21 08:46:17 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 42494 ms on localhost (91/170)
15/08/21 08:46:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:17 INFO Executor: Finished task 92.0 in stage 0.0 (TID 92). 2125 bytes result sent to driver
15/08/21 08:46:17 INFO TaskSetManager: Starting task 107.0 in stage 0.0 (TID 107, localhost, ANY, 1771 bytes)
15/08/21 08:46:17 INFO Executor: Running task 107.0 in stage 0.0 (TID 107)
15/08/21 08:46:17 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 25341 ms on localhost (92/170)
15/08/21 08:46:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 134217728 end: 257458240 length: 123240512 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:17 INFO Executor: Finished task 91.0 in stage 0.0 (TID 91). 2125 bytes result sent to driver
15/08/21 08:46:17 INFO TaskSetManager: Starting task 108.0 in stage 0.0 (TID 108, localhost, ANY, 1758 bytes)
15/08/21 08:46:17 INFO Executor: Running task 108.0 in stage 0.0 (TID 108)
15/08/21 08:46:17 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 26400 ms on localhost (93/170)
15/08/21 08:46:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:17 INFO Executor: Finished task 93.0 in stage 0.0 (TID 93). 2125 bytes result sent to driver
15/08/21 08:46:17 INFO TaskSetManager: Starting task 109.0 in stage 0.0 (TID 109, localhost, ANY, 1770 bytes)
15/08/21 08:46:17 INFO Executor: Running task 109.0 in stage 0.0 (TID 109)
15/08/21 08:46:17 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 25058 ms on localhost (94/170)
15/08/21 08:46:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 134217728 end: 257458739 length: 123241011 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:18 INFO Executor: Finished task 94.0 in stage 0.0 (TID 94). 2125 bytes result sent to driver
15/08/21 08:46:18 INFO TaskSetManager: Starting task 110.0 in stage 0.0 (TID 110, localhost, ANY, 1757 bytes)
15/08/21 08:46:18 INFO Executor: Running task 110.0 in stage 0.0 (TID 110)
15/08/21 08:46:18 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 24876 ms on localhost (95/170)
15/08/21 08:46:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
ug-2015 08:46:11 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 18125 ms: 193.10896 rec/ms, 386.21793 cell/ms
21-Aug-2015 08:46:11 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (94 ms) and 99% processing (18125 ms)
21-Aug-2015 08:46:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 22 ms. row count = 73908
21-Aug-2015 08:46:11 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:11 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573157 records.
21-Aug-2015 08:46:11 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:11 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 94 ms. row count = 3500100
21-Aug-2015 08:46:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 113 ms. row count = 3500100
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 195 ms. row count = 3500100
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 102 ms. row count = 3500100
21-Aug-2015 08:46:12 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573826 records.
21-Aug-2015 08:46:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 3965 ms. row count = 3500100
21-Aug-2015 08:46:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500786 records.
21-Aug-2015 08:46:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 151 ms. row count = 3500786
21-Aug-2015 08:46:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573221 records.
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 80 ms. row count = 3501171
21-Aug-2015 08:46:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502806 records.
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571385 records.
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500612 records from 2 columns in 17396 ms: 201.23085 rec/ms, 402.4617 cell/ms
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (105 ms) and 99% processing (17396 ms)
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500612. reading next block
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 20993 ms: 166.727 rec/ms, 333.454 cell/ms
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (86 ms) and 99% processing (20993 ms)
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 27 ms. row count = 73078
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 24 ms. row count = 74208
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 234 ms. row count = 3500100
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 296 ms. row count = 3502806
21-Aug-2015 08:46:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574219 records.
21-Aug-2015 08:46:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 182 ms. row count = 3501364
21-Aug-2015 08:46:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 311 ms. row count = 3500100
21-Aug-2015 08:46:18 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:18 INFO: parquet.hadoop.InternalParquetRecordRe15/08/21 08:46:22 INFO Executor: Finished task 96.0 in stage 0.0 (TID 96). 2125 bytes result sent to driver
15/08/21 08:46:22 INFO TaskSetManager: Starting task 111.0 in stage 0.0 (TID 111, localhost, ANY, 1771 bytes)
15/08/21 08:46:22 INFO Executor: Running task 111.0 in stage 0.0 (TID 111)
15/08/21 08:46:22 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 26232 ms on localhost (96/170)
15/08/21 08:46:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 134217728 end: 257439181 length: 123221453 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:23 INFO Executor: Finished task 95.0 in stage 0.0 (TID 95). 2125 bytes result sent to driver
15/08/21 08:46:23 INFO TaskSetManager: Starting task 112.0 in stage 0.0 (TID 112, localhost, ANY, 1757 bytes)
15/08/21 08:46:23 INFO Executor: Running task 112.0 in stage 0.0 (TID 112)
15/08/21 08:46:23 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 26759 ms on localhost (97/170)
15/08/21 08:46:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:23 INFO Executor: Finished task 97.0 in stage 0.0 (TID 97). 2125 bytes result sent to driver
15/08/21 08:46:23 INFO TaskSetManager: Starting task 113.0 in stage 0.0 (TID 113, localhost, ANY, 1770 bytes)
15/08/21 08:46:23 INFO Executor: Running task 113.0 in stage 0.0 (TID 113)
15/08/21 08:46:23 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 23326 ms on localhost (98/170)
15/08/21 08:46:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 134217728 end: 181459518 length: 47241790 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:28 INFO Executor: Finished task 98.0 in stage 0.0 (TID 98). 2125 bytes result sent to driver
15/08/21 08:46:28 INFO TaskSetManager: Starting task 114.0 in stage 0.0 (TID 114, localhost, ANY, 1757 bytes)
15/08/21 08:46:28 INFO Executor: Running task 114.0 in stage 0.0 (TID 114)
15/08/21 08:46:28 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 27526 ms on localhost (99/170)
15/08/21 08:46:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:56 INFO Executor: Finished task 113.0 in stage 0.0 (TID 113). 2125 bytes result sent to driver
15/08/21 08:46:56 INFO TaskSetManager: Starting task 115.0 in stage 0.0 (TID 115, localhost, ANY, 1768 bytes)
15/08/21 08:46:56 INFO Executor: Running task 115.0 in stage 0.0 (TID 115)
15/08/21 08:46:56 INFO TaskSetManager: Finished task 113.0 in stage 0.0 (TID 113) in 33158 ms on localhost (100/170)
15/08/21 08:46:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 134217728 end: 259458210 length: 125240482 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:57 INFO Executor: Finished task 100.0 in stage 0.0 (TID 100). 2125 bytes result sent to driver
15/08/21 08:46:57 INFO TaskSetManager: Starting task 116.0 in stage 0.0 (TID 116, localhost, ANY, 1758 bytes)
15/08/21 08:46:57 INFO Executor: Running task 116.0 in stage 0.0 (TID 116)
15/08/21 08:46:57 INFO TaskSetManager: Finished task 100.0 in stage 0.0 (TID 100) in 45051 ms on localhost (101/170)
15/08/21 08:46:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:57 INFO Executor: Finished task 102.0 in stage 0.0 (TID 102). 2125 bytes result sent to driver
15/08/21 08:46:57 INFO TaskSetManager: Starting task 117.0 in stage 0.0 (TID 117, localhost, ANY, 1769 bytes)
15/08/21 08:46:57 INFO Executor: Running task 117.0 in stage 0.0 (TID 117)
15/08/21 08:46:57 INFO TaskSetManager: Finished task 102.0 in stage 0.0 (TID 102) in 45008 ms on localhost (102/170)
15/08/21 08:46:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 134217728 end: 257467186 length: 123249458 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:57 INFO Executor: Finished task 99.0 in stage 0.0 (TID 99). 2125 bytes result sent to driver
15/08/21 08:46:57 INFO TaskSetManager: Starting task 118.0 in stage 0.0 (TID 118, localhost, ANY, 1756 bytes)
15/08/21 08:46:57 INFO Executor: Running task 118.0 in stage 0.0 (TID 118)
15/08/21 08:46:57 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 45693 ms on localhost (103/170)
15/08/21 08:46:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:46:57 INFO Executor: Finished task 101.0 in stage 0.0 (TID 101). 2125 bytes result sent to driver
15/08/21 08:46:57 INFO TaskSetManager: Starting task 119.0 in stage 0.0 (TID 119, localhost, ANY, 1769 bytes)
15/08/21 08:46:57 INFO Executor: Running task 119.0 in stage 0.0 (TID 119)
15/08/21 08:46:57 INFO TaskSetManager: Finished task 101.0 in stage 0.0 (TID 101) in 45916 ms on localhost (104/170)
15/08/21 08:46:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 134217728 end: 260141700 length: 125923972 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
ader: RecordReader initialized will read a total of 3501448 records.
21-Aug-2015 08:46:18 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:18 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 175 ms. row count = 3501448
21-Aug-2015 08:46:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572926 records.
21-Aug-2015 08:46:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 124 ms. row count = 3500100
21-Aug-2015 08:46:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:46:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1466882 records.
21-Aug-2015 08:46:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 189 ms. row count = 3500100
21-Aug-2015 08:46:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 170 ms. row count = 1466882
21-Aug-2015 08:46:28 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:28 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:46:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 99 ms. row count = 3500100
21-Aug-2015 08:46:33 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 21844 ms: 160.23164 rec/ms, 320.4633 cell/ms
21-Aug-2015 08:46:33 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (94 ms) and 99% processing (21844 ms)
21-Aug-2015 08:46:33 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:33 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 30 ms. row count = 73057
21-Aug-2015 08:46:34 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22160 ms: 157.94675 rec/ms, 315.8935 cell/ms
21-Aug-2015 08:46:34 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (113 ms) and 99% processing (22160 ms)
21-Aug-2015 08:46:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 23 ms. row count = 73871
21-Aug-2015 08:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 38559 ms: 90.77258 rec/ms, 181.54517 cell/ms
21-Aug-2015 08:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 9% reading (3965 ms) and 90% processing (38559 ms)
21-Aug-2015 08:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 31 ms. row count = 73726
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501171 records from 2 columns in 38771 ms: 90.30386 rec/ms, 180.60773 cell/ms
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (80 ms) and 99% processing (38771 ms)
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501171. reading next block
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 72050
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 38711 ms: 90.41616 rec/ms, 180.83232 cell/ms
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (234 ms) and 99% processing (38711 ms)
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 26 ms. row count = 71285
21-Aug-2015 08:46:56 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627682 records.
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:56 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 115 ms. row count = 3503132
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501364 records from 2 columns in 38899 ms: 90.01167 rec/ms, 180.02335 cell/ms
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (182 ms) and 99% processing (38899 ms)
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501364. reading next block
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 20 ms. row count = 72855
21-Aug-2015 08:46:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574192 records.
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 111 ms. row count = 3500100
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 91 ms. row count = 3501130
21-Aug-2015 08:46:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500631 records.
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 112 ms. row count = 3500631
21-Aug-2015 08:46:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize 15/08/21 08:46:58 INFO Executor: Finished task 103.0 in stage 0.0 (TID 103). 2125 bytes result sent to driver
15/08/21 08:46:58 INFO TaskSetManager: Starting task 120.0 in stage 0.0 (TID 120, localhost, ANY, 1756 bytes)
15/08/21 08:46:58 INFO Executor: Running task 120.0 in stage 0.0 (TID 120)
15/08/21 08:46:58 INFO TaskSetManager: Finished task 103.0 in stage 0.0 (TID 103) in 45985 ms on localhost (105/170)
15/08/21 08:46:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:00 INFO Executor: Finished task 104.0 in stage 0.0 (TID 104). 2125 bytes result sent to driver
15/08/21 08:47:00 INFO TaskSetManager: Starting task 121.0 in stage 0.0 (TID 121, localhost, ANY, 1769 bytes)
15/08/21 08:47:00 INFO Executor: Running task 121.0 in stage 0.0 (TID 121)
15/08/21 08:47:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 134217728 end: 259582440 length: 125364712 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:00 INFO TaskSetManager: Finished task 104.0 in stage 0.0 (TID 104) in 44079 ms on localhost (106/170)
15/08/21 08:47:01 INFO Executor: Finished task 105.0 in stage 0.0 (TID 105). 2125 bytes result sent to driver
15/08/21 08:47:01 INFO TaskSetManager: Starting task 122.0 in stage 0.0 (TID 122, localhost, ANY, 1758 bytes)
15/08/21 08:47:01 INFO Executor: Running task 122.0 in stage 0.0 (TID 122)
15/08/21 08:47:01 INFO TaskSetManager: Finished task 105.0 in stage 0.0 (TID 105) in 44152 ms on localhost (107/170)
15/08/21 08:47:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:01 INFO Executor: Finished task 107.0 in stage 0.0 (TID 107). 2125 bytes result sent to driver
15/08/21 08:47:01 INFO TaskSetManager: Starting task 123.0 in stage 0.0 (TID 123, localhost, ANY, 1770 bytes)
15/08/21 08:47:01 INFO Executor: Running task 123.0 in stage 0.0 (TID 123)
15/08/21 08:47:01 INFO TaskSetManager: Finished task 107.0 in stage 0.0 (TID 107) in 44443 ms on localhost (108/170)
15/08/21 08:47:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 134217728 end: 257838232 length: 123620504 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:01 INFO Executor: Finished task 108.0 in stage 0.0 (TID 108). 2125 bytes result sent to driver
15/08/21 08:47:01 INFO TaskSetManager: Starting task 124.0 in stage 0.0 (TID 124, localhost, ANY, 1757 bytes)
15/08/21 08:47:01 INFO Executor: Running task 124.0 in stage 0.0 (TID 124)
15/08/21 08:47:01 INFO TaskSetManager: Finished task 108.0 in stage 0.0 (TID 108) in 44119 ms on localhost (109/170)
15/08/21 08:47:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:02 INFO Executor: Finished task 109.0 in stage 0.0 (TID 109). 2125 bytes result sent to driver
15/08/21 08:47:02 INFO TaskSetManager: Starting task 125.0 in stage 0.0 (TID 125, localhost, ANY, 1770 bytes)
15/08/21 08:47:02 INFO Executor: Running task 125.0 in stage 0.0 (TID 125)
15/08/21 08:47:02 INFO TaskSetManager: Finished task 109.0 in stage 0.0 (TID 109) in 44265 ms on localhost (110/170)
15/08/21 08:47:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 134217728 end: 257849235 length: 123631507 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:06 INFO Executor: Finished task 110.0 in stage 0.0 (TID 110). 2125 bytes result sent to driver
15/08/21 08:47:06 INFO TaskSetManager: Starting task 126.0 in stage 0.0 (TID 126, localhost, ANY, 1758 bytes)
15/08/21 08:47:06 INFO Executor: Running task 126.0 in stage 0.0 (TID 126)
15/08/21 08:47:06 INFO TaskSetManager: Finished task 110.0 in stage 0.0 (TID 110) in 47969 ms on localhost (111/170)
15/08/21 08:47:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:06 INFO Executor: Finished task 106.0 in stage 0.0 (TID 106). 2125 bytes result sent to driver
15/08/21 08:47:06 INFO TaskSetManager: Starting task 127.0 in stage 0.0 (TID 127, localhost, ANY, 1770 bytes)
15/08/21 08:47:06 INFO Executor: Running task 127.0 in stage 0.0 (TID 127)
15/08/21 08:47:06 INFO TaskSetManager: Finished task 106.0 in stage 0.0 (TID 106) in 48843 ms on localhost (112/170)
15/08/21 08:47:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 134217728 end: 257181348 length: 122963620 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:07 INFO Executor: Finished task 111.0 in stage 0.0 (TID 111). 2125 bytes result sent to driver
15/08/21 08:47:07 INFO TaskSetManager: Starting task 128.0 in stage 0.0 (TID 128, localhost, ANY, 1758 bytes)
15/08/21 08:47:07 INFO Executor: Running task 128.0 in stage 0.0 (TID 128)
15/08/21 08:47:07 INFO TaskSetManager: Finished task 111.0 in stage 0.0 (TID 111) in 44426 ms on localhost (113/170)
15/08/21 08:47:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:07 INFO Executor: Finished task 112.0 in stage 0.0 (TID 112). 2125 bytes result sent to driver
15/08/21 08:47:07 INFO TaskSetManager: Starting task 129.0 in stage 0.0 (TID 129, localhost, ANY, 1769 bytes)
15/08/21 08:47:07 INFO Executor: Running task 129.0 in stage 0.0 (TID 129)
15/08/21 08:47:07 INFO TaskSetManager: Finished task 112.0 in stage 0.0 (TID 112) in 44197 ms on localhost (114/170)
15/08/21 08:47:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 134217728 end: 257473792 length: 123256064 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:16 INFO Executor: Finished task 114.0 in stage 0.0 (TID 114). 2125 bytes result sent to driver
15/08/21 08:47:16 INFO TaskSetManager: Starting task 130.0 in stage 0.0 (TID 130, localhost, ANY, 1757 bytes)
15/08/21 08:47:16 INFO Executor: Running task 130.0 in stage 0.0 (TID 130)
15/08/21 08:47:16 INFO TaskSetManager: Finished task 114.0 in stage 0.0 (TID 114) in 48733 ms on localhost (115/170)
15/08/21 08:47:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3648307 records.
21-Aug-2015 08:46:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:46:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 143 ms. row count = 3500100
21-Aug-2015 08:46:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:46:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501115 records.
21-Aug-2015 08:46:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2274 ms. row count = 3501115
21-Aug-2015 08:47:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626900 records.
21-Aug-2015 08:47:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 101 ms. row count = 3501235
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 38178 ms: 91.67845 rec/ms, 183.3569 cell/ms
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (124 ms) and 99% processing (38178 ms)
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 29 ms. row count = 72826
21-Aug-2015 08:47:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 105 ms. row count = 3500100
21-Aug-2015 08:47:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574036 records.
21-Aug-2015 08:47:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:01 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:02 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 119 ms. row count = 3501487
21-Aug-2015 08:47:02 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501235 records.
21-Aug-2015 08:47:02 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:02 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:02 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573259 records.
21-Aug-2015 08:47:02 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:02 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 131 ms. row count = 3501235
21-Aug-2015 08:47:02 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 109 ms. row count = 3500100
21-Aug-2015 08:47:06 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:06 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502933 records.
21-Aug-2015 08:47:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:06 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:06 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571471 records.
21-Aug-2015 08:47:06 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 65 ms. row count = 3501332
21-Aug-2015 08:47:06 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 160 ms. row count = 3502933
21-Aug-2015 08:47:07 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:07 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500833 records.
21-Aug-2015 08:47:07 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:07 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 87 ms. row count = 3500833
21-Aug-2015 08:47:07 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:07 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573329 records.
21-Aug-2015 08:47:07 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:07 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 98 ms. row count = 3500100
21-Aug-2015 08:47:12 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3503132 records from 2 columns in 15505 ms: 225.93564 rec/ms, 451.87128 cell/ms
21-Aug-2015 08:47:12 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (115 ms) and 99% processing (15505 ms)
21-Aug-2015 08:47:12 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3503132. reading next block
21-Aug-2015 08:47:12 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 33 ms. row count = 124550
21-Aug-2015 08:47:16 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501130 records from 2 columns in 19380 ms: 180.65686 rec/ms, 361.31372 cell/ms
21-Aug-2015 08:47:16 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (91 ms) and 99% processing (19380 ms)
21-Aug-2015 08:47:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501130. reading next block
21-Aug-2015 08:47:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 34 ms. row count = 73062
21-Aug-2015 08:47:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3502559 records.
21-Aug-215/08/21 08:47:22 INFO Executor: Finished task 115.0 in stage 0.0 (TID 115). 2125 bytes result sent to driver
15/08/21 08:47:22 INFO TaskSetManager: Starting task 131.0 in stage 0.0 (TID 131, localhost, ANY, 1770 bytes)
15/08/21 08:47:22 INFO Executor: Running task 131.0 in stage 0.0 (TID 131)
15/08/21 08:47:22 INFO TaskSetManager: Finished task 115.0 in stage 0.0 (TID 115) in 26387 ms on localhost (116/170)
15/08/21 08:47:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 134217728 end: 257455806 length: 123238078 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:23 INFO Executor: Finished task 118.0 in stage 0.0 (TID 118). 2125 bytes result sent to driver
15/08/21 08:47:23 INFO TaskSetManager: Starting task 132.0 in stage 0.0 (TID 132, localhost, ANY, 1758 bytes)
15/08/21 08:47:23 INFO Executor: Running task 132.0 in stage 0.0 (TID 132)
15/08/21 08:47:23 INFO TaskSetManager: Finished task 118.0 in stage 0.0 (TID 118) in 25489 ms on localhost (117/170)
15/08/21 08:47:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:23 INFO Executor: Finished task 117.0 in stage 0.0 (TID 117). 2125 bytes result sent to driver
15/08/21 08:47:23 INFO TaskSetManager: Starting task 133.0 in stage 0.0 (TID 133, localhost, ANY, 1773 bytes)
15/08/21 08:47:23 INFO Executor: Running task 133.0 in stage 0.0 (TID 133)
15/08/21 08:47:23 INFO TaskSetManager: Finished task 117.0 in stage 0.0 (TID 117) in 26046 ms on localhost (118/170)
15/08/21 08:47:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 134217728 end: 257547934 length: 123330206 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:23 INFO Executor: Finished task 119.0 in stage 0.0 (TID 119). 2125 bytes result sent to driver
15/08/21 08:47:23 INFO TaskSetManager: Starting task 134.0 in stage 0.0 (TID 134, localhost, ANY, 1758 bytes)
15/08/21 08:47:23 INFO Executor: Running task 134.0 in stage 0.0 (TID 134)
15/08/21 08:47:23 INFO TaskSetManager: Finished task 119.0 in stage 0.0 (TID 119) in 25600 ms on localhost (119/170)
15/08/21 08:47:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:23 INFO Executor: Finished task 116.0 in stage 0.0 (TID 116). 2125 bytes result sent to driver
15/08/21 08:47:23 INFO TaskSetManager: Starting task 135.0 in stage 0.0 (TID 135, localhost, ANY, 1769 bytes)
15/08/21 08:47:23 INFO Executor: Running task 135.0 in stage 0.0 (TID 135)
15/08/21 08:47:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 134217728 end: 257790576 length: 123572848 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:23 INFO TaskSetManager: Finished task 116.0 in stage 0.0 (TID 116) in 26661 ms on localhost (120/170)
15/08/21 08:47:23 INFO Executor: Finished task 120.0 in stage 0.0 (TID 120). 2125 bytes result sent to driver
15/08/21 08:47:23 INFO TaskSetManager: Starting task 136.0 in stage 0.0 (TID 136, localhost, ANY, 1757 bytes)
15/08/21 08:47:23 INFO Executor: Running task 136.0 in stage 0.0 (TID 136)
15/08/21 08:47:23 INFO TaskSetManager: Finished task 120.0 in stage 0.0 (TID 120) in 25309 ms on localhost (121/170)
15/08/21 08:47:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:29 INFO Executor: Finished task 122.0 in stage 0.0 (TID 122). 2125 bytes result sent to driver
15/08/21 08:47:29 INFO TaskSetManager: Starting task 137.0 in stage 0.0 (TID 137, localhost, ANY, 1770 bytes)
15/08/21 08:47:29 INFO Executor: Running task 137.0 in stage 0.0 (TID 137)
15/08/21 08:47:29 INFO TaskSetManager: Finished task 122.0 in stage 0.0 (TID 122) in 28279 ms on localhost (122/170)
15/08/21 08:47:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 134217728 end: 257571649 length: 123353921 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:29 INFO Executor: Finished task 123.0 in stage 0.0 (TID 123). 2125 bytes result sent to driver
15/08/21 08:47:29 INFO TaskSetManager: Starting task 138.0 in stage 0.0 (TID 138, localhost, ANY, 1758 bytes)
15/08/21 08:47:29 INFO Executor: Running task 138.0 in stage 0.0 (TID 138)
15/08/21 08:47:29 INFO TaskSetManager: Finished task 123.0 in stage 0.0 (TID 123) in 28046 ms on localhost (123/170)
15/08/21 08:47:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:29 INFO Executor: Finished task 124.0 in stage 0.0 (TID 124). 2125 bytes result sent to driver
15/08/21 08:47:29 INFO TaskSetManager: Starting task 139.0 in stage 0.0 (TID 139, localhost, ANY, 1770 bytes)
15/08/21 08:47:29 INFO Executor: Running task 139.0 in stage 0.0 (TID 139)
15/08/21 08:47:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 134217728 end: 257573201 length: 123355473 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:29 INFO TaskSetManager: Finished task 124.0 in stage 0.0 (TID 124) in 28035 ms on localhost (124/170)
015 08:47:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:17 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 18848 ms: 185.7014 rec/ms, 371.4028 cell/ms
21-Aug-2015 08:47:17 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (143 ms) and 99% processing (18848 ms)
21-Aug-2015 08:47:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:47:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 53 ms. row count = 148207
21-Aug-2015 08:47:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 106 ms. row count = 3502559
21-Aug-2015 08:47:21 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501235 records from 2 columns in 20747 ms: 168.75862 rec/ms, 337.51724 cell/ms
21-Aug-2015 08:47:21 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (101 ms) and 99% processing (20747 ms)
21-Aug-2015 08:47:21 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501235. reading next block
21-Aug-2015 08:47:21 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 38 ms. row count = 125665
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501487 records from 2 columns in 20178 ms: 173.52994 rec/ms, 347.05988 cell/ms
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (119 ms) and 99% processing (20178 ms)
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501487. reading next block
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 18 ms. row count = 72549
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 20462 ms: 171.05367 rec/ms, 342.10733 cell/ms
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (109 ms) and 99% processing (20462 ms)
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14 ms. row count = 73159
21-Aug-2015 08:47:22 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571559 records.
21-Aug-2015 08:47:22 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 135 ms. row count = 3500100
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501043 records.
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 149 ms. row count = 3501043
21-Aug-2015 08:47:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573366 records.
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501332 records from 2 columns in 16944 ms: 206.6414 rec/ms, 413.2828 cell/ms
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (65 ms) and 99% processing (16944 ms)
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501332. reading next block
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 17 ms. row count = 70139
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 168 ms. row count = 3501786
21-Aug-2015 08:47:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 73 ms. row count = 3500100
21-Aug-2015 08:47:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574145 records.
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:23 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 116 ms. row count = 3500100
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501169 records.
21-Aug-2015 08:47:23 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:24 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 122 ms. row count = 3501169
21-Aug-2015 08:47:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573313 records.
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 22140 ms: 158.08943 rec/ms, 316.17886 cell/ms
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (98 ms) and 99% processing (22140 ms)
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 73229
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 147 ms. row count = 3501584
21-Aug-2015 08:47:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:29 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501195 records.
21-Aug-2015 08:47:29 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordRe15/08/21 08:47:30 INFO Executor: Finished task 121.0 in stage 0.0 (TID 121). 2125 bytes result sent to driver
15/08/21 08:47:30 INFO TaskSetManager: Starting task 140.0 in stage 0.0 (TID 140, localhost, ANY, 1757 bytes)
15/08/21 08:47:30 INFO TaskSetManager: Finished task 121.0 in stage 0.0 (TID 121) in 29346 ms on localhost (125/170)
15/08/21 08:47:30 INFO Executor: Running task 140.0 in stage 0.0 (TID 140)
15/08/21 08:47:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:30 INFO Executor: Finished task 125.0 in stage 0.0 (TID 125). 2125 bytes result sent to driver
15/08/21 08:47:30 INFO TaskSetManager: Starting task 141.0 in stage 0.0 (TID 141, localhost, ANY, 1770 bytes)
15/08/21 08:47:30 INFO Executor: Running task 141.0 in stage 0.0 (TID 141)
15/08/21 08:47:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 134217728 end: 257466118 length: 123248390 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:30 INFO TaskSetManager: Finished task 125.0 in stage 0.0 (TID 125) in 28611 ms on localhost (126/170)
15/08/21 08:47:31 INFO Executor: Finished task 126.0 in stage 0.0 (TID 126). 2125 bytes result sent to driver
15/08/21 08:47:31 INFO TaskSetManager: Starting task 142.0 in stage 0.0 (TID 142, localhost, ANY, 1758 bytes)
15/08/21 08:47:31 INFO Executor: Running task 142.0 in stage 0.0 (TID 142)
15/08/21 08:47:31 INFO TaskSetManager: Finished task 126.0 in stage 0.0 (TID 126) in 24924 ms on localhost (127/170)
15/08/21 08:47:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:31 INFO Executor: Finished task 127.0 in stage 0.0 (TID 127). 2125 bytes result sent to driver
15/08/21 08:47:31 INFO TaskSetManager: Starting task 143.0 in stage 0.0 (TID 143, localhost, ANY, 1770 bytes)
15/08/21 08:47:31 INFO Executor: Running task 143.0 in stage 0.0 (TID 143)
15/08/21 08:47:31 INFO TaskSetManager: Finished task 127.0 in stage 0.0 (TID 127) in 25571 ms on localhost (128/170)
15/08/21 08:47:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 134217728 end: 257888240 length: 123670512 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:49 INFO Executor: Finished task 129.0 in stage 0.0 (TID 129). 2125 bytes result sent to driver
15/08/21 08:47:49 INFO Executor: Finished task 128.0 in stage 0.0 (TID 128). 2125 bytes result sent to driver
15/08/21 08:47:49 INFO TaskSetManager: Starting task 144.0 in stage 0.0 (TID 144, localhost, ANY, 1757 bytes)
15/08/21 08:47:49 INFO Executor: Running task 144.0 in stage 0.0 (TID 144)
15/08/21 08:47:49 INFO TaskSetManager: Finished task 128.0 in stage 0.0 (TID 128) in 42159 ms on localhost (129/170)
15/08/21 08:47:49 INFO TaskSetManager: Starting task 145.0 in stage 0.0 (TID 145, localhost, ANY, 1768 bytes)
15/08/21 08:47:49 INFO Executor: Running task 145.0 in stage 0.0 (TID 145)
15/08/21 08:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:47:49 INFO TaskSetManager: Finished task 129.0 in stage 0.0 (TID 129) in 41941 ms on localhost (130/170)
15/08/21 08:47:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 134217728 end: 258178393 length: 123960665 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:00 INFO Executor: Finished task 130.0 in stage 0.0 (TID 130). 2125 bytes result sent to driver
15/08/21 08:48:00 INFO TaskSetManager: Starting task 146.0 in stage 0.0 (TID 146, localhost, ANY, 1758 bytes)
15/08/21 08:48:00 INFO Executor: Running task 146.0 in stage 0.0 (TID 146)
15/08/21 08:48:00 INFO TaskSetManager: Finished task 130.0 in stage 0.0 (TID 130) in 43249 ms on localhost (131/170)
15/08/21 08:48:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:07 INFO Executor: Finished task 132.0 in stage 0.0 (TID 132). 2125 bytes result sent to driver
15/08/21 08:48:07 INFO TaskSetManager: Starting task 147.0 in stage 0.0 (TID 147, localhost, ANY, 1771 bytes)
15/08/21 08:48:07 INFO Executor: Running task 147.0 in stage 0.0 (TID 147)
15/08/21 08:48:07 INFO TaskSetManager: Finished task 132.0 in stage 0.0 (TID 132) in 44257 ms on localhost (132/170)
15/08/21 08:48:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 134217728 end: 257798680 length: 123580952 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:07 INFO Executor: Finished task 133.0 in stage 0.0 (TID 133). 2125 bytes result sent to driver
15/08/21 08:48:07 INFO TaskSetManager: Starting task 148.0 in stage 0.0 (TID 148, localhost, ANY, 1757 bytes)
15/08/21 08:48:07 INFO Executor: Running task 148.0 in stage 0.0 (TID 148)
15/08/21 08:48:07 INFO TaskSetManager: Finished task 133.0 in stage 0.0 (TID 133) in 44083 ms on localhost (133/170)
15/08/21 08:48:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:07 INFO Executor: Finished task 134.0 in stage 0.0 (TID 134). 2125 bytes result sent to driver
15/08/21 08:48:07 INFO TaskSetManager: Starting task 149.0 in stage 0.0 (TID 149, localhost, ANY, 1771 bytes)
15/08/21 08:48:07 INFO Executor: Running task 149.0 in stage 0.0 (TID 149)
15/08/21 08:48:07 INFO TaskSetManager: Finished task 134.0 in stage 0.0 (TID 134) in 43886 ms on localhost (134/170)
15/08/21 08:48:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 134217728 end: 257837778 length: 123620050 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
ader: RecordReader initialized will read a total of 3572913 records.
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 195 ms. row count = 3501195
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 212 ms. row count = 3500728
21-Aug-2015 08:47:30 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501339 records.
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 152 ms. row count = 3501339
21-Aug-2015 08:47:30 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572985 records.
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:30 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 105 ms. row count = 3500100
21-Aug-2015 08:47:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:47:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 88 ms. row count = 3500100
21-Aug-2015 08:47:31 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:31 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
21-Aug-2015 08:47:31 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:31 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 153 ms. row count = 3501076
21-Aug-2015 08:47:49 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:49 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:47:49 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:47:49 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573846 records.
21-Aug-2015 08:47:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:49 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:47:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 108 ms. row count = 3500100
21-Aug-2015 08:47:49 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 214 ms. row count = 3500100
21-Aug-2015 08:47:55 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501786 records from 2 columns in 31956 ms: 109.58149 rec/ms, 219.16298 cell/ms
21-Aug-2015 08:47:55 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (168 ms) and 99% processing (31956 ms)
21-Aug-2015 08:47:55 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501786. reading next block
21-Aug-2015 08:47:55 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 28 ms. row count = 71580
21-Aug-2015 08:48:00 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:00 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501395 records.
21-Aug-2015 08:48:00 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:00 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 216 ms. row count = 3501395
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 37244 ms: 93.977554 rec/ms, 187.95511 cell/ms
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (116 ms) and 99% processing (37244 ms)
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 43 ms. row count = 74045
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 38170 ms: 91.69767 rec/ms, 183.39534 cell/ms
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (135 ms) and 99% processing (38170 ms)
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 36 ms. row count = 71459
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501584 records from 2 columns in 31676 ms: 110.543755 rec/ms, 221.08751 cell/ms
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (147 ms) and 99% processing (31676 ms)
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501584. reading next block
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 71729
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500728 records from 2 columns in 31702 ms: 110.426094 rec/ms, 220.85219 cell/ms
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (212 ms) and 99% processing (31702 ms)
21-Aug-2015 08:48:01 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500728. reading next block
21-Aug-2015 08:48:02 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 74 ms. row count = 72185
21-Aug-2015 08:48:07 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:07 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572381 records.
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:07 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.ap15/08/21 08:48:07 INFO Executor: Finished task 136.0 in stage 0.0 (TID 136). 2125 bytes result sent to driver
15/08/21 08:48:07 INFO TaskSetManager: Starting task 150.0 in stage 0.0 (TID 150, localhost, ANY, 1757 bytes)
15/08/21 08:48:07 INFO Executor: Running task 150.0 in stage 0.0 (TID 150)
15/08/21 08:48:07 INFO TaskSetManager: Finished task 136.0 in stage 0.0 (TID 136) in 44052 ms on localhost (135/170)
15/08/21 08:48:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:08 INFO Executor: Finished task 135.0 in stage 0.0 (TID 135). 2125 bytes result sent to driver
15/08/21 08:48:08 INFO TaskSetManager: Starting task 151.0 in stage 0.0 (TID 151, localhost, ANY, 1771 bytes)
15/08/21 08:48:08 INFO Executor: Running task 151.0 in stage 0.0 (TID 151)
15/08/21 08:48:08 INFO TaskSetManager: Finished task 135.0 in stage 0.0 (TID 135) in 44664 ms on localhost (136/170)
15/08/21 08:48:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 134217728 end: 257748250 length: 123530522 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:08 INFO Executor: Finished task 131.0 in stage 0.0 (TID 131). 2125 bytes result sent to driver
15/08/21 08:48:08 INFO TaskSetManager: Starting task 152.0 in stage 0.0 (TID 152, localhost, ANY, 1757 bytes)
15/08/21 08:48:08 INFO Executor: Running task 152.0 in stage 0.0 (TID 152)
15/08/21 08:48:08 INFO TaskSetManager: Finished task 131.0 in stage 0.0 (TID 131) in 45676 ms on localhost (137/170)
15/08/21 08:48:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:08 INFO Executor: Finished task 137.0 in stage 0.0 (TID 137). 2125 bytes result sent to driver
15/08/21 08:48:08 INFO TaskSetManager: Starting task 153.0 in stage 0.0 (TID 153, localhost, ANY, 1772 bytes)
15/08/21 08:48:08 INFO Executor: Running task 153.0 in stage 0.0 (TID 153)
15/08/21 08:48:08 INFO TaskSetManager: Finished task 137.0 in stage 0.0 (TID 137) in 39370 ms on localhost (138/170)
15/08/21 08:48:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 134217728 end: 257331238 length: 123113510 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:09 INFO Executor: Finished task 138.0 in stage 0.0 (TID 138). 2125 bytes result sent to driver
15/08/21 08:48:09 INFO TaskSetManager: Starting task 154.0 in stage 0.0 (TID 154, localhost, ANY, 1758 bytes)
15/08/21 08:48:09 INFO Executor: Running task 154.0 in stage 0.0 (TID 154)
15/08/21 08:48:09 INFO TaskSetManager: Finished task 138.0 in stage 0.0 (TID 138) in 39844 ms on localhost (139/170)
15/08/21 08:48:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:09 INFO Executor: Finished task 139.0 in stage 0.0 (TID 139). 2125 bytes result sent to driver
15/08/21 08:48:09 INFO TaskSetManager: Starting task 155.0 in stage 0.0 (TID 155, localhost, ANY, 1771 bytes)
15/08/21 08:48:09 INFO Executor: Running task 155.0 in stage 0.0 (TID 155)
15/08/21 08:48:09 INFO TaskSetManager: Finished task 139.0 in stage 0.0 (TID 139) in 39956 ms on localhost (140/170)
15/08/21 08:48:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 134217728 end: 257176539 length: 122958811 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:14 INFO Executor: Finished task 142.0 in stage 0.0 (TID 142). 2125 bytes result sent to driver
15/08/21 08:48:14 INFO TaskSetManager: Starting task 156.0 in stage 0.0 (TID 156, localhost, ANY, 1758 bytes)
15/08/21 08:48:14 INFO Executor: Running task 156.0 in stage 0.0 (TID 156)
15/08/21 08:48:14 INFO TaskSetManager: Finished task 142.0 in stage 0.0 (TID 142) in 43925 ms on localhost (141/170)
15/08/21 08:48:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:15 INFO Executor: Finished task 141.0 in stage 0.0 (TID 141). 2125 bytes result sent to driver
15/08/21 08:48:15 INFO TaskSetManager: Starting task 157.0 in stage 0.0 (TID 157, localhost, ANY, 1771 bytes)
15/08/21 08:48:15 INFO Executor: Running task 157.0 in stage 0.0 (TID 157)
15/08/21 08:48:15 INFO TaskSetManager: Finished task 141.0 in stage 0.0 (TID 141) in 45107 ms on localhost (142/170)
15/08/21 08:48:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 134217728 end: 257446174 length: 123228446 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:16 INFO Executor: Finished task 143.0 in stage 0.0 (TID 143). 2125 bytes result sent to driver
15/08/21 08:48:16 INFO TaskSetManager: Starting task 158.0 in stage 0.0 (TID 158, localhost, ANY, 1758 bytes)
15/08/21 08:48:16 INFO TaskSetManager: Finished task 143.0 in stage 0.0 (TID 143) in 44581 ms on localhost (143/170)
15/08/21 08:48:16 INFO Executor: Running task 158.0 in stage 0.0 (TID 158)
15/08/21 08:48:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:17 INFO Executor: Finished task 144.0 in stage 0.0 (TID 144). 2125 bytes result sent to driver
15/08/21 08:48:17 INFO TaskSetManager: Starting task 159.0 in stage 0.0 (TID 159, localhost, ANY, 1769 bytes)
15/08/21 08:48:17 INFO Executor: Running task 159.0 in stage 0.0 (TID 159)
15/08/21 08:48:17 INFO TaskSetManager: Finished task 144.0 in stage 0.0 (TID 144) in 27810 ms on localhost (144/170)
15/08/21 08:48:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 134217728 end: 259884384 length: 125666656 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:17 INFO Executor: Finished task 140.0 in stage 0.0 (TID 140). 2125 bytes result sent to driver
15/08/21 08:48:17 INFO TaskSetManager: Starting task 160.0 in stage 0.0 (TID 160, localhost, ANY, 1758 bytes)
15/08/21 08:48:17 INFO Executor: Running task 160.0 in stage 0.0 (TID 160)
15/08/21 08:48:17 INFO Executor: Finished task 145.0 in stage 0.0 (TID 145). 2125 bytes result sent to driver
15/08/21 08:48:17 INFO TaskSetManager: Starting task 161.0 in stage 0.0 (TID 161, localhost, ANY, 1771 bytes)
15/08/21 08:48:17 INFO TaskSetManager: Finished task 140.0 in stage 0.0 (TID 140) in 46946 ms on localhost (145/170)
15/08/21 08:48:17 INFO Executor: Running task 161.0 in stage 0.0 (TID 161)
15/08/21 08:48:17 INFO TaskSetManager: Finished task 145.0 in stage 0.0 (TID 145) in 27957 ms on localhost (146/170)
15/08/21 08:48:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 134217728 end: 259183553 length: 124965825 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
ache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573120 records.
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 158 ms. row count = 3501121
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 154 ms. row count = 3500100
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 382 ms. row count = 3500100
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 36862 ms: 94.95144 rec/ms, 189.90288 cell/ms
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (105 ms) and 99% processing (36862 ms)
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 72885
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501076 records from 2 columns in 35923 ms: 97.46057 rec/ms, 194.92114 cell/ms
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (153 ms) and 99% processing (35923 ms)
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501076. reading next block
21-Aug-2015 08:48:07 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 45 ms. row count = 73262
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:48:07 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 99 ms. row count = 3500100
21-Aug-2015 08:48:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573993 records.
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 18931 ms: 184.88722 rec/ms, 369.77444 cell/ms
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (108 ms) and 99% processing (18931 ms)
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 73746
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 137 ms. row count = 3500100
21-Aug-2015 08:48:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 154 ms. row count = 3500100
21-Aug-2015 08:48:08 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
21-Aug-2015 08:48:08 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 140 ms. row count = 3500100
21-Aug-2015 08:48:09 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:09 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3503161 records.
21-Aug-2015 08:48:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:09 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:09 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 156 ms. row count = 3503161
21-Aug-2015 08:48:09 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3571357 records.
21-Aug-2015 08:48:09 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:14 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 4956 ms. row count = 3501317
21-Aug-2015 08:48:14 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501689 records.
21-Aug-2015 08:48:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 118 ms. row count = 3501689
21-Aug-2015 08:48:15 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:15 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3572541 records.
21-Aug-2015 08:48:15 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:15 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 106 ms. row count = 3500100
21-Aug-2015 08:48:16 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:16 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501046 records.
21-Aug-2015 08:48:16 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:16 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 119 ms. row count = 3501046
21-Aug-2015 08:48:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3626733 records.
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14415/08/21 08:48:34 INFO Executor: Finished task 146.0 in stage 0.0 (TID 146). 2125 bytes result sent to driver
15/08/21 08:48:34 INFO TaskSetManager: Starting task 162.0 in stage 0.0 (TID 162, localhost, ANY, 1757 bytes)
15/08/21 08:48:34 INFO Executor: Running task 162.0 in stage 0.0 (TID 162)
15/08/21 08:48:34 INFO TaskSetManager: Finished task 146.0 in stage 0.0 (TID 146) in 34416 ms on localhost (147/170)
15/08/21 08:48:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:35 INFO Executor: Finished task 148.0 in stage 0.0 (TID 148). 2125 bytes result sent to driver
15/08/21 08:48:35 INFO TaskSetManager: Starting task 163.0 in stage 0.0 (TID 163, localhost, ANY, 1771 bytes)
15/08/21 08:48:35 INFO Executor: Running task 163.0 in stage 0.0 (TID 163)
15/08/21 08:48:35 INFO TaskSetManager: Finished task 148.0 in stage 0.0 (TID 148) in 28657 ms on localhost (148/170)
15/08/21 08:48:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 134217728 end: 257504450 length: 123286722 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:36 INFO Executor: Finished task 147.0 in stage 0.0 (TID 147). 2125 bytes result sent to driver
15/08/21 08:48:36 INFO TaskSetManager: Starting task 164.0 in stage 0.0 (TID 164, localhost, ANY, 1758 bytes)
15/08/21 08:48:36 INFO Executor: Running task 164.0 in stage 0.0 (TID 164)
15/08/21 08:48:36 INFO TaskSetManager: Finished task 147.0 in stage 0.0 (TID 147) in 29186 ms on localhost (149/170)
15/08/21 08:48:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:57 INFO Executor: Finished task 152.0 in stage 0.0 (TID 152). 2125 bytes result sent to driver
15/08/21 08:48:57 INFO TaskSetManager: Starting task 165.0 in stage 0.0 (TID 165, localhost, ANY, 1769 bytes)
15/08/21 08:48:57 INFO Executor: Running task 165.0 in stage 0.0 (TID 165)
15/08/21 08:48:58 INFO TaskSetManager: Finished task 152.0 in stage 0.0 (TID 152) in 49502 ms on localhost (150/170)
15/08/21 08:48:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 134217728 end: 257173847 length: 122956119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:58 INFO Executor: Finished task 149.0 in stage 0.0 (TID 149). 2125 bytes result sent to driver
15/08/21 08:48:58 INFO TaskSetManager: Starting task 166.0 in stage 0.0 (TID 166, localhost, ANY, 1758 bytes)
15/08/21 08:48:58 INFO Executor: Running task 166.0 in stage 0.0 (TID 166)
15/08/21 08:48:58 INFO TaskSetManager: Finished task 149.0 in stage 0.0 (TID 149) in 50635 ms on localhost (151/170)
15/08/21 08:48:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:58 INFO Executor: Finished task 151.0 in stage 0.0 (TID 151). 2125 bytes result sent to driver
15/08/21 08:48:58 INFO TaskSetManager: Starting task 167.0 in stage 0.0 (TID 167, localhost, ANY, 1770 bytes)
15/08/21 08:48:58 INFO Executor: Running task 167.0 in stage 0.0 (TID 167)
15/08/21 08:48:58 INFO TaskSetManager: Finished task 151.0 in stage 0.0 (TID 151) in 50083 ms on localhost (152/170)
15/08/21 08:48:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 134217728 end: 257494956 length: 123277228 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:58 INFO Executor: Finished task 154.0 in stage 0.0 (TID 154). 2125 bytes result sent to driver
15/08/21 08:48:58 INFO TaskSetManager: Starting task 168.0 in stage 0.0 (TID 168, localhost, ANY, 1757 bytes)
15/08/21 08:48:58 INFO Executor: Running task 168.0 in stage 0.0 (TID 168)
15/08/21 08:48:58 INFO TaskSetManager: Finished task 154.0 in stage 0.0 (TID 154) in 49131 ms on localhost (153/170)
15/08/21 08:48:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
 ms. row count = 3500100
21-Aug-2015 08:48:17 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501221 records.
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3627071 records.
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 136 ms. row count = 3503161
21-Aug-2015 08:48:17 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 206 ms. row count = 3501221
21-Aug-2015 08:48:28 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 20867 ms: 167.73375 rec/ms, 335.4675 cell/ms
21-Aug-2015 08:48:28 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 1% reading (382 ms) and 98% processing (20867 ms)
21-Aug-2015 08:48:28 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:28 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 22 ms. row count = 72281
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 25636 ms: 136.53065 rec/ms, 273.0613 cell/ms
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (137 ms) and 99% processing (25636 ms)
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14 ms. row count = 73893
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 26621 ms: 131.47891 rec/ms, 262.95782 cell/ms
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (154 ms) and 99% processing (26621 ms)
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 69 ms. row count = 73020
21-Aug-2015 08:48:34 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 134 ms. row count = 3500100
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 25839 ms: 135.45802 rec/ms, 270.91605 cell/ms
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (140 ms) and 99% processing (25839 ms)
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:34 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 39 ms. row count = 73871
21-Aug-2015 08:48:35 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:35 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573920 records.
21-Aug-2015 08:48:35 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 118 ms. row count = 3500779
21-Aug-2015 08:48:36 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:36 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3501110 records.
21-Aug-2015 08:48:36 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 102 ms. row count = 3501110
21-Aug-2015 08:48:36 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3501317 records from 2 columns in 21686 ms: 161.45518 rec/ms, 322.91037 cell/ms
21-Aug-2015 08:48:36 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 18% reading (4956 ms) and 81% processing (21686 ms)
21-Aug-2015 08:48:36 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3501317. reading next block
21-Aug-2015 08:48:36 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 14 ms. row count = 70040
21-Aug-2015 08:48:37 INFO: parquet.hadoop.InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 21093 ms: 165.93657 rec/ms, 331.87314 cell/ms
21-Aug-2015 08:48:37 INFO: parquet.hadoop.InternalParquetRecordReader: time spent so far 0% reading (106 ms) and 99% processing (21093 ms)
21-Aug-2015 08:48:37 INFO: parquet.hadoop.InternalParquetRecordReader: at row 3500100. reading next block
21-Aug-2015 08:48:37 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 72441
21-Aug-2015 08:48:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573142 records.
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500519 records.
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 206 ms. row count = 3502917
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 170 ms. row count = 3500519
21-Aug-2015 08:48:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3573577 records.
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 123 ms. row count = 3500100
21-Aug-2015 08:48:58 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
21-Aug-2015 08:48:58 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
21-Aug-2015 08:48:59 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 136 ms. row cou15/08/21 08:48:59 INFO Executor: Finished task 153.0 in stage 0.0 (TID 153). 2125 bytes result sent to driver
15/08/21 08:48:59 INFO Executor: Finished task 156.0 in stage 0.0 (TID 156). 2125 bytes result sent to driver
15/08/21 08:48:59 INFO TaskSetManager: Starting task 169.0 in stage 0.0 (TID 169, localhost, ANY, 1771 bytes)
15/08/21 08:48:59 INFO Executor: Running task 169.0 in stage 0.0 (TID 169)
15/08/21 08:48:59 INFO TaskSetManager: Finished task 153.0 in stage 0.0 (TID 153) in 50175 ms on localhost (154/170)
15/08/21 08:48:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 134217728 end: 257035718 length: 122817990 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:48:59 INFO TaskSetManager: Finished task 156.0 in stage 0.0 (TID 156) in 44102 ms on localhost (155/170)
15/08/21 08:48:59 INFO Executor: Finished task 150.0 in stage 0.0 (TID 150). 2125 bytes result sent to driver
15/08/21 08:48:59 INFO TaskSetManager: Finished task 150.0 in stage 0.0 (TID 150) in 51331 ms on localhost (156/170)
15/08/21 08:49:00 INFO Executor: Finished task 155.0 in stage 0.0 (TID 155). 2125 bytes result sent to driver
15/08/21 08:49:00 INFO TaskSetManager: Finished task 155.0 in stage 0.0 (TID 155) in 50132 ms on localhost (157/170)
15/08/21 08:49:00 INFO Executor: Finished task 158.0 in stage 0.0 (TID 158). 2125 bytes result sent to driver
15/08/21 08:49:00 INFO TaskSetManager: Finished task 158.0 in stage 0.0 (TID 158) in 43701 ms on localhost (158/170)
15/08/21 08:49:00 INFO Executor: Finished task 157.0 in stage 0.0 (TID 157). 2125 bytes result sent to driver
15/08/21 08:49:00 INFO TaskSetManager: Finished task 157.0 in stage 0.0 (TID 157) in 44286 ms on localhost (159/170)
15/08/21 08:49:02 INFO Executor: Finished task 160.0 in stage 0.0 (TID 160). 2125 bytes result sent to driver
15/08/21 08:49:02 INFO TaskSetManager: Finished task 160.0 in stage 0.0 (TID 160) in 45490 ms on localhost (160/170)
15/08/21 08:49:03 INFO Executor: Finished task 161.0 in stage 0.0 (TID 161). 2125 bytes result sent to driver
15/08/21 08:49:03 INFO TaskSetManager: Finished task 161.0 in stage 0.0 (TID 161) in 45933 ms on localhost (161/170)
15/08/21 08:49:03 INFO Executor: Finished task 159.0 in stage 0.0 (TID 159). 2125 bytes result sent to driver
15/08/21 08:49:03 INFO TaskSetManager: Finished task 159.0 in stage 0.0 (TID 159) in 46134 ms on localhost (162/170)
15/08/21 08:49:09 INFO Executor: Finished task 162.0 in stage 0.0 (TID 162). 2125 bytes result sent to driver
15/08/21 08:49:09 INFO TaskSetManager: Finished task 162.0 in stage 0.0 (TID 162) in 34900 ms on localhost (163/170)
15/08/21 08:49:10 INFO Executor: Finished task 164.0 in stage 0.0 (TID 164). 2125 bytes result sent to driver
15/08/21 08:49:10 INFO TaskSetManager: Finished task 164.0 in stage 0.0 (TID 164) in 33787 ms on localhost (164/170)
15/08/21 08:49:10 INFO Executor: Finished task 163.0 in stage 0.0 (TID 163). 2125 bytes result sent to driver
15/08/21 08:49:10 INFO TaskSetManager: Finished task 163.0 in stage 0.0 (TID 163) in 34425 ms on localhost (165/170)
15/08/21 08:49:11 INFO Executor: Finished task 166.0 in stage 0.0 (TID 166). 2125 bytes result sent to driver
15/08/21 08:49:11 INFO TaskSetManager: Finished task 166.0 in stage 0.0 (TID 166) in 13056 ms on localhost (166/170)
15/08/21 08:49:11 INFO Executor: Finished task 165.0 in stage 0.0 (TID 165). 2125 bytes result sent to driver
15/08/21 08:49:11 INFO TaskSetManager: Finished task 165.0 in stage 0.0 (TID 165) in 13201 ms on localhost (167/170)
15/08/21 08:49:11 INFO Executor: Finished task 168.0 in stage 0.0 (TID 168). 2125 bytes result sent to driver
15/08/21 08:49:11 INFO TaskSetManager: Finished task 168.0 in stage 0.0 (TID 168) in 12512 ms on localhost (168/170)
15/08/21 08:49:11 INFO Executor: Finished task 167.0 in stage 0.0 (TID 167). 2125 bytes result sent to driver
15/08/21 08:49:11 INFO TaskSetManager: Finished task 167.0 in stage 0.0 (TID 167) in 13165 ms on localhost (169/170)
15/08/21 08:49:11 INFO Executor: Finished task 169.0 in stage 0.0 (TID 169). 2125 bytes result sent to driver
15/08/21 08:49:11 INFO TaskSetManager: Finished task 169.0 in stage 0.0 (TID 169) in 12721 ms on localhost (170/170)
15/08/21 08:49:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/21 08:49:11 INFO DAGScheduler: ShuffleMapStage 0 (processCmd at CliDriver.java:423) finished in 413.936 s
15/08/21 08:49:11 INFO DAGScheduler: looking for newly runnable stages
15/08/21 08:49:11 INFO DAGScheduler: running: Set()
15/08/21 08:49:11 INFO DAGScheduler: waiting: Set(ResultStage 1)
15/08/21 08:49:11 INFO DAGScheduler: failed: Set()
15/08/21 08:49:11 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@643f217e
15/08/21 08:49:11 INFO DAGScheduler: Missing parents for ResultStage 1: List()
15/08/21 08:49:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 08:49:11 INFO StatsReportListener: task runtime:(count: 170, mean: 38414.158824, stdev: 9985.288527, max: 62228.000000, min: 12512.000000)
15/08/21 08:49:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:49:11 INFO StatsReportListener: 	12.5 s	24.9 s	25.6 s	28.6 s	40.7 s	44.9 s	50.1 s	55.3 s	1.0 min
15/08/21 08:49:11 INFO StatsReportListener: shuffle bytes written:(count: 170, mean: 9629242.888235, stdev: 468850.402633, max: 10377140.000000, min: 4098365.000000)
15/08/21 08:49:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:49:11 INFO StatsReportListener: 	3.9 MB	9.0 MB	9.0 MB	9.1 MB	9.2 MB	9.3 MB	9.5 MB	9.6 MB	9.9 MB
15/08/21 08:49:11 INFO StatsReportListener: task result size:(count: 170, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 08:49:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:49:11 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 08:49:11 INFO StatsReportListener: executor (non-fetch) time pct: (count: 170, mean: 99.446299, stdev: 3.236690, max: 99.931926, min: 58.352511)
15/08/21 08:49:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:49:11 INFO StatsReportListener: 	58 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 08:49:11 INFO StatsReportListener: other time pct: (count: 170, mean: 0.553701, stdev: 3.236690, max: 41.647489, min: 0.068074)
15/08/21 08:49:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:49:11 INFO MemoryStore: ensureFreeSpace(78888) called with curMem=363081, maxMem=22226833244
15/08/21 08:49:11 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	42 %
15/08/21 08:49:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 77.0 KB, free 20.7 GB)
15/08/21 08:49:11 INFO MemoryStore: ensureFreeSpace(30049) called with curMem=441969, maxMem=22226833244
15/08/21 08:49:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.3 KB, free 20.7 GB)
15/08/21 08:49:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:51693 (size: 29.3 KB, free: 20.7 GB)
15/08/21 08:49:11 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
15/08/21 08:49:11 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423)
15/08/21 08:49:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
15/08/21 08:49:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:49:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 170)
15/08/21 08:49:11 INFO Executor: Running task 5.0 in stage 1.0 (TID 175)
15/08/21 08:49:11 INFO Executor: Running task 8.0 in stage 1.0 (TID 178)
15/08/21 08:49:11 INFO Executor: Running task 13.0 in stage 1.0 (TID 183)
15/08/21 08:49:11 INFO Executor: Running task 6.0 in stage 1.0 (TID 176)
15/08/21 08:49:11 INFO Executor: Running task 2.0 in stage 1.0 (TID 172)
15/08/21 08:49:11 INFO Executor: Running task 3.0 in stage 1.0 (TID 173)
15/08/21 08:49:11 INFO Executor: Running task 1.0 in stage 1.0 (TID 171)
15/08/21 08:49:11 INFO Executor: Running task 4.0 in stage 1.0 (TID 174)
15/08/21 08:49:11 INFO Executor: Running task 15.0 in stage 1.0 (TID 185)
15/08/21 08:49:11 INFO Executor: Running task 11.0 in stage 1.0 (TID 181)
15/08/21 08:49:11 INFO Executor: Running task 12.0 in stage 1.0 (TID 182)
15/08/21 08:49:11 INFO Executor: Running task 14.0 in stage 1.0 (TID 184)
15/08/21 08:49:11 INFO Executor: Running task 9.0 in stage 1.0 (TID 179)
15/08/21 08:49:11 INFO Executor: Running task 7.0 in stage 1.0 (TID 177)
15/08/21 08:49:11 INFO Executor: Running task 10.0 in stage 1.0 (TID 180)
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 08:49:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 08:49:36 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:36 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/21 08:49:36 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:49:37 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:49:37 INFO CodecConfig: Compression: GZIP
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:49:37 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:49:37 INFO ParquetOutputFormat: Validation is off
15/08/21 08:49:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:49:37 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:00 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:00 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:51693 in memory (size: 4.4 KB, free: 20.7 GB)
15/08/21 08:50:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:00 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:00 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:00 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:00 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:00 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:00 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:00 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:00 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,664,829B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,710B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 833,548B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,266B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,671,226B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,107B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 833,273B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,991B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:50:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000014
15/08/21 08:50:02 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000014_0: Committed
15/08/21 08:50:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000009
15/08/21 08:50:02 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000009_0: Committed
15/08/21 08:50:02 INFO Executor: Finished task 9.0 in stage 1.0 (TID 179). 843 bytes result sent to driver
15/08/21 08:50:02 INFO Executor: Finished task 14.0 in stage 1.0 (TID 184). 843 bytes result sent to driver
15/08/21 08:50:02 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:02 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:02 INFO Executor: Running task 17.0 in stage 1.0 (TID 187)
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:50:02 INFO Executor: Running task 16.0 in stage 1.0 (TID 186)
15/08/21 08:50:02 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 179) in 50647 ms on localhost (1/200)
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,671,049B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,930B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 834,267B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,985B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:50:02 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 184) in 50662 ms on localhost (2/200)
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,664,834B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,715B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 833,045B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,763B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:50:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000005
15/08/21 08:50:02 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000005_0: Committed
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,664,920B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,801B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:50:02 INFO Executor: Finished task 5.0 in stage 1.0 (TID 175). 843 bytes result sent to driver
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 833,650B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,368B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:50:02 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:02 INFO Executor: Running task 18.0 in stage 1.0 (TID 188)
15/08/21 08:50:02 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 175) in 50744 ms on localhost (3/200)
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:50:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000003
15/08/21 08:50:02 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000003_0: Committed
15/08/21 08:50:02 INFO Executor: Finished task 3.0 in stage 1.0 (TID 173). 843 bytes result sent to driver
15/08/21 08:50:02 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:02 INFO Executor: Running task 19.0 in stage 1.0 (TID 189)
15/08/21 08:50:02 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 173) in 50816 ms on localhost (4/200)
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,664,882B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,763B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 833,967B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,685B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:50:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000002
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,670,799B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,680B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000002_0: Committed
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 832,054B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,772B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:50:02 INFO Executor: Finished task 2.0 in stage 1.0 (TID 172). 843 bytes result sent to driver
15/08/21 08:50:02 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:02 INFO Executor: Running task 20.0 in stage 1.0 (TID 190)
15/08/21 08:50:02 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 172) in 50862 ms on localhost (5/200)
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000001
15/08/21 08:50:02 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000001_0: Committed
15/08/21 08:50:02 INFO Executor: Finished task 1.0 in stage 1.0 (TID 171). 843 bytes result sent to driver
15/08/21 08:50:02 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:02 INFO Executor: Running task 21.0 in stage 1.0 (TID 191)
15/08/21 08:50:02 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 171) in 50929 ms on localhost (6/200)
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,665,833B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,714B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000006
15/08/21 08:50:02 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000006_0: Committed
15/08/21 08:50:02 INFO Executor: Finished task 6.0 in stage 1.0 (TID 176). 843 bytes result sent to driver
15/08/21 08:50:02 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:02 INFO Executor: Running task 22.0 in stage 1.0 (TID 192)
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:50:02 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 176) in 50963 ms on localhost (7/200)
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 832,984B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,702B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:50:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000012
15/08/21 08:50:02 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000012_0: Committed
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:02 INFO Executor: Finished task 12.0 in stage 1.0 (TID 182). 843 bytes result sent to driver
15/08/21 08:50:02 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:02 INFO Executor: Running task 23.0 in stage 1.0 (TID 193)
15/08/21 08:50:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:50:02 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 182) in 51034 ms on localhost (8/200)
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,665,860B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,741B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 833,117B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,835B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,671,085B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,966B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 833,328B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,046B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:50:02 INFO ColumnChunkPageWriteStore: written 2,670,778B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,659B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 834,080B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,798B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 2,665,058B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,939B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 832,232B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,950B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:50:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,681
15/08/21 08:50:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000004
15/08/21 08:50:03 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000004_0: Committed
15/08/21 08:50:03 INFO Executor: Finished task 4.0 in stage 1.0 (TID 174). 843 bytes result sent to driver
15/08/21 08:50:03 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:03 INFO Executor: Running task 24.0 in stage 1.0 (TID 194)
15/08/21 08:50:03 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 174) in 51150 ms on localhost (9/200)
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 2,671,003B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,884B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 833,807B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,525B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:50:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000015
15/08/21 08:50:03 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000015_0: Committed
15/08/21 08:50:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210849_0001_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210849_0001_m_000007
15/08/21 08:50:03 INFO SparkHadoopMapRedUtil: attempt_201508210849_0001_m_000007_0: Committed
15/08/21 08:50:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000011
15/08/21 08:50:03 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000011_0: Committed
15/08/21 08:50:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:50:03 INFO Executor: Finished task 7.0 in stage 1.0 (TID 177). 843 bytes result sent to driver
15/08/21 08:50:03 INFO Executor: Finished task 15.0 in stage 1.0 (TID 185). 843 bytes result sent to driver
15/08/21 08:50:03 INFO Executor: Finished task 11.0 in stage 1.0 (TID 181). 843 bytes result sent to driver
15/08/21 08:50:03 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:03 INFO Executor: Running task 25.0 in stage 1.0 (TID 195)
15/08/21 08:50:03 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:03 INFO Executor: Running task 26.0 in stage 1.0 (TID 196)
15/08/21 08:50:03 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:03 INFO Executor: Running task 27.0 in stage 1.0 (TID 197)
15/08/21 08:50:03 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 177) in 51219 ms on localhost (10/200)
15/08/21 08:50:03 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 185) in 51215 ms on localhost (11/200)
15/08/21 08:50:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000013
15/08/21 08:50:03 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000013_0: Committed
15/08/21 08:50:03 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 181) in 51220 ms on localhost (12/200)
15/08/21 08:50:03 INFO Executor: Finished task 13.0 in stage 1.0 (TID 183). 843 bytes result sent to driver
15/08/21 08:50:03 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:03 INFO Executor: Running task 28.0 in stage 1.0 (TID 198)
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 2,665,013B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,894B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:03 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 183) in 51231 ms on localhost (13/200)
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 833,233B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,951B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 320 entries, 2,560B raw, 320B comp}
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000010
15/08/21 08:50:03 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000010_0: Committed
15/08/21 08:50:03 INFO Executor: Finished task 10.0 in stage 1.0 (TID 180). 843 bytes result sent to driver
15/08/21 08:50:03 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:03 INFO Executor: Running task 29.0 in stage 1.0 (TID 199)
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 2,670,872B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,753B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:03 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 180) in 51292 ms on localhost (14/200)
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 834,621B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,339B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000008
15/08/21 08:50:03 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000008_0: Committed
15/08/21 08:50:03 INFO Executor: Finished task 8.0 in stage 1.0 (TID 178). 843 bytes result sent to driver
15/08/21 08:50:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:50:03 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:03 INFO Executor: Running task 30.0 in stage 1.0 (TID 200)
15/08/21 08:50:03 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 178) in 51404 ms on localhost (15/200)
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 2,671,125B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,006B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:03 INFO ColumnChunkPageWriteStore: written 836,802B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 836,520B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000000
15/08/21 08:50:03 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000000_0: Committed
15/08/21 08:50:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 170). 843 bytes result sent to driver
15/08/21 08:50:03 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:03 INFO Executor: Running task 31.0 in stage 1.0 (TID 201)
15/08/21 08:50:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 170) in 51624 ms on localhost (16/200)
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:24 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:24 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:50:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:50:27 INFO ColumnChunkPageWriteStore: written 2,664,955B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,836B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:50:27 INFO ColumnChunkPageWriteStore: written 834,916B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,634B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:50:27 INFO ColumnChunkPageWriteStore: written 2,671,180B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,061B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:27 INFO ColumnChunkPageWriteStore: written 834,246B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,964B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:50:27 INFO ColumnChunkPageWriteStore: written 2,671,426B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,307B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:27 INFO ColumnChunkPageWriteStore: written 834,577B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,295B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:50:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000027
15/08/21 08:50:27 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000027_0: Committed
15/08/21 08:50:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000022
15/08/21 08:50:27 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000022_0: Committed
15/08/21 08:50:27 INFO Executor: Finished task 22.0 in stage 1.0 (TID 192). 843 bytes result sent to driver
15/08/21 08:50:27 INFO Executor: Finished task 27.0 in stage 1.0 (TID 197). 843 bytes result sent to driver
15/08/21 08:50:27 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:27 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:27 INFO Executor: Running task 32.0 in stage 1.0 (TID 202)
15/08/21 08:50:27 INFO Executor: Running task 33.0 in stage 1.0 (TID 203)
15/08/21 08:50:27 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 197) in 24330 ms on localhost (17/200)
15/08/21 08:50:27 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 192) in 24598 ms on localhost (18/200)
15/08/21 08:50:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000016
15/08/21 08:50:27 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000016_0: Committed
15/08/21 08:50:27 INFO Executor: Finished task 16.0 in stage 1.0 (TID 186). 843 bytes result sent to driver
15/08/21 08:50:27 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:27 INFO Executor: Running task 34.0 in stage 1.0 (TID 204)
15/08/21 08:50:27 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 186) in 24977 ms on localhost (19/200)
15/08/21 08:50:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:50:27 INFO ColumnChunkPageWriteStore: written 2,666,072B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,953B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:27 INFO ColumnChunkPageWriteStore: written 833,917B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,635B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:50:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000028
15/08/21 08:50:27 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000028_0: Committed
15/08/21 08:50:27 INFO Executor: Finished task 28.0 in stage 1.0 (TID 198). 843 bytes result sent to driver
15/08/21 08:50:27 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:50:27 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 198) in 24763 ms on localhost (20/200)
15/08/21 08:50:27 INFO Executor: Running task 35.0 in stage 1.0 (TID 205)
15/08/21 08:50:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:50:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:50:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 2,671,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,926B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 835,320B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,038B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 2,665,155B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,036B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 833,825B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,543B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 2,665,197B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,078B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 2,664,899B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,780B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 835,387B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,105B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 834,265B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,983B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:50:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000017
15/08/21 08:50:28 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000017_0: Committed
15/08/21 08:50:28 INFO Executor: Finished task 17.0 in stage 1.0 (TID 187). 843 bytes result sent to driver
15/08/21 08:50:28 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:28 INFO Executor: Running task 36.0 in stage 1.0 (TID 206)
15/08/21 08:50:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000031
15/08/21 08:50:28 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000031_0: Committed
15/08/21 08:50:28 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 187) in 25724 ms on localhost (21/200)
15/08/21 08:50:28 INFO Executor: Finished task 31.0 in stage 1.0 (TID 201). 843 bytes result sent to driver
15/08/21 08:50:28 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:28 INFO Executor: Running task 37.0 in stage 1.0 (TID 207)
15/08/21 08:50:28 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 201) in 24806 ms on localhost (22/200)
15/08/21 08:50:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000019
15/08/21 08:50:28 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000019_0: Committed
15/08/21 08:50:28 INFO Executor: Finished task 19.0 in stage 1.0 (TID 189). 843 bytes result sent to driver
15/08/21 08:50:28 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 208, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:28 INFO Executor: Running task 38.0 in stage 1.0 (TID 208)
15/08/21 08:50:28 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 189) in 25666 ms on localhost (23/200)
15/08/21 08:50:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000025
15/08/21 08:50:28 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000025_0: Committed
15/08/21 08:50:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:50:28 INFO Executor: Finished task 25.0 in stage 1.0 (TID 195). 843 bytes result sent to driver
15/08/21 08:50:28 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 209, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:28 INFO Executor: Running task 39.0 in stage 1.0 (TID 209)
15/08/21 08:50:28 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 195) in 25273 ms on localhost (24/200)
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 2,670,946B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,827B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 833,075B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,793B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:50:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 2,671,237B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,118B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 833,850B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,568B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:50:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000029
15/08/21 08:50:28 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000029_0: Committed
15/08/21 08:50:28 INFO Executor: Finished task 29.0 in stage 1.0 (TID 199). 843 bytes result sent to driver
15/08/21 08:50:28 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 210, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:28 INFO Executor: Running task 40.0 in stage 1.0 (TID 210)
15/08/21 08:50:28 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 199) in 25415 ms on localhost (25/200)
15/08/21 08:50:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 08:50:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000021
15/08/21 08:50:28 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000021_0: Committed
15/08/21 08:50:28 INFO Executor: Finished task 21.0 in stage 1.0 (TID 191). 843 bytes result sent to driver
15/08/21 08:50:28 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 211, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:28 INFO Executor: Running task 41.0 in stage 1.0 (TID 211)
15/08/21 08:50:28 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 191) in 25878 ms on localhost (26/200)
15/08/21 08:50:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 2,671,512B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,393B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 834,404B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,122B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 2,670,944B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,825B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:28 INFO ColumnChunkPageWriteStore: written 834,137B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,855B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:50:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000023
15/08/21 08:50:28 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000023_0: Committed
15/08/21 08:50:28 INFO Executor: Finished task 23.0 in stage 1.0 (TID 193). 843 bytes result sent to driver
15/08/21 08:50:28 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 212, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:28 INFO Executor: Running task 42.0 in stage 1.0 (TID 212)
15/08/21 08:50:28 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 193) in 25934 ms on localhost (27/200)
15/08/21 08:50:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000030
15/08/21 08:50:28 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000030_0: Committed
15/08/21 08:50:28 INFO Executor: Finished task 30.0 in stage 1.0 (TID 200). 843 bytes result sent to driver
15/08/21 08:50:28 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 213, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:28 INFO Executor: Running task 43.0 in stage 1.0 (TID 213)
15/08/21 08:50:28 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 200) in 25651 ms on localhost (28/200)
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:33 INFO ColumnChunkPageWriteStore: written 2,666,255B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,136B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:33 INFO ColumnChunkPageWriteStore: written 834,498B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,216B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:50:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:50:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000020
15/08/21 08:50:33 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000020_0: Committed
15/08/21 08:50:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:50:33 INFO Executor: Finished task 20.0 in stage 1.0 (TID 190). 843 bytes result sent to driver
15/08/21 08:50:33 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 214, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:33 INFO Executor: Running task 44.0 in stage 1.0 (TID 214)
15/08/21 08:50:33 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 190) in 30439 ms on localhost (29/200)
15/08/21 08:50:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:33 INFO ColumnChunkPageWriteStore: written 2,664,826B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,707B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:33 INFO ColumnChunkPageWriteStore: written 832,355B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,073B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:50:33 INFO ColumnChunkPageWriteStore: written 2,665,097B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,978B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:33 INFO ColumnChunkPageWriteStore: written 833,850B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,568B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:50:33 INFO ColumnChunkPageWriteStore: written 2,671,314B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,195B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:50:33 INFO ColumnChunkPageWriteStore: written 830,496B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,214B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:50:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000026
15/08/21 08:50:33 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000026_0: Committed
15/08/21 08:50:33 INFO Executor: Finished task 26.0 in stage 1.0 (TID 196). 843 bytes result sent to driver
15/08/21 08:50:33 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 215, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:33 INFO Executor: Running task 45.0 in stage 1.0 (TID 215)
15/08/21 08:50:33 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 196) in 30275 ms on localhost (30/200)
15/08/21 08:50:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000024
15/08/21 08:50:33 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000024_0: Committed
15/08/21 08:50:33 INFO Executor: Finished task 24.0 in stage 1.0 (TID 194). 843 bytes result sent to driver
15/08/21 08:50:33 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 216, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:33 INFO Executor: Running task 46.0 in stage 1.0 (TID 216)
15/08/21 08:50:33 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 194) in 30360 ms on localhost (31/200)
15/08/21 08:50:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000018
15/08/21 08:50:33 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000018_0: Committed
15/08/21 08:50:33 INFO Executor: Finished task 18.0 in stage 1.0 (TID 188). 843 bytes result sent to driver
15/08/21 08:50:33 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 217, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:50:33 INFO Executor: Running task 47.0 in stage 1.0 (TID 217)
15/08/21 08:50:33 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 188) in 30804 ms on localhost (32/200)
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:50:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:50:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:57 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:57 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:50:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:50:57 INFO CodecConfig: Compression: GZIP
15/08/21 08:50:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:50:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:50:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:50:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:50:57 INFO ParquetOutputFormat: Validation is off
15/08/21 08:50:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:50:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:00 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:00 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:00 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:00 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:00 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:00 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:00 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:00 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:00 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:02 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:02 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:51:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:51:06 INFO ColumnChunkPageWriteStore: written 2,664,648B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,529B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:06 INFO ColumnChunkPageWriteStore: written 832,636B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,354B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:51:06 INFO ColumnChunkPageWriteStore: written 2,664,808B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,689B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:06 INFO ColumnChunkPageWriteStore: written 832,830B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,548B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:51:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000034
15/08/21 08:51:06 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000034_0: Committed
15/08/21 08:51:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508210850_0001_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210850_0001_m_000033
15/08/21 08:51:06 INFO SparkHadoopMapRedUtil: attempt_201508210850_0001_m_000033_0: Committed
15/08/21 08:51:06 INFO Executor: Finished task 34.0 in stage 1.0 (TID 204). 843 bytes result sent to driver
15/08/21 08:51:06 INFO Executor: Finished task 33.0 in stage 1.0 (TID 203). 843 bytes result sent to driver
15/08/21 08:51:06 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 218, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:06 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 219, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:06 INFO Executor: Running task 48.0 in stage 1.0 (TID 218)
15/08/21 08:51:06 INFO Executor: Running task 49.0 in stage 1.0 (TID 219)
15/08/21 08:51:06 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 204) in 38828 ms on localhost (33/200)
15/08/21 08:51:06 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 203) in 38902 ms on localhost (34/200)
15/08/21 08:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:06 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:06 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:06 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:06 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:06 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:06 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:51:06 INFO ColumnChunkPageWriteStore: written 2,671,004B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,885B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:06 INFO ColumnChunkPageWriteStore: written 834,687B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,405B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000032
15/08/21 08:51:07 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000032_0: Committed
15/08/21 08:51:07 INFO Executor: Finished task 32.0 in stage 1.0 (TID 202). 843 bytes result sent to driver
15/08/21 08:51:07 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 220, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:07 INFO Executor: Running task 50.0 in stage 1.0 (TID 220)
15/08/21 08:51:07 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 202) in 39651 ms on localhost (35/200)
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 2,670,777B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,658B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 833,951B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,669B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000037
15/08/21 08:51:07 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000037_0: Committed
15/08/21 08:51:07 INFO Executor: Finished task 37.0 in stage 1.0 (TID 207). 843 bytes result sent to driver
15/08/21 08:51:07 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 221, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:07 INFO Executor: Running task 51.0 in stage 1.0 (TID 221)
15/08/21 08:51:07 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 207) in 39120 ms on localhost (36/200)
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 2,664,725B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,606B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 833,737B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,455B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 2,665,178B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,059B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 834,058B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,776B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000035
15/08/21 08:51:07 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000035_0: Committed
15/08/21 08:51:07 INFO Executor: Finished task 35.0 in stage 1.0 (TID 205). 843 bytes result sent to driver
15/08/21 08:51:07 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 222, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:07 INFO Executor: Running task 52.0 in stage 1.0 (TID 222)
15/08/21 08:51:07 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 205) in 39630 ms on localhost (37/200)
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 2,671,186B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,067B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000041
15/08/21 08:51:07 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000041_0: Committed
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 833,435B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,153B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:51:07 INFO Executor: Finished task 41.0 in stage 1.0 (TID 211). 843 bytes result sent to driver
15/08/21 08:51:07 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 223, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:07 INFO Executor: Running task 53.0 in stage 1.0 (TID 223)
15/08/21 08:51:07 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 211) in 38901 ms on localhost (38/200)
15/08/21 08:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000040
15/08/21 08:51:07 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000040_0: Committed
15/08/21 08:51:07 INFO Executor: Finished task 40.0 in stage 1.0 (TID 210). 843 bytes result sent to driver
15/08/21 08:51:07 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 224, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:07 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 210) in 39041 ms on localhost (39/200)
15/08/21 08:51:07 INFO Executor: Running task 54.0 in stage 1.0 (TID 224)
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 2,670,885B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,766B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 834,145B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,863B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:51:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 2,670,816B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,697B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:07 INFO ColumnChunkPageWriteStore: written 834,052B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,770B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000038
15/08/21 08:51:07 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000038_0: Committed
15/08/21 08:51:07 INFO Executor: Finished task 38.0 in stage 1.0 (TID 208). 843 bytes result sent to driver
15/08/21 08:51:07 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 225, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:07 INFO Executor: Running task 55.0 in stage 1.0 (TID 225)
15/08/21 08:51:07 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 208) in 39451 ms on localhost (40/200)
15/08/21 08:51:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000039
15/08/21 08:51:07 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000039_0: Committed
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:07 INFO Executor: Finished task 39.0 in stage 1.0 (TID 209). 843 bytes result sent to driver
15/08/21 08:51:07 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 226, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:07 INFO Executor: Running task 56.0 in stage 1.0 (TID 226)
15/08/21 08:51:07 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 209) in 39488 ms on localhost (41/200)
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 2,665,950B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,831B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 833,179B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,897B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 2,665,042B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,923B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 832,856B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,574B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 2,671,149B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,030B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 833,990B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,708B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000042
15/08/21 08:51:08 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000042_0: Committed
15/08/21 08:51:08 INFO Executor: Finished task 42.0 in stage 1.0 (TID 212). 843 bytes result sent to driver
15/08/21 08:51:08 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 227, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:08 INFO Executor: Running task 57.0 in stage 1.0 (TID 227)
15/08/21 08:51:08 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 212) in 39381 ms on localhost (42/200)
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 2,671,206B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,087B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 835,007B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,725B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000045
15/08/21 08:51:08 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000045_0: Committed
15/08/21 08:51:08 INFO Executor: Finished task 45.0 in stage 1.0 (TID 215). 843 bytes result sent to driver
15/08/21 08:51:08 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 228, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:08 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 215) in 34901 ms on localhost (43/200)
15/08/21 08:51:08 INFO Executor: Running task 58.0 in stage 1.0 (TID 228)
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000046
15/08/21 08:51:08 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000046_0: Committed
15/08/21 08:51:08 INFO Executor: Finished task 46.0 in stage 1.0 (TID 216). 843 bytes result sent to driver
15/08/21 08:51:08 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 229, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:08 INFO Executor: Running task 59.0 in stage 1.0 (TID 229)
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:08 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 216) in 34955 ms on localhost (44/200)
15/08/21 08:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:51:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 2,665,054B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,935B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 836,409B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 836,127B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 2,671,276B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,157B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 832,176B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,894B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000043
15/08/21 08:51:08 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000043_0: Committed
15/08/21 08:51:08 INFO Executor: Finished task 43.0 in stage 1.0 (TID 213). 843 bytes result sent to driver
15/08/21 08:51:08 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 230, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:08 INFO Executor: Running task 60.0 in stage 1.0 (TID 230)
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 2,665,941B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,822B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:08 INFO ColumnChunkPageWriteStore: written 833,292B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,010B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:51:08 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 213) in 39577 ms on localhost (45/200)
15/08/21 08:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000047
15/08/21 08:51:08 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000047_0: Committed
15/08/21 08:51:08 INFO Executor: Finished task 47.0 in stage 1.0 (TID 217). 843 bytes result sent to driver
15/08/21 08:51:08 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 231, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:08 INFO Executor: Running task 61.0 in stage 1.0 (TID 231)
15/08/21 08:51:08 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 217) in 35149 ms on localhost (46/200)
15/08/21 08:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000036
15/08/21 08:51:08 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000036_0: Committed
15/08/21 08:51:08 INFO Executor: Finished task 36.0 in stage 1.0 (TID 206). 843 bytes result sent to driver
15/08/21 08:51:08 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 232, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:08 INFO Executor: Running task 62.0 in stage 1.0 (TID 232)
15/08/21 08:51:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000044
15/08/21 08:51:08 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000044_0: Committed
15/08/21 08:51:08 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 206) in 40343 ms on localhost (47/200)
15/08/21 08:51:08 INFO Executor: Finished task 44.0 in stage 1.0 (TID 214). 843 bytes result sent to driver
15/08/21 08:51:08 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 233, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:08 INFO Executor: Running task 63.0 in stage 1.0 (TID 233)
15/08/21 08:51:08 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 214) in 35430 ms on localhost (48/200)
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:24 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:24 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:31 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:31 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:31 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:31 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:51:32 INFO ColumnChunkPageWriteStore: written 2,664,912B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,793B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:32 INFO ColumnChunkPageWriteStore: written 835,880B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,598B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:51:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000049
15/08/21 08:51:32 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000049_0: Committed
15/08/21 08:51:32 INFO Executor: Finished task 49.0 in stage 1.0 (TID 219). 843 bytes result sent to driver
15/08/21 08:51:32 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 234, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:32 INFO Executor: Running task 64.0 in stage 1.0 (TID 234)
15/08/21 08:51:32 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 219) in 26246 ms on localhost (49/200)
15/08/21 08:51:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:51:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:51:33 INFO ColumnChunkPageWriteStore: written 2,665,307B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,188B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:33 INFO ColumnChunkPageWriteStore: written 834,754B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,472B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:51:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000050
15/08/21 08:51:33 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000050_0: Committed
15/08/21 08:51:33 INFO Executor: Finished task 50.0 in stage 1.0 (TID 220). 843 bytes result sent to driver
15/08/21 08:51:33 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 235, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:33 INFO Executor: Running task 65.0 in stage 1.0 (TID 235)
15/08/21 08:51:33 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 220) in 26799 ms on localhost (50/200)
15/08/21 08:51:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:34 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:34 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:51:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:51:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:51:34 INFO ColumnChunkPageWriteStore: written 2,665,201B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,082B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:34 INFO ColumnChunkPageWriteStore: written 833,868B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,586B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:51:34 INFO ColumnChunkPageWriteStore: written 2,671,229B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,110B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:34 INFO ColumnChunkPageWriteStore: written 834,384B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,102B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:51:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000051
15/08/21 08:51:34 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000051_0: Committed
15/08/21 08:51:34 INFO Executor: Finished task 51.0 in stage 1.0 (TID 221). 843 bytes result sent to driver
15/08/21 08:51:34 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 236, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:34 INFO Executor: Running task 66.0 in stage 1.0 (TID 236)
15/08/21 08:51:34 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 221) in 27139 ms on localhost (51/200)
15/08/21 08:51:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:51:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000048
15/08/21 08:51:34 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000048_0: Committed
15/08/21 08:51:34 INFO Executor: Finished task 48.0 in stage 1.0 (TID 218). 843 bytes result sent to driver
15/08/21 08:51:34 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 237, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:34 INFO Executor: Running task 67.0 in stage 1.0 (TID 237)
15/08/21 08:51:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:34 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 218) in 28246 ms on localhost (52/200)
15/08/21 08:51:34 INFO ColumnChunkPageWriteStore: written 2,671,403B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,284B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:34 INFO ColumnChunkPageWriteStore: written 833,349B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,067B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:51:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000055
15/08/21 08:51:34 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000055_0: Committed
15/08/21 08:51:34 INFO Executor: Finished task 55.0 in stage 1.0 (TID 225). 843 bytes result sent to driver
15/08/21 08:51:34 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 238, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:34 INFO Executor: Running task 68.0 in stage 1.0 (TID 238)
15/08/21 08:51:34 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 225) in 27084 ms on localhost (53/200)
15/08/21 08:51:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:51:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:35 INFO ColumnChunkPageWriteStore: written 2,664,734B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,615B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:35 INFO ColumnChunkPageWriteStore: written 835,655B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,373B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:51:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000057
15/08/21 08:51:35 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000057_0: Committed
15/08/21 08:51:35 INFO Executor: Finished task 57.0 in stage 1.0 (TID 227). 843 bytes result sent to driver
15/08/21 08:51:35 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 239, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:35 INFO Executor: Running task 69.0 in stage 1.0 (TID 239)
15/08/21 08:51:35 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 227) in 26939 ms on localhost (54/200)
15/08/21 08:51:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 08:51:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:51:35 INFO ColumnChunkPageWriteStore: written 2,671,319B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,200B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:35 INFO ColumnChunkPageWriteStore: written 831,020B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,738B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:51:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000056
15/08/21 08:51:40 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000056_0: Committed
15/08/21 08:51:40 INFO Executor: Finished task 56.0 in stage 1.0 (TID 226). 843 bytes result sent to driver
15/08/21 08:51:40 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 240, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:40 INFO Executor: Running task 70.0 in stage 1.0 (TID 240)
15/08/21 08:51:40 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 226) in 33123 ms on localhost (55/200)
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:51:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 2,664,718B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,599B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 831,971B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,689B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 2,666,261B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,142B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 834,132B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,850B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:51:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:51:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:51:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000058
15/08/21 08:51:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000052
15/08/21 08:51:41 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000058_0: Committed
15/08/21 08:51:41 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000052_0: Committed
15/08/21 08:51:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:51:41 INFO Executor: Finished task 58.0 in stage 1.0 (TID 228). 843 bytes result sent to driver
15/08/21 08:51:41 INFO Executor: Finished task 52.0 in stage 1.0 (TID 222). 843 bytes result sent to driver
15/08/21 08:51:41 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 241, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:41 INFO Executor: Running task 71.0 in stage 1.0 (TID 241)
15/08/21 08:51:41 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 242, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:41 INFO Executor: Running task 72.0 in stage 1.0 (TID 242)
15/08/21 08:51:41 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 222) in 33980 ms on localhost (56/200)
15/08/21 08:51:41 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 228) in 33215 ms on localhost (57/200)
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 2,664,687B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,568B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 833,617B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,335B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:51:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 2,671,541B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,422B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 833,666B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,384B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 2,671,231B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,112B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 836,159B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,877B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:51:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000059
15/08/21 08:51:41 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000059_0: Committed
15/08/21 08:51:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000053
15/08/21 08:51:41 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000053_0: Committed
15/08/21 08:51:41 INFO Executor: Finished task 59.0 in stage 1.0 (TID 229). 843 bytes result sent to driver
15/08/21 08:51:41 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 243, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:41 INFO Executor: Finished task 53.0 in stage 1.0 (TID 223). 843 bytes result sent to driver
15/08/21 08:51:41 INFO Executor: Running task 73.0 in stage 1.0 (TID 243)
15/08/21 08:51:41 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 244, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:41 INFO Executor: Running task 74.0 in stage 1.0 (TID 244)
15/08/21 08:51:41 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 229) in 33287 ms on localhost (58/200)
15/08/21 08:51:41 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 223) in 34048 ms on localhost (59/200)
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 2,666,043B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,924B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:41 INFO ColumnChunkPageWriteStore: written 836,248B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,966B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:51:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000054
15/08/21 08:51:41 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000054_0: Committed
15/08/21 08:51:41 INFO Executor: Finished task 54.0 in stage 1.0 (TID 224). 843 bytes result sent to driver
15/08/21 08:51:41 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 245, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:41 INFO Executor: Running task 75.0 in stage 1.0 (TID 245)
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:51:41 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 224) in 34088 ms on localhost (60/200)
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000060
15/08/21 08:51:41 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000060_0: Committed
15/08/21 08:51:41 INFO Executor: Finished task 60.0 in stage 1.0 (TID 230). 843 bytes result sent to driver
15/08/21 08:51:41 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 246, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:41 INFO Executor: Running task 76.0 in stage 1.0 (TID 246)
15/08/21 08:51:41 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 230) in 33302 ms on localhost (61/200)
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 08:51:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:51:42 INFO ColumnChunkPageWriteStore: written 2,670,957B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,838B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:42 INFO ColumnChunkPageWriteStore: written 831,272B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,990B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:51:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:51:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000063
15/08/21 08:51:42 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000063_0: Committed
15/08/21 08:51:42 INFO ColumnChunkPageWriteStore: written 2,670,898B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,779B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:42 INFO Executor: Finished task 63.0 in stage 1.0 (TID 233). 843 bytes result sent to driver
15/08/21 08:51:42 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 247, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:42 INFO ColumnChunkPageWriteStore: written 834,402B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,120B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:51:42 INFO Executor: Running task 77.0 in stage 1.0 (TID 247)
15/08/21 08:51:42 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 233) in 33552 ms on localhost (62/200)
15/08/21 08:51:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000061
15/08/21 08:51:42 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000061_0: Committed
15/08/21 08:51:42 INFO Executor: Finished task 61.0 in stage 1.0 (TID 231). 843 bytes result sent to driver
15/08/21 08:51:42 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 248, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:42 INFO Executor: Running task 78.0 in stage 1.0 (TID 248)
15/08/21 08:51:42 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 231) in 33687 ms on localhost (63/200)
15/08/21 08:51:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:51:42 INFO ColumnChunkPageWriteStore: written 2,670,932B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,813B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:51:42 INFO ColumnChunkPageWriteStore: written 830,638B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,356B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:51:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000062
15/08/21 08:51:43 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000062_0: Committed
15/08/21 08:51:43 INFO Executor: Finished task 62.0 in stage 1.0 (TID 232). 843 bytes result sent to driver
15/08/21 08:51:43 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 249, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:51:43 INFO Executor: Running task 79.0 in stage 1.0 (TID 249)
15/08/21 08:51:43 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 232) in 34493 ms on localhost (64/200)
15/08/21 08:51:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:51:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:51:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:51:52 INFO CodecConfig: Compression: GZIP
15/08/21 08:51:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:51:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:51:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:51:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:51:52 INFO ParquetOutputFormat: Validation is off
15/08/21 08:51:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:51:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:52:09 INFO ColumnChunkPageWriteStore: written 2,671,438B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,319B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:09 INFO ColumnChunkPageWriteStore: written 831,981B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,699B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:52:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508210851_0001_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210851_0001_m_000064
15/08/21 08:52:09 INFO SparkHadoopMapRedUtil: attempt_201508210851_0001_m_000064_0: Committed
15/08/21 08:52:09 INFO Executor: Finished task 64.0 in stage 1.0 (TID 234). 843 bytes result sent to driver
15/08/21 08:52:09 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 250, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:09 INFO Executor: Running task 80.0 in stage 1.0 (TID 250)
15/08/21 08:52:09 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 234) in 37416 ms on localhost (65/200)
15/08/21 08:52:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:10 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:10 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:10 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:10 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:14 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:14 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:14 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:14 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:14 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:14 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:14 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:14 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:15 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:15 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:15 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:15 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:52:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:15 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:15 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:15 INFO ColumnChunkPageWriteStore: written 2,664,716B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,597B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:15 INFO ColumnChunkPageWriteStore: written 833,304B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,022B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:52:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000066
15/08/21 08:52:15 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000066_0: Committed
15/08/21 08:52:15 INFO Executor: Finished task 66.0 in stage 1.0 (TID 236). 843 bytes result sent to driver
15/08/21 08:52:15 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 251, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:15 INFO Executor: Running task 81.0 in stage 1.0 (TID 251)
15/08/21 08:52:15 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 236) in 40988 ms on localhost (66/200)
15/08/21 08:52:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:15 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:15 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:52:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:15 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:15 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:52:15 INFO ColumnChunkPageWriteStore: written 2,664,507B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,388B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:15 INFO ColumnChunkPageWriteStore: written 834,830B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,548B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:52:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:15 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:15 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000065
15/08/21 08:52:15 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000065_0: Committed
15/08/21 08:52:15 INFO Executor: Finished task 65.0 in stage 1.0 (TID 235). 843 bytes result sent to driver
15/08/21 08:52:15 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 252, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:15 INFO Executor: Running task 82.0 in stage 1.0 (TID 252)
15/08/21 08:52:15 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 235) in 42084 ms on localhost (67/200)
15/08/21 08:52:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:16 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:16 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 08:52:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:16 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:16 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:16 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:16 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:16 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:16 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:16 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:16 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:17 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:17 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:52:17 INFO ColumnChunkPageWriteStore: written 2,665,867B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,748B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:17 INFO ColumnChunkPageWriteStore: written 833,718B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,436B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:52:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000068
15/08/21 08:52:17 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000068_0: Committed
15/08/21 08:52:17 INFO Executor: Finished task 68.0 in stage 1.0 (TID 238). 843 bytes result sent to driver
15/08/21 08:52:17 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 253, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:17 INFO Executor: Running task 83.0 in stage 1.0 (TID 253)
15/08/21 08:52:17 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 238) in 42444 ms on localhost (68/200)
15/08/21 08:52:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:52:22 INFO ColumnChunkPageWriteStore: written 2,664,870B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,751B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:22 INFO ColumnChunkPageWriteStore: written 834,960B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,678B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:52:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000067
15/08/21 08:52:22 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000067_0: Committed
15/08/21 08:52:22 INFO Executor: Finished task 67.0 in stage 1.0 (TID 237). 843 bytes result sent to driver
15/08/21 08:52:22 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 254, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:22 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 237) in 48038 ms on localhost (69/200)
15/08/21 08:52:22 INFO Executor: Running task 84.0 in stage 1.0 (TID 254)
15/08/21 08:52:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:52:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:52:22 INFO ColumnChunkPageWriteStore: written 2,670,691B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,572B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:22 INFO ColumnChunkPageWriteStore: written 835,083B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,801B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:52:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:52:22 INFO ColumnChunkPageWriteStore: written 2,670,847B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,728B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:22 INFO ColumnChunkPageWriteStore: written 831,987B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,705B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:52:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000069
15/08/21 08:52:22 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000069_0: Committed
15/08/21 08:52:22 INFO Executor: Finished task 69.0 in stage 1.0 (TID 239). 843 bytes result sent to driver
15/08/21 08:52:22 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 255, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:22 INFO Executor: Running task 85.0 in stage 1.0 (TID 255)
15/08/21 08:52:22 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 239) in 47731 ms on localhost (70/200)
15/08/21 08:52:22 INFO ColumnChunkPageWriteStore: written 2,664,740B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,621B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:22 INFO ColumnChunkPageWriteStore: written 835,568B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,286B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:52:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000070
15/08/21 08:52:22 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000070_0: Committed
15/08/21 08:52:22 INFO Executor: Finished task 70.0 in stage 1.0 (TID 240). 843 bytes result sent to driver
15/08/21 08:52:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:22 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 256, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:52:22 INFO Executor: Running task 86.0 in stage 1.0 (TID 256)
15/08/21 08:52:22 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 240) in 42017 ms on localhost (71/200)
15/08/21 08:52:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000073
15/08/21 08:52:23 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000073_0: Committed
15/08/21 08:52:23 INFO Executor: Finished task 73.0 in stage 1.0 (TID 243). 843 bytes result sent to driver
15/08/21 08:52:23 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 257, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:23 INFO Executor: Running task 87.0 in stage 1.0 (TID 257)
15/08/21 08:52:23 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 243) in 41437 ms on localhost (72/200)
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 2,664,859B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,740B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 831,489B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,207B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000075
15/08/21 08:52:23 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000075_0: Committed
15/08/21 08:52:23 INFO Executor: Finished task 75.0 in stage 1.0 (TID 245). 843 bytes result sent to driver
15/08/21 08:52:23 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 258, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:23 INFO Executor: Running task 88.0 in stage 1.0 (TID 258)
15/08/21 08:52:23 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 245) in 41425 ms on localhost (73/200)
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 2,664,772B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,653B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 834,014B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,732B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000074
15/08/21 08:52:23 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000074_0: Committed
15/08/21 08:52:23 INFO Executor: Finished task 74.0 in stage 1.0 (TID 244). 843 bytes result sent to driver
15/08/21 08:52:23 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 259, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:23 INFO Executor: Running task 89.0 in stage 1.0 (TID 259)
15/08/21 08:52:23 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 244) in 41650 ms on localhost (74/200)
15/08/21 08:52:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 2,671,025B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,906B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 832,782B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,500B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:52:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000071
15/08/21 08:52:23 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000071_0: Committed
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 2,671,149B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,030B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:23 INFO Executor: Finished task 71.0 in stage 1.0 (TID 241). 843 bytes result sent to driver
15/08/21 08:52:23 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 260, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 833,093B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,811B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:52:23 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 241) in 42040 ms on localhost (75/200)
15/08/21 08:52:23 INFO Executor: Running task 90.0 in stage 1.0 (TID 260)
15/08/21 08:52:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000078
15/08/21 08:52:23 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000078_0: Committed
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:23 INFO Executor: Finished task 78.0 in stage 1.0 (TID 248). 843 bytes result sent to driver
15/08/21 08:52:23 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 261, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:23 INFO Executor: Running task 91.0 in stage 1.0 (TID 261)
15/08/21 08:52:23 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 248) in 41311 ms on localhost (76/200)
15/08/21 08:52:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:23 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 2,665,533B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,414B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 833,356B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,074B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 2,671,325B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,206B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:23 INFO ColumnChunkPageWriteStore: written 833,913B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,631B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:52:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000076
15/08/21 08:52:23 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000076_0: Committed
15/08/21 08:52:23 INFO Executor: Finished task 76.0 in stage 1.0 (TID 246). 843 bytes result sent to driver
15/08/21 08:52:23 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 262, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:23 INFO Executor: Running task 92.0 in stage 1.0 (TID 262)
15/08/21 08:52:23 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 246) in 41993 ms on localhost (77/200)
15/08/21 08:52:23 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000072
15/08/21 08:52:23 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000072_0: Committed
15/08/21 08:52:23 INFO Executor: Finished task 72.0 in stage 1.0 (TID 242). 843 bytes result sent to driver
15/08/21 08:52:23 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 263, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:23 INFO Executor: Running task 93.0 in stage 1.0 (TID 263)
15/08/21 08:52:23 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 242) in 42401 ms on localhost (78/200)
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:52:24 INFO ColumnChunkPageWriteStore: written 2,671,148B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,029B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:24 INFO ColumnChunkPageWriteStore: written 833,406B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,124B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:52:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000077
15/08/21 08:52:24 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000077_0: Committed
15/08/21 08:52:24 INFO Executor: Finished task 77.0 in stage 1.0 (TID 247). 843 bytes result sent to driver
15/08/21 08:52:24 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 264, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:24 INFO Executor: Running task 94.0 in stage 1.0 (TID 264)
15/08/21 08:52:24 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 247) in 42130 ms on localhost (79/200)
15/08/21 08:52:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:24 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:52:24 INFO ColumnChunkPageWriteStore: written 2,671,144B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,025B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:24 INFO ColumnChunkPageWriteStore: written 832,813B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,531B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:52:24 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000079
15/08/21 08:52:24 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000079_0: Committed
15/08/21 08:52:24 INFO Executor: Finished task 79.0 in stage 1.0 (TID 249). 843 bytes result sent to driver
15/08/21 08:52:24 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 265, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:24 INFO Executor: Running task 95.0 in stage 1.0 (TID 265)
15/08/21 08:52:24 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 249) in 41626 ms on localhost (80/200)
15/08/21 08:52:24 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:52:39 INFO ColumnChunkPageWriteStore: written 2,671,198B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,079B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:39 INFO ColumnChunkPageWriteStore: written 836,069B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,787B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:52:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000080
15/08/21 08:52:39 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000080_0: Committed
15/08/21 08:52:39 INFO Executor: Finished task 80.0 in stage 1.0 (TID 250). 843 bytes result sent to driver
15/08/21 08:52:39 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 266, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:39 INFO Executor: Running task 96.0 in stage 1.0 (TID 266)
15/08/21 08:52:39 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 250) in 29899 ms on localhost (81/200)
15/08/21 08:52:40 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:40 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:40 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:40 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
15/08/21 08:52:40 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:40 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:40 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:40 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:40 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:40 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:47 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:47 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:47 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:47 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:47 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:47 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:47 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:52:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:52:47 INFO ColumnChunkPageWriteStore: written 2,665,242B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,123B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:47 INFO ColumnChunkPageWriteStore: written 834,874B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,592B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:52:47 INFO ColumnChunkPageWriteStore: written 2,665,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,926B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:47 INFO ColumnChunkPageWriteStore: written 834,589B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,307B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:52:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000082
15/08/21 08:52:47 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000082_0: Committed
15/08/21 08:52:47 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000081
15/08/21 08:52:47 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000081_0: Committed
15/08/21 08:52:47 INFO Executor: Finished task 81.0 in stage 1.0 (TID 251). 843 bytes result sent to driver
15/08/21 08:52:47 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 267, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:47 INFO Executor: Running task 97.0 in stage 1.0 (TID 267)
15/08/21 08:52:47 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 251) in 32502 ms on localhost (82/200)
15/08/21 08:52:47 INFO Executor: Finished task 82.0 in stage 1.0 (TID 252). 843 bytes result sent to driver
15/08/21 08:52:47 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 268, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:47 INFO Executor: Running task 98.0 in stage 1.0 (TID 268)
15/08/21 08:52:47 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 252) in 32062 ms on localhost (83/200)
15/08/21 08:52:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:48 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:48 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:48 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:48 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:48 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:48 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:48 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:48 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:48 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:48 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:48 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:48 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:48 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:48 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:49 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:49 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:49 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:49 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:49 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:49 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:49 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:49 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:49 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:49 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:49 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:52:49 INFO ColumnChunkPageWriteStore: written 2,665,382B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,263B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:49 INFO ColumnChunkPageWriteStore: written 832,836B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,554B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:52:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:49 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:49 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000083
15/08/21 08:52:49 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000083_0: Committed
15/08/21 08:52:49 INFO Executor: Finished task 83.0 in stage 1.0 (TID 253). 843 bytes result sent to driver
15/08/21 08:52:49 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 269, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:49 INFO Executor: Running task 99.0 in stage 1.0 (TID 269)
15/08/21 08:52:49 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 253) in 32445 ms on localhost (84/200)
15/08/21 08:52:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 08:52:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:52:51 INFO ColumnChunkPageWriteStore: written 2,671,438B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,319B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:51 INFO ColumnChunkPageWriteStore: written 834,780B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,498B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:52:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000086
15/08/21 08:52:51 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000086_0: Committed
15/08/21 08:52:51 INFO Executor: Finished task 86.0 in stage 1.0 (TID 256). 843 bytes result sent to driver
15/08/21 08:52:51 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 270, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:51 INFO Executor: Running task 100.0 in stage 1.0 (TID 270)
15/08/21 08:52:51 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 256) in 28240 ms on localhost (85/200)
15/08/21 08:52:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:52:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:55 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:55 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:52:55 INFO ColumnChunkPageWriteStore: written 2,671,429B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,310B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:55 INFO ColumnChunkPageWriteStore: written 833,569B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,287B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:52:55 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000087
15/08/21 08:52:55 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000087_0: Committed
15/08/21 08:52:55 INFO Executor: Finished task 87.0 in stage 1.0 (TID 257). 843 bytes result sent to driver
15/08/21 08:52:55 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 271, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:55 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 257) in 32945 ms on localhost (86/200)
15/08/21 08:52:55 INFO Executor: Running task 101.0 in stage 1.0 (TID 271)
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:52:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:52:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,673
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 2,665,045B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,926B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 832,530B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,248B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 2,671,325B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,206B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 835,794B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,512B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 319 entries, 2,552B raw, 319B comp}
15/08/21 08:52:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:52:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000091
15/08/21 08:52:56 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000091_0: Committed
15/08/21 08:52:56 INFO Executor: Finished task 91.0 in stage 1.0 (TID 261). 843 bytes result sent to driver
15/08/21 08:52:56 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 272, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:56 INFO Executor: Running task 102.0 in stage 1.0 (TID 272)
15/08/21 08:52:56 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 261) in 32658 ms on localhost (87/200)
15/08/21 08:52:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000088
15/08/21 08:52:56 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000088_0: Committed
15/08/21 08:52:56 INFO Executor: Finished task 88.0 in stage 1.0 (TID 258). 843 bytes result sent to driver
15/08/21 08:52:56 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 273, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:56 INFO Executor: Running task 103.0 in stage 1.0 (TID 273)
15/08/21 08:52:56 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 258) in 33134 ms on localhost (88/200)
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 2,671,098B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,979B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 832,460B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,178B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 08:52:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000085
15/08/21 08:52:56 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000085_0: Committed
15/08/21 08:52:56 INFO Executor: Finished task 85.0 in stage 1.0 (TID 255). 843 bytes result sent to driver
15/08/21 08:52:56 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 274, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:56 INFO Executor: Running task 104.0 in stage 1.0 (TID 274)
15/08/21 08:52:56 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 255) in 33499 ms on localhost (89/200)
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 2,664,909B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,790B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 830,715B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,433B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:52:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000090
15/08/21 08:52:56 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000090_0: Committed
15/08/21 08:52:56 INFO Executor: Finished task 90.0 in stage 1.0 (TID 260). 843 bytes result sent to driver
15/08/21 08:52:56 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 275, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:56 INFO Executor: Running task 105.0 in stage 1.0 (TID 275)
15/08/21 08:52:56 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 260) in 33021 ms on localhost (90/200)
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 2,665,894B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,775B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 833,016B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,734B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:52:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:52:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000092
15/08/21 08:52:56 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000092_0: Committed
15/08/21 08:52:56 INFO Executor: Finished task 92.0 in stage 1.0 (TID 262). 843 bytes result sent to driver
15/08/21 08:52:56 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 276, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:56 INFO Executor: Running task 106.0 in stage 1.0 (TID 276)
15/08/21 08:52:56 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 262) in 32864 ms on localhost (91/200)
15/08/21 08:52:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,585
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 2,666,194B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,075B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 831,501B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,219B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 2,671,043B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,924B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:56 INFO ColumnChunkPageWriteStore: written 834,855B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,573B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 308 entries, 2,464B raw, 308B comp}
15/08/21 08:52:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000084
15/08/21 08:52:56 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000084_0: Committed
15/08/21 08:52:56 INFO Executor: Finished task 84.0 in stage 1.0 (TID 254). 843 bytes result sent to driver
15/08/21 08:52:56 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 277, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:56 INFO Executor: Running task 107.0 in stage 1.0 (TID 277)
15/08/21 08:52:56 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 254) in 34196 ms on localhost (92/200)
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000093
15/08/21 08:52:56 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000093_0: Committed
15/08/21 08:52:56 INFO Executor: Finished task 93.0 in stage 1.0 (TID 263). 843 bytes result sent to driver
15/08/21 08:52:56 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 278, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:56 INFO Executor: Running task 108.0 in stage 1.0 (TID 278)
15/08/21 08:52:56 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 263) in 33005 ms on localhost (93/200)
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:52:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 08:52:57 INFO ColumnChunkPageWriteStore: written 2,670,977B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,858B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:57 INFO ColumnChunkPageWriteStore: written 832,600B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,318B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:52:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000094
15/08/21 08:52:57 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000094_0: Committed
15/08/21 08:52:57 INFO ColumnChunkPageWriteStore: written 2,665,088B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,969B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:57 INFO Executor: Finished task 94.0 in stage 1.0 (TID 264). 843 bytes result sent to driver
15/08/21 08:52:57 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 279, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:57 INFO ColumnChunkPageWriteStore: written 834,046B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,764B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 08:52:57 INFO Executor: Running task 109.0 in stage 1.0 (TID 279)
15/08/21 08:52:57 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 264) in 32956 ms on localhost (94/200)
15/08/21 08:52:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000089
15/08/21 08:52:57 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000089_0: Committed
15/08/21 08:52:57 INFO Executor: Finished task 89.0 in stage 1.0 (TID 259). 843 bytes result sent to driver
15/08/21 08:52:57 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 280, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:57 INFO Executor: Running task 110.0 in stage 1.0 (TID 280)
15/08/21 08:52:57 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 259) in 34105 ms on localhost (95/200)
15/08/21 08:52:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:52:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:52:57 INFO ColumnChunkPageWriteStore: written 2,670,798B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,679B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:52:57 INFO ColumnChunkPageWriteStore: written 833,138B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,856B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:52:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000095
15/08/21 08:52:57 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000095_0: Committed
15/08/21 08:52:57 INFO Executor: Finished task 95.0 in stage 1.0 (TID 265). 843 bytes result sent to driver
15/08/21 08:52:57 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 281, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:52:57 INFO Executor: Running task 111.0 in stage 1.0 (TID 281)
15/08/21 08:52:57 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 265) in 33110 ms on localhost (96/200)
15/08/21 08:52:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:52:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:52:58 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:52:58 INFO CodecConfig: Compression: GZIP
15/08/21 08:52:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:52:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:52:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:52:58 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:52:58 INFO ParquetOutputFormat: Validation is off
15/08/21 08:52:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:52:58 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:53:05 INFO ColumnChunkPageWriteStore: written 2,671,367B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,248B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:05 INFO ColumnChunkPageWriteStore: written 835,262B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,980B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:53:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508210852_0001_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210852_0001_m_000096
15/08/21 08:53:05 INFO SparkHadoopMapRedUtil: attempt_201508210852_0001_m_000096_0: Committed
15/08/21 08:53:05 INFO Executor: Finished task 96.0 in stage 1.0 (TID 266). 843 bytes result sent to driver
15/08/21 08:53:05 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 282, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:05 INFO Executor: Running task 112.0 in stage 1.0 (TID 282)
15/08/21 08:53:05 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 266) in 25285 ms on localhost (97/200)
15/08/21 08:53:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:06 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:06 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:06 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:20 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:20 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,673
15/08/21 08:53:22 INFO ColumnChunkPageWriteStore: written 2,665,205B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,086B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:22 INFO ColumnChunkPageWriteStore: written 833,315B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,033B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 319 entries, 2,552B raw, 319B comp}
15/08/21 08:53:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000097
15/08/21 08:53:22 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000097_0: Committed
15/08/21 08:53:22 INFO Executor: Finished task 97.0 in stage 1.0 (TID 267). 843 bytes result sent to driver
15/08/21 08:53:22 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 283, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:22 INFO Executor: Running task 113.0 in stage 1.0 (TID 283)
15/08/21 08:53:22 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 267) in 34176 ms on localhost (98/200)
15/08/21 08:53:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:53:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:22 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:22 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:22 INFO ColumnChunkPageWriteStore: written 2,664,831B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,712B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:22 INFO ColumnChunkPageWriteStore: written 834,250B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,968B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:53:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000098
15/08/21 08:53:22 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000098_0: Committed
15/08/21 08:53:22 INFO Executor: Finished task 98.0 in stage 1.0 (TID 268). 843 bytes result sent to driver
15/08/21 08:53:22 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 284, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:22 INFO Executor: Running task 114.0 in stage 1.0 (TID 284)
15/08/21 08:53:22 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 268) in 34506 ms on localhost (99/200)
15/08/21 08:53:22 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:22 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:22 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:22 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:22 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:22 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:22 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:22 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:22 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:22 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:53:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:27 INFO ColumnChunkPageWriteStore: written 2,664,905B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,786B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:27 INFO ColumnChunkPageWriteStore: written 834,064B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,782B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:53:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000099
15/08/21 08:53:27 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000099_0: Committed
15/08/21 08:53:27 INFO Executor: Finished task 99.0 in stage 1.0 (TID 269). 843 bytes result sent to driver
15/08/21 08:53:27 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 285, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:27 INFO Executor: Running task 115.0 in stage 1.0 (TID 285)
15/08/21 08:53:27 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 269) in 37705 ms on localhost (100/200)
15/08/21 08:53:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:27 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:53:27 INFO ColumnChunkPageWriteStore: written 2,665,707B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,588B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:27 INFO ColumnChunkPageWriteStore: written 833,973B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,691B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:53:27 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000100
15/08/21 08:53:27 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000100_0: Committed
15/08/21 08:53:27 INFO Executor: Finished task 100.0 in stage 1.0 (TID 270). 843 bytes result sent to driver
15/08/21 08:53:27 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 286, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:27 INFO Executor: Running task 116.0 in stage 1.0 (TID 286)
15/08/21 08:53:27 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 270) in 36568 ms on localhost (101/200)
15/08/21 08:53:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:53:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:53:28 INFO ColumnChunkPageWriteStore: written 2,671,228B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,109B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:28 INFO ColumnChunkPageWriteStore: written 837,388B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 837,106B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:53:28 INFO ColumnChunkPageWriteStore: written 2,670,961B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,842B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:28 INFO ColumnChunkPageWriteStore: written 834,751B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,469B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:53:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000101
15/08/21 08:53:28 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000101_0: Committed
15/08/21 08:53:28 INFO Executor: Finished task 101.0 in stage 1.0 (TID 271). 843 bytes result sent to driver
15/08/21 08:53:28 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 287, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:28 INFO Executor: Running task 117.0 in stage 1.0 (TID 287)
15/08/21 08:53:28 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 271) in 32898 ms on localhost (102/200)
15/08/21 08:53:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:28 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000102
15/08/21 08:53:28 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000102_0: Committed
15/08/21 08:53:28 INFO Executor: Finished task 102.0 in stage 1.0 (TID 272). 843 bytes result sent to driver
15/08/21 08:53:28 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 288, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:28 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 272) in 32775 ms on localhost (103/200)
15/08/21 08:53:28 INFO Executor: Running task 118.0 in stage 1.0 (TID 288)
15/08/21 08:53:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:53:33 INFO ColumnChunkPageWriteStore: written 2,671,439B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,320B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:33 INFO ColumnChunkPageWriteStore: written 834,740B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,458B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:53:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000104
15/08/21 08:53:34 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000104_0: Committed
15/08/21 08:53:34 INFO Executor: Finished task 104.0 in stage 1.0 (TID 274). 843 bytes result sent to driver
15/08/21 08:53:34 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 289, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:34 INFO Executor: Running task 119.0 in stage 1.0 (TID 289)
15/08/21 08:53:34 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 274) in 37653 ms on localhost (104/200)
15/08/21 08:53:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:53:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 2,665,021B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,902B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 832,356B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,074B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 2,666,321B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,202B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 833,519B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,237B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:53:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000106
15/08/21 08:53:34 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000106_0: Committed
15/08/21 08:53:34 INFO Executor: Finished task 106.0 in stage 1.0 (TID 276). 843 bytes result sent to driver
15/08/21 08:53:34 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 290, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:34 INFO Executor: Running task 120.0 in stage 1.0 (TID 290)
15/08/21 08:53:34 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 276) in 37582 ms on localhost (105/200)
15/08/21 08:53:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:34 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:34 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000108
15/08/21 08:53:34 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000108_0: Committed
15/08/21 08:53:34 INFO Executor: Finished task 108.0 in stage 1.0 (TID 278). 843 bytes result sent to driver
15/08/21 08:53:34 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 291, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:34 INFO Executor: Running task 121.0 in stage 1.0 (TID 291)
15/08/21 08:53:34 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 278) in 37406 ms on localhost (106/200)
15/08/21 08:53:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 2,671,218B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,099B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 831,764B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,482B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:53:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 2,671,095B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,976B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 830,535B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,253B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:53:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000103
15/08/21 08:53:34 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000103_0: Committed
15/08/21 08:53:34 INFO Executor: Finished task 103.0 in stage 1.0 (TID 273). 843 bytes result sent to driver
15/08/21 08:53:34 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 292, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:34 INFO Executor: Running task 122.0 in stage 1.0 (TID 292)
15/08/21 08:53:34 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 273) in 38551 ms on localhost (107/200)
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:53:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000109
15/08/21 08:53:34 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000109_0: Committed
15/08/21 08:53:34 INFO Executor: Finished task 109.0 in stage 1.0 (TID 279). 843 bytes result sent to driver
15/08/21 08:53:34 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 293, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:34 INFO Executor: Running task 123.0 in stage 1.0 (TID 293)
15/08/21 08:53:34 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 279) in 37673 ms on localhost (108/200)
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 2,664,933B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,814B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:34 INFO ColumnChunkPageWriteStore: written 834,256B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,974B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:53:35 INFO ColumnChunkPageWriteStore: written 2,671,480B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,361B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:35 INFO ColumnChunkPageWriteStore: written 834,612B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,330B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:53:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000105
15/08/21 08:53:35 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000105_0: Committed
15/08/21 08:53:35 INFO Executor: Finished task 105.0 in stage 1.0 (TID 275). 843 bytes result sent to driver
15/08/21 08:53:35 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 294, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:35 INFO Executor: Running task 124.0 in stage 1.0 (TID 294)
15/08/21 08:53:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000111
15/08/21 08:53:35 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000111_0: Committed
15/08/21 08:53:35 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 275) in 38614 ms on localhost (109/200)
15/08/21 08:53:35 INFO Executor: Finished task 111.0 in stage 1.0 (TID 281). 843 bytes result sent to driver
15/08/21 08:53:35 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 295, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:35 INFO Executor: Running task 125.0 in stage 1.0 (TID 295)
15/08/21 08:53:35 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 281) in 37334 ms on localhost (110/200)
15/08/21 08:53:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:53:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:53:35 INFO ColumnChunkPageWriteStore: written 2,665,132B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,013B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:35 INFO ColumnChunkPageWriteStore: written 833,912B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,630B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:53:35 INFO ColumnChunkPageWriteStore: written 2,671,231B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,112B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:35 INFO ColumnChunkPageWriteStore: written 834,861B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,579B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:53:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000107
15/08/21 08:53:35 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000107_0: Committed
15/08/21 08:53:35 INFO Executor: Finished task 107.0 in stage 1.0 (TID 277). 843 bytes result sent to driver
15/08/21 08:53:35 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 296, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:35 INFO Executor: Running task 126.0 in stage 1.0 (TID 296)
15/08/21 08:53:35 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 277) in 38639 ms on localhost (111/200)
15/08/21 08:53:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000110
15/08/21 08:53:35 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000110_0: Committed
15/08/21 08:53:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:35 INFO Executor: Finished task 110.0 in stage 1.0 (TID 280). 843 bytes result sent to driver
15/08/21 08:53:35 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 297, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:35 INFO Executor: Running task 127.0 in stage 1.0 (TID 297)
15/08/21 08:53:35 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 280) in 38161 ms on localhost (112/200)
15/08/21 08:53:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:36 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:53:36 INFO ColumnChunkPageWriteStore: written 2,671,213B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,094B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:36 INFO ColumnChunkPageWriteStore: written 833,615B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,333B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:53:36 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000112
15/08/21 08:53:36 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000112_0: Committed
15/08/21 08:53:36 INFO Executor: Finished task 112.0 in stage 1.0 (TID 282). 843 bytes result sent to driver
15/08/21 08:53:36 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 298, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:36 INFO Executor: Running task 128.0 in stage 1.0 (TID 298)
15/08/21 08:53:36 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 282) in 31183 ms on localhost (113/200)
15/08/21 08:53:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:43 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:43 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:43 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:43 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:43 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:43 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:43 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:43 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:43 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:44 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:44 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:53:49 INFO ColumnChunkPageWriteStore: written 2,665,196B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,077B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:49 INFO ColumnChunkPageWriteStore: written 835,647B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,365B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:53:49 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000113
15/08/21 08:53:49 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000113_0: Committed
15/08/21 08:53:49 INFO Executor: Finished task 113.0 in stage 1.0 (TID 283). 843 bytes result sent to driver
15/08/21 08:53:49 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 299, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:49 INFO Executor: Running task 129.0 in stage 1.0 (TID 299)
15/08/21 08:53:49 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 283) in 26987 ms on localhost (114/200)
15/08/21 08:53:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:53:50 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:53:50 INFO ColumnChunkPageWriteStore: written 2,665,084B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,965B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:50 INFO ColumnChunkPageWriteStore: written 834,882B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,600B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:53:50 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000114
15/08/21 08:53:50 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000114_0: Committed
15/08/21 08:53:50 INFO Executor: Finished task 114.0 in stage 1.0 (TID 284). 843 bytes result sent to driver
15/08/21 08:53:50 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 300, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:50 INFO Executor: Running task 130.0 in stage 1.0 (TID 300)
15/08/21 08:53:50 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 284) in 27734 ms on localhost (115/200)
15/08/21 08:53:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:50 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:50 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:50 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:50 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:50 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:50 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:50 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:51 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:51 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:51 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:51 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:51 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:51 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:51 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:51 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:51 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:51 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:51 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:56 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:56 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:53:56 INFO ColumnChunkPageWriteStore: written 2,666,031B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,912B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:56 INFO ColumnChunkPageWriteStore: written 837,303B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 837,021B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:53:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000116
15/08/21 08:53:57 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000116_0: Committed
15/08/21 08:53:57 INFO Executor: Finished task 116.0 in stage 1.0 (TID 286). 843 bytes result sent to driver
15/08/21 08:53:57 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 301, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:57 INFO Executor: Running task 131.0 in stage 1.0 (TID 301)
15/08/21 08:53:57 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 286) in 29328 ms on localhost (116/200)
15/08/21 08:53:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:57 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:57 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:53:57 INFO ColumnChunkPageWriteStore: written 2,665,047B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,928B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:57 INFO ColumnChunkPageWriteStore: written 834,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,713B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:53:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:57 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:57 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:53:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000115
15/08/21 08:53:57 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000115_0: Committed
15/08/21 08:53:57 INFO Executor: Finished task 115.0 in stage 1.0 (TID 285). 843 bytes result sent to driver
15/08/21 08:53:57 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 302, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:57 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 285) in 30136 ms on localhost (117/200)
15/08/21 08:53:57 INFO Executor: Running task 132.0 in stage 1.0 (TID 302)
15/08/21 08:53:57 INFO ColumnChunkPageWriteStore: written 2,671,269B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,150B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:57 INFO ColumnChunkPageWriteStore: written 834,770B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,488B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:53:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000118
15/08/21 08:53:57 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000118_0: Committed
15/08/21 08:53:57 INFO Executor: Finished task 118.0 in stage 1.0 (TID 288). 843 bytes result sent to driver
15/08/21 08:53:57 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 303, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:57 INFO Executor: Running task 133.0 in stage 1.0 (TID 303)
15/08/21 08:53:57 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 288) in 28685 ms on localhost (118/200)
15/08/21 08:53:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:53:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:57 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:57 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:57 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:57 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:58 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:58 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:58 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:58 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:53:58 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:58 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:58 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:58 INFO ColumnChunkPageWriteStore: written 2,671,112B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,993B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:58 INFO ColumnChunkPageWriteStore: written 832,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,713B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:53:58 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:58 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:58 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:58 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:58 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:58 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000117
15/08/21 08:53:58 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000117_0: Committed
15/08/21 08:53:58 INFO Executor: Finished task 117.0 in stage 1.0 (TID 287). 843 bytes result sent to driver
15/08/21 08:53:58 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 304, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:58 INFO Executor: Running task 134.0 in stage 1.0 (TID 304)
15/08/21 08:53:58 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 287) in 29478 ms on localhost (119/200)
15/08/21 08:53:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:53:58 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:58 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:58 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:58 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:58 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:58 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:58 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:58 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:58 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:53:58 INFO CodecConfig: Compression: GZIP
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:53:58 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:53:58 INFO ParquetOutputFormat: Validation is off
15/08/21 08:53:58 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:53:58 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:53:58 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:53:59 INFO ColumnChunkPageWriteStore: written 2,671,208B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,089B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:53:59 INFO ColumnChunkPageWriteStore: written 832,665B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,383B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:53:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:53:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000120
15/08/21 08:53:59 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000120_0: Committed
15/08/21 08:53:59 INFO Executor: Finished task 120.0 in stage 1.0 (TID 290). 843 bytes result sent to driver
15/08/21 08:53:59 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 305, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:53:59 INFO Executor: Running task 135.0 in stage 1.0 (TID 305)
15/08/21 08:53:59 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 290) in 24908 ms on localhost (120/200)
15/08/21 08:53:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:53:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:03 INFO ColumnChunkPageWriteStore: written 2,671,005B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,886B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:03 INFO ColumnChunkPageWriteStore: written 834,168B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,886B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:54:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000119
15/08/21 08:54:03 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000119_0: Committed
15/08/21 08:54:03 INFO Executor: Finished task 119.0 in stage 1.0 (TID 289). 843 bytes result sent to driver
15/08/21 08:54:03 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 306, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:03 INFO Executor: Running task 136.0 in stage 1.0 (TID 306)
15/08/21 08:54:03 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 289) in 29255 ms on localhost (121/200)
15/08/21 08:54:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:54:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:03 INFO ColumnChunkPageWriteStore: written 2,665,084B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,965B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:03 INFO ColumnChunkPageWriteStore: written 836,016B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,734B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:54:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000121
15/08/21 08:54:03 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000121_0: Committed
15/08/21 08:54:03 INFO Executor: Finished task 121.0 in stage 1.0 (TID 291). 843 bytes result sent to driver
15/08/21 08:54:03 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 307, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:03 INFO Executor: Running task 137.0 in stage 1.0 (TID 307)
15/08/21 08:54:03 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 291) in 29186 ms on localhost (122/200)
15/08/21 08:54:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:54:03 INFO ColumnChunkPageWriteStore: written 2,664,913B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,794B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:03 INFO ColumnChunkPageWriteStore: written 833,998B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,716B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:54:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:54:03 INFO ColumnChunkPageWriteStore: written 2,671,062B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,943B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:03 INFO ColumnChunkPageWriteStore: written 834,555B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,273B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:54:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000122
15/08/21 08:54:03 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000122_0: Committed
15/08/21 08:54:03 INFO Executor: Finished task 122.0 in stage 1.0 (TID 292). 843 bytes result sent to driver
15/08/21 08:54:03 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 308, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:03 INFO Executor: Running task 138.0 in stage 1.0 (TID 308)
15/08/21 08:54:03 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 292) in 29101 ms on localhost (123/200)
15/08/21 08:54:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000126
15/08/21 08:54:03 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000126_0: Committed
15/08/21 08:54:03 INFO Executor: Finished task 126.0 in stage 1.0 (TID 296). 843 bytes result sent to driver
15/08/21 08:54:03 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 309, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:03 INFO Executor: Running task 139.0 in stage 1.0 (TID 309)
15/08/21 08:54:03 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 296) in 28525 ms on localhost (124/200)
15/08/21 08:54:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:54:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 2,665,944B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,825B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 835,183B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,901B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 2,665,196B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,077B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 834,711B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,429B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:54:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000124
15/08/21 08:54:04 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000124_0: Committed
15/08/21 08:54:04 INFO Executor: Finished task 124.0 in stage 1.0 (TID 294). 843 bytes result sent to driver
15/08/21 08:54:04 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 310, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:04 INFO Executor: Running task 140.0 in stage 1.0 (TID 310)
15/08/21 08:54:04 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 294) in 29054 ms on localhost (125/200)
15/08/21 08:54:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000123
15/08/21 08:54:04 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000123_0: Committed
15/08/21 08:54:04 INFO Executor: Finished task 123.0 in stage 1.0 (TID 293). 843 bytes result sent to driver
15/08/21 08:54:04 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 311, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:04 INFO Executor: Running task 141.0 in stage 1.0 (TID 311)
15/08/21 08:54:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:04 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 293) in 29346 ms on localhost (126/200)
15/08/21 08:54:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 2,671,421B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,302B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 832,191B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,909B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:54:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000127
15/08/21 08:54:04 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000127_0: Committed
15/08/21 08:54:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:54:04 INFO Executor: Finished task 127.0 in stage 1.0 (TID 297). 843 bytes result sent to driver
15/08/21 08:54:04 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 312, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:04 INFO Executor: Running task 142.0 in stage 1.0 (TID 312)
15/08/21 08:54:04 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 297) in 28970 ms on localhost (127/200)
15/08/21 08:54:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 2,671,132B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,013B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 833,511B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,229B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:54:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000125
15/08/21 08:54:04 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000125_0: Committed
15/08/21 08:54:04 INFO Executor: Finished task 125.0 in stage 1.0 (TID 295). 843 bytes result sent to driver
15/08/21 08:54:04 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 313, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:04 INFO Executor: Running task 143.0 in stage 1.0 (TID 313)
15/08/21 08:54:04 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 295) in 29554 ms on localhost (128/200)
15/08/21 08:54:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 2,671,683B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,564B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:04 INFO ColumnChunkPageWriteStore: written 834,278B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,996B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:54:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508210853_0001_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210853_0001_m_000128
15/08/21 08:54:04 INFO SparkHadoopMapRedUtil: attempt_201508210853_0001_m_000128_0: Committed
15/08/21 08:54:04 INFO Executor: Finished task 128.0 in stage 1.0 (TID 298). 843 bytes result sent to driver
15/08/21 08:54:04 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 314, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:04 INFO Executor: Running task 144.0 in stage 1.0 (TID 314)
15/08/21 08:54:04 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 298) in 28622 ms on localhost (129/200)
15/08/21 08:54:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:05 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:05 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:05 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:18 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:18 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:19 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:54:20 INFO ColumnChunkPageWriteStore: written 2,665,074B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,955B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:20 INFO ColumnChunkPageWriteStore: written 831,680B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,398B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:54:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000129
15/08/21 08:54:20 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000129_0: Committed
15/08/21 08:54:20 INFO Executor: Finished task 129.0 in stage 1.0 (TID 299). 843 bytes result sent to driver
15/08/21 08:54:20 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 315, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:20 INFO Executor: Running task 145.0 in stage 1.0 (TID 315)
15/08/21 08:54:20 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 299) in 31297 ms on localhost (130/200)
15/08/21 08:54:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:54:21 INFO ColumnChunkPageWriteStore: written 2,664,800B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,681B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:21 INFO ColumnChunkPageWriteStore: written 834,840B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,558B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:54:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000130
15/08/21 08:54:21 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000130_0: Committed
15/08/21 08:54:21 INFO Executor: Finished task 130.0 in stage 1.0 (TID 300). 843 bytes result sent to driver
15/08/21 08:54:21 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 316, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:21 INFO Executor: Running task 146.0 in stage 1.0 (TID 316)
15/08/21 08:54:21 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 300) in 31025 ms on localhost (131/200)
15/08/21 08:54:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:21 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:21 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:21 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:21 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:21 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:24 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:24 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:24 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:24 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:24 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:24 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:24 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:24 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:24 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:24 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:24 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:25 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:25 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:25 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:25 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:25 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:25 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:25 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:54:25 INFO ColumnChunkPageWriteStore: written 2,665,010B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,891B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:25 INFO ColumnChunkPageWriteStore: written 835,357B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,075B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000131
15/08/21 08:54:26 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000131_0: Committed
15/08/21 08:54:26 INFO Executor: Finished task 131.0 in stage 1.0 (TID 301). 843 bytes result sent to driver
15/08/21 08:54:26 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 317, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:26 INFO Executor: Running task 147.0 in stage 1.0 (TID 317)
15/08/21 08:54:26 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 301) in 28948 ms on localhost (132/200)
15/08/21 08:54:26 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 08:54:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:26 INFO ColumnChunkPageWriteStore: written 2,665,953B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,834B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:26 INFO ColumnChunkPageWriteStore: written 835,522B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,240B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 08:54:26 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000132
15/08/21 08:54:26 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000132_0: Committed
15/08/21 08:54:26 INFO Executor: Finished task 132.0 in stage 1.0 (TID 302). 843 bytes result sent to driver
15/08/21 08:54:26 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 318, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:26 INFO Executor: Running task 148.0 in stage 1.0 (TID 318)
15/08/21 08:54:26 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 302) in 28642 ms on localhost (133/200)
15/08/21 08:54:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:54:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:31 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:31 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:31 INFO ColumnChunkPageWriteStore: written 2,671,115B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,996B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:31 INFO ColumnChunkPageWriteStore: written 833,766B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,484B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:54:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000133
15/08/21 08:54:31 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000133_0: Committed
15/08/21 08:54:31 INFO Executor: Finished task 133.0 in stage 1.0 (TID 303). 843 bytes result sent to driver
15/08/21 08:54:31 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 319, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:31 INFO Executor: Running task 149.0 in stage 1.0 (TID 319)
15/08/21 08:54:31 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 303) in 33847 ms on localhost (134/200)
15/08/21 08:54:31 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:54:31 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:54:31 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:31 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:31 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:31 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:31 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:31 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:31 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:31 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:31 INFO ColumnChunkPageWriteStore: written 2,670,779B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,660B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:31 INFO ColumnChunkPageWriteStore: written 834,758B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,476B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:54:31 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:31 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000134
15/08/21 08:54:31 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000134_0: Committed
15/08/21 08:54:31 INFO Executor: Finished task 134.0 in stage 1.0 (TID 304). 843 bytes result sent to driver
15/08/21 08:54:31 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 320, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:31 INFO Executor: Running task 150.0 in stage 1.0 (TID 320)
15/08/21 08:54:31 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 304) in 33644 ms on localhost (135/200)
15/08/21 08:54:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:54:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:33 INFO ColumnChunkPageWriteStore: written 2,665,250B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,131B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:33 INFO ColumnChunkPageWriteStore: written 831,478B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,196B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:54:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:54:33 INFO ColumnChunkPageWriteStore: written 2,670,949B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,830B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:33 INFO ColumnChunkPageWriteStore: written 834,040B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,758B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:54:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000138
15/08/21 08:54:33 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000138_0: Committed
15/08/21 08:54:33 INFO Executor: Finished task 138.0 in stage 1.0 (TID 308). 843 bytes result sent to driver
15/08/21 08:54:33 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 321, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:33 INFO Executor: Running task 151.0 in stage 1.0 (TID 321)
15/08/21 08:54:33 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 308) in 29833 ms on localhost (136/200)
15/08/21 08:54:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000136
15/08/21 08:54:33 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000136_0: Committed
15/08/21 08:54:33 INFO Executor: Finished task 136.0 in stage 1.0 (TID 306). 843 bytes result sent to driver
15/08/21 08:54:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:33 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 322, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:33 INFO Executor: Running task 152.0 in stage 1.0 (TID 322)
15/08/21 08:54:33 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 306) in 30592 ms on localhost (137/200)
15/08/21 08:54:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 08:54:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:34 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:34 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:54:34 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:54:34 INFO ColumnChunkPageWriteStore: written 2,665,032B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,913B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:34 INFO ColumnChunkPageWriteStore: written 832,389B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,107B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:54:34 INFO ColumnChunkPageWriteStore: written 2,671,262B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,143B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:34 INFO ColumnChunkPageWriteStore: written 833,200B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,918B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:54:34 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000137
15/08/21 08:54:34 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000137_0: Committed
15/08/21 08:54:34 INFO Executor: Finished task 137.0 in stage 1.0 (TID 307). 843 bytes result sent to driver
15/08/21 08:54:34 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 323, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:34 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 307) in 31538 ms on localhost (138/200)
15/08/21 08:54:34 INFO Executor: Running task 153.0 in stage 1.0 (TID 323)
15/08/21 08:54:35 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000135
15/08/21 08:54:35 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000135_0: Committed
15/08/21 08:54:35 INFO Executor: Finished task 135.0 in stage 1.0 (TID 305). 843 bytes result sent to driver
15/08/21 08:54:35 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 324, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:35 INFO Executor: Running task 154.0 in stage 1.0 (TID 324)
15/08/21 08:54:35 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 305) in 35889 ms on localhost (139/200)
15/08/21 08:54:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:54:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:54:40 INFO ColumnChunkPageWriteStore: written 2,665,071B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,952B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:40 INFO ColumnChunkPageWriteStore: written 835,821B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,539B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:54:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000139
15/08/21 08:54:40 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000139_0: Committed
15/08/21 08:54:40 INFO Executor: Finished task 139.0 in stage 1.0 (TID 309). 843 bytes result sent to driver
15/08/21 08:54:40 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 325, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:40 INFO Executor: Running task 155.0 in stage 1.0 (TID 325)
15/08/21 08:54:40 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 309) in 36533 ms on localhost (140/200)
15/08/21 08:54:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:54:40 INFO ColumnChunkPageWriteStore: written 2,671,252B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,133B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:40 INFO ColumnChunkPageWriteStore: written 833,254B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,972B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:54:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000142
15/08/21 08:54:40 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000142_0: Committed
15/08/21 08:54:40 INFO Executor: Finished task 142.0 in stage 1.0 (TID 312). 843 bytes result sent to driver
15/08/21 08:54:40 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 326, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:40 INFO Executor: Running task 156.0 in stage 1.0 (TID 326)
15/08/21 08:54:40 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 312) in 36346 ms on localhost (141/200)
15/08/21 08:54:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:54:40 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:54:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:54:40 INFO ColumnChunkPageWriteStore: written 2,671,240B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,121B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:40 INFO ColumnChunkPageWriteStore: written 832,390B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,108B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:54:40 INFO ColumnChunkPageWriteStore: written 2,665,955B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,836B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:40 INFO ColumnChunkPageWriteStore: written 832,522B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,240B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:54:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000143
15/08/21 08:54:41 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000143_0: Committed
15/08/21 08:54:41 INFO Executor: Finished task 143.0 in stage 1.0 (TID 313). 843 bytes result sent to driver
15/08/21 08:54:41 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 327, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:41 INFO Executor: Running task 157.0 in stage 1.0 (TID 327)
15/08/21 08:54:41 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 313) in 36349 ms on localhost (142/200)
15/08/21 08:54:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000140
15/08/21 08:54:41 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000140_0: Committed
15/08/21 08:54:41 INFO Executor: Finished task 140.0 in stage 1.0 (TID 310). 843 bytes result sent to driver
15/08/21 08:54:41 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 328, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:41 INFO Executor: Running task 158.0 in stage 1.0 (TID 328)
15/08/21 08:54:41 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 310) in 36902 ms on localhost (143/200)
15/08/21 08:54:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 08:54:41 INFO ColumnChunkPageWriteStore: written 2,670,980B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,861B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:41 INFO ColumnChunkPageWriteStore: written 834,170B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,888B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 08:54:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:54:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000141
15/08/21 08:54:41 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000141_0: Committed
15/08/21 08:54:41 INFO Executor: Finished task 141.0 in stage 1.0 (TID 311). 843 bytes result sent to driver
15/08/21 08:54:41 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 329, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:41 INFO Executor: Running task 159.0 in stage 1.0 (TID 329)
15/08/21 08:54:41 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 311) in 37242 ms on localhost (144/200)
15/08/21 08:54:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:41 INFO ColumnChunkPageWriteStore: written 2,671,505B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,386B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:41 INFO ColumnChunkPageWriteStore: written 834,710B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,428B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:54:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000144
15/08/21 08:54:41 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000144_0: Committed
15/08/21 08:54:41 INFO Executor: Finished task 144.0 in stage 1.0 (TID 314). 843 bytes result sent to driver
15/08/21 08:54:41 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 330, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:41 INFO Executor: Running task 160.0 in stage 1.0 (TID 330)
15/08/21 08:54:41 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 314) in 36757 ms on localhost (145/200)
15/08/21 08:54:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:42 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:42 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:42 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:42 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:42 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:42 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:54:48 INFO ColumnChunkPageWriteStore: written 2,664,977B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,858B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:48 INFO ColumnChunkPageWriteStore: written 832,215B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,933B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:54:48 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000145
15/08/21 08:54:48 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000145_0: Committed
15/08/21 08:54:48 INFO Executor: Finished task 145.0 in stage 1.0 (TID 315). 843 bytes result sent to driver
15/08/21 08:54:48 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 331, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:48 INFO Executor: Running task 161.0 in stage 1.0 (TID 331)
15/08/21 08:54:48 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 315) in 28078 ms on localhost (146/200)
15/08/21 08:54:48 INFO ColumnChunkPageWriteStore: written 2,665,008B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,889B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:48 INFO ColumnChunkPageWriteStore: written 835,046B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,764B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:54:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:48 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000146
15/08/21 08:54:48 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000146_0: Committed
15/08/21 08:54:48 INFO Executor: Finished task 146.0 in stage 1.0 (TID 316). 843 bytes result sent to driver
15/08/21 08:54:48 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 332, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:48 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 316) in 27383 ms on localhost (147/200)
15/08/21 08:54:48 INFO Executor: Running task 162.0 in stage 1.0 (TID 332)
15/08/21 08:54:48 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:49 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:49 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:49 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:49 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:49 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:49 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:49 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:49 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:49 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:50 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:50 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:50 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:50 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:50 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:50 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:50 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:50 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:50 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:55 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:55 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:55 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:55 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:55 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:55 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:55 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:55 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:55 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:55 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:55 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:55 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:54:55 INFO ColumnChunkPageWriteStore: written 2,664,987B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,868B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:55 INFO ColumnChunkPageWriteStore: written 833,748B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,466B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:54:56 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000147
15/08/21 08:54:56 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000147_0: Committed
15/08/21 08:54:56 INFO Executor: Finished task 147.0 in stage 1.0 (TID 317). 843 bytes result sent to driver
15/08/21 08:54:56 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 333, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:56 INFO Executor: Running task 163.0 in stage 1.0 (TID 333)
15/08/21 08:54:56 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 317) in 30150 ms on localhost (148/200)
15/08/21 08:54:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:56 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:56 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:56 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:56 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:56 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:56 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:56 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:56 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:56 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:56 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:54:56 INFO ColumnChunkPageWriteStore: written 2,671,374B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,255B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:56 INFO ColumnChunkPageWriteStore: written 833,179B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,897B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:54:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000149
15/08/21 08:54:57 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000149_0: Committed
15/08/21 08:54:57 INFO Executor: Finished task 149.0 in stage 1.0 (TID 319). 843 bytes result sent to driver
15/08/21 08:54:57 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 334, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:57 INFO Executor: Running task 164.0 in stage 1.0 (TID 334)
15/08/21 08:54:57 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 319) in 25520 ms on localhost (149/200)
15/08/21 08:54:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:54:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:57 INFO ColumnChunkPageWriteStore: written 2,666,190B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,071B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:57 INFO ColumnChunkPageWriteStore: written 831,826B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,544B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:54:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000148
15/08/21 08:54:57 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000148_0: Committed
15/08/21 08:54:57 INFO Executor: Finished task 148.0 in stage 1.0 (TID 318). 843 bytes result sent to driver
15/08/21 08:54:57 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 335, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:57 INFO Executor: Running task 165.0 in stage 1.0 (TID 335)
15/08/21 08:54:57 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 318) in 30945 ms on localhost (150/200)
15/08/21 08:54:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:54:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:57 INFO ColumnChunkPageWriteStore: written 2,671,251B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,132B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:57 INFO ColumnChunkPageWriteStore: written 833,479B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,197B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:54:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000150
15/08/21 08:54:57 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000150_0: Committed
15/08/21 08:54:57 INFO Executor: Finished task 150.0 in stage 1.0 (TID 320). 843 bytes result sent to driver
15/08/21 08:54:57 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 336, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:57 INFO Executor: Running task 166.0 in stage 1.0 (TID 336)
15/08/21 08:54:57 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 320) in 25339 ms on localhost (151/200)
15/08/21 08:54:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:54:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:57 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:57 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:54:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:57 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:54:57 INFO ColumnChunkPageWriteStore: written 2,671,152B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,033B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:54:57 INFO ColumnChunkPageWriteStore: written 831,567B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,285B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:54:57 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000151
15/08/21 08:54:57 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000151_0: Committed
15/08/21 08:54:57 INFO Executor: Finished task 151.0 in stage 1.0 (TID 321). 843 bytes result sent to driver
15/08/21 08:54:57 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 337, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:54:57 INFO Executor: Running task 167.0 in stage 1.0 (TID 337)
15/08/21 08:54:57 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 321) in 23993 ms on localhost (152/200)
15/08/21 08:54:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:54:57 INFO CodecConfig: Compression: GZIP
15/08/21 08:54:57 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:54:57 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:54:57 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:54:57 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:54:57 INFO ParquetOutputFormat: Validation is off
15/08/21 08:54:57 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:54:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:54:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:54:57 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:02 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:02 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:02 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:02 INFO ColumnChunkPageWriteStore: written 2,671,444B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,325B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:02 INFO ColumnChunkPageWriteStore: written 832,700B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,418B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000152
15/08/21 08:55:02 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000152_0: Committed
15/08/21 08:55:02 INFO Executor: Finished task 152.0 in stage 1.0 (TID 322). 843 bytes result sent to driver
15/08/21 08:55:02 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 338, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:02 INFO Executor: Running task 168.0 in stage 1.0 (TID 338)
15/08/21 08:55:02 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 322) in 29007 ms on localhost (153/200)
15/08/21 08:55:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:03 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:03 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:03 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:03 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:55:03 INFO ColumnChunkPageWriteStore: written 2,665,172B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,053B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:03 INFO ColumnChunkPageWriteStore: written 833,923B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,641B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:55:03 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000153
15/08/21 08:55:03 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000153_0: Committed
15/08/21 08:55:03 INFO Executor: Finished task 153.0 in stage 1.0 (TID 323). 843 bytes result sent to driver
15/08/21 08:55:03 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 339, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:03 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 323) in 28645 ms on localhost (154/200)
15/08/21 08:55:03 INFO Executor: Running task 169.0 in stage 1.0 (TID 339)
15/08/21 08:55:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:03 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:03 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:03 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:03 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:03 INFO ColumnChunkPageWriteStore: written 2,665,303B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,184B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:03 INFO ColumnChunkPageWriteStore: written 834,375B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,093B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:04 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:04 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508210854_0001_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210854_0001_m_000154
15/08/21 08:55:04 INFO SparkHadoopMapRedUtil: attempt_201508210854_0001_m_000154_0: Committed
15/08/21 08:55:04 INFO Executor: Finished task 154.0 in stage 1.0 (TID 324). 843 bytes result sent to driver
15/08/21 08:55:04 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 340, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:04 INFO Executor: Running task 170.0 in stage 1.0 (TID 340)
15/08/21 08:55:04 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 324) in 29038 ms on localhost (155/200)
15/08/21 08:55:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:55:04 INFO ColumnChunkPageWriteStore: written 2,666,080B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,961B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:04 INFO ColumnChunkPageWriteStore: written 834,306B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,024B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:55:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000156
15/08/21 08:55:04 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000156_0: Committed
15/08/21 08:55:04 INFO Executor: Finished task 156.0 in stage 1.0 (TID 326). 843 bytes result sent to driver
15/08/21 08:55:04 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 341, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:04 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 326) in 23717 ms on localhost (156/200)
15/08/21 08:55:04 INFO Executor: Running task 171.0 in stage 1.0 (TID 341)
15/08/21 08:55:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:04 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:04 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:04 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:55:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:05 INFO ColumnChunkPageWriteStore: written 2,671,373B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,254B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:05 INFO ColumnChunkPageWriteStore: written 830,066B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 829,784B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:55:05 INFO ColumnChunkPageWriteStore: written 2,665,144B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,025B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:05 INFO ColumnChunkPageWriteStore: written 832,740B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,458B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000158
15/08/21 08:55:05 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000158_0: Committed
15/08/21 08:55:05 INFO Executor: Finished task 158.0 in stage 1.0 (TID 328). 843 bytes result sent to driver
15/08/21 08:55:05 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 342, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:05 INFO Executor: Running task 172.0 in stage 1.0 (TID 342)
15/08/21 08:55:05 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 328) in 24379 ms on localhost (157/200)
15/08/21 08:55:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000155
15/08/21 08:55:05 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000155_0: Committed
15/08/21 08:55:05 INFO Executor: Finished task 155.0 in stage 1.0 (TID 325). 843 bytes result sent to driver
15/08/21 08:55:05 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 343, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:05 INFO Executor: Running task 173.0 in stage 1.0 (TID 343)
15/08/21 08:55:05 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 325) in 25048 ms on localhost (158/200)
15/08/21 08:55:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:55:05 INFO ColumnChunkPageWriteStore: written 2,671,157B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,038B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:05 INFO ColumnChunkPageWriteStore: written 833,432B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,150B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:55:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000159
15/08/21 08:55:10 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000159_0: Committed
15/08/21 08:55:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:10 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:10 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:10 INFO Executor: Finished task 159.0 in stage 1.0 (TID 329). 843 bytes result sent to driver
15/08/21 08:55:10 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 344, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:10 INFO Executor: Running task 174.0 in stage 1.0 (TID 344)
15/08/21 08:55:10 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 329) in 28607 ms on localhost (159/200)
15/08/21 08:55:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:55:10 INFO ColumnChunkPageWriteStore: written 2,670,940B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,821B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:10 INFO ColumnChunkPageWriteStore: written 833,949B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,667B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:55:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000157
15/08/21 08:55:10 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000157_0: Committed
15/08/21 08:55:10 INFO Executor: Finished task 157.0 in stage 1.0 (TID 327). 843 bytes result sent to driver
15/08/21 08:55:10 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 345, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:10 INFO Executor: Running task 175.0 in stage 1.0 (TID 345)
15/08/21 08:55:10 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 327) in 29369 ms on localhost (160/200)
15/08/21 08:55:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:10 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:10 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:10 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:10 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:11 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:55:11 INFO ColumnChunkPageWriteStore: written 2,671,419B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,300B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:11 INFO ColumnChunkPageWriteStore: written 831,056B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,774B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:55:11 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000160
15/08/21 08:55:11 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000160_0: Committed
15/08/21 08:55:11 INFO Executor: Finished task 160.0 in stage 1.0 (TID 330). 843 bytes result sent to driver
15/08/21 08:55:11 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 346, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:11 INFO Executor: Running task 176.0 in stage 1.0 (TID 346)
15/08/21 08:55:11 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 330) in 29628 ms on localhost (161/200)
15/08/21 08:55:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:55:12 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:12 INFO ColumnChunkPageWriteStore: written 2,664,814B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,695B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:12 INFO ColumnChunkPageWriteStore: written 833,829B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,547B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:55:12 INFO ColumnChunkPageWriteStore: written 2,665,004B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,885B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:12 INFO ColumnChunkPageWriteStore: written 832,594B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,312B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000161
15/08/21 08:55:12 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000161_0: Committed
15/08/21 08:55:12 INFO Executor: Finished task 161.0 in stage 1.0 (TID 331). 843 bytes result sent to driver
15/08/21 08:55:12 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000162
15/08/21 08:55:12 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000162_0: Committed
15/08/21 08:55:12 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 347, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:12 INFO Executor: Finished task 162.0 in stage 1.0 (TID 332). 843 bytes result sent to driver
15/08/21 08:55:12 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 348, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:12 INFO Executor: Running task 177.0 in stage 1.0 (TID 347)
15/08/21 08:55:12 INFO Executor: Running task 178.0 in stage 1.0 (TID 348)
15/08/21 08:55:12 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 332) in 23700 ms on localhost (162/200)
15/08/21 08:55:12 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 331) in 23849 ms on localhost (163/200)
15/08/21 08:55:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:26 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:26 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:26 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:26 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:26 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:26 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:26 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:26 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:26 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:27 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:27 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:27 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:27 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:27 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:27 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:27 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:27 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:27 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:28 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:55:28 INFO ColumnChunkPageWriteStore: written 2,665,751B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,632B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:28 INFO ColumnChunkPageWriteStore: written 833,909B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,627B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:55:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:55:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000164
15/08/21 08:55:32 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000164_0: Committed
15/08/21 08:55:32 INFO Executor: Finished task 164.0 in stage 1.0 (TID 334). 843 bytes result sent to driver
15/08/21 08:55:32 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 349, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:32 INFO Executor: Running task 179.0 in stage 1.0 (TID 349)
15/08/21 08:55:32 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 334) in 35078 ms on localhost (164/200)
15/08/21 08:55:32 INFO ColumnChunkPageWriteStore: written 2,665,184B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,065B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:32 INFO ColumnChunkPageWriteStore: written 833,514B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,232B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:55:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:32 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000163
15/08/21 08:55:32 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000163_0: Committed
15/08/21 08:55:32 INFO Executor: Finished task 163.0 in stage 1.0 (TID 333). 843 bytes result sent to driver
15/08/21 08:55:32 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 350, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:32 INFO Executor: Running task 180.0 in stage 1.0 (TID 350)
15/08/21 08:55:32 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 333) in 36096 ms on localhost (165/200)
15/08/21 08:55:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:55:32 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:32 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:32 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:32 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:32 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:32 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:32 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:33 INFO ColumnChunkPageWriteStore: written 2,671,029B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,910B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:33 INFO ColumnChunkPageWriteStore: written 836,258B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,976B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000165
15/08/21 08:55:33 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000165_0: Committed
15/08/21 08:55:33 INFO Executor: Finished task 165.0 in stage 1.0 (TID 335). 843 bytes result sent to driver
15/08/21 08:55:33 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 351, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:33 INFO Executor: Running task 181.0 in stage 1.0 (TID 351)
15/08/21 08:55:33 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 335) in 36143 ms on localhost (166/200)
15/08/21 08:55:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:33 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:33 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:33 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:33 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:33 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:33 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:33 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:33 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:33 INFO ColumnChunkPageWriteStore: written 2,670,794B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,675B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:33 INFO ColumnChunkPageWriteStore: written 832,013B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,731B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:33 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000166
15/08/21 08:55:33 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000166_0: Committed
15/08/21 08:55:33 INFO Executor: Finished task 166.0 in stage 1.0 (TID 336). 843 bytes result sent to driver
15/08/21 08:55:33 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 352, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:33 INFO Executor: Running task 182.0 in stage 1.0 (TID 352)
15/08/21 08:55:33 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 336) in 36504 ms on localhost (167/200)
15/08/21 08:55:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:34 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:34 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:34 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:34 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:34 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:34 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:34 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:34 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:34 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:34 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:34 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:35 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:35 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:35 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:35 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:35 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:35 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:35 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:55:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:55:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:41 INFO ColumnChunkPageWriteStore: written 2,665,329B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,210B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:41 INFO ColumnChunkPageWriteStore: written 832,235B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,953B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:55:41 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:41 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:41 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:41 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:41 INFO ColumnChunkPageWriteStore: written 2,671,233B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,114B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:41 INFO ColumnChunkPageWriteStore: written 2,671,218B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,099B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:41 INFO ColumnChunkPageWriteStore: written 834,702B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,420B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:55:41 INFO ColumnChunkPageWriteStore: written 832,689B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,407B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:41 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000167
15/08/21 08:55:41 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000167_0: Committed
15/08/21 08:55:41 INFO Executor: Finished task 167.0 in stage 1.0 (TID 337). 843 bytes result sent to driver
15/08/21 08:55:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000170
15/08/21 08:55:41 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000170_0: Committed
15/08/21 08:55:41 INFO Executor: Finished task 170.0 in stage 1.0 (TID 340). 843 bytes result sent to driver
15/08/21 08:55:41 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 353, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:41 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 354, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:41 INFO Executor: Running task 184.0 in stage 1.0 (TID 354)
15/08/21 08:55:41 INFO Executor: Running task 183.0 in stage 1.0 (TID 353)
15/08/21 08:55:41 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 337) in 44240 ms on localhost (168/200)
15/08/21 08:55:41 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 340) in 37911 ms on localhost (169/200)
15/08/21 08:55:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000168
15/08/21 08:55:41 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000168_0: Committed
15/08/21 08:55:41 INFO Executor: Finished task 168.0 in stage 1.0 (TID 338). 843 bytes result sent to driver
15/08/21 08:55:41 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 355, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:42 INFO Executor: Running task 185.0 in stage 1.0 (TID 355)
15/08/21 08:55:42 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 338) in 39141 ms on localhost (170/200)
15/08/21 08:55:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 08:55:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:55:42 INFO ColumnChunkPageWriteStore: written 2,665,025B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,906B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:42 INFO ColumnChunkPageWriteStore: written 831,175B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,893B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:55:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000171
15/08/21 08:55:42 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000171_0: Committed
15/08/21 08:55:42 INFO Executor: Finished task 171.0 in stage 1.0 (TID 341). 843 bytes result sent to driver
15/08/21 08:55:42 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 356, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:42 INFO Executor: Running task 186.0 in stage 1.0 (TID 356)
15/08/21 08:55:42 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 341) in 37953 ms on localhost (171/200)
15/08/21 08:55:42 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 08:55:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:42 INFO ColumnChunkPageWriteStore: written 2,671,354B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,235B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:42 INFO ColumnChunkPageWriteStore: written 833,246B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,964B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:42 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:42 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:42 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:42 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:42 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000174
15/08/21 08:55:43 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000174_0: Committed
15/08/21 08:55:43 INFO Executor: Finished task 174.0 in stage 1.0 (TID 344). 843 bytes result sent to driver
15/08/21 08:55:43 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 357, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:43 INFO Executor: Running task 187.0 in stage 1.0 (TID 357)
15/08/21 08:55:43 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 344) in 32967 ms on localhost (172/200)
15/08/21 08:55:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:55:43 INFO ColumnChunkPageWriteStore: written 2,665,102B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,983B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:43 INFO ColumnChunkPageWriteStore: written 830,633B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,351B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:55:43 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000169
15/08/21 08:55:43 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000169_0: Committed
15/08/21 08:55:43 INFO Executor: Finished task 169.0 in stage 1.0 (TID 339). 843 bytes result sent to driver
15/08/21 08:55:43 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 358, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:43 INFO Executor: Running task 188.0 in stage 1.0 (TID 358)
15/08/21 08:55:43 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 339) in 40174 ms on localhost (173/200)
15/08/21 08:55:43 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,665
15/08/21 08:55:43 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:44 INFO ColumnChunkPageWriteStore: written 2,671,396B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,277B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:44 INFO ColumnChunkPageWriteStore: written 836,094B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,812B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 318 entries, 2,544B raw, 318B comp}
15/08/21 08:55:44 INFO ColumnChunkPageWriteStore: written 2,666,198B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,079B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:44 INFO ColumnChunkPageWriteStore: written 834,962B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,680B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000173
15/08/21 08:55:44 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000173_0: Committed
15/08/21 08:55:44 INFO Executor: Finished task 173.0 in stage 1.0 (TID 343). 843 bytes result sent to driver
15/08/21 08:55:44 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 359, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:44 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 343) in 38672 ms on localhost (174/200)
15/08/21 08:55:44 INFO Executor: Running task 189.0 in stage 1.0 (TID 359)
15/08/21 08:55:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000172
15/08/21 08:55:44 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000172_0: Committed
15/08/21 08:55:44 INFO Executor: Finished task 172.0 in stage 1.0 (TID 342). 843 bytes result sent to driver
15/08/21 08:55:44 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 360, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:44 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 342) in 38821 ms on localhost (175/200)
15/08/21 08:55:44 INFO Executor: Running task 190.0 in stage 1.0 (TID 360)
15/08/21 08:55:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:44 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:44 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,649
15/08/21 08:55:44 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:44 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:44 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:44 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:44 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:44 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:44 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:44 INFO ColumnChunkPageWriteStore: written 2,671,213B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,094B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:44 INFO ColumnChunkPageWriteStore: written 832,436B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,154B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 316 entries, 2,528B raw, 316B comp}
15/08/21 08:55:44 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000175
15/08/21 08:55:44 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000175_0: Committed
15/08/21 08:55:44 INFO Executor: Finished task 175.0 in stage 1.0 (TID 345). 843 bytes result sent to driver
15/08/21 08:55:44 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 361, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:44 INFO Executor: Running task 191.0 in stage 1.0 (TID 361)
15/08/21 08:55:44 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 345) in 34448 ms on localhost (176/200)
15/08/21 08:55:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:55:51 INFO ColumnChunkPageWriteStore: written 2,671,155B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,036B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:51 INFO ColumnChunkPageWriteStore: written 831,754B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,472B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:55:51 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000176
15/08/21 08:55:51 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000176_0: Committed
15/08/21 08:55:51 INFO Executor: Finished task 176.0 in stage 1.0 (TID 346). 843 bytes result sent to driver
15/08/21 08:55:51 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 362, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:51 INFO Executor: Running task 192.0 in stage 1.0 (TID 362)
15/08/21 08:55:51 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 346) in 39900 ms on localhost (177/200)
15/08/21 08:55:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:55:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:55:52 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:55:52 INFO ColumnChunkPageWriteStore: written 2,665,190B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,071B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:52 INFO ColumnChunkPageWriteStore: written 833,011B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,729B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:55:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000178
15/08/21 08:55:52 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000178_0: Committed
15/08/21 08:55:52 INFO Executor: Finished task 178.0 in stage 1.0 (TID 348). 843 bytes result sent to driver
15/08/21 08:55:52 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 363, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:52 INFO Executor: Running task 193.0 in stage 1.0 (TID 363)
15/08/21 08:55:52 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 348) in 40042 ms on localhost (178/200)
15/08/21 08:55:52 INFO ColumnChunkPageWriteStore: written 2,665,184B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,065B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:52 INFO ColumnChunkPageWriteStore: written 835,844B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,562B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:55:52 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000177
15/08/21 08:55:52 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000177_0: Committed
15/08/21 08:55:52 INFO Executor: Finished task 177.0 in stage 1.0 (TID 347). 843 bytes result sent to driver
15/08/21 08:55:52 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 364, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:52 INFO Executor: Running task 194.0 in stage 1.0 (TID 364)
15/08/21 08:55:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:55:52 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 347) in 40147 ms on localhost (179/200)
15/08/21 08:55:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:55:52 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:52 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:52 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:52 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:52 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:52 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:52 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:52 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:52 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:59 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:59 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:59 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:59 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:59 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:55:59 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:55:59 INFO CodecConfig: Compression: GZIP
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:55:59 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:55:59 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:55:59 INFO ParquetOutputFormat: Validation is off
15/08/21 08:55:59 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:55:59 INFO ColumnChunkPageWriteStore: written 2,665,124B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,005B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:55:59 INFO ColumnChunkPageWriteStore: written 834,872B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,590B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:55:59 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:55:59 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000179
15/08/21 08:55:59 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000179_0: Committed
15/08/21 08:55:59 INFO Executor: Finished task 179.0 in stage 1.0 (TID 349). 843 bytes result sent to driver
15/08/21 08:55:59 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 365, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:55:59 INFO Executor: Running task 195.0 in stage 1.0 (TID 365)
15/08/21 08:55:59 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 349) in 27696 ms on localhost (180/200)
15/08/21 08:55:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:55:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:56:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:56:01 INFO ColumnChunkPageWriteStore: written 2,671,236B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,117B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:01 INFO ColumnChunkPageWriteStore: written 833,872B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,590B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:56:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000182
15/08/21 08:56:01 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000182_0: Committed
15/08/21 08:56:01 INFO Executor: Finished task 182.0 in stage 1.0 (TID 352). 843 bytes result sent to driver
15/08/21 08:56:01 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 366, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:56:01 INFO Executor: Running task 196.0 in stage 1.0 (TID 366)
15/08/21 08:56:01 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 352) in 27552 ms on localhost (181/200)
15/08/21 08:56:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,633
15/08/21 08:56:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:56:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/21 08:56:01 INFO ColumnChunkPageWriteStore: written 2,666,139B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,020B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:01 INFO ColumnChunkPageWriteStore: written 833,416B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,134B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 314 entries, 2,512B raw, 314B comp}
15/08/21 08:56:01 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000180
15/08/21 08:56:01 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000180_0: Committed
15/08/21 08:56:01 INFO Executor: Finished task 180.0 in stage 1.0 (TID 350). 843 bytes result sent to driver
15/08/21 08:56:01 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 367, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:56:01 INFO Executor: Running task 197.0 in stage 1.0 (TID 367)
15/08/21 08:56:01 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 350) in 29418 ms on localhost (182/200)
15/08/21 08:56:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:56:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:56:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:01 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:01 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:01 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:01 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:01 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:01 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:01 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:01 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:01 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:56:01 INFO ColumnChunkPageWriteStore: written 2,671,432B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,313B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:01 INFO ColumnChunkPageWriteStore: written 830,734B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,452B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:56:02 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:02 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:02 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:02 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:02 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:02 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:02 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:02 INFO FileOutputCommitter: Saved output of task 'attempt_201508210855_0001_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210855_0001_m_000181
15/08/21 08:56:02 INFO SparkHadoopMapRedUtil: attempt_201508210855_0001_m_000181_0: Committed
15/08/21 08:56:02 INFO Executor: Finished task 181.0 in stage 1.0 (TID 351). 843 bytes result sent to driver
15/08/21 08:56:02 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 368, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:56:07 INFO Executor: Running task 198.0 in stage 1.0 (TID 368)
15/08/21 08:56:07 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 351) in 33889 ms on localhost (183/200)
15/08/21 08:56:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:56:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:56:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:07 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:07 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:07 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:07 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:07 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:08 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:08 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:08 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:08 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:08 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:56:09 INFO ColumnChunkPageWriteStore: written 2,664,952B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,833B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:09 INFO ColumnChunkPageWriteStore: written 835,727B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,445B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:56:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:56:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:09 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:09 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000186
15/08/21 08:56:09 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000186_0: Committed
15/08/21 08:56:09 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:09 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:09 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:09 INFO Executor: Finished task 186.0 in stage 1.0 (TID 356). 843 bytes result sent to driver
15/08/21 08:56:09 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 369, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/21 08:56:09 INFO Executor: Running task 199.0 in stage 1.0 (TID 369)
15/08/21 08:56:09 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 356) in 26858 ms on localhost (184/200)
15/08/21 08:56:09 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:09 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:09 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:09 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:09 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:09 INFO ColumnChunkPageWriteStore: written 2,671,394B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,275B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:09 INFO ColumnChunkPageWriteStore: written 833,536B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,254B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:56:09 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 08:56:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/21 08:56:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000183
15/08/21 08:56:09 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000183_0: Committed
15/08/21 08:56:09 INFO Executor: Finished task 183.0 in stage 1.0 (TID 353). 843 bytes result sent to driver
15/08/21 08:56:09 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 353) in 27560 ms on localhost (185/200)
15/08/21 08:56:09 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:56:09 INFO ColumnChunkPageWriteStore: written 2,665,090B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,971B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:09 INFO ColumnChunkPageWriteStore: written 832,815B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,533B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:56:09 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000185
15/08/21 08:56:09 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000185_0: Committed
15/08/21 08:56:09 INFO Executor: Finished task 185.0 in stage 1.0 (TID 355). 843 bytes result sent to driver
15/08/21 08:56:09 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 355) in 27724 ms on localhost (186/200)
15/08/21 08:56:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:56:10 INFO ColumnChunkPageWriteStore: written 2,671,254B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,135B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:10 INFO ColumnChunkPageWriteStore: written 835,132B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,850B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:56:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000184
15/08/21 08:56:10 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000184_0: Committed
15/08/21 08:56:10 INFO Executor: Finished task 184.0 in stage 1.0 (TID 354). 843 bytes result sent to driver
15/08/21 08:56:10 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 354) in 28217 ms on localhost (187/200)
15/08/21 08:56:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:56:10 INFO ColumnChunkPageWriteStore: written 2,665,171B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,052B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:10 INFO ColumnChunkPageWriteStore: written 834,877B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,595B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:56:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000187
15/08/21 08:56:10 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000187_0: Committed
15/08/21 08:56:10 INFO Executor: Finished task 187.0 in stage 1.0 (TID 357). 843 bytes result sent to driver
15/08/21 08:56:10 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 357) in 27337 ms on localhost (188/200)
15/08/21 08:56:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:56:10 INFO ColumnChunkPageWriteStore: written 2,666,015B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,896B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:10 INFO ColumnChunkPageWriteStore: written 834,611B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,329B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:56:10 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000188
15/08/21 08:56:10 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000188_0: Committed
15/08/21 08:56:10 INFO Executor: Finished task 188.0 in stage 1.0 (TID 358). 843 bytes result sent to driver
15/08/21 08:56:10 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 358) in 26852 ms on localhost (189/200)
15/08/21 08:56:10 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:10 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:10 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:10 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:15 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:15 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:56:16 INFO ColumnChunkPageWriteStore: written 2,671,442B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,323B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:16 INFO ColumnChunkPageWriteStore: written 833,079B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,797B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:56:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:16 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:16 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000190
15/08/21 08:56:16 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000190_0: Committed
15/08/21 08:56:16 INFO Executor: Finished task 190.0 in stage 1.0 (TID 360). 843 bytes result sent to driver
15/08/21 08:56:16 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 360) in 32138 ms on localhost (190/200)
15/08/21 08:56:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:56:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:56:16 INFO ColumnChunkPageWriteStore: written 2,671,529B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,410B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:16 INFO ColumnChunkPageWriteStore: written 833,348B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,066B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:56:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000191
15/08/21 08:56:16 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000191_0: Committed
15/08/21 08:56:16 INFO Executor: Finished task 191.0 in stage 1.0 (TID 361). 843 bytes result sent to driver
15/08/21 08:56:16 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 361) in 31851 ms on localhost (191/200)
15/08/21 08:56:16 INFO ColumnChunkPageWriteStore: written 2,671,450B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,331B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:16 INFO ColumnChunkPageWriteStore: written 831,022B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 830,740B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:56:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000189
15/08/21 08:56:16 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000189_0: Committed
15/08/21 08:56:16 INFO Executor: Finished task 189.0 in stage 1.0 (TID 359). 843 bytes result sent to driver
15/08/21 08:56:16 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 359) in 32624 ms on localhost (192/200)
15/08/21 08:56:16 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:16 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:16 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:16 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:16 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:17 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:17 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:17 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:17 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:17 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,617
15/08/21 08:56:17 INFO ColumnChunkPageWriteStore: written 2,671,177B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,058B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:17 INFO ColumnChunkPageWriteStore: written 835,289B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 835,007B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 312 entries, 2,496B raw, 312B comp}
15/08/21 08:56:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000192
15/08/21 08:56:17 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000192_0: Committed
15/08/21 08:56:17 INFO Executor: Finished task 192.0 in stage 1.0 (TID 362). 843 bytes result sent to driver
15/08/21 08:56:17 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 362) in 26768 ms on localhost (193/200)
15/08/21 08:56:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,609
15/08/21 08:56:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:18 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:18 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:18 INFO ColumnChunkPageWriteStore: written 2,665,340B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,665,221B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:18 INFO ColumnChunkPageWriteStore: written 831,556B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 831,274B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 311 entries, 2,488B raw, 311B comp}
15/08/21 08:56:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000194
15/08/21 08:56:18 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000194_0: Committed
15/08/21 08:56:18 INFO Executor: Finished task 194.0 in stage 1.0 (TID 364). 843 bytes result sent to driver
15/08/21 08:56:18 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 364) in 26195 ms on localhost (194/200)
15/08/21 08:56:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:18 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:18 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:18 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:18 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:18 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:18 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:18 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:56:19 INFO ColumnChunkPageWriteStore: written 2,664,917B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,798B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:19 INFO ColumnChunkPageWriteStore: written 833,921B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,639B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:56:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000193
15/08/21 08:56:19 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000193_0: Committed
15/08/21 08:56:19 INFO Executor: Finished task 193.0 in stage 1.0 (TID 363). 843 bytes result sent to driver
15/08/21 08:56:19 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 363) in 26820 ms on localhost (195/200)
15/08/21 08:56:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:56:19 INFO ColumnChunkPageWriteStore: written 2,665,052B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,664,933B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:19 INFO ColumnChunkPageWriteStore: written 834,559B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 834,277B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:56:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000195
15/08/21 08:56:19 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000195_0: Committed
15/08/21 08:56:19 INFO Executor: Finished task 195.0 in stage 1.0 (TID 365). 843 bytes result sent to driver
15/08/21 08:56:19 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 365) in 20133 ms on localhost (196/200)
15/08/21 08:56:20 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 08:56:20 INFO CodecConfig: Compression: GZIP
15/08/21 08:56:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 08:56:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 08:56:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 08:56:20 INFO ParquetOutputFormat: Dictionary is on
15/08/21 08:56:20 INFO ParquetOutputFormat: Validation is off
15/08/21 08:56:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 08:56:20 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 08:56:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,641
15/08/21 08:56:20 INFO ColumnChunkPageWriteStore: written 2,666,316B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,666,197B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:20 INFO ColumnChunkPageWriteStore: written 832,995B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,713B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 315 entries, 2,520B raw, 315B comp}
15/08/21 08:56:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,601
15/08/21 08:56:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000196
15/08/21 08:56:20 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000196_0: Committed
15/08/21 08:56:20 INFO Executor: Finished task 196.0 in stage 1.0 (TID 366). 843 bytes result sent to driver
15/08/21 08:56:20 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 366) in 19285 ms on localhost (197/200)
15/08/21 08:56:20 INFO ColumnChunkPageWriteStore: written 2,671,199B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,080B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:20 INFO ColumnChunkPageWriteStore: written 832,452B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,170B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 310 entries, 2,480B raw, 310B comp}
15/08/21 08:56:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000197
15/08/21 08:56:20 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000197_0: Committed
15/08/21 08:56:20 INFO Executor: Finished task 197.0 in stage 1.0 (TID 367). 843 bytes result sent to driver
15/08/21 08:56:20 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 367) in 19126 ms on localhost (198/200)
15/08/21 08:56:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,625
15/08/21 08:56:20 INFO ColumnChunkPageWriteStore: written 2,671,458B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,671,339B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:21 INFO ColumnChunkPageWriteStore: written 833,958B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 833,676B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 313 entries, 2,504B raw, 313B comp}
15/08/21 08:56:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000198
15/08/21 08:56:21 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000198_0: Committed
15/08/21 08:56:21 INFO Executor: Finished task 198.0 in stage 1.0 (TID 368). 843 bytes result sent to driver
15/08/21 08:56:21 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 368) in 18946 ms on localhost (199/200)
15/08/21 08:56:22 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 28,574,657
15/08/21 08:56:22 INFO ColumnChunkPageWriteStore: written 2,671,110B for [l_orderkey] INT32: 750,000 values, 3,000,024B raw, 2,670,991B comp, 3 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 08:56:22 INFO ColumnChunkPageWriteStore: written 832,892B for [t_sum_quantity] DOUBLE: 750,000 values, 845,342B raw, 832,610B comp, 6 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 317 entries, 2,536B raw, 317B comp}
15/08/21 08:56:22 INFO FileOutputCommitter: Saved output of task 'attempt_201508210856_0001_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_temporary/0/task_201508210856_0001_m_000199
15/08/21 08:56:22 INFO SparkHadoopMapRedUtil: attempt_201508210856_0001_m_000199_0: Committed
15/08/21 08:56:22 INFO Executor: Finished task 199.0 in stage 1.0 (TID 369). 843 bytes result sent to driver
15/08/21 08:56:22 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 369) in 12842 ms on localhost (200/200)
15/08/21 08:56:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/21 08:56:22 INFO DAGScheduler: ResultStage 1 (processCmd at CliDriver.java:423) finished in 430.289 s
15/08/21 08:56:22 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@494eafa6
15/08/21 08:56:22 INFO StatsReportListener: task runtime:(count: 200, mean: 33983.390000, stdev: 7621.036678, max: 51624.000000, min: 12842.000000)
15/08/21 08:56:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:22 INFO StatsReportListener: 	12.8 s	24.4 s	25.4 s	28.6 s	33.1 s	38.8 s	42.4 s	51.0 s	51.6 s
15/08/21 08:56:22 INFO StatsReportListener: fetch wait time:(count: 200, mean: 1.190000, stdev: 1.744678, max: 12.000000, min: 0.000000)
15/08/21 08:56:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:22 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	3.0 ms	4.0 ms	12.0 ms
15/08/21 08:56:22 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 08:56:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:22 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 08:56:22 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/21 08:56:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:22 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/21 08:56:22 INFO DAGScheduler: Job 0 finished: processCmd at CliDriver.java:423, took 845.000410 s
15/08/21 08:56:22 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 99.365946, stdev: 2.425883, max: 99.892997, min: 72.875541)
15/08/21 08:56:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:22 INFO StatsReportListener: 	73 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 08:56:22 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.003620, stdev: 0.005702, max: 0.038778, min: 0.000000)
15/08/21 08:56:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:22 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 08:56:22 INFO StatsReportListener: other time pct: (count: 200, mean: 0.630433, stdev: 2.424463, max: 27.092790, min: 0.107003)
15/08/21 08:56:22 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:22 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	27 %
15/08/21 08:56:23 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 08:56:23 INFO DefaultWriterContainer: Job job_201508210842_0000 committed.
15/08/21 08:56:23 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 08:56:23 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/_common_metadata
15/08/21 08:56:23 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 08:56:23 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 08:56:23 INFO DAGScheduler: Final stage: ResultStage 2(processCmd at CliDriver.java:423)
15/08/21 08:56:23 INFO DAGScheduler: Parents of final stage: List()
15/08/21 08:56:23 INFO DAGScheduler: Missing parents: List()
15/08/21 08:56:23 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 08:56:23 INFO MemoryStore: ensureFreeSpace(2976) called with curMem=458258, maxMem=22226833244
15/08/21 08:56:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 20.7 GB)
15/08/21 08:56:23 INFO MemoryStore: ensureFreeSpace(1784) called with curMem=461234, maxMem=22226833244
15/08/21 08:56:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1784.0 B, free 20.7 GB)
15/08/21 08:56:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:51693 (size: 1784.0 B, free: 20.7 GB)
15/08/21 08:56:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/21 08:56:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423)
15/08/21 08:56:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/21 08:56:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 370, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 08:56:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 370)
15/08/21 08:56:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 370). 606 bytes result sent to driver
15/08/21 08:56:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 370) in 43 ms on localhost (1/1)
15/08/21 08:56:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/21 08:56:23 INFO DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:423) finished in 0.045 s
15/08/21 08:56:23 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@683222d
15/08/21 08:56:23 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 0.063590 s
15/08/21 08:56:23 INFO StatsReportListener: task runtime:(count: 1, mean: 43.000000, stdev: 0.000000, max: 43.000000, min: 43.000000)
15/08/21 08:56:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:23 INFO StatsReportListener: 	43.0 ms	43.0 ms	43.0 ms	43.0 ms	43.0 ms	43.0 ms	43.0 ms	43.0 ms	43.0 ms
15/08/21 08:56:23 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 08:56:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:23 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/21 08:56:23 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 39.534884, stdev: 0.000000, max: 39.534884, min: 39.534884)
15/08/21 08:56:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:23 INFO StatsReportListener: 	40 %	40 %	40 %	40 %	40 %	40 %	40 %	40 %	40 %
15/08/21 08:56:23 INFO StatsReportListener: other time pct: (count: 1, mean: 60.465116, stdev: 0.000000, max: 60.465116, min: 60.465116)
15/08/21 08:56:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:23 INFO StatsReportListener: 	60 %	60 %	60 %	60 %	60 %	60 %	60 %	60 %	60 %
Time taken: 851.13 seconds
15/08/21 08:56:23 INFO CliDriver: Time taken: 851.13 seconds
15/08/21 08:56:24 INFO ParseDriver: Parsing command: insert into table q18_large_volume_customer_par
select 
  c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice,sum(l_quantity)
from 
  customer_par c join orders_par o 
  on 
    c.c_custkey = o.o_custkey
  join q18_tmp_par t 
  on 
    o.o_orderkey = t.l_orderkey and t.t_sum_quantity > 315
  join lineitem_par l 
  on 
    o.o_orderkey = l.l_orderkey
group by c_name,c_custkey,o_orderkey,o_orderdate,o_totalprice
order by o_totalprice desc,o_orderdate
limit 100
15/08/21 08:56:24 INFO ParseDriver: Parse Completed
15/08/21 08:56:24 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 08:56:24 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 08:56:24 INFO ParquetFileReader: reading another 10 footers
15/08/21 08:56:24 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 08:56:24 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 08:56:24 INFO ParquetFileReader: reading another 19 footers
15/08/21 08:56:24 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 08:56:24 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=463018, maxMem=22226833244
15/08/21 08:56:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 08:56:24 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=789626, maxMem=22226833244
15/08/21 08:56:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 08:56:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:51693 (size: 22.3 KB, free: 20.7 GB)
15/08/21 08:56:24 INFO SparkContext: Created broadcast 4 from processCmd at CliDriver.java:423
15/08/21 08:56:24 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=812419, maxMem=22226833244
15/08/21 08:56:24 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 08:56:24 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1139027, maxMem=22226833244
15/08/21 08:56:24 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 08:56:24 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:51693 (size: 22.3 KB, free: 20.7 GB)
15/08/21 08:56:24 INFO SparkContext: Created broadcast 5 from processCmd at CliDriver.java:423
15/08/21 08:56:24 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1161820, maxMem=22226833244
15/08/21 08:56:24 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 08:56:24 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1488428, maxMem=22226833244
15/08/21 08:56:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 08:56:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:51693 (size: 22.3 KB, free: 20.7 GB)
15/08/21 08:56:24 INFO SparkContext: Created broadcast 6 from processCmd at CliDriver.java:423
15/08/21 08:56:24 INFO MemoryStore: ensureFreeSpace(326608) called with curMem=1511221, maxMem=22226833244
15/08/21 08:56:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 319.0 KB, free 20.7 GB)
15/08/21 08:56:25 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1837829, maxMem=22226833244
15/08/21 08:56:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 22.3 KB, free 20.7 GB)
15/08/21 08:56:25 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:51693 (size: 22.3 KB, free: 20.7 GB)
15/08/21 08:56:25 INFO SparkContext: Created broadcast 7 from processCmd at CliDriver.java:423
15/08/21 08:56:25 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 08:56:25 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 08:56:25 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 08:56:25 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 08:56:25 INFO Exchange: Using SparkSqlSerializer2.
15/08/21 08:56:25 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 08:56:25 INFO DAGScheduler: Registering RDD 25 (processCmd at CliDriver.java:423)
15/08/21 08:56:25 INFO DAGScheduler: Registering RDD 28 (processCmd at CliDriver.java:423)
15/08/21 08:56:25 INFO DAGScheduler: Registering RDD 33 (processCmd at CliDriver.java:423)
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 08:56:25 INFO DAGScheduler: Registering RDD 22 (processCmd at CliDriver.java:423)
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/21 08:56:25 INFO DAGScheduler: Registering RDD 17 (processCmd at CliDriver.java:423)
15/08/21 08:56:25 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/21 08:56:25 INFO DAGScheduler: Final stage: ResultStage 8(processCmd at CliDriver.java:423)
15/08/21 08:56:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 08:56:25 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 08:56:25 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 08:56:25 INFO MemoryStore: ensureFreeSpace(6632) called with curMem=1860622, maxMem=22226833244
15/08/21 08:56:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.5 KB, free 20.7 GB)
15/08/21 08:56:25 INFO MemoryStore: ensureFreeSpace(3624) called with curMem=1867254, maxMem=22226833244
15/08/21 08:56:25 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.5 KB, free 20.7 GB)
15/08/21 08:56:25 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:51693 (size: 3.5 KB, free: 20.7 GB)
15/08/21 08:56:25 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874
15/08/21 08:56:25 INFO DAGScheduler: Submitting 19 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423)
15/08/21 08:56:25 INFO TaskSchedulerImpl: Adding task set 3.0 with 19 tasks
15/08/21 08:56:25 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 371, localhost, ANY, 1706 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 372, localhost, ANY, 1715 bytes)
15/08/21 08:56:25 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 08:56:25 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 373, localhost, ANY, 1709 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 374, localhost, ANY, 1706 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 375, localhost, ANY, 1714 bytes)
15/08/21 08:56:25 INFO MemoryStore: ensureFreeSpace(6728) called with curMem=1870878, maxMem=22226833244
15/08/21 08:56:25 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 6.6 KB, free 20.7 GB)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 376, localhost, ANY, 1706 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 377, localhost, ANY, 1714 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 378, localhost, ANY, 1706 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 379, localhost, ANY, 1713 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 380, localhost, ANY, 1705 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 381, localhost, ANY, 1715 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 382, localhost, ANY, 1706 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 383, localhost, ANY, 1716 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 384, localhost, ANY, 1706 bytes)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 385, localhost, ANY, 1713 bytes)
15/08/21 08:56:25 INFO MemoryStore: ensureFreeSpace(3650) called with curMem=1877606, maxMem=22226833244
15/08/21 08:56:25 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.6 KB, free 20.7 GB)
15/08/21 08:56:25 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 386, localhost, ANY, 1706 bytes)
15/08/21 08:56:25 INFO Executor: Running task 1.0 in stage 3.0 (TID 372)
15/08/21 08:56:25 INFO Executor: Running task 0.0 in stage 3.0 (TID 371)
15/08/21 08:56:25 INFO Executor: Running task 7.0 in stage 3.0 (TID 378)
15/08/21 08:56:25 INFO Executor: Running task 10.0 in stage 3.0 (TID 381)
15/08/21 08:56:25 INFO Executor: Running task 8.0 in stage 3.0 (TID 379)
15/08/21 08:56:25 INFO Executor: Running task 4.0 in stage 3.0 (TID 375)
15/08/21 08:56:25 INFO Executor: Running task 3.0 in stage 3.0 (TID 374)
15/08/21 08:56:25 INFO Executor: Running task 2.0 in stage 3.0 (TID 373)
15/08/21 08:56:25 INFO Executor: Running task 5.0 in stage 3.0 (TID 376)
15/08/21 08:56:25 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:51693 (size: 3.6 KB, free: 20.7 GB)
15/08/21 08:56:25 INFO Executor: Running task 14.0 in stage 3.0 (TID 385)
15/08/21 08:56:25 INFO Executor: Running task 9.0 in stage 3.0 (TID 380)
15/08/21 08:56:25 INFO Executor: Running task 6.0 in stage 3.0 (TID 377)
15/08/21 08:56:25 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:874
15/08/21 08:56:25 INFO Executor: Running task 12.0 in stage 3.0 (TID 383)
15/08/21 08:56:25 INFO Executor: Running task 13.0 in stage 3.0 (TID 384)
15/08/21 08:56:25 INFO Executor: Running task 11.0 in stage 3.0 (TID 382)
15/08/21 08:56:25 INFO Executor: Running task 15.0 in stage 3.0 (TID 386)
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000000_0 start: 134217728 end: 138596223 length: 4378495 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000006_0 start: 134217728 end: 137279350 length: 3061622 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:25 INFO DAGScheduler: Submitting 57 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423)
15/08/21 08:56:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:25 INFO TaskSchedulerImpl: Adding task set 4.0 with 57 tasks
15/08/21 08:56:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000009_0 start: 0 end: 24310349 length: 24310349 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000007_0 start: 134217728 end: 137178560 length: 2960832 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000004_0 start: 134217728 end: 138024796 length: 3807068 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000002_0 start: 134217728 end: 138011074 length: 3793346 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000001_0 start: 134217728 end: 138025614 length: 3807886 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000005_0 start: 134217728 end: 138026641 length: 3808913 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 68086 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 83694 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560003 records.
15/08/21 08:56:26 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[22] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560031 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 67074 records.
15/08/21 08:56:26 INFO MemoryStore: ensureFreeSpace(7320) called with curMem=1881256, maxMem=22226833244
15/08/21 08:56:26 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.1 KB, free 20.7 GB)
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560100 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560144 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1559896 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76962 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 288335 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 77014 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560462 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560128 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO MemoryStore: ensureFreeSpace(3874) called with curMem=1888576, maxMem=22226833244
15/08/21 08:56:26 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.8 KB, free 20.7 GB)
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560100 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76866 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:51693 (size: 3.8 KB, free: 20.7 GB)
15/08/21 08:56:26 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:874
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76861 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 67074
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 78 ms. row count = 68086
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 83694
15/08/21 08:56:26 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[22] at processCmd at CliDriver.java:423)
15/08/21 08:56:26 INFO TaskSchedulerImpl: Adding task set 6.0 with 200 tasks
15/08/21 08:56:26 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[17] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 08:56:26 INFO MemoryStore: ensureFreeSpace(6952) called with curMem=1892450, maxMem=22226833244
15/08/21 08:56:26 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 6.8 KB, free 20.7 GB)
15/08/21 08:56:26 INFO MemoryStore: ensureFreeSpace(3749) called with curMem=1899402, maxMem=22226833244
15/08/21 08:56:26 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KB, free 20.7 GB)
15/08/21 08:56:26 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:51693 (size: 3.7 KB, free: 20.7 GB)
15/08/21 08:56:26 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:874
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 76866
15/08/21 08:56:26 INFO DAGScheduler: Submitting 170 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[17] at processCmd at CliDriver.java:423)
15/08/21 08:56:26 INFO TaskSchedulerImpl: Adding task set 7.0 with 170 tasks
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 248 ms. row count = 76962
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 226 ms. row count = 76861
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 294 ms. row count = 288335
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 323 ms. row count = 77014
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 435 ms. row count = 1560100
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 495 ms. row count = 1560003
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 500 ms. row count = 1560128
15/08/21 08:56:26 INFO Executor: Finished task 14.0 in stage 3.0 (TID 385). 2125 bytes result sent to driver
15/08/21 08:56:26 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 387, localhost, ANY, 1714 bytes)
15/08/21 08:56:26 INFO Executor: Running task 16.0 in stage 3.0 (TID 387)
15/08/21 08:56:26 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 385) in 648 ms on localhost (1/19)
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000008_0 start: 134217728 end: 137173148 length: 2955420 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 583 ms. row count = 1560462
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 66882 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO Executor: Finished task 6.0 in stage 3.0 (TID 377). 2125 bytes result sent to driver
15/08/21 08:56:26 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 388, localhost, ANY, 1706 bytes)
15/08/21 08:56:26 INFO Executor: Running task 17.0 in stage 3.0 (TID 388)
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 377) in 757 ms on localhost (2/19)
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1560365 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 109 ms. row count = 66882
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 699 ms. row count = 1560100
15/08/21 08:56:26 INFO Executor: Finished task 12.0 in stage 3.0 (TID 383). 2125 bytes result sent to driver
15/08/21 08:56:26 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 389, localhost, ANY, 1712 bytes)
15/08/21 08:56:26 INFO Executor: Running task 18.0 in stage 3.0 (TID 389)
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 797 ms. row count = 1560144
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/customer_par/000003_0 start: 134217728 end: 138036519 length: 3818791 hosts: [] requestedSchema: message root {
  optional int32 c_custkey;
  optional binary c_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}},{"name":"c_address","type":"string","nullable":true,"metadata":{}},{"name":"c_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_phone","type":"string","nullable":true,"metadata":{}},{"name":"c_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"c_mktsegment","type":"string","nullable":true,"metadata":{}},{"name":"c_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"c_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"c_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 383) in 876 ms on localhost (3/19)
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 76997 records.
15/08/21 08:56:26 INFO Executor: Finished task 10.0 in stage 3.0 (TID 381). 2125 bytes result sent to driver
15/08/21 08:56:26 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 390, localhost, ANY, 1740 bytes)
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO Executor: Running task 0.0 in stage 4.0 (TID 390)
15/08/21 08:56:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:26 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 381) in 937 ms on localhost (4/19)
15/08/21 08:56:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 76997
15/08/21 08:56:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061701 records.
15/08/21 08:56:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:26 INFO InternalParquetRecordReader: block read in memory in 909 ms. row count = 1559896
15/08/21 08:56:26 INFO Executor: Finished task 4.0 in stage 3.0 (TID 375). 2125 bytes result sent to driver
15/08/21 08:56:27 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 391, localhost, ANY, 1746 bytes)
15/08/21 08:56:27 INFO Executor: Running task 1.0 in stage 4.0 (TID 391)
15/08/21 08:56:27 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 375) in 1043 ms on localhost (5/19)
15/08/21 08:56:27 INFO Executor: Finished task 8.0 in stage 3.0 (TID 379). 2125 bytes result sent to driver
15/08/21 08:56:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:27 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 392, localhost, ANY, 1752 bytes)
15/08/21 08:56:27 INFO Executor: Running task 2.0 in stage 4.0 (TID 392)
15/08/21 08:56:27 INFO Executor: Finished task 1.0 in stage 3.0 (TID 372). 2125 bytes result sent to driver
15/08/21 08:56:27 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 393, localhost, ANY, 1739 bytes)
15/08/21 08:56:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:27 INFO Executor: Running task 3.0 in stage 4.0 (TID 393)
15/08/21 08:56:27 INFO InternalParquetRecordReader: block read in memory in 988 ms. row count = 1560031
15/08/21 08:56:27 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 379) in 1054 ms on localhost (6/19)
15/08/21 08:56:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000008_0 start: 268435456 end: 340165230 length: 71729774 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:27 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 372) in 1067 ms on localhost (7/19)
15/08/21 08:56:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061449 records.
15/08/21 08:56:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061593 records.
15/08/21 08:56:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1785023 records.
15/08/21 08:56:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:27 INFO Executor: Finished task 16.0 in stage 3.0 (TID 387). 2125 bytes result sent to driver
15/08/21 08:56:27 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 394, localhost, ANY, 1745 bytes)
15/08/21 08:56:27 INFO Executor: Running task 4.0 in stage 4.0 (TID 394)
15/08/21 08:56:27 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 387) in 649 ms on localhost (8/19)
15/08/21 08:56:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061600 records.
15/08/21 08:56:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:27 INFO Executor: Finished task 18.0 in stage 3.0 (TID 389). 2125 bytes result sent to driver
15/08/21 08:56:27 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 395, localhost, ANY, 1756 bytes)
15/08/21 08:56:27 INFO Executor: Running task 5.0 in stage 4.0 (TID 395)
15/08/21 08:56:27 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 389) in 623 ms on localhost (9/19)
15/08/21 08:56:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000000_0 start: 268435456 end: 344047583 length: 75612127 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1875761 records.
15/08/21 08:56:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:27 INFO Executor: Finished task 2.0 in stage 3.0 (TID 373). 2125 bytes result sent to driver
15/08/21 08:56:27 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 396, localhost, ANY, 1741 bytes)
15/08/21 08:56:27 INFO Executor: Running task 6.0 in stage 4.0 (TID 396)
15/08/21 08:56:27 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 373) in 1759 ms on localhost (10/19)
15/08/21 08:56:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:27 INFO InternalParquetRecordReader: block read in memory in 966 ms. row count = 1560365
15/08/21 08:56:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061554 records.
15/08/21 08:56:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:51693 in memory (size: 1784.0 B, free: 20.7 GB)
15/08/21 08:56:32 INFO InternalParquetRecordReader: block read in memory in 5859 ms. row count = 1785023
15/08/21 08:56:33 INFO InternalParquetRecordReader: block read in memory in 6888 ms. row count = 3061593
15/08/21 08:56:34 INFO InternalParquetRecordReader: block read in memory in 6506 ms. row count = 1875761
15/08/21 08:56:34 INFO InternalParquetRecordReader: block read in memory in 7104 ms. row count = 3061449
15/08/21 08:56:34 INFO InternalParquetRecordReader: block read in memory in 7350 ms. row count = 3061701
15/08/21 08:56:34 INFO InternalParquetRecordReader: block read in memory in 6826 ms. row count = 3061554
15/08/21 08:56:34 INFO InternalParquetRecordReader: block read in memory in 7351 ms. row count = 3061600
15/08/21 08:56:35 INFO Executor: Finished task 9.0 in stage 3.0 (TID 380). 2125 bytes result sent to driver
15/08/21 08:56:35 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 397, localhost, ANY, 1746 bytes)
15/08/21 08:56:35 INFO Executor: Running task 7.0 in stage 4.0 (TID 397)
15/08/21 08:56:35 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 380) in 9678 ms on localhost (11/19)
15/08/21 08:56:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061231 records.
15/08/21 08:56:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:36 INFO InternalParquetRecordReader: block read in memory in 601 ms. row count = 3061231
15/08/21 08:56:36 INFO Executor: Finished task 5.0 in stage 3.0 (TID 376). 2125 bytes result sent to driver
15/08/21 08:56:36 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 398, localhost, ANY, 1752 bytes)
15/08/21 08:56:36 INFO Executor: Running task 8.0 in stage 4.0 (TID 398)
15/08/21 08:56:36 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 376) in 10704 ms on localhost (12/19)
15/08/21 08:56:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000003_0 start: 268435456 end: 340526195 length: 72090739 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1793520 records.
15/08/21 08:56:36 INFO Executor: Finished task 13.0 in stage 3.0 (TID 384). 2125 bytes result sent to driver
15/08/21 08:56:36 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 399, localhost, ANY, 1742 bytes)
15/08/21 08:56:36 INFO Executor: Running task 9.0 in stage 4.0 (TID 399)
15/08/21 08:56:36 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 384) in 10747 ms on localhost (13/19)
15/08/21 08:56:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061301 records.
15/08/21 08:56:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:36 INFO Executor: Finished task 11.0 in stage 3.0 (TID 382). 2125 bytes result sent to driver
15/08/21 08:56:36 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 400, localhost, ANY, 1747 bytes)
15/08/21 08:56:36 INFO Executor: Running task 10.0 in stage 4.0 (TID 400)
15/08/21 08:56:36 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 382) in 10930 ms on localhost (14/19)
15/08/21 08:56:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061650 records.
15/08/21 08:56:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:37 INFO Executor: Finished task 7.0 in stage 3.0 (TID 378). 2125 bytes result sent to driver
15/08/21 08:56:37 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 401, localhost, ANY, 1753 bytes)
15/08/21 08:56:37 INFO Executor: Running task 11.0 in stage 4.0 (TID 401)
15/08/21 08:56:37 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 378) in 11110 ms on localhost (15/19)
15/08/21 08:56:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000015_0 start: 268435456 end: 340154417 length: 71718961 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784493 records.
15/08/21 08:56:37 INFO Executor: Finished task 15.0 in stage 3.0 (TID 386). 2125 bytes result sent to driver
15/08/21 08:56:37 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 402, localhost, ANY, 1742 bytes)
15/08/21 08:56:37 INFO Executor: Running task 12.0 in stage 4.0 (TID 402)
15/08/21 08:56:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:37 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 386) in 11135 ms on localhost (16/19)
15/08/21 08:56:37 INFO InternalParquetRecordReader: block read in memory in 369 ms. row count = 1793520
15/08/21 08:56:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:37 INFO Executor: Finished task 3.0 in stage 3.0 (TID 374). 2125 bytes result sent to driver
15/08/21 08:56:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061813 records.
15/08/21 08:56:37 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 403, localhost, ANY, 1747 bytes)
15/08/21 08:56:37 INFO Executor: Running task 13.0 in stage 4.0 (TID 403)
15/08/21 08:56:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:37 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 374) in 11188 ms on localhost (17/19)
15/08/21 08:56:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:37 INFO Executor: Finished task 17.0 in stage 3.0 (TID 388). 2125 bytes result sent to driver
15/08/21 08:56:37 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 404, localhost, ANY, 1754 bytes)
15/08/21 08:56:37 INFO Executor: Running task 14.0 in stage 4.0 (TID 404)
15/08/21 08:56:37 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 388) in 10473 ms on localhost (18/19)
15/08/21 08:56:37 INFO Executor: Finished task 0.0 in stage 3.0 (TID 371). 2125 bytes result sent to driver
15/08/21 08:56:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000016_0 start: 268435456 end: 340147334 length: 71711878 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:37 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 405, localhost, ANY, 1740 bytes)
15/08/21 08:56:37 INFO Executor: Running task 15.0 in stage 4.0 (TID 405)
15/08/21 08:56:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061669 records.
15/08/21 08:56:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:37 INFO DAGScheduler: ShuffleMapStage 3 (processCmd at CliDriver.java:423) finished in 11.253 s
15/08/21 08:56:37 INFO DAGScheduler: looking for newly runnable stages
15/08/21 08:56:37 INFO DAGScheduler: running: Set(ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 4)
15/08/21 08:56:37 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/21 08:56:37 INFO DAGScheduler: failed: Set()
15/08/21 08:56:37 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1ddf9542
15/08/21 08:56:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784858 records.
15/08/21 08:56:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:37 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 371) in 11252 ms on localhost (19/19)
15/08/21 08:56:37 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/21 08:56:37 INFO StatsReportListener: task runtime:(count: 19, mean: 5612.105263, stdev: 4939.344138, max: 11252.000000, min: 623.000000)
15/08/21 08:56:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:37 INFO StatsReportListener: 	623.0 ms	623.0 ms	648.0 ms	876.0 ms	1.8 s	10.9 s	11.2 s	11.3 s	11.3 s
15/08/21 08:56:37 INFO StatsReportListener: shuffle bytes written:(count: 19, mean: 8918860.526316, stdev: 8262561.827176, max: 17626413.000000, min: 769785.000000)
15/08/21 08:56:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:37 INFO StatsReportListener: 	751.7 KB	751.7 KB	753.5 KB	860.8 KB	3.1 MB	16.8 MB	16.8 MB	16.8 MB	16.8 MB
15/08/21 08:56:37 INFO StatsReportListener: task result size:(count: 19, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 08:56:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:37 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 08:56:37 INFO StatsReportListener: executor (non-fetch) time pct: (count: 19, mean: 96.299148, stdev: 3.386921, max: 99.685676, min: 90.123457)
15/08/21 08:56:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:37 INFO StatsReportListener: 	90 %	90 %	92 %	93 %	97 %	100 %	100 %	100 %	100 %
15/08/21 08:56:37 INFO StatsReportListener: other time pct: (count: 19, mean: 3.700852, stdev: 3.386921, max: 9.876543, min: 0.314324)
15/08/21 08:56:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:56:37 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 3 %	 7 %	 8 %	10 %	10 %
15/08/21 08:56:37 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061498 records.
15/08/21 08:56:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:37 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List(ShuffleMapStage 4)
15/08/21 08:56:37 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:37 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 08:56:38 INFO InternalParquetRecordReader: block read in memory in 1333 ms. row count = 3061301
15/08/21 08:56:38 INFO InternalParquetRecordReader: block read in memory in 1097 ms. row count = 1784858
15/08/21 08:56:38 INFO InternalParquetRecordReader: block read in memory in 1536 ms. row count = 3061813
15/08/21 08:56:38 INFO InternalParquetRecordReader: block read in memory in 1953 ms. row count = 3061650
15/08/21 08:56:38 INFO InternalParquetRecordReader: block read in memory in 1768 ms. row count = 3061669
15/08/21 08:56:39 INFO InternalParquetRecordReader: block read in memory in 1893 ms. row count = 1784493
15/08/21 08:56:39 INFO InternalParquetRecordReader: block read in memory in 1875 ms. row count = 3061498
15/08/21 08:56:40 INFO Executor: Finished task 2.0 in stage 4.0 (TID 392). 2125 bytes result sent to driver
15/08/21 08:56:40 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 406, localhost, ANY, 1746 bytes)
15/08/21 08:56:40 INFO Executor: Running task 16.0 in stage 4.0 (TID 406)
15/08/21 08:56:40 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 392) in 13030 ms on localhost (1/57)
15/08/21 08:56:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061799 records.
15/08/21 08:56:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:51693 in memory (size: 22.3 KB, free: 20.7 GB)
15/08/21 08:56:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:51693 in memory (size: 3.5 KB, free: 20.7 GB)
15/08/21 08:56:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:51693 in memory (size: 29.3 KB, free: 20.7 GB)
15/08/21 08:56:45 INFO ContextCleaner: Cleaned shuffle 0
15/08/21 08:56:46 INFO InternalParquetRecordReader: block read in memory in 5882 ms. row count = 3061799
15/08/21 08:56:47 INFO Executor: Finished task 5.0 in stage 4.0 (TID 395). 2125 bytes result sent to driver
15/08/21 08:56:47 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 407, localhost, ANY, 1752 bytes)
15/08/21 08:56:47 INFO Executor: Running task 17.0 in stage 4.0 (TID 407)
15/08/21 08:56:47 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 395) in 20190 ms on localhost (2/57)
15/08/21 08:56:47 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000002_0 start: 268435456 end: 343033905 length: 74598449 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:47 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:47 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1851904 records.
15/08/21 08:56:47 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:47 INFO InternalParquetRecordReader: block read in memory in 289 ms. row count = 1851904
15/08/21 08:56:49 INFO Executor: Finished task 3.0 in stage 4.0 (TID 393). 2125 bytes result sent to driver
15/08/21 08:56:49 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 408, localhost, ANY, 1741 bytes)
15/08/21 08:56:49 INFO Executor: Running task 18.0 in stage 4.0 (TID 408)
15/08/21 08:56:49 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 393) in 22588 ms on localhost (3/57)
15/08/21 08:56:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061581 records.
15/08/21 08:56:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:50 INFO Executor: Finished task 14.0 in stage 4.0 (TID 404). 2125 bytes result sent to driver
15/08/21 08:56:50 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 409, localhost, ANY, 1747 bytes)
15/08/21 08:56:50 INFO Executor: Running task 19.0 in stage 4.0 (TID 409)
15/08/21 08:56:50 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 404) in 12883 ms on localhost (4/57)
15/08/21 08:56:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061649 records.
15/08/21 08:56:50 INFO InternalParquetRecordReader: block read in memory in 460 ms. row count = 3061581
15/08/21 08:56:50 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:50 INFO Executor: Finished task 8.0 in stage 4.0 (TID 398). 2125 bytes result sent to driver
15/08/21 08:56:50 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 410, localhost, ANY, 1753 bytes)
15/08/21 08:56:50 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 398) in 13718 ms on localhost (5/57)
15/08/21 08:56:50 INFO Executor: Running task 20.0 in stage 4.0 (TID 410)
15/08/21 08:56:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000004_0 start: 268435456 end: 340167695 length: 71732239 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:50 INFO InternalParquetRecordReader: block read in memory in 379 ms. row count = 3061649
15/08/21 08:56:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784882 records.
15/08/21 08:56:50 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:50 INFO Executor: Finished task 11.0 in stage 4.0 (TID 401). 2125 bytes result sent to driver
15/08/21 08:56:50 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 411, localhost, ANY, 1741 bytes)
15/08/21 08:56:50 INFO Executor: Running task 21.0 in stage 4.0 (TID 411)
15/08/21 08:56:50 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:50 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 401) in 13478 ms on localhost (6/57)
15/08/21 08:56:50 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:50 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061580 records.
15/08/21 08:56:50 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:50 INFO InternalParquetRecordReader: block read in memory in 366 ms. row count = 1784882
15/08/21 08:56:51 INFO InternalParquetRecordReader: block read in memory in 607 ms. row count = 3061580
15/08/21 08:56:51 INFO Executor: Finished task 1.0 in stage 4.0 (TID 391). 2125 bytes result sent to driver
15/08/21 08:56:51 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 412, localhost, ANY, 1747 bytes)
15/08/21 08:56:51 INFO Executor: Running task 22.0 in stage 4.0 (TID 412)
15/08/21 08:56:51 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 391) in 24215 ms on localhost (7/57)
15/08/21 08:56:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061754 records.
15/08/21 08:56:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:51 INFO Executor: Finished task 0.0 in stage 4.0 (TID 390). 2125 bytes result sent to driver
15/08/21 08:56:51 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 413, localhost, ANY, 1754 bytes)
15/08/21 08:56:51 INFO Executor: Running task 23.0 in stage 4.0 (TID 413)
15/08/21 08:56:51 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 390) in 24511 ms on localhost (8/57)
15/08/21 08:56:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000011_0 start: 268435456 end: 340141911 length: 71706455 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784589 records.
15/08/21 08:56:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:51 INFO Executor: Finished task 4.0 in stage 4.0 (TID 394). 2125 bytes result sent to driver
15/08/21 08:56:51 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 414, localhost, ANY, 1742 bytes)
15/08/21 08:56:51 INFO Executor: Running task 24.0 in stage 4.0 (TID 414)
15/08/21 08:56:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:51 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 394) in 24434 ms on localhost (9/57)
15/08/21 08:56:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061826 records.
15/08/21 08:56:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:51 INFO InternalParquetRecordReader: block read in memory in 402 ms. row count = 1784589
15/08/21 08:56:51 INFO InternalParquetRecordReader: block read in memory in 645 ms. row count = 3061754
15/08/21 08:56:52 INFO Executor: Finished task 6.0 in stage 4.0 (TID 396). 2125 bytes result sent to driver
15/08/21 08:56:52 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 415, localhost, ANY, 1747 bytes)
15/08/21 08:56:52 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 396) in 24453 ms on localhost (10/57)
15/08/21 08:56:52 INFO Executor: Running task 25.0 in stage 4.0 (TID 415)
15/08/21 08:56:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061906 records.
15/08/21 08:56:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:52 INFO InternalParquetRecordReader: block read in memory in 759 ms. row count = 3061826
15/08/21 08:56:52 INFO Executor: Finished task 7.0 in stage 4.0 (TID 397). 2125 bytes result sent to driver
15/08/21 08:56:52 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 416, localhost, ANY, 1755 bytes)
15/08/21 08:56:52 INFO Executor: Running task 26.0 in stage 4.0 (TID 416)
15/08/21 08:56:52 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 397) in 16915 ms on localhost (11/57)
15/08/21 08:56:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000014_0 start: 268435456 end: 340152459 length: 71717003 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784988 records.
15/08/21 08:56:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:52 INFO InternalParquetRecordReader: block read in memory in 307 ms. row count = 1784988
15/08/21 08:56:52 INFO InternalParquetRecordReader: block read in memory in 462 ms. row count = 3061906
15/08/21 08:56:53 INFO Executor: Finished task 9.0 in stage 4.0 (TID 399). 2125 bytes result sent to driver
15/08/21 08:56:53 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 417, localhost, ANY, 1741 bytes)
15/08/21 08:56:53 INFO Executor: Running task 27.0 in stage 4.0 (TID 417)
15/08/21 08:56:53 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 399) in 16508 ms on localhost (12/57)
15/08/21 08:56:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061659 records.
15/08/21 08:56:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:53 INFO InternalParquetRecordReader: block read in memory in 353 ms. row count = 3061659
15/08/21 08:56:53 INFO Executor: Finished task 17.0 in stage 4.0 (TID 407). 2125 bytes result sent to driver
15/08/21 08:56:53 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 418, localhost, ANY, 1747 bytes)
15/08/21 08:56:53 INFO Executor: Running task 28.0 in stage 4.0 (TID 418)
15/08/21 08:56:53 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 407) in 6289 ms on localhost (13/57)
15/08/21 08:56:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061649 records.
15/08/21 08:56:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:54 INFO Executor: Finished task 10.0 in stage 4.0 (TID 400). 2125 bytes result sent to driver
15/08/21 08:56:54 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 419, localhost, ANY, 1754 bytes)
15/08/21 08:56:54 INFO Executor: Running task 29.0 in stage 4.0 (TID 419)
15/08/21 08:56:54 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 400) in 17336 ms on localhost (14/57)
15/08/21 08:56:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000012_0 start: 268435456 end: 340149052 length: 71713596 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784790 records.
15/08/21 08:56:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:54 INFO InternalParquetRecordReader: block read in memory in 415 ms. row count = 3061649
15/08/21 08:56:54 INFO Executor: Finished task 12.0 in stage 4.0 (TID 402). 2125 bytes result sent to driver
15/08/21 08:56:54 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 420, localhost, ANY, 1742 bytes)
15/08/21 08:56:54 INFO Executor: Running task 30.0 in stage 4.0 (TID 420)
15/08/21 08:56:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:54 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 402) in 17283 ms on localhost (15/57)
15/08/21 08:56:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061804 records.
15/08/21 08:56:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:54 INFO InternalParquetRecordReader: block read in memory in 300 ms. row count = 1784790
15/08/21 08:56:54 INFO InternalParquetRecordReader: block read in memory in 449 ms. row count = 3061804
15/08/21 08:56:55 INFO Executor: Finished task 13.0 in stage 4.0 (TID 403). 2125 bytes result sent to driver
15/08/21 08:56:55 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 421, localhost, ANY, 1747 bytes)
15/08/21 08:56:55 INFO Executor: Running task 31.0 in stage 4.0 (TID 421)
15/08/21 08:56:55 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 403) in 18159 ms on localhost (16/57)
15/08/21 08:56:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061148 records.
15/08/21 08:56:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:55 INFO Executor: Finished task 15.0 in stage 4.0 (TID 405). 2125 bytes result sent to driver
15/08/21 08:56:55 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 422, localhost, ANY, 1754 bytes)
15/08/21 08:56:55 INFO Executor: Running task 32.0 in stage 4.0 (TID 422)
15/08/21 08:56:55 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 405) in 18172 ms on localhost (17/57)
15/08/21 08:56:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000017_0 start: 268435456 end: 340153987 length: 71718531 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784820 records.
15/08/21 08:56:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:55 INFO InternalParquetRecordReader: block read in memory in 325 ms. row count = 1784820
15/08/21 08:56:55 INFO InternalParquetRecordReader: block read in memory in 469 ms. row count = 3061148
15/08/21 08:56:56 INFO Executor: Finished task 16.0 in stage 4.0 (TID 406). 2125 bytes result sent to driver
15/08/21 08:56:56 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 423, localhost, ANY, 1741 bytes)
15/08/21 08:56:56 INFO Executor: Running task 33.0 in stage 4.0 (TID 423)
15/08/21 08:56:56 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 406) in 16536 ms on localhost (18/57)
15/08/21 08:56:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061090 records.
15/08/21 08:56:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:56 INFO InternalParquetRecordReader: block read in memory in 351 ms. row count = 3061090
15/08/21 08:56:57 INFO Executor: Finished task 20.0 in stage 4.0 (TID 410). 2125 bytes result sent to driver
15/08/21 08:56:57 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 424, localhost, ANY, 1747 bytes)
15/08/21 08:56:57 INFO Executor: Running task 34.0 in stage 4.0 (TID 424)
15/08/21 08:56:57 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 410) in 6641 ms on localhost (19/57)
15/08/21 08:56:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061657 records.
15/08/21 08:56:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:57 INFO InternalParquetRecordReader: block read in memory in 420 ms. row count = 3061657
15/08/21 08:56:57 INFO Executor: Finished task 23.0 in stage 4.0 (TID 413). 2125 bytes result sent to driver
15/08/21 08:56:57 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 425, localhost, ANY, 1755 bytes)
15/08/21 08:56:57 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 413) in 6374 ms on localhost (20/57)
15/08/21 08:56:57 INFO Executor: Running task 35.0 in stage 4.0 (TID 425)
15/08/21 08:56:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000006_0 start: 268435456 end: 340154038 length: 71718582 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784628 records.
15/08/21 08:56:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:56:58 INFO InternalParquetRecordReader: block read in memory in 232 ms. row count = 1784628
15/08/21 08:56:59 INFO Executor: Finished task 26.0 in stage 4.0 (TID 416). 2125 bytes result sent to driver
15/08/21 08:56:59 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 426, localhost, ANY, 1741 bytes)
15/08/21 08:56:59 INFO Executor: Running task 36.0 in stage 4.0 (TID 426)
15/08/21 08:56:59 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 416) in 7213 ms on localhost (21/57)
15/08/21 08:56:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:56:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:56:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061565 records.
15/08/21 08:56:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:00 INFO InternalParquetRecordReader: block read in memory in 610 ms. row count = 3061565
15/08/21 08:57:01 INFO Executor: Finished task 18.0 in stage 4.0 (TID 408). 2125 bytes result sent to driver
15/08/21 08:57:01 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 427, localhost, ANY, 1747 bytes)
15/08/21 08:57:01 INFO Executor: Running task 37.0 in stage 4.0 (TID 427)
15/08/21 08:57:01 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 408) in 12284 ms on localhost (22/57)
15/08/21 08:57:01 INFO Executor: Finished task 29.0 in stage 4.0 (TID 419). 2125 bytes result sent to driver
15/08/21 08:57:01 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 428, localhost, ANY, 1753 bytes)
15/08/21 08:57:01 INFO Executor: Running task 38.0 in stage 4.0 (TID 428)
15/08/21 08:57:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:01 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 419) in 7666 ms on localhost (23/57)
15/08/21 08:57:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000013_0 start: 268435456 end: 340156978 length: 71721522 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1785031 records.
15/08/21 08:57:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061553 records.
15/08/21 08:57:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:02 INFO Executor: Finished task 19.0 in stage 4.0 (TID 409). 2125 bytes result sent to driver
15/08/21 08:57:02 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 429, localhost, ANY, 1741 bytes)
15/08/21 08:57:02 INFO Executor: Running task 39.0 in stage 4.0 (TID 429)
15/08/21 08:57:02 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 409) in 12105 ms on localhost (24/57)
15/08/21 08:57:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061805 records.
15/08/21 08:57:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:02 INFO InternalParquetRecordReader: block read in memory in 536 ms. row count = 1785031
15/08/21 08:57:02 INFO Executor: Finished task 21.0 in stage 4.0 (TID 411). 2125 bytes result sent to driver
15/08/21 08:57:02 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 430, localhost, ANY, 1747 bytes)
15/08/21 08:57:02 INFO Executor: Running task 40.0 in stage 4.0 (TID 430)
15/08/21 08:57:02 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 411) in 11945 ms on localhost (25/57)
15/08/21 08:57:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061589 records.
15/08/21 08:57:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:02 INFO Executor: Finished task 32.0 in stage 4.0 (TID 422). 2125 bytes result sent to driver
15/08/21 08:57:02 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 431, localhost, ANY, 1754 bytes)
15/08/21 08:57:02 INFO Executor: Running task 41.0 in stage 4.0 (TID 431)
15/08/21 08:57:02 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 422) in 7370 ms on localhost (26/57)
15/08/21 08:57:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000007_0 start: 268435456 end: 340173405 length: 71737949 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784699 records.
15/08/21 08:57:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:02 INFO InternalParquetRecordReader: block read in memory in 846 ms. row count = 3061553
15/08/21 08:57:03 INFO InternalParquetRecordReader: block read in memory in 1220 ms. row count = 3061805
15/08/21 08:57:03 INFO InternalParquetRecordReader: block read in memory in 797 ms. row count = 1784699
15/08/21 08:57:03 INFO InternalParquetRecordReader: block read in memory in 1094 ms. row count = 3061589
15/08/21 08:57:03 INFO Executor: Finished task 24.0 in stage 4.0 (TID 414). 2125 bytes result sent to driver
15/08/21 08:57:03 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 432, localhost, ANY, 1741 bytes)
15/08/21 08:57:03 INFO Executor: Running task 42.0 in stage 4.0 (TID 432)
15/08/21 08:57:03 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 414) in 11972 ms on localhost (27/57)
15/08/21 08:57:03 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061319 records.
15/08/21 08:57:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:04 INFO InternalParquetRecordReader: block read in memory in 439 ms. row count = 3061319
15/08/21 08:57:04 INFO Executor: Finished task 22.0 in stage 4.0 (TID 412). 2125 bytes result sent to driver
15/08/21 08:57:04 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 433, localhost, ANY, 1747 bytes)
15/08/21 08:57:04 INFO Executor: Running task 43.0 in stage 4.0 (TID 433)
15/08/21 08:57:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:04 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 412) in 13117 ms on localhost (28/57)
15/08/21 08:57:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061685 records.
15/08/21 08:57:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:04 INFO InternalParquetRecordReader: block read in memory in 482 ms. row count = 3061685
15/08/21 08:57:05 INFO Executor: Finished task 27.0 in stage 4.0 (TID 417). 2125 bytes result sent to driver
15/08/21 08:57:05 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 434, localhost, ANY, 1752 bytes)
15/08/21 08:57:05 INFO Executor: Running task 44.0 in stage 4.0 (TID 434)
15/08/21 08:57:05 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 417) in 12126 ms on localhost (29/57)
15/08/21 08:57:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000009_0 start: 268435456 end: 340161363 length: 71725907 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784830 records.
15/08/21 08:57:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:05 INFO Executor: Finished task 25.0 in stage 4.0 (TID 415). 2125 bytes result sent to driver
15/08/21 08:57:05 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 435, localhost, ANY, 1741 bytes)
15/08/21 08:57:05 INFO Executor: Running task 45.0 in stage 4.0 (TID 435)
15/08/21 08:57:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:05 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 415) in 13256 ms on localhost (30/57)
15/08/21 08:57:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061466 records.
15/08/21 08:57:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:05 INFO Executor: Finished task 35.0 in stage 4.0 (TID 425). 2125 bytes result sent to driver
15/08/21 08:57:05 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 436, localhost, ANY, 1747 bytes)
15/08/21 08:57:05 INFO Executor: Running task 46.0 in stage 4.0 (TID 436)
15/08/21 08:57:05 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 425) in 7767 ms on localhost (31/57)
15/08/21 08:57:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061853 records.
15/08/21 08:57:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:05 INFO InternalParquetRecordReader: block read in memory in 255 ms. row count = 1784830
15/08/21 08:57:05 INFO Executor: Finished task 28.0 in stage 4.0 (TID 418). 2125 bytes result sent to driver
15/08/21 08:57:05 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 437, localhost, ANY, 1755 bytes)
15/08/21 08:57:05 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 418) in 11943 ms on localhost (32/57)
15/08/21 08:57:05 INFO Executor: Running task 47.0 in stage 4.0 (TID 437)
15/08/21 08:57:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000018_0 start: 268435456 end: 319211896 length: 50776440 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1297836 records.
15/08/21 08:57:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:06 INFO InternalParquetRecordReader: block read in memory in 1235 ms. row count = 3061466
15/08/21 08:57:07 INFO InternalParquetRecordReader: block read in memory in 1441 ms. row count = 3061853
15/08/21 08:57:07 INFO InternalParquetRecordReader: block read in memory in 1192 ms. row count = 1297836
15/08/21 08:57:07 INFO Executor: Finished task 30.0 in stage 4.0 (TID 420). 2125 bytes result sent to driver
15/08/21 08:57:07 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 438, localhost, ANY, 1741 bytes)
15/08/21 08:57:07 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 420) in 13015 ms on localhost (33/57)
15/08/21 08:57:07 INFO Executor: Running task 48.0 in stage 4.0 (TID 438)
15/08/21 08:57:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062023 records.
15/08/21 08:57:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:07 INFO InternalParquetRecordReader: block read in memory in 490 ms. row count = 3062023
15/08/21 08:57:08 INFO Executor: Finished task 31.0 in stage 4.0 (TID 421). 2125 bytes result sent to driver
15/08/21 08:57:08 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 439, localhost, ANY, 1747 bytes)
15/08/21 08:57:08 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 421) in 13100 ms on localhost (34/57)
15/08/21 08:57:08 INFO Executor: Running task 49.0 in stage 4.0 (TID 439)
15/08/21 08:57:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061647 records.
15/08/21 08:57:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:08 INFO InternalParquetRecordReader: block read in memory in 299 ms. row count = 3061647
15/08/21 08:57:09 INFO Executor: Finished task 38.0 in stage 4.0 (TID 428). 2125 bytes result sent to driver
15/08/21 08:57:09 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 440, localhost, ANY, 1755 bytes)
15/08/21 08:57:09 INFO Executor: Running task 50.0 in stage 4.0 (TID 440)
15/08/21 08:57:09 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 428) in 7267 ms on localhost (35/57)
15/08/21 08:57:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000010_0 start: 268435456 end: 340156068 length: 71720612 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784477 records.
15/08/21 08:57:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:09 INFO Executor: Finished task 33.0 in stage 4.0 (TID 423). 2125 bytes result sent to driver
15/08/21 08:57:09 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 441, localhost, ANY, 1741 bytes)
15/08/21 08:57:09 INFO Executor: Running task 51.0 in stage 4.0 (TID 441)
15/08/21 08:57:09 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 423) in 12712 ms on localhost (36/57)
15/08/21 08:57:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062000 records.
15/08/21 08:57:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:09 INFO InternalParquetRecordReader: block read in memory in 205 ms. row count = 1784477
15/08/21 08:57:09 INFO Executor: Finished task 41.0 in stage 4.0 (TID 431). 2125 bytes result sent to driver
15/08/21 08:57:09 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 442, localhost, ANY, 1746 bytes)
15/08/21 08:57:09 INFO Executor: Running task 52.0 in stage 4.0 (TID 442)
15/08/21 08:57:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:09 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 431) in 6902 ms on localhost (37/57)
15/08/21 08:57:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062082 records.
15/08/21 08:57:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:09 INFO Executor: Finished task 34.0 in stage 4.0 (TID 424). 2125 bytes result sent to driver
15/08/21 08:57:09 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 443, localhost, ANY, 1754 bytes)
15/08/21 08:57:09 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 424) in 12639 ms on localhost (38/57)
15/08/21 08:57:09 INFO Executor: Running task 53.0 in stage 4.0 (TID 443)
15/08/21 08:57:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000001_0 start: 268435456 end: 343036825 length: 74601369 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1851764 records.
15/08/21 08:57:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:09 INFO InternalParquetRecordReader: block read in memory in 416 ms. row count = 3062000
15/08/21 08:57:09 INFO InternalParquetRecordReader: block read in memory in 282 ms. row count = 1851764
15/08/21 08:57:10 INFO InternalParquetRecordReader: block read in memory in 636 ms. row count = 3062082
15/08/21 08:57:11 INFO Executor: Finished task 36.0 in stage 4.0 (TID 426). 2125 bytes result sent to driver
15/08/21 08:57:11 INFO Executor: Finished task 47.0 in stage 4.0 (TID 437). 2125 bytes result sent to driver
15/08/21 08:57:11 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 444, localhost, ANY, 1741 bytes)
15/08/21 08:57:11 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 445, localhost, ANY, 1747 bytes)
15/08/21 08:57:11 INFO Executor: Running task 54.0 in stage 4.0 (TID 444)
15/08/21 08:57:11 INFO Executor: Running task 55.0 in stage 4.0 (TID 445)
15/08/21 08:57:11 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 426) in 11461 ms on localhost (39/57)
15/08/21 08:57:11 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 437) in 5351 ms on localhost (40/57)
15/08/21 08:57:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 134217728 end: 268435456 length: 134217728 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3061869 records.
15/08/21 08:57:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3062071 records.
15/08/21 08:57:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:11 INFO Executor: Finished task 44.0 in stage 4.0 (TID 434). 2125 bytes result sent to driver
15/08/21 08:57:11 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 446, localhost, ANY, 1753 bytes)
15/08/21 08:57:11 INFO Executor: Running task 56.0 in stage 4.0 (TID 446)
15/08/21 08:57:11 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 434) in 6041 ms on localhost (41/57)
15/08/21 08:57:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/orders_par/000005_0 start: 268435456 end: 340157695 length: 71722239 hosts: [] requestedSchema: message root {
  optional int32 o_custkey;
  optional int32 o_orderkey;
  optional double o_totalprice;
  optional binary o_orderdate (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderstatus","type":"string","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}},{"name":"o_orderpriority","type":"string","nullable":true,"metadata":{}},{"name":"o_clerk","type":"string","nullable":true,"metadata":{}},{"name":"o_shippriority","type":"integer","nullable":true,"metadata":{}},{"name":"o_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"o_custkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"o_totalprice","type":"double","nullable":true,"metadata":{}},{"name":"o_orderdate","type":"string","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1784419 records.
15/08/21 08:57:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:12 INFO InternalParquetRecordReader: block read in memory in 1318 ms. row count = 3061869
15/08/21 08:57:12 INFO InternalParquetRecordReader: block read in memory in 1309 ms. row count = 1784419
15/08/21 08:57:12 INFO InternalParquetRecordReader: block read in memory in 1557 ms. row count = 3062071
15/08/21 08:57:14 INFO Executor: Finished task 37.0 in stage 4.0 (TID 427). 2125 bytes result sent to driver
15/08/21 08:57:14 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 447, localhost, ANY, 1692 bytes)
15/08/21 08:57:14 INFO Executor: Running task 0.0 in stage 6.0 (TID 447)
15/08/21 08:57:14 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 427) in 12421 ms on localhost (42/57)
15/08/21 08:57:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00030-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506260 length: 3506260 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:14 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:14 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 750000
15/08/21 08:57:14 INFO Executor: Finished task 39.0 in stage 4.0 (TID 429). 2125 bytes result sent to driver
15/08/21 08:57:14 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 448, localhost, ANY, 1692 bytes)
15/08/21 08:57:14 INFO Executor: Running task 1.0 in stage 6.0 (TID 448)
15/08/21 08:57:14 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 429) in 12298 ms on localhost (43/57)
15/08/21 08:57:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00172-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502333 length: 3502333 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:14 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:14 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 750000
15/08/21 08:57:14 INFO Executor: Finished task 40.0 in stage 4.0 (TID 430). 2125 bytes result sent to driver
15/08/21 08:57:14 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 449, localhost, ANY, 1691 bytes)
15/08/21 08:57:14 INFO Executor: Running task 2.0 in stage 6.0 (TID 449)
15/08/21 08:57:14 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 430) in 12092 ms on localhost (44/57)
15/08/21 08:57:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00112-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506027 length: 3506027 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:14 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:14 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 08:57:14 INFO Executor: Finished task 42.0 in stage 4.0 (TID 432). 2125 bytes result sent to driver
15/08/21 08:57:14 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 450, localhost, ANY, 1693 bytes)
15/08/21 08:57:14 INFO Executor: Running task 3.0 in stage 6.0 (TID 450)
15/08/21 08:57:14 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 432) in 11087 ms on localhost (45/57)
15/08/21 08:57:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00110-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507283 length: 3507283 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:14 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:14 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:14 INFO Executor: Finished task 1.0 in stage 6.0 (TID 448). 2125 bytes result sent to driver
15/08/21 08:57:14 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 451, localhost, ANY, 1690 bytes)
15/08/21 08:57:14 INFO Executor: Running task 4.0 in stage 6.0 (TID 451)
15/08/21 08:57:14 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 448) in 434 ms on localhost (1/200)
15/08/21 08:57:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00025-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500335 length: 3500335 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:14 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 750000
15/08/21 08:57:15 INFO Executor: Finished task 3.0 in stage 6.0 (TID 450). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 452, localhost, ANY, 1691 bytes)
15/08/21 08:57:15 INFO Executor: Running task 5.0 in stage 6.0 (TID 452)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 450) in 311 ms on localhost (2/200)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00190-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505708 length: 3505708 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO Executor: Finished task 0.0 in stage 6.0 (TID 447). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 453, localhost, ANY, 1694 bytes)
15/08/21 08:57:15 INFO Executor: Running task 6.0 in stage 6.0 (TID 453)
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 447) in 778 ms on localhost (3/200)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00016-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507180 length: 3507180 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO Executor: Finished task 2.0 in stage 6.0 (TID 449). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 454, localhost, ANY, 1693 bytes)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 449) in 547 ms on localhost (4/200)
15/08/21 08:57:15 INFO Executor: Running task 7.0 in stage 6.0 (TID 454)
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00073-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501504 length: 3501504 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 750000
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO Executor: Finished task 4.0 in stage 6.0 (TID 451). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 455, localhost, ANY, 1692 bytes)
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 08:57:15 INFO Executor: Running task 8.0 in stage 6.0 (TID 455)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 451) in 339 ms on localhost (5/200)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00077-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505733 length: 3505733 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO Executor: Finished task 50.0 in stage 4.0 (TID 440). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 456, localhost, ANY, 1694 bytes)
15/08/21 08:57:15 INFO Executor: Running task 9.0 in stage 6.0 (TID 456)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 440) in 6087 ms on localhost (46/57)
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00069-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506964 length: 3506964 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 08:57:15 INFO Executor: Finished task 5.0 in stage 6.0 (TID 452). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 457, localhost, ANY, 1690 bytes)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 452) in 416 ms on localhost (6/200)
15/08/21 08:57:15 INFO Executor: Running task 10.0 in stage 6.0 (TID 457)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00028-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501169 length: 3501169 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 08:57:15 INFO Executor: Finished task 6.0 in stage 6.0 (TID 453). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 458, localhost, ANY, 1690 bytes)
15/08/21 08:57:15 INFO Executor: Running task 11.0 in stage 6.0 (TID 458)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 453) in 463 ms on localhost (7/200)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00084-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498871 length: 3498871 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO Executor: Finished task 9.0 in stage 6.0 (TID 456). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 459, localhost, ANY, 1692 bytes)
15/08/21 08:57:15 INFO Executor: Running task 12.0 in stage 6.0 (TID 459)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 456) in 307 ms on localhost (8/200)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00048-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506806 length: 3506806 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 750000
15/08/21 08:57:15 INFO Executor: Finished task 7.0 in stage 6.0 (TID 454). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 460, localhost, ANY, 1692 bytes)
15/08/21 08:57:15 INFO Executor: Running task 13.0 in stage 6.0 (TID 460)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 454) in 561 ms on localhost (9/200)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00013-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505997 length: 3505997 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO Executor: Finished task 8.0 in stage 6.0 (TID 455). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 461, localhost, ANY, 1693 bytes)
15/08/21 08:57:15 INFO Executor: Running task 14.0 in stage 6.0 (TID 461)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00096-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507797 length: 3507797 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 455) in 465 ms on localhost (10/200)
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 08:57:15 INFO Executor: Finished task 10.0 in stage 6.0 (TID 457). 2125 bytes result sent to driver
15/08/21 08:57:15 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 462, localhost, ANY, 1693 bytes)
15/08/21 08:57:15 INFO Executor: Running task 15.0 in stage 6.0 (TID 462)
15/08/21 08:57:15 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 457) in 505 ms on localhost (11/200)
15/08/21 08:57:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00118-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507229 length: 3507229 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:15 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:16 INFO Executor: Finished task 11.0 in stage 6.0 (TID 458). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 463, localhost, ANY, 1694 bytes)
15/08/21 08:57:16 INFO Executor: Running task 16.0 in stage 6.0 (TID 463)
15/08/21 08:57:16 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 458) in 549 ms on localhost (12/200)
15/08/21 08:57:16 INFO Executor: Finished task 12.0 in stage 6.0 (TID 459). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00144-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507412 length: 3507412 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 464, localhost, ANY, 1694 bytes)
15/08/21 08:57:16 INFO Executor: Running task 17.0 in stage 6.0 (TID 464)
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 459) in 550 ms on localhost (13/200)
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00086-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507402 length: 3507402 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:16 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 08:57:16 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 08:57:16 INFO Executor: Finished task 14.0 in stage 6.0 (TID 461). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO Executor: Finished task 13.0 in stage 6.0 (TID 460). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 465, localhost, ANY, 1691 bytes)
15/08/21 08:57:16 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 466, localhost, ANY, 1692 bytes)
15/08/21 08:57:16 INFO Executor: Running task 18.0 in stage 6.0 (TID 465)
15/08/21 08:57:16 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 461) in 567 ms on localhost (14/200)
15/08/21 08:57:16 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 460) in 584 ms on localhost (15/200)
15/08/21 08:57:16 INFO Executor: Running task 19.0 in stage 6.0 (TID 466)
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00105-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500366 length: 3500366 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00019-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501762 length: 3501762 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:16 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 08:57:16 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 750000
15/08/21 08:57:16 INFO Executor: Finished task 15.0 in stage 6.0 (TID 462). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 467, localhost, ANY, 1692 bytes)
15/08/21 08:57:16 INFO Executor: Running task 20.0 in stage 6.0 (TID 467)
15/08/21 08:57:16 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 462) in 488 ms on localhost (16/200)
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00108-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501015 length: 3501015 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:16 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 08:57:16 INFO Executor: Finished task 18.0 in stage 6.0 (TID 465). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 468, localhost, ANY, 1693 bytes)
15/08/21 08:57:16 INFO Executor: Running task 21.0 in stage 6.0 (TID 468)
15/08/21 08:57:16 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 465) in 297 ms on localhost (17/200)
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00022-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506607 length: 3506607 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO Executor: Finished task 16.0 in stage 6.0 (TID 463). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 469, localhost, ANY, 1689 bytes)
15/08/21 08:57:16 INFO Executor: Running task 22.0 in stage 6.0 (TID 469)
15/08/21 08:57:16 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 463) in 504 ms on localhost (18/200)
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00004-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500159 length: 3500159 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:16 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 08:57:16 INFO Executor: Finished task 17.0 in stage 6.0 (TID 464). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 470, localhost, ANY, 1691 bytes)
15/08/21 08:57:16 INFO Executor: Running task 23.0 in stage 6.0 (TID 470)
15/08/21 08:57:16 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 464) in 545 ms on localhost (19/200)
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00005-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506506 length: 3506506 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:16 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 08:57:16 INFO Executor: Finished task 19.0 in stage 6.0 (TID 466). 2125 bytes result sent to driver
15/08/21 08:57:16 INFO TaskSetManager: Starting task 24.0 in stage 6.0 (TID 471, localhost, ANY, 1690 bytes)
15/08/21 08:57:16 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 466) in 523 ms on localhost (20/200)
15/08/21 08:57:16 INFO Executor: Running task 24.0 in stage 6.0 (TID 471)
15/08/21 08:57:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00078-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505418 length: 3505418 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:17 INFO InternalParquetRecordReader: block read in memory in 703 ms. row count = 750000
15/08/21 08:57:17 INFO Executor: Finished task 53.0 in stage 4.0 (TID 443). 2125 bytes result sent to driver
15/08/21 08:57:17 INFO TaskSetManager: Starting task 25.0 in stage 6.0 (TID 472, localhost, ANY, 1692 bytes)
15/08/21 08:57:17 INFO Executor: Running task 25.0 in stage 6.0 (TID 472)
15/08/21 08:57:17 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 443) in 7873 ms on localhost (47/57)
15/08/21 08:57:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00182-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506307 length: 3506307 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:17 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:17 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 08:57:17 INFO Executor: Finished task 20.0 in stage 6.0 (TID 467). 2125 bytes result sent to driver
15/08/21 08:57:17 INFO TaskSetManager: Starting task 26.0 in stage 6.0 (TID 473, localhost, ANY, 1693 bytes)
15/08/21 08:57:17 INFO Executor: Running task 26.0 in stage 6.0 (TID 473)
15/08/21 08:57:17 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 467) in 1378 ms on localhost (21/200)
15/08/21 08:57:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00127-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504800 length: 3504800 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:17 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 750000
15/08/21 08:57:17 INFO Executor: Finished task 22.0 in stage 6.0 (TID 469). 2125 bytes result sent to driver
15/08/21 08:57:17 INFO TaskSetManager: Starting task 27.0 in stage 6.0 (TID 474, localhost, ANY, 1691 bytes)
15/08/21 08:57:17 INFO Executor: Running task 27.0 in stage 6.0 (TID 474)
15/08/21 08:57:17 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 469) in 1340 ms on localhost (22/200)
15/08/21 08:57:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00039-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506046 length: 3506046 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:17 INFO Executor: Finished task 23.0 in stage 6.0 (TID 470). 2125 bytes result sent to driver
15/08/21 08:57:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:17 INFO TaskSetManager: Starting task 28.0 in stage 6.0 (TID 475, localhost, ANY, 1691 bytes)
15/08/21 08:57:17 INFO Executor: Running task 28.0 in stage 6.0 (TID 475)
15/08/21 08:57:17 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 470) in 1295 ms on localhost (23/200)
15/08/21 08:57:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00137-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498602 length: 3498602 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:17 INFO Executor: Finished task 21.0 in stage 6.0 (TID 468). 2125 bytes result sent to driver
15/08/21 08:57:17 INFO TaskSetManager: Starting task 29.0 in stage 6.0 (TID 476, localhost, ANY, 1689 bytes)
15/08/21 08:57:17 INFO Executor: Running task 29.0 in stage 6.0 (TID 476)
15/08/21 08:57:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00020-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501937 length: 3501937 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:17 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 468) in 1412 ms on localhost (24/200)
15/08/21 08:57:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:17 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 08:57:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:17 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 08:57:18 INFO Executor: Finished task 24.0 in stage 6.0 (TID 471). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Starting task 30.0 in stage 6.0 (TID 477, localhost, ANY, 1693 bytes)
15/08/21 08:57:18 INFO Executor: Running task 30.0 in stage 6.0 (TID 477)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 24.0 in stage 6.0 (TID 471) in 1267 ms on localhost (25/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00104-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507353 length: 3507353 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO Executor: Finished task 43.0 in stage 4.0 (TID 433). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO TaskSetManager: Starting task 31.0 in stage 6.0 (TID 478, localhost, ANY, 1692 bytes)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 433) in 13722 ms on localhost (48/57)
15/08/21 08:57:18 INFO Executor: Running task 31.0 in stage 6.0 (TID 478)
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00194-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498074 length: 3498074 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO Executor: Finished task 25.0 in stage 6.0 (TID 472). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Starting task 32.0 in stage 6.0 (TID 479, localhost, ANY, 1692 bytes)
15/08/21 08:57:18 INFO Executor: Running task 32.0 in stage 6.0 (TID 479)
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 08:57:18 INFO TaskSetManager: Finished task 25.0 in stage 6.0 (TID 472) in 580 ms on localhost (26/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00122-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500106 length: 3500106 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 750000
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 750000
15/08/21 08:57:18 INFO Executor: Finished task 26.0 in stage 6.0 (TID 473). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Starting task 33.0 in stage 6.0 (TID 480, localhost, ANY, 1692 bytes)
15/08/21 08:57:18 INFO Executor: Running task 33.0 in stage 6.0 (TID 480)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 26.0 in stage 6.0 (TID 473) in 479 ms on localhost (27/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00091-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498759 length: 3498759 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 08:57:18 INFO Executor: Finished task 31.0 in stage 6.0 (TID 478). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Starting task 34.0 in stage 6.0 (TID 481, localhost, ANY, 1692 bytes)
15/08/21 08:57:18 INFO Executor: Running task 34.0 in stage 6.0 (TID 481)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 31.0 in stage 6.0 (TID 478) in 325 ms on localhost (28/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00124-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502312 length: 3502312 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 INFO Executor: Finished task 27.0 in stage 6.0 (TID 474). 2125 bytes result sent to driver
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO TaskSetManager: Starting task 35.0 in stage 6.0 (TID 482, localhost, ANY, 1691 bytes)
15/08/21 08:57:18 INFO Executor: Running task 35.0 in stage 6.0 (TID 482)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 27.0 in stage 6.0 (TID 474) in 493 ms on localhost (29/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00103-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504162 length: 3504162 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 750000
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 08:57:18 INFO Executor: Finished task 28.0 in stage 6.0 (TID 475). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Starting task 36.0 in stage 6.0 (TID 483, localhost, ANY, 1693 bytes)
15/08/21 08:57:18 INFO Executor: Running task 36.0 in stage 6.0 (TID 483)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 28.0 in stage 6.0 (TID 475) in 564 ms on localhost (30/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00183-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506119 length: 3506119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO Executor: Finished task 29.0 in stage 6.0 (TID 476). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO TaskSetManager: Starting task 37.0 in stage 6.0 (TID 484, localhost, ANY, 1692 bytes)
15/08/21 08:57:18 INFO Executor: Running task 37.0 in stage 6.0 (TID 484)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00131-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501547 length: 3501547 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 INFO TaskSetManager: Finished task 29.0 in stage 6.0 (TID 476) in 601 ms on localhost (31/200)
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:18 INFO Executor: Finished task 30.0 in stage 6.0 (TID 477). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Starting task 38.0 in stage 6.0 (TID 485, localhost, ANY, 1692 bytes)
15/08/21 08:57:18 INFO Executor: Running task 38.0 in stage 6.0 (TID 485)
15/08/21 08:57:18 INFO Executor: Finished task 32.0 in stage 6.0 (TID 479). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Finished task 30.0 in stage 6.0 (TID 477) in 646 ms on localhost (32/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00148-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499209 length: 3499209 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO TaskSetManager: Starting task 39.0 in stage 6.0 (TID 486, localhost, ANY, 1694 bytes)
15/08/21 08:57:18 INFO Executor: Running task 39.0 in stage 6.0 (TID 486)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 32.0 in stage 6.0 (TID 479) in 587 ms on localhost (33/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00167-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507097 length: 3507097 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO Executor: Finished task 35.0 in stage 6.0 (TID 482). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO TaskSetManager: Starting task 40.0 in stage 6.0 (TID 487, localhost, ANY, 1691 bytes)
15/08/21 08:57:18 INFO Executor: Running task 40.0 in stage 6.0 (TID 487)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 35.0 in stage 6.0 (TID 482) in 321 ms on localhost (34/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00106-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498575 length: 3498575 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 750000
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 750000
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 750000
15/08/21 08:57:18 INFO Executor: Finished task 33.0 in stage 6.0 (TID 480). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Starting task 41.0 in stage 6.0 (TID 488, localhost, ANY, 1693 bytes)
15/08/21 08:57:18 INFO Executor: Running task 41.0 in stage 6.0 (TID 488)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 33.0 in stage 6.0 (TID 480) in 580 ms on localhost (35/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00107-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500234 length: 3500234 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 08:57:18 INFO Executor: Finished task 34.0 in stage 6.0 (TID 481). 2125 bytes result sent to driver
15/08/21 08:57:18 INFO TaskSetManager: Starting task 42.0 in stage 6.0 (TID 489, localhost, ANY, 1690 bytes)
15/08/21 08:57:18 INFO Executor: Running task 42.0 in stage 6.0 (TID 489)
15/08/21 08:57:18 INFO TaskSetManager: Finished task 34.0 in stage 6.0 (TID 481) in 576 ms on localhost (36/200)
15/08/21 08:57:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00008-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506690 length: 3506690 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:18 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 08:57:19 INFO Executor: Finished task 36.0 in stage 6.0 (TID 483). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 43.0 in stage 6.0 (TID 490, localhost, ANY, 1689 bytes)
15/08/21 08:57:19 INFO Executor: Running task 43.0 in stage 6.0 (TID 490)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00003-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499057 length: 3499057 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 INFO TaskSetManager: Finished task 36.0 in stage 6.0 (TID 483) in 622 ms on localhost (37/200)
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO Executor: Finished task 37.0 in stage 6.0 (TID 484). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 44.0 in stage 6.0 (TID 491, localhost, ANY, 1693 bytes)
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO Executor: Running task 44.0 in stage 6.0 (TID 491)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 37.0 in stage 6.0 (TID 484) in 602 ms on localhost (38/200)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00032-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506885 length: 3506885 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 750000
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO Executor: Finished task 45.0 in stage 4.0 (TID 435). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO Executor: Finished task 46.0 in stage 4.0 (TID 436). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 45.0 in stage 6.0 (TID 492, localhost, ANY, 1691 bytes)
15/08/21 08:57:19 INFO Executor: Running task 45.0 in stage 6.0 (TID 492)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00042-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499078 length: 3499078 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 INFO TaskSetManager: Starting task 46.0 in stage 6.0 (TID 493, localhost, ANY, 1690 bytes)
15/08/21 08:57:19 INFO Executor: Running task 46.0 in stage 6.0 (TID 493)
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 750000
15/08/21 08:57:19 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 436) in 13678 ms on localhost (49/57)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00043-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502639 length: 3502639 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 435) in 13769 ms on localhost (50/57)
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO Executor: Finished task 39.0 in stage 6.0 (TID 486). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 47.0 in stage 6.0 (TID 494, localhost, ANY, 1693 bytes)
15/08/21 08:57:19 INFO Executor: Running task 47.0 in stage 6.0 (TID 494)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 39.0 in stage 6.0 (TID 486) in 567 ms on localhost (39/200)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00193-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500019 length: 3500019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 750000
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO Executor: Finished task 38.0 in stage 6.0 (TID 485). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO Executor: Finished task 40.0 in stage 6.0 (TID 487). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 48.0 in stage 6.0 (TID 495, localhost, ANY, 1691 bytes)
15/08/21 08:57:19 INFO Executor: Running task 48.0 in stage 6.0 (TID 495)
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00158-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502636 length: 3502636 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 INFO TaskSetManager: Starting task 49.0 in stage 6.0 (TID 496, localhost, ANY, 1692 bytes)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 38.0 in stage 6.0 (TID 485) in 641 ms on localhost (40/200)
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO Executor: Running task 49.0 in stage 6.0 (TID 496)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 40.0 in stage 6.0 (TID 487) in 592 ms on localhost (41/200)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00031-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507536 length: 3507536 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 750000
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 08:57:19 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 08:57:19 INFO Executor: Finished task 41.0 in stage 6.0 (TID 488). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 50.0 in stage 6.0 (TID 497, localhost, ANY, 1694 bytes)
15/08/21 08:57:19 INFO Executor: Running task 50.0 in stage 6.0 (TID 497)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 41.0 in stage 6.0 (TID 488) in 587 ms on localhost (42/200)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00046-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507400 length: 3507400 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 44 ms. row count = 750000
15/08/21 08:57:19 INFO Executor: Finished task 42.0 in stage 6.0 (TID 489). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 51.0 in stage 6.0 (TID 498, localhost, ANY, 1692 bytes)
15/08/21 08:57:19 INFO Executor: Running task 51.0 in stage 6.0 (TID 498)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 42.0 in stage 6.0 (TID 489) in 695 ms on localhost (43/200)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00021-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506265 length: 3506265 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 08:57:19 INFO Executor: Finished task 43.0 in stage 6.0 (TID 490). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 52.0 in stage 6.0 (TID 499, localhost, ANY, 1690 bytes)
15/08/21 08:57:19 INFO Executor: Running task 52.0 in stage 6.0 (TID 499)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 43.0 in stage 6.0 (TID 490) in 700 ms on localhost (44/200)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00035-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499639 length: 3499639 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 08:57:19 INFO Executor: Finished task 45.0 in stage 6.0 (TID 492). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO Executor: Finished task 44.0 in stage 6.0 (TID 491). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 53.0 in stage 6.0 (TID 500, localhost, ANY, 1693 bytes)
15/08/21 08:57:19 INFO Executor: Running task 53.0 in stage 6.0 (TID 500)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00180-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500745 length: 3500745 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO TaskSetManager: Starting task 54.0 in stage 6.0 (TID 501, localhost, ANY, 1692 bytes)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 45.0 in stage 6.0 (TID 492) in 740 ms on localhost (45/200)
15/08/21 08:57:19 INFO Executor: Running task 54.0 in stage 6.0 (TID 501)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 44.0 in stage 6.0 (TID 491) in 808 ms on localhost (46/200)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00126-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506809 length: 3506809 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO Executor: Finished task 51.0 in stage 6.0 (TID 498). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO Executor: Finished task 46.0 in stage 6.0 (TID 493). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO TaskSetManager: Starting task 55.0 in stage 6.0 (TID 502, localhost, ANY, 1693 bytes)
15/08/21 08:57:19 INFO Executor: Running task 55.0 in stage 6.0 (TID 502)
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00198-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506610 length: 3506610 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO TaskSetManager: Starting task 56.0 in stage 6.0 (TID 503, localhost, ANY, 1690 bytes)
15/08/21 08:57:19 INFO Executor: Running task 56.0 in stage 6.0 (TID 503)
15/08/21 08:57:19 INFO TaskSetManager: Finished task 51.0 in stage 6.0 (TID 498) in 330 ms on localhost (47/200)
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00006-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504043 length: 3504043 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 INFO TaskSetManager: Finished task 46.0 in stage 6.0 (TID 493) in 765 ms on localhost (48/200)
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 08:57:19 INFO Executor: Finished task 47.0 in stage 6.0 (TID 494). 2125 bytes result sent to driver
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:19 INFO TaskSetManager: Starting task 57.0 in stage 6.0 (TID 504, localhost, ANY, 1692 bytes)
15/08/21 08:57:19 INFO Executor: Running task 57.0 in stage 6.0 (TID 504)
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 08:57:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00040-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505811 length: 3505811 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:19 INFO TaskSetManager: Finished task 47.0 in stage 6.0 (TID 494) in 766 ms on localhost (49/200)
15/08/21 08:57:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:19 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO Executor: Finished task 48.0 in stage 6.0 (TID 495). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO Executor: Finished task 49.0 in stage 6.0 (TID 496). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 58.0 in stage 6.0 (TID 505, localhost, ANY, 1693 bytes)
15/08/21 08:57:20 INFO Executor: Running task 58.0 in stage 6.0 (TID 505)
15/08/21 08:57:20 INFO TaskSetManager: Starting task 59.0 in stage 6.0 (TID 506, localhost, ANY, 1692 bytes)
15/08/21 08:57:20 INFO Executor: Running task 59.0 in stage 6.0 (TID 506)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 48.0 in stage 6.0 (TID 495) in 782 ms on localhost (50/200)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 49.0 in stage 6.0 (TID 496) in 769 ms on localhost (51/200)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00095-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505119 length: 3505119 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00012-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500010 length: 3500010 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO Executor: Finished task 50.0 in stage 6.0 (TID 497). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 60.0 in stage 6.0 (TID 507, localhost, ANY, 1692 bytes)
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 750000
15/08/21 08:57:20 INFO Executor: Running task 60.0 in stage 6.0 (TID 507)
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO TaskSetManager: Finished task 50.0 in stage 6.0 (TID 497) in 684 ms on localhost (52/200)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00027-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501052 length: 3501052 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 750000
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 08:57:20 INFO Executor: Finished task 52.0 in stage 6.0 (TID 499). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 61.0 in stage 6.0 (TID 508, localhost, ANY, 1692 bytes)
15/08/21 08:57:20 INFO Executor: Running task 61.0 in stage 6.0 (TID 508)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00188-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501816 length: 3501816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 INFO TaskSetManager: Finished task 52.0 in stage 6.0 (TID 499) in 576 ms on localhost (53/200)
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO Executor: Finished task 48.0 in stage 4.0 (TID 438). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 62.0 in stage 6.0 (TID 509, localhost, ANY, 1692 bytes)
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO Executor: Running task 62.0 in stage 6.0 (TID 509)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 438) in 12972 ms on localhost (51/57)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00051-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500251 length: 3500251 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 750000
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO Executor: Finished task 56.0 in stage 4.0 (TID 446). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 63.0 in stage 6.0 (TID 510, localhost, ANY, 1691 bytes)
15/08/21 08:57:20 INFO Executor: Running task 63.0 in stage 6.0 (TID 510)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 446) in 9034 ms on localhost (52/57)
15/08/21 08:57:20 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00058-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3497871 length: 3497871 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 750000
15/08/21 08:57:20 INFO Executor: Finished task 54.0 in stage 6.0 (TID 501). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO Executor: Finished task 53.0 in stage 6.0 (TID 500). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 64.0 in stage 6.0 (TID 511, localhost, ANY, 1689 bytes)
15/08/21 08:57:20 INFO Executor: Running task 64.0 in stage 6.0 (TID 511)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00009-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499561 length: 3499561 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO TaskSetManager: Starting task 65.0 in stage 6.0 (TID 512, localhost, ANY, 1692 bytes)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 54.0 in stage 6.0 (TID 501) in 652 ms on localhost (54/200)
15/08/21 08:57:20 INFO Executor: Running task 65.0 in stage 6.0 (TID 512)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 53.0 in stage 6.0 (TID 500) in 659 ms on localhost (55/200)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00055-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505938 length: 3505938 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 INFO Executor: Finished task 55.0 in stage 6.0 (TID 502). 2125 bytes result sent to driver
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO TaskSetManager: Starting task 66.0 in stage 6.0 (TID 513, localhost, ANY, 1693 bytes)
15/08/21 08:57:20 INFO Executor: Running task 66.0 in stage 6.0 (TID 513)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00037-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505909 length: 3505909 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 INFO TaskSetManager: Finished task 55.0 in stage 6.0 (TID 502) in 636 ms on localhost (56/200)
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO Executor: Finished task 56.0 in stage 6.0 (TID 503). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO TaskSetManager: Starting task 67.0 in stage 6.0 (TID 514, localhost, ANY, 1691 bytes)
15/08/21 08:57:20 INFO Executor: Running task 67.0 in stage 6.0 (TID 514)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 56.0 in stage 6.0 (TID 503) in 666 ms on localhost (57/200)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00179-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501177 length: 3501177 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 08:57:20 INFO Executor: Finished task 58.0 in stage 6.0 (TID 505). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO Executor: Finished task 57.0 in stage 6.0 (TID 504). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 750000
15/08/21 08:57:20 INFO Executor: Finished task 59.0 in stage 6.0 (TID 506). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 68.0 in stage 6.0 (TID 515, localhost, ANY, 1690 bytes)
15/08/21 08:57:20 INFO Executor: Running task 68.0 in stage 6.0 (TID 515)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00154-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500858 length: 3500858 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO TaskSetManager: Starting task 69.0 in stage 6.0 (TID 516, localhost, ANY, 1691 bytes)
15/08/21 08:57:20 INFO Executor: Running task 69.0 in stage 6.0 (TID 516)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00139-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502079 length: 3502079 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO TaskSetManager: Starting task 70.0 in stage 6.0 (TID 517, localhost, ANY, 1691 bytes)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 58.0 in stage 6.0 (TID 505) in 655 ms on localhost (58/200)
15/08/21 08:57:20 INFO Executor: Running task 70.0 in stage 6.0 (TID 517)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00163-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499870 length: 3499870 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 INFO TaskSetManager: Finished task 59.0 in stage 6.0 (TID 506) in 655 ms on localhost (59/200)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 57.0 in stage 6.0 (TID 504) in 724 ms on localhost (60/200)
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO Executor: Finished task 60.0 in stage 6.0 (TID 507). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 08:57:20 INFO TaskSetManager: Starting task 71.0 in stage 6.0 (TID 518, localhost, ANY, 1691 bytes)
15/08/21 08:57:20 INFO Executor: Running task 71.0 in stage 6.0 (TID 518)
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:20 INFO TaskSetManager: Finished task 60.0 in stage 6.0 (TID 507) in 653 ms on localhost (61/200)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00010-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499442 length: 3499442 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 08:57:20 INFO Executor: Finished task 61.0 in stage 6.0 (TID 508). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 72.0 in stage 6.0 (TID 519, localhost, ANY, 1692 bytes)
15/08/21 08:57:20 INFO Executor: Running task 72.0 in stage 6.0 (TID 519)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 61.0 in stage 6.0 (TID 508) in 559 ms on localhost (62/200)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00160-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3503635 length: 3503635 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO Executor: Finished task 62.0 in stage 6.0 (TID 509). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 73.0 in stage 6.0 (TID 520, localhost, ANY, 1693 bytes)
15/08/21 08:57:20 INFO Executor: Running task 73.0 in stage 6.0 (TID 520)
15/08/21 08:57:20 INFO TaskSetManager: Finished task 62.0 in stage 6.0 (TID 509) in 562 ms on localhost (63/200)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00067-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501014 length: 3501014 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO Executor: Finished task 63.0 in stage 6.0 (TID 510). 2125 bytes result sent to driver
15/08/21 08:57:20 INFO TaskSetManager: Starting task 74.0 in stage 6.0 (TID 521, localhost, ANY, 1692 bytes)
15/08/21 08:57:20 INFO Executor: Running task 74.0 in stage 6.0 (TID 521)
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:20 INFO TaskSetManager: Finished task 63.0 in stage 6.0 (TID 510) in 540 ms on localhost (64/200)
15/08/21 08:57:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00130-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500816 length: 3500816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:20 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 08:57:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 750000
15/08/21 08:57:21 INFO Executor: Finished task 64.0 in stage 6.0 (TID 511). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 75.0 in stage 6.0 (TID 522, localhost, ANY, 1691 bytes)
15/08/21 08:57:21 INFO Executor: Running task 75.0 in stage 6.0 (TID 522)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00036-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500319 length: 3500319 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 INFO TaskSetManager: Finished task 64.0 in stage 6.0 (TID 511) in 616 ms on localhost (65/200)
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO Executor: Finished task 66.0 in stage 6.0 (TID 513). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO TaskSetManager: Starting task 76.0 in stage 6.0 (TID 523, localhost, ANY, 1692 bytes)
15/08/21 08:57:21 INFO Executor: Running task 76.0 in stage 6.0 (TID 523)
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO TaskSetManager: Finished task 66.0 in stage 6.0 (TID 513) in 613 ms on localhost (66/200)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00033-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498478 length: 3498478 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO Executor: Finished task 67.0 in stage 6.0 (TID 514). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO TaskSetManager: Starting task 77.0 in stage 6.0 (TID 524, localhost, ANY, 1691 bytes)
15/08/21 08:57:21 INFO Executor: Running task 77.0 in stage 6.0 (TID 524)
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 08:57:21 INFO TaskSetManager: Finished task 67.0 in stage 6.0 (TID 514) in 615 ms on localhost (67/200)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00063-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3503402 length: 3503402 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 08:57:21 INFO Executor: Finished task 65.0 in stage 6.0 (TID 512). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 78.0 in stage 6.0 (TID 525, localhost, ANY, 1692 bytes)
15/08/21 08:57:21 INFO Executor: Running task 78.0 in stage 6.0 (TID 525)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 65.0 in stage 6.0 (TID 512) in 709 ms on localhost (68/200)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00109-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502812 length: 3502812 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO Executor: Finished task 68.0 in stage 6.0 (TID 515). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 79.0 in stage 6.0 (TID 526, localhost, ANY, 1693 bytes)
15/08/21 08:57:21 INFO Executor: Running task 79.0 in stage 6.0 (TID 526)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 68.0 in stage 6.0 (TID 515) in 641 ms on localhost (69/200)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00169-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3496922 length: 3496922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 750000
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO Executor: Finished task 69.0 in stage 6.0 (TID 516). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 80.0 in stage 6.0 (TID 527, localhost, ANY, 1693 bytes)
15/08/21 08:57:21 INFO Executor: Running task 80.0 in stage 6.0 (TID 527)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 69.0 in stage 6.0 (TID 516) in 650 ms on localhost (70/200)
15/08/21 08:57:21 INFO Executor: Finished task 70.0 in stage 6.0 (TID 517). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00074-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499991 length: 3499991 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO TaskSetManager: Starting task 81.0 in stage 6.0 (TID 528, localhost, ANY, 1691 bytes)
15/08/21 08:57:21 INFO Executor: Running task 81.0 in stage 6.0 (TID 528)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 70.0 in stage 6.0 (TID 517) in 651 ms on localhost (71/200)
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00029-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505200 length: 3505200 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO Executor: Finished task 71.0 in stage 6.0 (TID 518). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 82.0 in stage 6.0 (TID 529, localhost, ANY, 1690 bytes)
15/08/21 08:57:21 INFO Executor: Running task 82.0 in stage 6.0 (TID 529)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 71.0 in stage 6.0 (TID 518) in 667 ms on localhost (72/200)
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00161-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499824 length: 3499824 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 750000
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 750000
15/08/21 08:57:21 INFO Executor: Finished task 72.0 in stage 6.0 (TID 519). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 83.0 in stage 6.0 (TID 530, localhost, ANY, 1692 bytes)
15/08/21 08:57:21 INFO Executor: Running task 83.0 in stage 6.0 (TID 530)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 72.0 in stage 6.0 (TID 519) in 624 ms on localhost (73/200)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00141-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506341 length: 3506341 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO Executor: Finished task 73.0 in stage 6.0 (TID 520). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO TaskSetManager: Starting task 84.0 in stage 6.0 (TID 531, localhost, ANY, 1691 bytes)
15/08/21 08:57:21 INFO Executor: Running task 84.0 in stage 6.0 (TID 531)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 73.0 in stage 6.0 (TID 520) in 604 ms on localhost (74/200)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00152-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505322 length: 3505322 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO Executor: Finished task 74.0 in stage 6.0 (TID 521). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO TaskSetManager: Starting task 85.0 in stage 6.0 (TID 532, localhost, ANY, 1693 bytes)
15/08/21 08:57:21 INFO Executor: Running task 85.0 in stage 6.0 (TID 532)
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 08:57:21 INFO TaskSetManager: Finished task 74.0 in stage 6.0 (TID 521) in 605 ms on localhost (75/200)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00197-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504816 length: 3504816 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 08:57:21 INFO Executor: Finished task 49.0 in stage 4.0 (TID 439). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 86.0 in stage 6.0 (TID 533, localhost, ANY, 1693 bytes)
15/08/21 08:57:21 INFO Executor: Running task 86.0 in stage 6.0 (TID 533)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 439) in 13181 ms on localhost (53/57)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00080-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3508460 length: 3508460 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO Executor: Finished task 75.0 in stage 6.0 (TID 522). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 87.0 in stage 6.0 (TID 534, localhost, ANY, 1692 bytes)
15/08/21 08:57:21 INFO Executor: Running task 87.0 in stage 6.0 (TID 534)
15/08/21 08:57:21 INFO TaskSetManager: Finished task 75.0 in stage 6.0 (TID 522) in 702 ms on localhost (76/200)
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00113-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502019 length: 3502019 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO Executor: Finished task 76.0 in stage 6.0 (TID 523). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO TaskSetManager: Starting task 88.0 in stage 6.0 (TID 535, localhost, ANY, 1692 bytes)
15/08/21 08:57:21 INFO Executor: Running task 88.0 in stage 6.0 (TID 535)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00185-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499076 length: 3499076 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 INFO TaskSetManager: Finished task 76.0 in stage 6.0 (TID 523) in 733 ms on localhost (77/200)
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO Executor: Finished task 77.0 in stage 6.0 (TID 524). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 89.0 in stage 6.0 (TID 536, localhost, ANY, 1693 bytes)
15/08/21 08:57:21 INFO Executor: Running task 89.0 in stage 6.0 (TID 536)
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO TaskSetManager: Finished task 77.0 in stage 6.0 (TID 524) in 706 ms on localhost (78/200)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00072-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506406 length: 3506406 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 08:57:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:21 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:21 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 08:57:21 INFO Executor: Finished task 78.0 in stage 6.0 (TID 525). 2125 bytes result sent to driver
15/08/21 08:57:21 INFO TaskSetManager: Starting task 90.0 in stage 6.0 (TID 537, localhost, ANY, 1690 bytes)
15/08/21 08:57:21 INFO Executor: Running task 90.0 in stage 6.0 (TID 537)
15/08/21 08:57:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00083-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499387 length: 3499387 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:21 INFO TaskSetManager: Finished task 78.0 in stage 6.0 (TID 525) in 743 ms on localhost (79/200)
15/08/21 08:57:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO Executor: Finished task 80.0 in stage 6.0 (TID 527). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 91.0 in stage 6.0 (TID 538, localhost, ANY, 1693 bytes)
15/08/21 08:57:22 INFO Executor: Running task 91.0 in stage 6.0 (TID 538)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 80.0 in stage 6.0 (TID 527) in 709 ms on localhost (80/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00102-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506891 length: 3506891 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO Executor: Finished task 79.0 in stage 6.0 (TID 526). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO TaskSetManager: Starting task 92.0 in stage 6.0 (TID 539, localhost, ANY, 1691 bytes)
15/08/21 08:57:22 INFO Executor: Running task 92.0 in stage 6.0 (TID 539)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 79.0 in stage 6.0 (TID 526) in 753 ms on localhost (81/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00070-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504014 length: 3504014 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO Executor: Finished task 81.0 in stage 6.0 (TID 528). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 93.0 in stage 6.0 (TID 540, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 93.0 in stage 6.0 (TID 540)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 81.0 in stage 6.0 (TID 528) in 762 ms on localhost (82/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00052-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501586 length: 3501586 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 08:57:22 INFO Executor: Finished task 82.0 in stage 6.0 (TID 529). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO TaskSetManager: Starting task 94.0 in stage 6.0 (TID 541, localhost, ANY, 1693 bytes)
15/08/21 08:57:22 INFO Executor: Running task 94.0 in stage 6.0 (TID 541)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 82.0 in stage 6.0 (TID 529) in 710 ms on localhost (83/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00196-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500486 length: 3500486 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 08:57:22 INFO Executor: Finished task 89.0 in stage 6.0 (TID 536). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 95.0 in stage 6.0 (TID 542, localhost, ANY, 1690 bytes)
15/08/21 08:57:22 INFO Executor: Running task 95.0 in stage 6.0 (TID 542)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00114-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501155 length: 3501155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO TaskSetManager: Finished task 89.0 in stage 6.0 (TID 536) in 268 ms on localhost (84/200)
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO Executor: Finished task 83.0 in stage 6.0 (TID 530). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 08:57:22 INFO TaskSetManager: Starting task 96.0 in stage 6.0 (TID 543, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 96.0 in stage 6.0 (TID 543)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 83.0 in stage 6.0 (TID 530) in 714 ms on localhost (85/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00146-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501235 length: 3501235 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO Executor: Finished task 90.0 in stage 6.0 (TID 537). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO Executor: Finished task 85.0 in stage 6.0 (TID 532). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO Executor: Finished task 84.0 in stage 6.0 (TID 531). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 08:57:22 INFO TaskSetManager: Starting task 97.0 in stage 6.0 (TID 544, localhost, ANY, 1691 bytes)
15/08/21 08:57:22 INFO Executor: Running task 97.0 in stage 6.0 (TID 544)
15/08/21 08:57:22 INFO TaskSetManager: Starting task 98.0 in stage 6.0 (TID 545, localhost, ANY, 1693 bytes)
15/08/21 08:57:22 INFO Executor: Running task 98.0 in stage 6.0 (TID 545)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 90.0 in stage 6.0 (TID 537) in 262 ms on localhost (86/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00092-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500083 length: 3500083 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00061-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506496 length: 3506496 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO TaskSetManager: Starting task 99.0 in stage 6.0 (TID 546, localhost, ANY, 1691 bytes)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 85.0 in stage 6.0 (TID 532) in 721 ms on localhost (87/200)
15/08/21 08:57:22 INFO Executor: Running task 99.0 in stage 6.0 (TID 546)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 84.0 in stage 6.0 (TID 531) in 750 ms on localhost (88/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00145-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498381 length: 3498381 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO Executor: Finished task 86.0 in stage 6.0 (TID 533). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 100.0 in stage 6.0 (TID 547, localhost, ANY, 1691 bytes)
15/08/21 08:57:22 INFO Executor: Running task 100.0 in stage 6.0 (TID 547)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 86.0 in stage 6.0 (TID 533) in 710 ms on localhost (89/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00000-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3509124 length: 3509124 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO Executor: Finished task 92.0 in stage 6.0 (TID 539). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO TaskSetManager: Starting task 101.0 in stage 6.0 (TID 548, localhost, ANY, 1693 bytes)
15/08/21 08:57:22 INFO Executor: Running task 101.0 in stage 6.0 (TID 548)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 92.0 in stage 6.0 (TID 539) in 284 ms on localhost (90/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00093-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507082 length: 3507082 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 53 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO Executor: Finished task 87.0 in stage 6.0 (TID 534). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 102.0 in stage 6.0 (TID 549, localhost, ANY, 1693 bytes)
15/08/21 08:57:22 INFO Executor: Running task 102.0 in stage 6.0 (TID 549)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 87.0 in stage 6.0 (TID 534) in 537 ms on localhost (91/200)
15/08/21 08:57:22 INFO Executor: Finished task 88.0 in stage 6.0 (TID 535). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00135-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505648 length: 3505648 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO TaskSetManager: Starting task 103.0 in stage 6.0 (TID 550, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 103.0 in stage 6.0 (TID 550)
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO TaskSetManager: Finished task 88.0 in stage 6.0 (TID 535) in 509 ms on localhost (92/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00115-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501230 length: 3501230 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 750000
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 71 ms. row count = 750000
15/08/21 08:57:22 INFO Executor: Finished task 91.0 in stage 6.0 (TID 538). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 104.0 in stage 6.0 (TID 551, localhost, ANY, 1693 bytes)
15/08/21 08:57:22 INFO Executor: Running task 104.0 in stage 6.0 (TID 551)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 91.0 in stage 6.0 (TID 538) in 498 ms on localhost (93/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00111-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507275 length: 3507275 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 750000
15/08/21 08:57:22 INFO Executor: Finished task 93.0 in stage 6.0 (TID 540). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO Executor: Finished task 94.0 in stage 6.0 (TID 541). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 105.0 in stage 6.0 (TID 552, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 105.0 in stage 6.0 (TID 552)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00024-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502988 length: 3502988 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO TaskSetManager: Starting task 106.0 in stage 6.0 (TID 553, localhost, ANY, 1690 bytes)
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO Executor: Running task 106.0 in stage 6.0 (TID 553)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 93.0 in stage 6.0 (TID 540) in 520 ms on localhost (94/200)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 94.0 in stage 6.0 (TID 541) in 502 ms on localhost (95/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00155-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499054 length: 3499054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 08:57:22 INFO Executor: Finished task 95.0 in stage 6.0 (TID 542). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 107.0 in stage 6.0 (TID 554, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 107.0 in stage 6.0 (TID 554)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 95.0 in stage 6.0 (TID 542) in 556 ms on localhost (96/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00175-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504840 length: 3504840 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO Executor: Finished task 96.0 in stage 6.0 (TID 543). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO TaskSetManager: Starting task 108.0 in stage 6.0 (TID 555, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 108.0 in stage 6.0 (TID 555)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 96.0 in stage 6.0 (TID 543) in 534 ms on localhost (97/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00094-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504770 length: 3504770 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 08:57:22 INFO Executor: Finished task 97.0 in stage 6.0 (TID 544). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 109.0 in stage 6.0 (TID 556, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 109.0 in stage 6.0 (TID 556)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 97.0 in stage 6.0 (TID 544) in 633 ms on localhost (98/200)
15/08/21 08:57:22 INFO Executor: Finished task 51.0 in stage 4.0 (TID 441). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00014-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505682 length: 3505682 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO Executor: Finished task 99.0 in stage 6.0 (TID 546). 2125 bytes result sent to driver
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO TaskSetManager: Starting task 110.0 in stage 6.0 (TID 557, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 110.0 in stage 6.0 (TID 557)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00156-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501563 length: 3501563 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO Executor: Finished task 98.0 in stage 6.0 (TID 545). 2125 bytes result sent to driver
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO Executor: Finished task 100.0 in stage 6.0 (TID 547). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO TaskSetManager: Starting task 111.0 in stage 6.0 (TID 558, localhost, ANY, 1691 bytes)
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO TaskSetManager: Finished task 99.0 in stage 6.0 (TID 546) in 652 ms on localhost (99/200)
15/08/21 08:57:22 INFO Executor: Running task 111.0 in stage 6.0 (TID 558)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 441) in 13615 ms on localhost (54/57)
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00191-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506064 length: 3506064 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO TaskSetManager: Starting task 112.0 in stage 6.0 (TID 559, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 112.0 in stage 6.0 (TID 559)
15/08/21 08:57:22 INFO TaskSetManager: Starting task 113.0 in stage 6.0 (TID 560, localhost, ANY, 1693 bytes)
15/08/21 08:57:22 INFO Executor: Running task 113.0 in stage 6.0 (TID 560)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 98.0 in stage 6.0 (TID 545) in 673 ms on localhost (100/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00181-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3503343 length: 3503343 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00157-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506076 length: 3506076 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 08:57:22 INFO TaskSetManager: Finished task 100.0 in stage 6.0 (TID 547) in 632 ms on localhost (101/200)
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 750000
15/08/21 08:57:22 INFO Executor: Finished task 101.0 in stage 6.0 (TID 548). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 23 ms. row count = 750000
15/08/21 08:57:22 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 08:57:22 INFO TaskSetManager: Starting task 114.0 in stage 6.0 (TID 561, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 114.0 in stage 6.0 (TID 561)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 101.0 in stage 6.0 (TID 548) in 661 ms on localhost (102/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00098-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500259 length: 3500259 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:22 INFO Executor: Finished task 103.0 in stage 6.0 (TID 550). 2125 bytes result sent to driver
15/08/21 08:57:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:22 INFO TaskSetManager: Starting task 115.0 in stage 6.0 (TID 562, localhost, ANY, 1692 bytes)
15/08/21 08:57:22 INFO Executor: Running task 115.0 in stage 6.0 (TID 562)
15/08/21 08:57:22 INFO TaskSetManager: Finished task 103.0 in stage 6.0 (TID 550) in 644 ms on localhost (103/200)
15/08/21 08:57:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00138-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3497916 length: 3497916 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 750000
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 08:57:23 INFO Executor: Finished task 102.0 in stage 6.0 (TID 549). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO TaskSetManager: Starting task 116.0 in stage 6.0 (TID 563, localhost, ANY, 1691 bytes)
15/08/21 08:57:23 INFO Executor: Running task 116.0 in stage 6.0 (TID 563)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 102.0 in stage 6.0 (TID 549) in 738 ms on localhost (104/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00132-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502651 length: 3502651 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO Executor: Finished task 104.0 in stage 6.0 (TID 551). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO TaskSetManager: Starting task 117.0 in stage 6.0 (TID 564, localhost, ANY, 1692 bytes)
15/08/21 08:57:23 INFO Executor: Running task 117.0 in stage 6.0 (TID 564)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 104.0 in stage 6.0 (TID 551) in 667 ms on localhost (105/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00015-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505595 length: 3505595 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 64 ms. row count = 750000
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO Executor: Finished task 106.0 in stage 6.0 (TID 553). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO Executor: Finished task 105.0 in stage 6.0 (TID 552). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO TaskSetManager: Starting task 118.0 in stage 6.0 (TID 565, localhost, ANY, 1690 bytes)
15/08/21 08:57:23 INFO Executor: Running task 118.0 in stage 6.0 (TID 565)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00150-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505922 length: 3505922 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO TaskSetManager: Starting task 119.0 in stage 6.0 (TID 566, localhost, ANY, 1691 bytes)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 106.0 in stage 6.0 (TID 553) in 649 ms on localhost (106/200)
15/08/21 08:57:23 INFO Executor: Running task 119.0 in stage 6.0 (TID 566)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 105.0 in stage 6.0 (TID 552) in 667 ms on localhost (107/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00116-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504528 length: 3504528 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 750000
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 750000
15/08/21 08:57:23 INFO Executor: Finished task 107.0 in stage 6.0 (TID 554). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 750000
15/08/21 08:57:23 INFO TaskSetManager: Starting task 120.0 in stage 6.0 (TID 567, localhost, ANY, 1691 bytes)
15/08/21 08:57:23 INFO Executor: Running task 120.0 in stage 6.0 (TID 567)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 107.0 in stage 6.0 (TID 554) in 641 ms on localhost (108/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00076-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500080 length: 3500080 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 750000
15/08/21 08:57:23 INFO Executor: Finished task 108.0 in stage 6.0 (TID 555). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO TaskSetManager: Starting task 121.0 in stage 6.0 (TID 568, localhost, ANY, 1692 bytes)
15/08/21 08:57:23 INFO Executor: Running task 121.0 in stage 6.0 (TID 568)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 108.0 in stage 6.0 (TID 555) in 748 ms on localhost (109/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00187-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501231 length: 3501231 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 750000
15/08/21 08:57:23 INFO Executor: Finished task 110.0 in stage 6.0 (TID 557). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO Executor: Finished task 112.0 in stage 6.0 (TID 559). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO Executor: Finished task 111.0 in stage 6.0 (TID 558). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO Executor: Finished task 109.0 in stage 6.0 (TID 556). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO TaskSetManager: Starting task 122.0 in stage 6.0 (TID 569, localhost, ANY, 1690 bytes)
15/08/21 08:57:23 INFO Executor: Running task 122.0 in stage 6.0 (TID 569)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 110.0 in stage 6.0 (TID 557) in 792 ms on localhost (110/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00075-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3497524 length: 3497524 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO TaskSetManager: Starting task 123.0 in stage 6.0 (TID 570, localhost, ANY, 1690 bytes)
15/08/21 08:57:23 INFO Executor: Running task 123.0 in stage 6.0 (TID 570)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 112.0 in stage 6.0 (TID 559) in 796 ms on localhost (111/200)
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00186-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501858 length: 3501858 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 750000
15/08/21 08:57:23 INFO TaskSetManager: Starting task 124.0 in stage 6.0 (TID 571, localhost, ANY, 1692 bytes)
15/08/21 08:57:23 INFO Executor: Running task 124.0 in stage 6.0 (TID 571)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 111.0 in stage 6.0 (TID 558) in 854 ms on localhost (112/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00087-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506179 length: 3506179 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO TaskSetManager: Starting task 125.0 in stage 6.0 (TID 572, localhost, ANY, 1692 bytes)
15/08/21 08:57:23 INFO Executor: Running task 125.0 in stage 6.0 (TID 572)
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 750000
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00062-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502754 length: 3502754 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 INFO TaskSetManager: Finished task 109.0 in stage 6.0 (TID 556) in 915 ms on localhost (113/200)
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO Executor: Finished task 113.0 in stage 6.0 (TID 560). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO TaskSetManager: Starting task 126.0 in stage 6.0 (TID 573, localhost, ANY, 1692 bytes)
15/08/21 08:57:23 INFO Executor: Running task 126.0 in stage 6.0 (TID 573)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 113.0 in stage 6.0 (TID 560) in 873 ms on localhost (114/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00100-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500864 length: 3500864 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO Executor: Finished task 114.0 in stage 6.0 (TID 561). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO TaskSetManager: Starting task 127.0 in stage 6.0 (TID 574, localhost, ANY, 1691 bytes)
15/08/21 08:57:23 INFO Executor: Running task 127.0 in stage 6.0 (TID 574)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 114.0 in stage 6.0 (TID 561) in 863 ms on localhost (115/200)
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 750000
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00147-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499919 length: 3499919 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 INFO Executor: Finished task 115.0 in stage 6.0 (TID 562). 2125 bytes result sent to driver
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 750000
15/08/21 08:57:23 INFO TaskSetManager: Starting task 128.0 in stage 6.0 (TID 575, localhost, ANY, 1694 bytes)
15/08/21 08:57:23 INFO Executor: Running task 128.0 in stage 6.0 (TID 575)
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00173-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3508672 length: 3508672 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO TaskSetManager: Finished task 115.0 in stage 6.0 (TID 562) in 867 ms on localhost (116/200)
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 78 ms. row count = 750000
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO Executor: Finished task 116.0 in stage 6.0 (TID 563). 2125 bytes result sent to driver
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 750000
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 750000
15/08/21 08:57:23 INFO TaskSetManager: Starting task 129.0 in stage 6.0 (TID 576, localhost, ANY, 1691 bytes)
15/08/21 08:57:23 INFO Executor: Running task 129.0 in stage 6.0 (TID 576)
15/08/21 08:57:23 INFO TaskSetManager: Finished task 116.0 in stage 6.0 (TID 563) in 851 ms on localhost (117/200)
15/08/21 08:57:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00195-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500801 length: 3500801 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:23 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 08:57:23 INFO Executor: Finished task 118.0 in stage 6.0 (TID 565). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO TaskSetManager: Starting task 130.0 in stage 6.0 (TID 577, localhost, ANY, 1692 bytes)
15/08/21 08:57:24 INFO Executor: Running task 130.0 in stage 6.0 (TID 577)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 118.0 in stage 6.0 (TID 565) in 810 ms on localhost (118/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00082-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501309 length: 3501309 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO Executor: Finished task 117.0 in stage 6.0 (TID 564). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO Executor: Finished task 119.0 in stage 6.0 (TID 566). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO TaskSetManager: Starting task 131.0 in stage 6.0 (TID 578, localhost, ANY, 1694 bytes)
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO Executor: Running task 131.0 in stage 6.0 (TID 578)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00128-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507147 length: 3507147 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO TaskSetManager: Starting task 132.0 in stage 6.0 (TID 579, localhost, ANY, 1692 bytes)
15/08/21 08:57:24 INFO Executor: Running task 132.0 in stage 6.0 (TID 579)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 117.0 in stage 6.0 (TID 564) in 921 ms on localhost (119/200)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 119.0 in stage 6.0 (TID 566) in 857 ms on localhost (120/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00050-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501241 length: 3501241 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO Executor: Finished task 120.0 in stage 6.0 (TID 567). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO TaskSetManager: Starting task 133.0 in stage 6.0 (TID 580, localhost, ANY, 1690 bytes)
15/08/21 08:57:24 INFO Executor: Running task 133.0 in stage 6.0 (TID 580)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 120.0 in stage 6.0 (TID 567) in 812 ms on localhost (121/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00089-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500320 length: 3500320 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 112 ms. row count = 750000
15/08/21 08:57:24 INFO Executor: Finished task 121.0 in stage 6.0 (TID 568). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO TaskSetManager: Starting task 134.0 in stage 6.0 (TID 581, localhost, ANY, 1693 bytes)
15/08/21 08:57:24 INFO Executor: Running task 134.0 in stage 6.0 (TID 581)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 121.0 in stage 6.0 (TID 568) in 769 ms on localhost (122/200)
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 750000
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00143-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504817 length: 3504817 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 750000
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 750000
15/08/21 08:57:24 INFO Executor: Finished task 123.0 in stage 6.0 (TID 570). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO TaskSetManager: Starting task 135.0 in stage 6.0 (TID 582, localhost, ANY, 1691 bytes)
15/08/21 08:57:24 INFO Executor: Running task 135.0 in stage 6.0 (TID 582)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 123.0 in stage 6.0 (TID 570) in 838 ms on localhost (123/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00170-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498753 length: 3498753 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 750000
15/08/21 08:57:24 INFO Executor: Finished task 122.0 in stage 6.0 (TID 569). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO TaskSetManager: Starting task 136.0 in stage 6.0 (TID 583, localhost, ANY, 1693 bytes)
15/08/21 08:57:24 INFO Executor: Running task 136.0 in stage 6.0 (TID 583)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 122.0 in stage 6.0 (TID 569) in 1114 ms on localhost (124/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00168-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505102 length: 3505102 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 INFO Executor: Finished task 125.0 in stage 6.0 (TID 572). 2125 bytes result sent to driver
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO TaskSetManager: Starting task 137.0 in stage 6.0 (TID 584, localhost, ANY, 1693 bytes)
15/08/21 08:57:24 INFO Executor: Running task 137.0 in stage 6.0 (TID 584)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 125.0 in stage 6.0 (TID 572) in 1043 ms on localhost (125/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00097-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499721 length: 3499721 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 750000
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO Executor: Finished task 124.0 in stage 6.0 (TID 571). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO TaskSetManager: Starting task 138.0 in stage 6.0 (TID 585, localhost, ANY, 1691 bytes)
15/08/21 08:57:24 INFO Executor: Running task 138.0 in stage 6.0 (TID 585)
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 08:57:24 INFO TaskSetManager: Finished task 124.0 in stage 6.0 (TID 571) in 1147 ms on localhost (126/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00026-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498365 length: 3498365 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 INFO Executor: Finished task 126.0 in stage 6.0 (TID 573). 2125 bytes result sent to driver
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO TaskSetManager: Starting task 139.0 in stage 6.0 (TID 586, localhost, ANY, 1692 bytes)
15/08/21 08:57:24 INFO Executor: Running task 139.0 in stage 6.0 (TID 586)
15/08/21 08:57:24 INFO Executor: Finished task 128.0 in stage 6.0 (TID 575). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00049-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501976 length: 3501976 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO TaskSetManager: Starting task 140.0 in stage 6.0 (TID 587, localhost, ANY, 1692 bytes)
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO Executor: Running task 140.0 in stage 6.0 (TID 587)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 126.0 in stage 6.0 (TID 573) in 1142 ms on localhost (127/200)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 128.0 in stage 6.0 (TID 575) in 1067 ms on localhost (128/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00120-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505053 length: 3505053 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO Executor: Finished task 127.0 in stage 6.0 (TID 574). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO TaskSetManager: Starting task 141.0 in stage 6.0 (TID 588, localhost, ANY, 1693 bytes)
15/08/21 08:57:24 INFO Executor: Running task 141.0 in stage 6.0 (TID 588)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 127.0 in stage 6.0 (TID 574) in 1132 ms on localhost (129/200)
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00192-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507651 length: 3507651 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO Executor: Finished task 129.0 in stage 6.0 (TID 576). 2125 bytes result sent to driver
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO TaskSetManager: Starting task 142.0 in stage 6.0 (TID 589, localhost, ANY, 1692 bytes)
15/08/21 08:57:24 INFO Executor: Running task 142.0 in stage 6.0 (TID 589)
15/08/21 08:57:24 INFO TaskSetManager: Finished task 129.0 in stage 6.0 (TID 576) in 1090 ms on localhost (130/200)
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 750000
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00162-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498776 length: 3498776 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 INFO Executor: Finished task 131.0 in stage 6.0 (TID 578). 2125 bytes result sent to driver
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:24 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 750000
15/08/21 08:57:24 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:24 INFO TaskSetManager: Starting task 143.0 in stage 6.0 (TID 590, localhost, ANY, 1693 bytes)
15/08/21 08:57:24 INFO Executor: Running task 143.0 in stage 6.0 (TID 590)
15/08/21 08:57:24 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:24 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00176-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504086 length: 3504086 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:24 INFO TaskSetManager: Finished task 131.0 in stage 6.0 (TID 578) in 965 ms on localhost (131/200)
15/08/21 08:57:24 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO Executor: Finished task 130.0 in stage 6.0 (TID 577). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO TaskSetManager: Starting task 144.0 in stage 6.0 (TID 591, localhost, ANY, 1692 bytes)
15/08/21 08:57:25 INFO Executor: Running task 144.0 in stage 6.0 (TID 591)
15/08/21 08:57:25 INFO Executor: Finished task 133.0 in stage 6.0 (TID 580). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00142-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505692 length: 3505692 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 INFO TaskSetManager: Starting task 145.0 in stage 6.0 (TID 592, localhost, ANY, 1691 bytes)
15/08/21 08:57:25 INFO Executor: Running task 145.0 in stage 6.0 (TID 592)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 130.0 in stage 6.0 (TID 577) in 1030 ms on localhost (132/200)
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO TaskSetManager: Finished task 133.0 in stage 6.0 (TID 580) in 921 ms on localhost (133/200)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00017-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500155 length: 3500155 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO Executor: Finished task 132.0 in stage 6.0 (TID 579). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO TaskSetManager: Starting task 146.0 in stage 6.0 (TID 593, localhost, ANY, 1692 bytes)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 132.0 in stage 6.0 (TID 579) in 1028 ms on localhost (134/200)
15/08/21 08:57:25 INFO Executor: Running task 146.0 in stage 6.0 (TID 593)
15/08/21 08:57:25 INFO Executor: Finished task 134.0 in stage 6.0 (TID 581). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO TaskSetManager: Starting task 147.0 in stage 6.0 (TID 594, localhost, ANY, 1691 bytes)
15/08/21 08:57:25 INFO Executor: Running task 147.0 in stage 6.0 (TID 594)
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 99 ms. row count = 750000
15/08/21 08:57:25 INFO TaskSetManager: Finished task 134.0 in stage 6.0 (TID 581) in 914 ms on localhost (135/200)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00177-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502215 length: 3502215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00166-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3503996 length: 3503996 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 59 ms. row count = 750000
15/08/21 08:57:25 INFO Executor: Finished task 135.0 in stage 6.0 (TID 582). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO TaskSetManager: Starting task 148.0 in stage 6.0 (TID 595, localhost, ANY, 1691 bytes)
15/08/21 08:57:25 INFO Executor: Running task 148.0 in stage 6.0 (TID 595)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00159-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505777 length: 3505777 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 INFO TaskSetManager: Finished task 135.0 in stage 6.0 (TID 582) in 726 ms on localhost (136/200)
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 750000
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 78 ms. row count = 750000
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 750000
15/08/21 08:57:25 INFO Executor: Finished task 145.0 in stage 6.0 (TID 592). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO TaskSetManager: Starting task 149.0 in stage 6.0 (TID 596, localhost, ANY, 1692 bytes)
15/08/21 08:57:25 INFO Executor: Running task 149.0 in stage 6.0 (TID 596)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 145.0 in stage 6.0 (TID 592) in 404 ms on localhost (137/200)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00125-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505822 length: 3505822 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO Executor: Finished task 136.0 in stage 6.0 (TID 583). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO TaskSetManager: Starting task 150.0 in stage 6.0 (TID 597, localhost, ANY, 1691 bytes)
15/08/21 08:57:25 INFO Executor: Running task 150.0 in stage 6.0 (TID 597)
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 750000
15/08/21 08:57:25 INFO TaskSetManager: Finished task 136.0 in stage 6.0 (TID 583) in 801 ms on localhost (138/200)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00041-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500431 length: 3500431 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 750000
15/08/21 08:57:25 INFO Executor: Finished task 137.0 in stage 6.0 (TID 584). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO TaskSetManager: Starting task 151.0 in stage 6.0 (TID 598, localhost, ANY, 1691 bytes)
15/08/21 08:57:25 INFO Executor: Running task 151.0 in stage 6.0 (TID 598)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 137.0 in stage 6.0 (TID 584) in 919 ms on localhost (139/200)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00007-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506054 length: 3506054 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO Executor: Finished task 138.0 in stage 6.0 (TID 585). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO TaskSetManager: Starting task 152.0 in stage 6.0 (TID 599, localhost, ANY, 1692 bytes)
15/08/21 08:57:25 INFO Executor: Running task 152.0 in stage 6.0 (TID 599)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 138.0 in stage 6.0 (TID 585) in 911 ms on localhost (140/200)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00129-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3497927 length: 3497927 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 08:57:25 INFO Executor: Finished task 139.0 in stage 6.0 (TID 586). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO Executor: Finished task 141.0 in stage 6.0 (TID 588). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO TaskSetManager: Starting task 153.0 in stage 6.0 (TID 600, localhost, ANY, 1692 bytes)
15/08/21 08:57:25 INFO Executor: Finished task 140.0 in stage 6.0 (TID 587). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO Executor: Running task 153.0 in stage 6.0 (TID 600)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00171-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3497374 length: 3497374 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 INFO TaskSetManager: Starting task 154.0 in stage 6.0 (TID 601, localhost, ANY, 1691 bytes)
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO Executor: Running task 154.0 in stage 6.0 (TID 601)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 139.0 in stage 6.0 (TID 586) in 974 ms on localhost (141/200)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00123-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501098 length: 3501098 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO TaskSetManager: Starting task 155.0 in stage 6.0 (TID 602, localhost, ANY, 1692 bytes)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 141.0 in stage 6.0 (TID 588) in 944 ms on localhost (142/200)
15/08/21 08:57:25 INFO Executor: Running task 155.0 in stage 6.0 (TID 602)
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 750000
15/08/21 08:57:25 INFO TaskSetManager: Finished task 140.0 in stage 6.0 (TID 587) in 991 ms on localhost (143/200)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00057-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3501590 length: 3501590 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 83 ms. row count = 750000
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 750000
15/08/21 08:57:25 INFO Executor: Finished task 142.0 in stage 6.0 (TID 589). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO TaskSetManager: Starting task 156.0 in stage 6.0 (TID 603, localhost, ANY, 1692 bytes)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 142.0 in stage 6.0 (TID 589) in 976 ms on localhost (144/200)
15/08/21 08:57:25 INFO Executor: Running task 156.0 in stage 6.0 (TID 603)
15/08/21 08:57:25 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 750000
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00153-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500279 length: 3500279 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO Executor: Finished task 146.0 in stage 6.0 (TID 593). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:25 INFO Executor: Finished task 144.0 in stage 6.0 (TID 591). 2125 bytes result sent to driver
15/08/21 08:57:25 INFO TaskSetManager: Starting task 157.0 in stage 6.0 (TID 604, localhost, ANY, 1691 bytes)
15/08/21 08:57:25 INFO Executor: Running task 157.0 in stage 6.0 (TID 604)
15/08/21 08:57:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00002-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499762 length: 3499762 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:25 INFO TaskSetManager: Starting task 158.0 in stage 6.0 (TID 605, localhost, ANY, 1692 bytes)
15/08/21 08:57:25 INFO TaskSetManager: Finished task 146.0 in stage 6.0 (TID 593) in 915 ms on localhost (145/200)
15/08/21 08:57:25 INFO Executor: Running task 158.0 in stage 6.0 (TID 605)
15/08/21 08:57:25 INFO Executor: Finished task 147.0 in stage 6.0 (TID 594). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00121-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3502280 length: 3502280 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO TaskSetManager: Starting task 159.0 in stage 6.0 (TID 606, localhost, ANY, 1692 bytes)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 144.0 in stage 6.0 (TID 591) in 998 ms on localhost (146/200)
15/08/21 08:57:26 INFO Executor: Running task 159.0 in stage 6.0 (TID 606)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 147.0 in stage 6.0 (TID 594) in 929 ms on localhost (147/200)
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 750000
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00149-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505750 length: 3505750 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 750000
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 76 ms. row count = 750000
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 368 ms. row count = 750000
15/08/21 08:57:26 INFO Executor: Finished task 152.0 in stage 6.0 (TID 599). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO TaskSetManager: Starting task 160.0 in stage 6.0 (TID 607, localhost, ANY, 1691 bytes)
15/08/21 08:57:26 INFO Executor: Running task 160.0 in stage 6.0 (TID 607)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 152.0 in stage 6.0 (TID 599) in 852 ms on localhost (148/200)
15/08/21 08:57:26 INFO Executor: Finished task 143.0 in stage 6.0 (TID 590). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00151-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3503903 length: 3503903 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO TaskSetManager: Starting task 161.0 in stage 6.0 (TID 608, localhost, ANY, 1692 bytes)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 143.0 in stage 6.0 (TID 590) in 1593 ms on localhost (149/200)
15/08/21 08:57:26 INFO Executor: Running task 161.0 in stage 6.0 (TID 608)
15/08/21 08:57:26 INFO Executor: Finished task 148.0 in stage 6.0 (TID 595). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00038-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506215 length: 3506215 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO TaskSetManager: Starting task 162.0 in stage 6.0 (TID 609, localhost, ANY, 1691 bytes)
15/08/21 08:57:26 INFO Executor: Running task 162.0 in stage 6.0 (TID 609)
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO TaskSetManager: Finished task 148.0 in stage 6.0 (TID 595) in 1453 ms on localhost (150/200)
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00164-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500845 length: 3500845 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO Executor: Finished task 52.0 in stage 4.0 (TID 442). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO TaskSetManager: Starting task 163.0 in stage 6.0 (TID 610, localhost, ANY, 1692 bytes)
15/08/21 08:57:26 INFO Executor: Running task 163.0 in stage 6.0 (TID 610)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 442) in 17037 ms on localhost (55/57)
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00117-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505296 length: 3505296 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO Executor: Finished task 149.0 in stage 6.0 (TID 596). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO TaskSetManager: Starting task 164.0 in stage 6.0 (TID 611, localhost, ANY, 1690 bytes)
15/08/21 08:57:26 INFO Executor: Running task 164.0 in stage 6.0 (TID 611)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 149.0 in stage 6.0 (TID 596) in 1278 ms on localhost (151/200)
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00060-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3503479 length: 3503479 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO Executor: Finished task 150.0 in stage 6.0 (TID 597). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO TaskSetManager: Starting task 165.0 in stage 6.0 (TID 612, localhost, ANY, 1691 bytes)
15/08/21 08:57:26 INFO Executor: Running task 165.0 in stage 6.0 (TID 612)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 150.0 in stage 6.0 (TID 597) in 1208 ms on localhost (152/200)
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00184-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507565 length: 3507565 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO Executor: Finished task 151.0 in stage 6.0 (TID 598). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO TaskSetManager: Starting task 166.0 in stage 6.0 (TID 613, localhost, ANY, 1691 bytes)
15/08/21 08:57:26 INFO Executor: Running task 166.0 in stage 6.0 (TID 613)
15/08/21 08:57:26 INFO Executor: Finished task 153.0 in stage 6.0 (TID 600). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO Executor: Finished task 154.0 in stage 6.0 (TID 601). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 08:57:26 INFO TaskSetManager: Starting task 167.0 in stage 6.0 (TID 614, localhost, ANY, 1694 bytes)
15/08/21 08:57:26 INFO Executor: Running task 167.0 in stage 6.0 (TID 614)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 151.0 in stage 6.0 (TID 598) in 1126 ms on localhost (153/200)
15/08/21 08:57:26 INFO TaskSetManager: Starting task 168.0 in stage 6.0 (TID 615, localhost, ANY, 1692 bytes)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 153.0 in stage 6.0 (TID 600) in 984 ms on localhost (154/200)
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00034-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498833 length: 3498833 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 INFO Executor: Running task 168.0 in stage 6.0 (TID 615)
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00165-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3508457 length: 3508457 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO TaskSetManager: Finished task 154.0 in stage 6.0 (TID 601) in 963 ms on localhost (155/200)
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00011-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3498475 length: 3498475 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 186 ms. row count = 750000
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 199 ms. row count = 750000
15/08/21 08:57:26 INFO Executor: Finished task 158.0 in stage 6.0 (TID 605). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 227 ms. row count = 750000
15/08/21 08:57:26 INFO TaskSetManager: Starting task 169.0 in stage 6.0 (TID 616, localhost, ANY, 1693 bytes)
15/08/21 08:57:26 INFO Executor: Running task 169.0 in stage 6.0 (TID 616)
15/08/21 08:57:26 INFO TaskSetManager: Finished task 158.0 in stage 6.0 (TID 605) in 886 ms on localhost (156/200)
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00136-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506182 length: 3506182 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 205 ms. row count = 750000
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 193 ms. row count = 750000
15/08/21 08:57:26 INFO Executor: Finished task 155.0 in stage 6.0 (TID 602). 2125 bytes result sent to driver
15/08/21 08:57:26 INFO TaskSetManager: Starting task 170.0 in stage 6.0 (TID 617, localhost, ANY, 1689 bytes)
15/08/21 08:57:26 INFO Executor: Running task 170.0 in stage 6.0 (TID 617)
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 161 ms. row count = 750000
15/08/21 08:57:26 INFO TaskSetManager: Finished task 155.0 in stage 6.0 (TID 602) in 1143 ms on localhost (157/200)
15/08/21 08:57:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00001-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500034 length: 3500034 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 182 ms. row count = 750000
15/08/21 08:57:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:26 INFO InternalParquetRecordReader: block read in memory in 211 ms. row count = 750000
15/08/21 08:57:27 INFO Executor: Finished task 156.0 in stage 6.0 (TID 603). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 171.0 in stage 6.0 (TID 618, localhost, ANY, 1690 bytes)
15/08/21 08:57:27 INFO Executor: Running task 171.0 in stage 6.0 (TID 618)
15/08/21 08:57:27 INFO Executor: Finished task 157.0 in stage 6.0 (TID 604). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Finished task 156.0 in stage 6.0 (TID 603) in 1142 ms on localhost (158/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00018-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500127 length: 3500127 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 INFO TaskSetManager: Starting task 172.0 in stage 6.0 (TID 619, localhost, ANY, 1692 bytes)
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO Executor: Running task 172.0 in stage 6.0 (TID 619)
15/08/21 08:57:27 INFO Executor: Finished task 159.0 in stage 6.0 (TID 606). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Finished task 157.0 in stage 6.0 (TID 604) in 1110 ms on localhost (159/200)
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 750000
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00044-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500433 length: 3500433 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 INFO TaskSetManager: Starting task 173.0 in stage 6.0 (TID 620, localhost, ANY, 1692 bytes)
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO Executor: Running task 173.0 in stage 6.0 (TID 620)
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO TaskSetManager: Finished task 159.0 in stage 6.0 (TID 606) in 1061 ms on localhost (160/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00088-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3508314 length: 3508314 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 187 ms. row count = 750000
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 750000
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 750000
15/08/21 08:57:27 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 750000
15/08/21 08:57:27 INFO Executor: Finished task 165.0 in stage 6.0 (TID 612). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 174.0 in stage 6.0 (TID 621, localhost, ANY, 1693 bytes)
15/08/21 08:57:27 INFO Executor: Running task 174.0 in stage 6.0 (TID 621)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 165.0 in stage 6.0 (TID 612) in 530 ms on localhost (161/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00053-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506395 length: 3506395 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO Executor: Finished task 54.0 in stage 4.0 (TID 444). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 175.0 in stage 6.0 (TID 622, localhost, ANY, 1692 bytes)
15/08/21 08:57:27 INFO Executor: Running task 175.0 in stage 6.0 (TID 622)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 444) in 16098 ms on localhost (56/57)
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 24 ms. row count = 750000
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00059-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499497 length: 3499497 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 39 ms. row count = 750000
15/08/21 08:57:27 INFO Executor: Finished task 160.0 in stage 6.0 (TID 607). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 176.0 in stage 6.0 (TID 623, localhost, ANY, 1692 bytes)
15/08/21 08:57:27 INFO Executor: Running task 176.0 in stage 6.0 (TID 623)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 160.0 in stage 6.0 (TID 607) in 884 ms on localhost (162/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00140-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499669 length: 3499669 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 750000
15/08/21 08:57:27 INFO Executor: Finished task 161.0 in stage 6.0 (TID 608). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO Executor: Finished task 162.0 in stage 6.0 (TID 609). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 177.0 in stage 6.0 (TID 624, localhost, ANY, 1692 bytes)
15/08/21 08:57:27 INFO Executor: Running task 177.0 in stage 6.0 (TID 624)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00081-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500815 length: 3500815 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO TaskSetManager: Starting task 178.0 in stage 6.0 (TID 625, localhost, ANY, 1693 bytes)
15/08/21 08:57:27 INFO Executor: Running task 178.0 in stage 6.0 (TID 625)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 161.0 in stage 6.0 (TID 608) in 977 ms on localhost (163/200)
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO Executor: Finished task 164.0 in stage 6.0 (TID 611). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00064-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504621 length: 3504621 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO TaskSetManager: Starting task 179.0 in stage 6.0 (TID 626, localhost, ANY, 1692 bytes)
15/08/21 08:57:27 INFO Executor: Running task 179.0 in stage 6.0 (TID 626)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 162.0 in stage 6.0 (TID 609) in 980 ms on localhost (164/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00079-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505149 length: 3505149 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 INFO TaskSetManager: Finished task 164.0 in stage 6.0 (TID 611) in 890 ms on localhost (165/200)
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO Executor: Finished task 163.0 in stage 6.0 (TID 610). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO TaskSetManager: Starting task 180.0 in stage 6.0 (TID 627, localhost, ANY, 1690 bytes)
15/08/21 08:57:27 INFO Executor: Running task 180.0 in stage 6.0 (TID 627)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 163.0 in stage 6.0 (TID 610) in 970 ms on localhost (166/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00071-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505003 length: 3505003 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 750000
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 750000
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 750000
15/08/21 08:57:27 INFO Executor: Finished task 168.0 in stage 6.0 (TID 615). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 181.0 in stage 6.0 (TID 628, localhost, ANY, 1692 bytes)
15/08/21 08:57:27 INFO Executor: Running task 181.0 in stage 6.0 (TID 628)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 168.0 in stage 6.0 (TID 615) in 946 ms on localhost (167/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00056-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3503531 length: 3503531 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO Executor: Finished task 166.0 in stage 6.0 (TID 613). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO TaskSetManager: Starting task 182.0 in stage 6.0 (TID 629, localhost, ANY, 1693 bytes)
15/08/21 08:57:27 INFO Executor: Running task 182.0 in stage 6.0 (TID 629)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 166.0 in stage 6.0 (TID 613) in 992 ms on localhost (168/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00023-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3507108 length: 3507108 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO Executor: Finished task 167.0 in stage 6.0 (TID 614). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO Executor: Finished task 170.0 in stage 6.0 (TID 617). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO TaskSetManager: Starting task 183.0 in stage 6.0 (TID 630, localhost, ANY, 1691 bytes)
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 750000
15/08/21 08:57:27 INFO Executor: Running task 183.0 in stage 6.0 (TID 630)
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO TaskSetManager: Starting task 184.0 in stage 6.0 (TID 631, localhost, ANY, 1691 bytes)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 167.0 in stage 6.0 (TID 614) in 1019 ms on localhost (169/200)
15/08/21 08:57:27 INFO Executor: Running task 184.0 in stage 6.0 (TID 631)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00101-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3509794 length: 3509794 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 INFO TaskSetManager: Finished task 170.0 in stage 6.0 (TID 617) in 855 ms on localhost (170/200)
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00174-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505785 length: 3505785 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 INFO Executor: Finished task 171.0 in stage 6.0 (TID 618). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 40 ms. row count = 750000
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO TaskSetManager: Starting task 185.0 in stage 6.0 (TID 632, localhost, ANY, 1693 bytes)
15/08/21 08:57:27 INFO Executor: Running task 185.0 in stage 6.0 (TID 632)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 171.0 in stage 6.0 (TID 618) in 788 ms on localhost (171/200)
15/08/21 08:57:27 INFO Executor: Finished task 169.0 in stage 6.0 (TID 616). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00134-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506718 length: 3506718 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO TaskSetManager: Starting task 186.0 in stage 6.0 (TID 633, localhost, ANY, 1690 bytes)
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO TaskSetManager: Finished task 169.0 in stage 6.0 (TID 616) in 988 ms on localhost (172/200)
15/08/21 08:57:27 INFO Executor: Running task 186.0 in stage 6.0 (TID 633)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00047-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504640 length: 3504640 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO Executor: Finished task 172.0 in stage 6.0 (TID 619). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 750000
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 750000
15/08/21 08:57:27 INFO TaskSetManager: Starting task 187.0 in stage 6.0 (TID 634, localhost, ANY, 1691 bytes)
15/08/21 08:57:27 INFO Executor: Running task 187.0 in stage 6.0 (TID 634)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 172.0 in stage 6.0 (TID 619) in 844 ms on localhost (173/200)
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00189-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3503657 length: 3503657 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO Executor: Finished task 173.0 in stage 6.0 (TID 620). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 188.0 in stage 6.0 (TID 635, localhost, ANY, 1690 bytes)
15/08/21 08:57:27 INFO Executor: Running task 188.0 in stage 6.0 (TID 635)
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 750000
15/08/21 08:57:27 INFO TaskSetManager: Finished task 173.0 in stage 6.0 (TID 620) in 867 ms on localhost (174/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00178-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499383 length: 3499383 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 750000
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:27 INFO Executor: Finished task 175.0 in stage 6.0 (TID 622). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 189.0 in stage 6.0 (TID 636, localhost, ANY, 1691 bytes)
15/08/21 08:57:27 INFO Executor: Running task 189.0 in stage 6.0 (TID 636)
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 750000
15/08/21 08:57:27 INFO TaskSetManager: Finished task 175.0 in stage 6.0 (TID 622) in 697 ms on localhost (175/200)
15/08/21 08:57:27 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 750000
15/08/21 08:57:27 INFO Executor: Finished task 174.0 in stage 6.0 (TID 621). 2125 bytes result sent to driver
15/08/21 08:57:27 INFO TaskSetManager: Starting task 190.0 in stage 6.0 (TID 637, localhost, ANY, 1693 bytes)
15/08/21 08:57:27 INFO Executor: Running task 190.0 in stage 6.0 (TID 637)
15/08/21 08:57:27 INFO TaskSetManager: Finished task 174.0 in stage 6.0 (TID 621) in 772 ms on localhost (176/200)
15/08/21 08:57:27 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00068-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500774 length: 3500774 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00099-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500145 length: 3500145 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:28 INFO Executor: Finished task 176.0 in stage 6.0 (TID 623). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 191.0 in stage 6.0 (TID 638, localhost, ANY, 1692 bytes)
15/08/21 08:57:28 INFO Executor: Running task 191.0 in stage 6.0 (TID 638)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 176.0 in stage 6.0 (TID 623) in 623 ms on localhost (177/200)
15/08/21 08:57:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00054-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3508574 length: 3508574 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:28 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 750000
15/08/21 08:57:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:28 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 750000
15/08/21 08:57:28 INFO InternalParquetRecordReader: block read in memory in 36 ms. row count = 750000
15/08/21 08:57:28 INFO Executor: Finished task 55.0 in stage 4.0 (TID 445). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 192.0 in stage 6.0 (TID 639, localhost, ANY, 1692 bytes)
15/08/21 08:57:28 INFO Executor: Running task 192.0 in stage 6.0 (TID 639)
15/08/21 08:57:28 INFO DAGScheduler: ShuffleMapStage 4 (processCmd at CliDriver.java:423) finished in 62.204 s
15/08/21 08:57:28 INFO DAGScheduler: looking for newly runnable stages
15/08/21 08:57:28 INFO DAGScheduler: running: Set(ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 08:57:28 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 8)
15/08/21 08:57:28 INFO DAGScheduler: failed: Set()
15/08/21 08:57:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00045-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506318 length: 3506318 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:28 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@32b5a3f1
15/08/21 08:57:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:28 INFO StatsReportListener: task runtime:(count: 234, mean: 3785.465812, stdev: 5872.654812, max: 24511.000000, min: 262.000000)
15/08/21 08:57:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:57:28 INFO StatsReportListener: 	262.0 ms	416.0 ms	520.0 ms	632.0 ms	838.0 ms	1.5 s	13.2 s	17.0 s	24.5 s
15/08/21 08:57:28 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 445) in 17003 ms on localhost (57/57)
15/08/21 08:57:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/21 08:57:28 INFO StatsReportListener: shuffle bytes written:(count: 234, mean: 15971787.760684, stdev: 29124128.789467, max: 76375224.000000, min: 0.000000)
15/08/21 08:57:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:57:28 INFO StatsReportListener: 	0.0 B	0.0 B	3.1 KB	3.2 KB	3.2 KB	3.2 KB	72.8 MB	72.8 MB	72.8 MB
15/08/21 08:57:28 INFO StatsReportListener: task result size:(count: 234, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 08:57:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:57:28 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 08:57:28 INFO DAGScheduler: Missing parents for ShuffleMapStage 5: List()
15/08/21 08:57:28 INFO StatsReportListener: executor (non-fetch) time pct: (count: 234, mean: 95.628828, stdev: 3.738108, max: 99.905894, min: 66.666667)
15/08/21 08:57:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:57:28 INFO StatsReportListener: 	67 %	90 %	92 %	94 %	96 %	98 %	100 %	100 %	100 %
15/08/21 08:57:28 INFO StatsReportListener: other time pct: (count: 234, mean: 4.371172, stdev: 3.738108, max: 33.333333, min: 0.094106)
15/08/21 08:57:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:57:28 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 2 %	 4 %	 6 %	 8 %	10 %	33 %
15/08/21 08:57:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:28 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7)
15/08/21 08:57:28 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[33] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 08:57:28 INFO Executor: Finished task 177.0 in stage 6.0 (TID 624). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 193.0 in stage 6.0 (TID 640, localhost, ANY, 1692 bytes)
15/08/21 08:57:28 INFO Executor: Running task 193.0 in stage 6.0 (TID 640)
15/08/21 08:57:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:28 INFO TaskSetManager: Finished task 177.0 in stage 6.0 (TID 624) in 754 ms on localhost (178/200)
15/08/21 08:57:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00133-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506051 length: 3506051 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:28 INFO MemoryStore: ensureFreeSpace(9704) called with curMem=1429877, maxMem=22226833244
15/08/21 08:57:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.5 KB, free 20.7 GB)
15/08/21 08:57:28 INFO Executor: Finished task 178.0 in stage 6.0 (TID 625). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 194.0 in stage 6.0 (TID 641, localhost, ANY, 1692 bytes)
15/08/21 08:57:28 INFO Executor: Finished task 179.0 in stage 6.0 (TID 626). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:28 INFO Executor: Running task 194.0 in stage 6.0 (TID 641)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 178.0 in stage 6.0 (TID 625) in 782 ms on localhost (179/200)
15/08/21 08:57:28 INFO TaskSetManager: Starting task 195.0 in stage 6.0 (TID 642, localhost, ANY, 1693 bytes)
15/08/21 08:57:28 INFO Executor: Running task 195.0 in stage 6.0 (TID 642)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 179.0 in stage 6.0 (TID 626) in 783 ms on localhost (180/200)
15/08/21 08:57:28 INFO InternalParquetRecordReader: block read in memory in 67 ms. row count = 750000
15/08/21 08:57:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00119-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3506360 length: 3506360 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00066-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3499206 length: 3499206 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:28 INFO MemoryStore: ensureFreeSpace(5005) called with curMem=1439581, maxMem=22226833244
15/08/21 08:57:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.9 KB, free 20.7 GB)
15/08/21 08:57:28 INFO Executor: Finished task 180.0 in stage 6.0 (TID 627). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:51693 (size: 4.9 KB, free: 20.7 GB)
15/08/21 08:57:28 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874
15/08/21 08:57:28 INFO TaskSetManager: Starting task 196.0 in stage 6.0 (TID 643, localhost, ANY, 1691 bytes)
15/08/21 08:57:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:28 INFO Executor: Running task 196.0 in stage 6.0 (TID 643)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 180.0 in stage 6.0 (TID 627) in 754 ms on localhost (181/200)
15/08/21 08:57:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00199-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3505199 length: 3505199 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:57:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:28 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 750000
15/08/21 08:57:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:57:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:28 INFO DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[33] at processCmd at CliDriver.java:423)
15/08/21 08:57:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 200 tasks
15/08/21 08:57:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:57:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:57:28 INFO Executor: Finished task 181.0 in stage 6.0 (TID 628). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 750000
15/08/21 08:57:28 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 750000
15/08/21 08:57:28 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 644, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 0.0 in stage 5.0 (TID 644)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 181.0 in stage 6.0 (TID 628) in 788 ms on localhost (182/200)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 750000
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO Executor: Finished task 184.0 in stage 6.0 (TID 631). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 645, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 1.0 in stage 5.0 (TID 645)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 184.0 in stage 6.0 (TID 631) in 781 ms on localhost (183/200)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO Executor: Finished task 188.0 in stage 6.0 (TID 635). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO Executor: Finished task 182.0 in stage 6.0 (TID 629). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 646, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 2.0 in stage 5.0 (TID 646)
15/08/21 08:57:28 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 647, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 3.0 in stage 5.0 (TID 647)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 188.0 in stage 6.0 (TID 635) in 764 ms on localhost (184/200)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 182.0 in stage 6.0 (TID 629) in 960 ms on localhost (185/200)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:28 INFO Executor: Finished task 186.0 in stage 6.0 (TID 633). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 648, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 4.0 in stage 5.0 (TID 648)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 186.0 in stage 6.0 (TID 633) in 934 ms on localhost (186/200)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO Executor: Finished task 185.0 in stage 6.0 (TID 632). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 649, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 5.0 in stage 5.0 (TID 649)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 185.0 in stage 6.0 (TID 632) in 1011 ms on localhost (187/200)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:28 INFO Executor: Finished task 183.0 in stage 6.0 (TID 630). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 650, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 6.0 in stage 5.0 (TID 650)
15/08/21 08:57:28 INFO Executor: Finished task 190.0 in stage 6.0 (TID 637). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Finished task 183.0 in stage 6.0 (TID 630) in 1135 ms on localhost (188/200)
15/08/21 08:57:28 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 651, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 7.0 in stage 5.0 (TID 651)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO TaskSetManager: Finished task 190.0 in stage 6.0 (TID 637) in 897 ms on localhost (189/200)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:28 INFO Executor: Finished task 187.0 in stage 6.0 (TID 634). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:28 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 652, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 8.0 in stage 5.0 (TID 652)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO TaskSetManager: Finished task 187.0 in stage 6.0 (TID 634) in 1001 ms on localhost (190/200)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:28 INFO Executor: Finished task 189.0 in stage 6.0 (TID 636). 2125 bytes result sent to driver
15/08/21 08:57:28 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 653, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:28 INFO Executor: Running task 9.0 in stage 5.0 (TID 653)
15/08/21 08:57:28 INFO TaskSetManager: Finished task 189.0 in stage 6.0 (TID 636) in 948 ms on localhost (191/200)
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO Executor: Finished task 191.0 in stage 6.0 (TID 638). 2125 bytes result sent to driver
15/08/21 08:57:29 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 654, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:29 INFO Executor: Running task 10.0 in stage 5.0 (TID 654)
15/08/21 08:57:29 INFO TaskSetManager: Finished task 191.0 in stage 6.0 (TID 638) in 995 ms on localhost (192/200)
15/08/21 08:57:29 INFO Executor: Finished task 193.0 in stage 6.0 (TID 640). 2125 bytes result sent to driver
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:29 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 655, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:29 INFO Executor: Running task 11.0 in stage 5.0 (TID 655)
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:29 INFO TaskSetManager: Finished task 193.0 in stage 6.0 (TID 640) in 792 ms on localhost (193/200)
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO Executor: Finished task 195.0 in stage 6.0 (TID 642). 2125 bytes result sent to driver
15/08/21 08:57:29 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 656, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:29 INFO Executor: Finished task 196.0 in stage 6.0 (TID 643). 2125 bytes result sent to driver
15/08/21 08:57:29 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 657, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:29 INFO TaskSetManager: Finished task 195.0 in stage 6.0 (TID 642) in 835 ms on localhost (194/200)
15/08/21 08:57:29 INFO Executor: Running task 13.0 in stage 5.0 (TID 657)
15/08/21 08:57:29 INFO Executor: Running task 12.0 in stage 5.0 (TID 656)
15/08/21 08:57:29 INFO TaskSetManager: Finished task 196.0 in stage 6.0 (TID 643) in 808 ms on localhost (195/200)
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO Executor: Finished task 192.0 in stage 6.0 (TID 639). 2125 bytes result sent to driver
15/08/21 08:57:29 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 658, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:29 INFO Executor: Running task 14.0 in stage 5.0 (TID 658)
15/08/21 08:57:29 INFO TaskSetManager: Finished task 192.0 in stage 6.0 (TID 639) in 982 ms on localhost (196/200)
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:29 INFO Executor: Finished task 194.0 in stage 6.0 (TID 641). 2125 bytes result sent to driver
15/08/21 08:57:29 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 659, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:29 INFO Executor: Running task 15.0 in stage 5.0 (TID 659)
15/08/21 08:57:29 INFO TaskSetManager: Finished task 194.0 in stage 6.0 (TID 641) in 952 ms on localhost (197/200)
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:35 INFO Executor: Finished task 3.0 in stage 5.0 (TID 647). 1219 bytes result sent to driver
15/08/21 08:57:35 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 660, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:35 INFO Executor: Running task 16.0 in stage 5.0 (TID 660)
15/08/21 08:57:35 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 647) in 6401 ms on localhost (1/200)
15/08/21 08:57:35 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:35 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:35 INFO Executor: Finished task 14.0 in stage 5.0 (TID 658). 1219 bytes result sent to driver
15/08/21 08:57:35 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 661, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:35 INFO Executor: Running task 17.0 in stage 5.0 (TID 661)
15/08/21 08:57:35 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 658) in 5939 ms on localhost (2/200)
15/08/21 08:57:35 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:35 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:36 INFO Executor: Finished task 1.0 in stage 5.0 (TID 645). 1219 bytes result sent to driver
15/08/21 08:57:36 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 662, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:36 INFO Executor: Running task 18.0 in stage 5.0 (TID 662)
15/08/21 08:57:36 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 645) in 7813 ms on localhost (3/200)
15/08/21 08:57:36 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:36 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:36 INFO Executor: Finished task 6.0 in stage 5.0 (TID 650). 1219 bytes result sent to driver
15/08/21 08:57:36 INFO TaskSetManager: Starting task 19.0 in stage 5.0 (TID 663, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:36 INFO Executor: Running task 19.0 in stage 5.0 (TID 663)
15/08/21 08:57:36 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 650) in 7848 ms on localhost (4/200)
15/08/21 08:57:36 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:36 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO Executor: Finished task 2.0 in stage 5.0 (TID 646). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO Executor: Finished task 9.0 in stage 5.0 (TID 653). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 20.0 in stage 5.0 (TID 664, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 20.0 in stage 5.0 (TID 664)
15/08/21 08:57:37 INFO TaskSetManager: Starting task 21.0 in stage 5.0 (TID 665, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 21.0 in stage 5.0 (TID 665)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 646) in 8458 ms on localhost (5/200)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 653) in 8219 ms on localhost (6/200)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:37 INFO Executor: Finished task 8.0 in stage 5.0 (TID 652). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 22.0 in stage 5.0 (TID 666, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 22.0 in stage 5.0 (TID 666)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 652) in 8373 ms on localhost (7/200)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO Executor: Finished task 13.0 in stage 5.0 (TID 657). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 23.0 in stage 5.0 (TID 667, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 23.0 in stage 5.0 (TID 667)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 657) in 8185 ms on localhost (8/200)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:37 INFO Executor: Finished task 5.0 in stage 5.0 (TID 649). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 24.0 in stage 5.0 (TID 668, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 24.0 in stage 5.0 (TID 668)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 649) in 8537 ms on localhost (9/200)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO Executor: Finished task 4.0 in stage 5.0 (TID 648). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 25.0 in stage 5.0 (TID 669, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 25.0 in stage 5.0 (TID 669)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 648) in 8751 ms on localhost (10/200)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO Executor: Finished task 12.0 in stage 5.0 (TID 656). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO Executor: Finished task 0.0 in stage 5.0 (TID 644). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 26.0 in stage 5.0 (TID 670, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO TaskSetManager: Starting task 27.0 in stage 5.0 (TID 671, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 656) in 8410 ms on localhost (11/200)
15/08/21 08:57:37 INFO Executor: Running task 27.0 in stage 5.0 (TID 671)
15/08/21 08:57:37 INFO Executor: Finished task 11.0 in stage 5.0 (TID 655). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 644) in 9134 ms on localhost (12/200)
15/08/21 08:57:37 INFO TaskSetManager: Starting task 28.0 in stage 5.0 (TID 672, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 28.0 in stage 5.0 (TID 672)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 655) in 8515 ms on localhost (13/200)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:37 INFO Executor: Running task 26.0 in stage 5.0 (TID 670)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 08:57:37 INFO Executor: Finished task 15.0 in stage 5.0 (TID 659). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 29.0 in stage 5.0 (TID 673, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 29.0 in stage 5.0 (TID 673)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 659) in 8371 ms on localhost (14/200)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:37 INFO Executor: Finished task 7.0 in stage 5.0 (TID 651). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 30.0 in stage 5.0 (TID 674, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 30.0 in stage 5.0 (TID 674)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 651) in 8796 ms on localhost (15/200)
15/08/21 08:57:37 INFO Executor: Finished task 10.0 in stage 5.0 (TID 654). 1219 bytes result sent to driver
15/08/21 08:57:37 INFO TaskSetManager: Starting task 31.0 in stage 5.0 (TID 675, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:37 INFO Executor: Running task 31.0 in stage 5.0 (TID 675)
15/08/21 08:57:37 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 654) in 8651 ms on localhost (16/200)
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:41 INFO Executor: Finished task 16.0 in stage 5.0 (TID 660). 1219 bytes result sent to driver
15/08/21 08:57:41 INFO TaskSetManager: Starting task 32.0 in stage 5.0 (TID 676, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:41 INFO Executor: Running task 32.0 in stage 5.0 (TID 676)
15/08/21 08:57:41 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 660) in 6626 ms on localhost (17/200)
15/08/21 08:57:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:41 INFO Executor: Finished task 17.0 in stage 5.0 (TID 661). 1219 bytes result sent to driver
15/08/21 08:57:41 INFO TaskSetManager: Starting task 33.0 in stage 5.0 (TID 677, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:41 INFO Executor: Running task 33.0 in stage 5.0 (TID 677)
15/08/21 08:57:41 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 661) in 6848 ms on localhost (18/200)
15/08/21 08:57:41 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:41 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:42 INFO Executor: Finished task 18.0 in stage 5.0 (TID 662). 1219 bytes result sent to driver
15/08/21 08:57:42 INFO TaskSetManager: Starting task 34.0 in stage 5.0 (TID 678, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:42 INFO Executor: Running task 34.0 in stage 5.0 (TID 678)
15/08/21 08:57:42 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 662) in 6181 ms on localhost (19/200)
15/08/21 08:57:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:57:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:43 INFO Executor: Finished task 21.0 in stage 5.0 (TID 665). 1219 bytes result sent to driver
15/08/21 08:57:43 INFO TaskSetManager: Starting task 35.0 in stage 5.0 (TID 679, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:43 INFO TaskSetManager: Finished task 21.0 in stage 5.0 (TID 665) in 6031 ms on localhost (20/200)
15/08/21 08:57:43 INFO Executor: Finished task 23.0 in stage 5.0 (TID 667). 1219 bytes result sent to driver
15/08/21 08:57:43 INFO Executor: Running task 35.0 in stage 5.0 (TID 679)
15/08/21 08:57:43 INFO TaskSetManager: Starting task 36.0 in stage 5.0 (TID 680, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:43 INFO Executor: Running task 36.0 in stage 5.0 (TID 680)
15/08/21 08:57:43 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:43 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:43 INFO TaskSetManager: Finished task 23.0 in stage 5.0 (TID 667) in 5868 ms on localhost (21/200)
15/08/21 08:57:43 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 08:57:43 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO Executor: Finished task 19.0 in stage 5.0 (TID 663). 1219 bytes result sent to driver
15/08/21 08:57:44 INFO TaskSetManager: Starting task 37.0 in stage 5.0 (TID 681, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:44 INFO Executor: Running task 37.0 in stage 5.0 (TID 681)
15/08/21 08:57:44 INFO TaskSetManager: Finished task 19.0 in stage 5.0 (TID 663) in 7518 ms on localhost (22/200)
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO Executor: Finished task 27.0 in stage 5.0 (TID 671). 1219 bytes result sent to driver
15/08/21 08:57:44 INFO TaskSetManager: Starting task 38.0 in stage 5.0 (TID 682, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:44 INFO Executor: Running task 38.0 in stage 5.0 (TID 682)
15/08/21 08:57:44 INFO TaskSetManager: Finished task 27.0 in stage 5.0 (TID 671) in 6908 ms on localhost (23/200)
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO Executor: Finished task 20.0 in stage 5.0 (TID 664). 1219 bytes result sent to driver
15/08/21 08:57:44 INFO TaskSetManager: Starting task 39.0 in stage 5.0 (TID 683, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:44 INFO Executor: Running task 39.0 in stage 5.0 (TID 683)
15/08/21 08:57:44 INFO TaskSetManager: Finished task 20.0 in stage 5.0 (TID 664) in 7629 ms on localhost (24/200)
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO Executor: Finished task 24.0 in stage 5.0 (TID 668). 1219 bytes result sent to driver
15/08/21 08:57:44 INFO TaskSetManager: Starting task 40.0 in stage 5.0 (TID 684, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:44 INFO Executor: Running task 40.0 in stage 5.0 (TID 684)
15/08/21 08:57:44 INFO TaskSetManager: Finished task 24.0 in stage 5.0 (TID 668) in 7538 ms on localhost (25/200)
15/08/21 08:57:44 INFO Executor: Finished task 26.0 in stage 5.0 (TID 670). 1219 bytes result sent to driver
15/08/21 08:57:44 INFO TaskSetManager: Starting task 41.0 in stage 5.0 (TID 685, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:44 INFO Executor: Running task 41.0 in stage 5.0 (TID 685)
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO TaskSetManager: Finished task 26.0 in stage 5.0 (TID 670) in 7372 ms on localhost (26/200)
15/08/21 08:57:44 INFO Executor: Finished task 22.0 in stage 5.0 (TID 666). 1219 bytes result sent to driver
15/08/21 08:57:44 INFO TaskSetManager: Starting task 42.0 in stage 5.0 (TID 686, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:44 INFO TaskSetManager: Finished task 22.0 in stage 5.0 (TID 666) in 7720 ms on localhost (27/200)
15/08/21 08:57:44 INFO Executor: Running task 42.0 in stage 5.0 (TID 686)
15/08/21 08:57:44 INFO Executor: Finished task 25.0 in stage 5.0 (TID 669). 1219 bytes result sent to driver
15/08/21 08:57:44 INFO TaskSetManager: Starting task 43.0 in stage 5.0 (TID 687, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:44 INFO Executor: Running task 43.0 in stage 5.0 (TID 687)
15/08/21 08:57:44 INFO TaskSetManager: Finished task 25.0 in stage 5.0 (TID 669) in 7484 ms on localhost (28/200)
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:45 INFO Executor: Finished task 29.0 in stage 5.0 (TID 673). 1219 bytes result sent to driver
15/08/21 08:57:45 INFO Executor: Finished task 28.0 in stage 5.0 (TID 672). 1219 bytes result sent to driver
15/08/21 08:57:45 INFO TaskSetManager: Starting task 44.0 in stage 5.0 (TID 688, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:45 INFO TaskSetManager: Starting task 45.0 in stage 5.0 (TID 689, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:45 INFO Executor: Running task 44.0 in stage 5.0 (TID 688)
15/08/21 08:57:45 INFO Executor: Running task 45.0 in stage 5.0 (TID 689)
15/08/21 08:57:45 INFO TaskSetManager: Finished task 29.0 in stage 5.0 (TID 673) in 7505 ms on localhost (29/200)
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:45 INFO TaskSetManager: Finished task 28.0 in stage 5.0 (TID 672) in 7544 ms on localhost (30/200)
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:45 INFO Executor: Finished task 30.0 in stage 5.0 (TID 674). 1219 bytes result sent to driver
15/08/21 08:57:45 INFO TaskSetManager: Starting task 46.0 in stage 5.0 (TID 690, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:45 INFO Executor: Running task 46.0 in stage 5.0 (TID 690)
15/08/21 08:57:45 INFO TaskSetManager: Finished task 30.0 in stage 5.0 (TID 674) in 7599 ms on localhost (31/200)
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:45 INFO Executor: Finished task 31.0 in stage 5.0 (TID 675). 1219 bytes result sent to driver
15/08/21 08:57:45 INFO TaskSetManager: Starting task 47.0 in stage 5.0 (TID 691, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:45 INFO TaskSetManager: Finished task 31.0 in stage 5.0 (TID 675) in 7794 ms on localhost (32/200)
15/08/21 08:57:45 INFO Executor: Running task 47.0 in stage 5.0 (TID 691)
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:48 INFO Executor: Finished task 33.0 in stage 5.0 (TID 677). 1219 bytes result sent to driver
15/08/21 08:57:48 INFO TaskSetManager: Starting task 48.0 in stage 5.0 (TID 692, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:48 INFO Executor: Running task 48.0 in stage 5.0 (TID 692)
15/08/21 08:57:48 INFO TaskSetManager: Finished task 33.0 in stage 5.0 (TID 677) in 6544 ms on localhost (33/200)
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:48 INFO Executor: Finished task 34.0 in stage 5.0 (TID 678). 1219 bytes result sent to driver
15/08/21 08:57:48 INFO TaskSetManager: Starting task 49.0 in stage 5.0 (TID 693, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:48 INFO TaskSetManager: Finished task 34.0 in stage 5.0 (TID 678) in 6216 ms on localhost (34/200)
15/08/21 08:57:48 INFO Executor: Running task 49.0 in stage 5.0 (TID 693)
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:48 INFO Executor: Finished task 32.0 in stage 5.0 (TID 676). 1219 bytes result sent to driver
15/08/21 08:57:48 INFO TaskSetManager: Starting task 50.0 in stage 5.0 (TID 694, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:48 INFO TaskSetManager: Finished task 32.0 in stage 5.0 (TID 676) in 7296 ms on localhost (35/200)
15/08/21 08:57:48 INFO Executor: Running task 50.0 in stage 5.0 (TID 694)
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:49 INFO Executor: Finished task 36.0 in stage 5.0 (TID 680). 1219 bytes result sent to driver
15/08/21 08:57:49 INFO TaskSetManager: Starting task 51.0 in stage 5.0 (TID 695, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:49 INFO Executor: Running task 51.0 in stage 5.0 (TID 695)
15/08/21 08:57:49 INFO TaskSetManager: Finished task 36.0 in stage 5.0 (TID 680) in 6167 ms on localhost (36/200)
15/08/21 08:57:49 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:49 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:50 INFO Executor: Finished task 35.0 in stage 5.0 (TID 679). 1219 bytes result sent to driver
15/08/21 08:57:50 INFO TaskSetManager: Starting task 52.0 in stage 5.0 (TID 696, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:50 INFO Executor: Running task 52.0 in stage 5.0 (TID 696)
15/08/21 08:57:50 INFO TaskSetManager: Finished task 35.0 in stage 5.0 (TID 679) in 7325 ms on localhost (37/200)
15/08/21 08:57:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 08:57:50 INFO Executor: Finished task 37.0 in stage 5.0 (TID 681). 1219 bytes result sent to driver
15/08/21 08:57:50 INFO TaskSetManager: Starting task 53.0 in stage 5.0 (TID 697, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:50 INFO Executor: Running task 53.0 in stage 5.0 (TID 697)
15/08/21 08:57:50 INFO TaskSetManager: Finished task 37.0 in stage 5.0 (TID 681) in 6466 ms on localhost (38/200)
15/08/21 08:57:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:51 INFO Executor: Finished task 39.0 in stage 5.0 (TID 683). 1219 bytes result sent to driver
15/08/21 08:57:51 INFO TaskSetManager: Starting task 54.0 in stage 5.0 (TID 698, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:51 INFO Executor: Running task 54.0 in stage 5.0 (TID 698)
15/08/21 08:57:51 INFO TaskSetManager: Finished task 39.0 in stage 5.0 (TID 683) in 6430 ms on localhost (39/200)
15/08/21 08:57:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:51 INFO Executor: Finished task 43.0 in stage 5.0 (TID 687). 1219 bytes result sent to driver
15/08/21 08:57:51 INFO TaskSetManager: Starting task 55.0 in stage 5.0 (TID 699, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:51 INFO Executor: Running task 55.0 in stage 5.0 (TID 699)
15/08/21 08:57:51 INFO TaskSetManager: Finished task 43.0 in stage 5.0 (TID 687) in 6893 ms on localhost (40/200)
15/08/21 08:57:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:57:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:57:51 INFO Executor: Finished task 44.0 in stage 5.0 (TID 688). 1219 bytes result sent to driver
15/08/21 08:57:51 INFO TaskSetManager: Starting task 56.0 in stage 5.0 (TID 700, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:51 INFO Executor: Running task 56.0 in stage 5.0 (TID 700)
15/08/21 08:57:51 INFO TaskSetManager: Finished task 44.0 in stage 5.0 (TID 688) in 6917 ms on localhost (41/200)
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO Executor: Finished task 45.0 in stage 5.0 (TID 689). 1219 bytes result sent to driver
15/08/21 08:57:52 INFO TaskSetManager: Starting task 57.0 in stage 5.0 (TID 701, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:52 INFO Executor: Running task 57.0 in stage 5.0 (TID 701)
15/08/21 08:57:52 INFO TaskSetManager: Finished task 45.0 in stage 5.0 (TID 689) in 6976 ms on localhost (42/200)
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:52 INFO Executor: Finished task 40.0 in stage 5.0 (TID 684). 1219 bytes result sent to driver
15/08/21 08:57:52 INFO TaskSetManager: Starting task 58.0 in stage 5.0 (TID 702, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:52 INFO Executor: Running task 58.0 in stage 5.0 (TID 702)
15/08/21 08:57:52 INFO TaskSetManager: Finished task 40.0 in stage 5.0 (TID 684) in 7711 ms on localhost (43/200)
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO Executor: Finished task 46.0 in stage 5.0 (TID 690). 1219 bytes result sent to driver
15/08/21 08:57:52 INFO TaskSetManager: Starting task 59.0 in stage 5.0 (TID 703, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:52 INFO Executor: Running task 59.0 in stage 5.0 (TID 703)
15/08/21 08:57:52 INFO TaskSetManager: Finished task 46.0 in stage 5.0 (TID 690) in 7384 ms on localhost (44/200)
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO Executor: Finished task 47.0 in stage 5.0 (TID 691). 1219 bytes result sent to driver
15/08/21 08:57:52 INFO TaskSetManager: Starting task 60.0 in stage 5.0 (TID 704, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:52 INFO Executor: Running task 60.0 in stage 5.0 (TID 704)
15/08/21 08:57:52 INFO TaskSetManager: Finished task 47.0 in stage 5.0 (TID 691) in 7187 ms on localhost (45/200)
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO Executor: Finished task 42.0 in stage 5.0 (TID 686). 1219 bytes result sent to driver
15/08/21 08:57:52 INFO TaskSetManager: Starting task 61.0 in stage 5.0 (TID 705, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:52 INFO Executor: Running task 61.0 in stage 5.0 (TID 705)
15/08/21 08:57:52 INFO TaskSetManager: Finished task 42.0 in stage 5.0 (TID 686) in 7865 ms on localhost (46/200)
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO Executor: Finished task 38.0 in stage 5.0 (TID 682). 1219 bytes result sent to driver
15/08/21 08:57:52 INFO TaskSetManager: Starting task 62.0 in stage 5.0 (TID 706, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:52 INFO Executor: Running task 62.0 in stage 5.0 (TID 706)
15/08/21 08:57:52 INFO TaskSetManager: Finished task 38.0 in stage 5.0 (TID 682) in 8481 ms on localhost (47/200)
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:52 INFO Executor: Finished task 41.0 in stage 5.0 (TID 685). 1219 bytes result sent to driver
15/08/21 08:57:52 INFO TaskSetManager: Starting task 63.0 in stage 5.0 (TID 707, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:52 INFO Executor: Running task 63.0 in stage 5.0 (TID 707)
15/08/21 08:57:52 INFO TaskSetManager: Finished task 41.0 in stage 5.0 (TID 685) in 8072 ms on localhost (48/200)
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:55 INFO Executor: Finished task 48.0 in stage 5.0 (TID 692). 1219 bytes result sent to driver
15/08/21 08:57:55 INFO TaskSetManager: Starting task 64.0 in stage 5.0 (TID 708, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:55 INFO TaskSetManager: Finished task 48.0 in stage 5.0 (TID 692) in 7250 ms on localhost (49/200)
15/08/21 08:57:55 INFO Executor: Running task 64.0 in stage 5.0 (TID 708)
15/08/21 08:57:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:55 INFO Executor: Finished task 50.0 in stage 5.0 (TID 694). 1219 bytes result sent to driver
15/08/21 08:57:55 INFO TaskSetManager: Starting task 65.0 in stage 5.0 (TID 709, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:55 INFO Executor: Running task 65.0 in stage 5.0 (TID 709)
15/08/21 08:57:55 INFO TaskSetManager: Finished task 50.0 in stage 5.0 (TID 694) in 6962 ms on localhost (50/200)
15/08/21 08:57:55 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:55 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:57:56 INFO Executor: Finished task 49.0 in stage 5.0 (TID 693). 1219 bytes result sent to driver
15/08/21 08:57:56 INFO TaskSetManager: Starting task 66.0 in stage 5.0 (TID 710, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:56 INFO Executor: Finished task 51.0 in stage 5.0 (TID 695). 1219 bytes result sent to driver
15/08/21 08:57:56 INFO Executor: Running task 66.0 in stage 5.0 (TID 710)
15/08/21 08:57:56 INFO TaskSetManager: Starting task 67.0 in stage 5.0 (TID 711, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:56 INFO Executor: Running task 67.0 in stage 5.0 (TID 711)
15/08/21 08:57:56 INFO TaskSetManager: Finished task 51.0 in stage 5.0 (TID 695) in 6745 ms on localhost (51/200)
15/08/21 08:57:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:56 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:56 INFO TaskSetManager: Finished task 49.0 in stage 5.0 (TID 693) in 7372 ms on localhost (52/200)
15/08/21 08:57:56 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:57 INFO Executor: Finished task 52.0 in stage 5.0 (TID 696). 1219 bytes result sent to driver
15/08/21 08:57:57 INFO TaskSetManager: Starting task 68.0 in stage 5.0 (TID 712, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:57 INFO Executor: Running task 68.0 in stage 5.0 (TID 712)
15/08/21 08:57:57 INFO TaskSetManager: Finished task 52.0 in stage 5.0 (TID 696) in 6832 ms on localhost (53/200)
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:57 INFO Executor: Finished task 53.0 in stage 5.0 (TID 697). 1219 bytes result sent to driver
15/08/21 08:57:57 INFO TaskSetManager: Starting task 69.0 in stage 5.0 (TID 713, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:57 INFO TaskSetManager: Finished task 53.0 in stage 5.0 (TID 697) in 6708 ms on localhost (54/200)
15/08/21 08:57:57 INFO Executor: Running task 69.0 in stage 5.0 (TID 713)
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:57 INFO Executor: Finished task 54.0 in stage 5.0 (TID 698). 1219 bytes result sent to driver
15/08/21 08:57:57 INFO TaskSetManager: Starting task 70.0 in stage 5.0 (TID 714, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:57 INFO Executor: Running task 70.0 in stage 5.0 (TID 714)
15/08/21 08:57:57 INFO TaskSetManager: Finished task 54.0 in stage 5.0 (TID 698) in 6390 ms on localhost (55/200)
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:57 INFO Executor: Finished task 55.0 in stage 5.0 (TID 699). 1219 bytes result sent to driver
15/08/21 08:57:57 INFO TaskSetManager: Starting task 71.0 in stage 5.0 (TID 715, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:57 INFO TaskSetManager: Finished task 55.0 in stage 5.0 (TID 699) in 6125 ms on localhost (56/200)
15/08/21 08:57:57 INFO Executor: Running task 71.0 in stage 5.0 (TID 715)
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:58 INFO Executor: Finished task 56.0 in stage 5.0 (TID 700). 1219 bytes result sent to driver
15/08/21 08:57:58 INFO TaskSetManager: Starting task 72.0 in stage 5.0 (TID 716, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:58 INFO Executor: Running task 72.0 in stage 5.0 (TID 716)
15/08/21 08:57:58 INFO TaskSetManager: Finished task 56.0 in stage 5.0 (TID 700) in 6522 ms on localhost (57/200)
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:58 INFO Executor: Finished task 58.0 in stage 5.0 (TID 702). 1219 bytes result sent to driver
15/08/21 08:57:58 INFO TaskSetManager: Starting task 73.0 in stage 5.0 (TID 717, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:58 INFO Executor: Running task 73.0 in stage 5.0 (TID 717)
15/08/21 08:57:58 INFO TaskSetManager: Finished task 58.0 in stage 5.0 (TID 702) in 6234 ms on localhost (58/200)
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:58 INFO Executor: Finished task 57.0 in stage 5.0 (TID 701). 1219 bytes result sent to driver
15/08/21 08:57:58 INFO TaskSetManager: Starting task 74.0 in stage 5.0 (TID 718, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:58 INFO Executor: Running task 74.0 in stage 5.0 (TID 718)
15/08/21 08:57:58 INFO TaskSetManager: Finished task 57.0 in stage 5.0 (TID 701) in 6789 ms on localhost (59/200)
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:59 INFO Executor: Finished task 60.0 in stage 5.0 (TID 704). 1219 bytes result sent to driver
15/08/21 08:57:59 INFO TaskSetManager: Starting task 75.0 in stage 5.0 (TID 719, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:59 INFO Executor: Running task 75.0 in stage 5.0 (TID 719)
15/08/21 08:57:59 INFO TaskSetManager: Finished task 60.0 in stage 5.0 (TID 704) in 6474 ms on localhost (60/200)
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:59 INFO Executor: Finished task 59.0 in stage 5.0 (TID 703). 1219 bytes result sent to driver
15/08/21 08:57:59 INFO TaskSetManager: Starting task 76.0 in stage 5.0 (TID 720, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:59 INFO Executor: Running task 76.0 in stage 5.0 (TID 720)
15/08/21 08:57:59 INFO TaskSetManager: Finished task 59.0 in stage 5.0 (TID 703) in 6648 ms on localhost (61/200)
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:59 INFO Executor: Finished task 61.0 in stage 5.0 (TID 705). 1219 bytes result sent to driver
15/08/21 08:57:59 INFO TaskSetManager: Starting task 77.0 in stage 5.0 (TID 721, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:59 INFO Executor: Running task 77.0 in stage 5.0 (TID 721)
15/08/21 08:57:59 INFO TaskSetManager: Finished task 61.0 in stage 5.0 (TID 705) in 6508 ms on localhost (62/200)
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:59 INFO Executor: Finished task 62.0 in stage 5.0 (TID 706). 1219 bytes result sent to driver
15/08/21 08:57:59 INFO TaskSetManager: Starting task 78.0 in stage 5.0 (TID 722, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:59 INFO Executor: Running task 78.0 in stage 5.0 (TID 722)
15/08/21 08:57:59 INFO TaskSetManager: Finished task 62.0 in stage 5.0 (TID 706) in 6517 ms on localhost (63/200)
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:57:59 INFO Executor: Finished task 63.0 in stage 5.0 (TID 707). 1219 bytes result sent to driver
15/08/21 08:57:59 INFO TaskSetManager: Starting task 79.0 in stage 5.0 (TID 723, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:57:59 INFO TaskSetManager: Finished task 63.0 in stage 5.0 (TID 707) in 6572 ms on localhost (64/200)
15/08/21 08:57:59 INFO Executor: Running task 79.0 in stage 5.0 (TID 723)
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:57:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:01 INFO Executor: Finished task 65.0 in stage 5.0 (TID 709). 1219 bytes result sent to driver
15/08/21 08:58:01 INFO TaskSetManager: Starting task 80.0 in stage 5.0 (TID 724, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:01 INFO Executor: Running task 80.0 in stage 5.0 (TID 724)
15/08/21 08:58:01 INFO TaskSetManager: Finished task 65.0 in stage 5.0 (TID 709) in 5362 ms on localhost (65/200)
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:01 INFO Executor: Finished task 64.0 in stage 5.0 (TID 708). 1219 bytes result sent to driver
15/08/21 08:58:01 INFO TaskSetManager: Starting task 81.0 in stage 5.0 (TID 725, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:01 INFO TaskSetManager: Finished task 64.0 in stage 5.0 (TID 708) in 5607 ms on localhost (66/200)
15/08/21 08:58:01 INFO Executor: Running task 81.0 in stage 5.0 (TID 725)
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:01 INFO Executor: Finished task 67.0 in stage 5.0 (TID 711). 1219 bytes result sent to driver
15/08/21 08:58:01 INFO TaskSetManager: Starting task 82.0 in stage 5.0 (TID 726, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:01 INFO Executor: Running task 82.0 in stage 5.0 (TID 726)
15/08/21 08:58:01 INFO TaskSetManager: Finished task 67.0 in stage 5.0 (TID 711) in 5877 ms on localhost (67/200)
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:02 INFO Executor: Finished task 66.0 in stage 5.0 (TID 710). 1219 bytes result sent to driver
15/08/21 08:58:02 INFO TaskSetManager: Starting task 83.0 in stage 5.0 (TID 727, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:02 INFO Executor: Running task 83.0 in stage 5.0 (TID 727)
15/08/21 08:58:02 INFO TaskSetManager: Finished task 66.0 in stage 5.0 (TID 710) in 6281 ms on localhost (68/200)
15/08/21 08:58:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:02 INFO Executor: Finished task 68.0 in stage 5.0 (TID 712). 1219 bytes result sent to driver
15/08/21 08:58:02 INFO TaskSetManager: Starting task 84.0 in stage 5.0 (TID 728, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:02 INFO Executor: Running task 84.0 in stage 5.0 (TID 728)
15/08/21 08:58:02 INFO TaskSetManager: Finished task 68.0 in stage 5.0 (TID 712) in 5424 ms on localhost (69/200)
15/08/21 08:58:02 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:02 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:03 INFO Executor: Finished task 70.0 in stage 5.0 (TID 714). 1219 bytes result sent to driver
15/08/21 08:58:03 INFO TaskSetManager: Starting task 85.0 in stage 5.0 (TID 729, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:03 INFO TaskSetManager: Finished task 70.0 in stage 5.0 (TID 714) in 5457 ms on localhost (70/200)
15/08/21 08:58:03 INFO Executor: Finished task 69.0 in stage 5.0 (TID 713). 1219 bytes result sent to driver
15/08/21 08:58:03 INFO TaskSetManager: Starting task 86.0 in stage 5.0 (TID 730, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:03 INFO Executor: Running task 85.0 in stage 5.0 (TID 729)
15/08/21 08:58:03 INFO Executor: Running task 86.0 in stage 5.0 (TID 730)
15/08/21 08:58:03 INFO TaskSetManager: Finished task 69.0 in stage 5.0 (TID 713) in 5638 ms on localhost (71/200)
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:03 INFO Executor: Finished task 71.0 in stage 5.0 (TID 715). 1219 bytes result sent to driver
15/08/21 08:58:03 INFO TaskSetManager: Starting task 87.0 in stage 5.0 (TID 731, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:03 INFO Executor: Running task 87.0 in stage 5.0 (TID 731)
15/08/21 08:58:03 INFO TaskSetManager: Finished task 71.0 in stage 5.0 (TID 715) in 5380 ms on localhost (72/200)
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:03 INFO Executor: Finished task 72.0 in stage 5.0 (TID 716). 1219 bytes result sent to driver
15/08/21 08:58:03 INFO TaskSetManager: Starting task 88.0 in stage 5.0 (TID 732, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:03 INFO TaskSetManager: Finished task 72.0 in stage 5.0 (TID 716) in 5114 ms on localhost (73/200)
15/08/21 08:58:03 INFO Executor: Running task 88.0 in stage 5.0 (TID 732)
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:03 INFO Executor: Finished task 73.0 in stage 5.0 (TID 717). 1219 bytes result sent to driver
15/08/21 08:58:03 INFO TaskSetManager: Starting task 89.0 in stage 5.0 (TID 733, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:03 INFO Executor: Running task 89.0 in stage 5.0 (TID 733)
15/08/21 08:58:03 INFO TaskSetManager: Finished task 73.0 in stage 5.0 (TID 717) in 5176 ms on localhost (74/200)
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:04 INFO Executor: Finished task 75.0 in stage 5.0 (TID 719). 1219 bytes result sent to driver
15/08/21 08:58:04 INFO TaskSetManager: Starting task 90.0 in stage 5.0 (TID 734, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:04 INFO Executor: Running task 90.0 in stage 5.0 (TID 734)
15/08/21 08:58:04 INFO TaskSetManager: Finished task 75.0 in stage 5.0 (TID 719) in 5163 ms on localhost (75/200)
15/08/21 08:58:04 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:04 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:04 INFO Executor: Finished task 76.0 in stage 5.0 (TID 720). 1219 bytes result sent to driver
15/08/21 08:58:04 INFO TaskSetManager: Starting task 91.0 in stage 5.0 (TID 735, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:04 INFO Executor: Running task 91.0 in stage 5.0 (TID 735)
15/08/21 08:58:04 INFO TaskSetManager: Finished task 76.0 in stage 5.0 (TID 720) in 5342 ms on localhost (76/200)
15/08/21 08:58:04 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:04 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:05 INFO Executor: Finished task 77.0 in stage 5.0 (TID 721). 1219 bytes result sent to driver
15/08/21 08:58:05 INFO TaskSetManager: Starting task 92.0 in stage 5.0 (TID 736, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:05 INFO Executor: Running task 92.0 in stage 5.0 (TID 736)
15/08/21 08:58:05 INFO TaskSetManager: Finished task 77.0 in stage 5.0 (TID 721) in 5745 ms on localhost (77/200)
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:05 INFO Executor: Finished task 74.0 in stage 5.0 (TID 718). 1219 bytes result sent to driver
15/08/21 08:58:05 INFO TaskSetManager: Starting task 93.0 in stage 5.0 (TID 737, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:05 INFO Executor: Running task 93.0 in stage 5.0 (TID 737)
15/08/21 08:58:05 INFO TaskSetManager: Finished task 74.0 in stage 5.0 (TID 718) in 6255 ms on localhost (78/200)
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:05 INFO Executor: Finished task 78.0 in stage 5.0 (TID 722). 1219 bytes result sent to driver
15/08/21 08:58:05 INFO Executor: Finished task 79.0 in stage 5.0 (TID 723). 1219 bytes result sent to driver
15/08/21 08:58:05 INFO TaskSetManager: Starting task 94.0 in stage 5.0 (TID 738, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:05 INFO TaskSetManager: Starting task 95.0 in stage 5.0 (TID 739, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:05 INFO Executor: Running task 95.0 in stage 5.0 (TID 739)
15/08/21 08:58:05 INFO TaskSetManager: Finished task 78.0 in stage 5.0 (TID 722) in 5911 ms on localhost (79/200)
15/08/21 08:58:05 INFO TaskSetManager: Finished task 79.0 in stage 5.0 (TID 723) in 5817 ms on localhost (80/200)
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:05 INFO Executor: Running task 94.0 in stage 5.0 (TID 738)
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:06 INFO Executor: Finished task 80.0 in stage 5.0 (TID 724). 1219 bytes result sent to driver
15/08/21 08:58:06 INFO TaskSetManager: Starting task 96.0 in stage 5.0 (TID 740, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:06 INFO Executor: Running task 96.0 in stage 5.0 (TID 740)
15/08/21 08:58:06 INFO Executor: Finished task 82.0 in stage 5.0 (TID 726). 1219 bytes result sent to driver
15/08/21 08:58:06 INFO TaskSetManager: Finished task 80.0 in stage 5.0 (TID 724) in 5633 ms on localhost (81/200)
15/08/21 08:58:06 INFO TaskSetManager: Starting task 97.0 in stage 5.0 (TID 741, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:06 INFO Executor: Running task 97.0 in stage 5.0 (TID 741)
15/08/21 08:58:06 INFO TaskSetManager: Finished task 82.0 in stage 5.0 (TID 726) in 4983 ms on localhost (82/200)
15/08/21 08:58:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:06 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:06 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:07 INFO Executor: Finished task 81.0 in stage 5.0 (TID 725). 1219 bytes result sent to driver
15/08/21 08:58:07 INFO TaskSetManager: Starting task 98.0 in stage 5.0 (TID 742, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:07 INFO Executor: Running task 98.0 in stage 5.0 (TID 742)
15/08/21 08:58:07 INFO TaskSetManager: Finished task 81.0 in stage 5.0 (TID 725) in 5726 ms on localhost (83/200)
15/08/21 08:58:07 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:07 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:07 INFO Executor: Finished task 83.0 in stage 5.0 (TID 727). 1219 bytes result sent to driver
15/08/21 08:58:07 INFO TaskSetManager: Starting task 99.0 in stage 5.0 (TID 743, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:07 INFO Executor: Running task 99.0 in stage 5.0 (TID 743)
15/08/21 08:58:07 INFO TaskSetManager: Finished task 83.0 in stage 5.0 (TID 727) in 4943 ms on localhost (84/200)
15/08/21 08:58:07 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:07 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:08 INFO Executor: Finished task 84.0 in stage 5.0 (TID 728). 1219 bytes result sent to driver
15/08/21 08:58:08 INFO TaskSetManager: Starting task 100.0 in stage 5.0 (TID 744, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:08 INFO Executor: Running task 100.0 in stage 5.0 (TID 744)
15/08/21 08:58:08 INFO TaskSetManager: Finished task 84.0 in stage 5.0 (TID 728) in 5996 ms on localhost (85/200)
15/08/21 08:58:08 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:08 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:09 INFO Executor: Finished task 86.0 in stage 5.0 (TID 730). 1219 bytes result sent to driver
15/08/21 08:58:09 INFO TaskSetManager: Starting task 101.0 in stage 5.0 (TID 745, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:09 INFO Executor: Running task 101.0 in stage 5.0 (TID 745)
15/08/21 08:58:09 INFO TaskSetManager: Finished task 86.0 in stage 5.0 (TID 730) in 6081 ms on localhost (86/200)
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO Executor: Finished task 85.0 in stage 5.0 (TID 729). 1219 bytes result sent to driver
15/08/21 08:58:09 INFO TaskSetManager: Starting task 102.0 in stage 5.0 (TID 746, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:09 INFO Executor: Running task 102.0 in stage 5.0 (TID 746)
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO TaskSetManager: Finished task 85.0 in stage 5.0 (TID 729) in 6241 ms on localhost (87/200)
15/08/21 08:58:09 INFO Executor: Finished task 87.0 in stage 5.0 (TID 731). 1219 bytes result sent to driver
15/08/21 08:58:09 INFO TaskSetManager: Starting task 103.0 in stage 5.0 (TID 747, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:09 INFO Executor: Running task 103.0 in stage 5.0 (TID 747)
15/08/21 08:58:09 INFO TaskSetManager: Finished task 87.0 in stage 5.0 (TID 731) in 5973 ms on localhost (88/200)
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO Executor: Finished task 88.0 in stage 5.0 (TID 732). 1219 bytes result sent to driver
15/08/21 08:58:09 INFO TaskSetManager: Starting task 104.0 in stage 5.0 (TID 748, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:09 INFO Executor: Running task 104.0 in stage 5.0 (TID 748)
15/08/21 08:58:09 INFO TaskSetManager: Finished task 88.0 in stage 5.0 (TID 732) in 5812 ms on localhost (89/200)
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO Executor: Finished task 89.0 in stage 5.0 (TID 733). 1219 bytes result sent to driver
15/08/21 08:58:09 INFO TaskSetManager: Starting task 105.0 in stage 5.0 (TID 749, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:09 INFO Executor: Running task 105.0 in stage 5.0 (TID 749)
15/08/21 08:58:09 INFO TaskSetManager: Finished task 89.0 in stage 5.0 (TID 733) in 5468 ms on localhost (90/200)
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO Executor: Finished task 90.0 in stage 5.0 (TID 734). 1219 bytes result sent to driver
15/08/21 08:58:09 INFO TaskSetManager: Starting task 106.0 in stage 5.0 (TID 750, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:09 INFO Executor: Running task 106.0 in stage 5.0 (TID 750)
15/08/21 08:58:09 INFO TaskSetManager: Finished task 90.0 in stage 5.0 (TID 734) in 5301 ms on localhost (91/200)
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:09 INFO Executor: Finished task 91.0 in stage 5.0 (TID 735). 1219 bytes result sent to driver
15/08/21 08:58:09 INFO TaskSetManager: Starting task 107.0 in stage 5.0 (TID 751, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:09 INFO TaskSetManager: Finished task 91.0 in stage 5.0 (TID 735) in 5310 ms on localhost (92/200)
15/08/21 08:58:09 INFO Executor: Running task 107.0 in stage 5.0 (TID 751)
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:10 INFO Executor: Finished task 92.0 in stage 5.0 (TID 736). 1219 bytes result sent to driver
15/08/21 08:58:10 INFO Executor: Finished task 93.0 in stage 5.0 (TID 737). 1219 bytes result sent to driver
15/08/21 08:58:10 INFO TaskSetManager: Starting task 108.0 in stage 5.0 (TID 752, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:10 INFO Executor: Running task 108.0 in stage 5.0 (TID 752)
15/08/21 08:58:10 INFO TaskSetManager: Starting task 109.0 in stage 5.0 (TID 753, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:10 INFO Executor: Running task 109.0 in stage 5.0 (TID 753)
15/08/21 08:58:10 INFO TaskSetManager: Finished task 92.0 in stage 5.0 (TID 736) in 5543 ms on localhost (93/200)
15/08/21 08:58:10 INFO TaskSetManager: Finished task 93.0 in stage 5.0 (TID 737) in 5523 ms on localhost (94/200)
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:10 INFO Executor: Finished task 94.0 in stage 5.0 (TID 738). 1219 bytes result sent to driver
15/08/21 08:58:10 INFO TaskSetManager: Starting task 110.0 in stage 5.0 (TID 754, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:10 INFO Executor: Running task 110.0 in stage 5.0 (TID 754)
15/08/21 08:58:10 INFO TaskSetManager: Finished task 94.0 in stage 5.0 (TID 738) in 5299 ms on localhost (95/200)
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:11 INFO Executor: Finished task 95.0 in stage 5.0 (TID 739). 1219 bytes result sent to driver
15/08/21 08:58:11 INFO TaskSetManager: Starting task 111.0 in stage 5.0 (TID 755, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:11 INFO Executor: Running task 111.0 in stage 5.0 (TID 755)
15/08/21 08:58:11 INFO TaskSetManager: Finished task 95.0 in stage 5.0 (TID 739) in 5712 ms on localhost (96/200)
15/08/21 08:58:11 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:11 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:12 INFO Executor: Finished task 96.0 in stage 5.0 (TID 740). 1219 bytes result sent to driver
15/08/21 08:58:12 INFO TaskSetManager: Starting task 112.0 in stage 5.0 (TID 756, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:12 INFO Executor: Running task 112.0 in stage 5.0 (TID 756)
15/08/21 08:58:12 INFO TaskSetManager: Finished task 96.0 in stage 5.0 (TID 740) in 5095 ms on localhost (97/200)
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:12 INFO Executor: Finished task 97.0 in stage 5.0 (TID 741). 1219 bytes result sent to driver
15/08/21 08:58:12 INFO TaskSetManager: Starting task 113.0 in stage 5.0 (TID 757, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:12 INFO TaskSetManager: Finished task 97.0 in stage 5.0 (TID 741) in 5344 ms on localhost (98/200)
15/08/21 08:58:12 INFO Executor: Running task 113.0 in stage 5.0 (TID 757)
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:12 INFO Executor: Finished task 98.0 in stage 5.0 (TID 742). 1219 bytes result sent to driver
15/08/21 08:58:12 INFO Executor: Finished task 99.0 in stage 5.0 (TID 743). 1219 bytes result sent to driver
15/08/21 08:58:12 INFO TaskSetManager: Starting task 114.0 in stage 5.0 (TID 758, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:12 INFO Executor: Running task 114.0 in stage 5.0 (TID 758)
15/08/21 08:58:12 INFO TaskSetManager: Starting task 115.0 in stage 5.0 (TID 759, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:12 INFO Executor: Running task 115.0 in stage 5.0 (TID 759)
15/08/21 08:58:12 INFO TaskSetManager: Finished task 98.0 in stage 5.0 (TID 742) in 5297 ms on localhost (99/200)
15/08/21 08:58:12 INFO TaskSetManager: Finished task 99.0 in stage 5.0 (TID 743) in 5071 ms on localhost (100/200)
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:13 INFO Executor: Finished task 100.0 in stage 5.0 (TID 744). 1219 bytes result sent to driver
15/08/21 08:58:13 INFO TaskSetManager: Starting task 116.0 in stage 5.0 (TID 760, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:13 INFO Executor: Running task 116.0 in stage 5.0 (TID 760)
15/08/21 08:58:13 INFO TaskSetManager: Finished task 100.0 in stage 5.0 (TID 744) in 4847 ms on localhost (101/200)
15/08/21 08:58:13 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:13 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:14 INFO Executor: Finished task 102.0 in stage 5.0 (TID 746). 1219 bytes result sent to driver
15/08/21 08:58:14 INFO TaskSetManager: Starting task 117.0 in stage 5.0 (TID 761, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:14 INFO Executor: Running task 117.0 in stage 5.0 (TID 761)
15/08/21 08:58:14 INFO TaskSetManager: Finished task 102.0 in stage 5.0 (TID 746) in 5119 ms on localhost (102/200)
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:14 INFO Executor: Finished task 105.0 in stage 5.0 (TID 749). 1219 bytes result sent to driver
15/08/21 08:58:14 INFO TaskSetManager: Starting task 118.0 in stage 5.0 (TID 762, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:14 INFO Executor: Running task 118.0 in stage 5.0 (TID 762)
15/08/21 08:58:14 INFO TaskSetManager: Finished task 105.0 in stage 5.0 (TID 749) in 5102 ms on localhost (103/200)
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:14 INFO Executor: Finished task 103.0 in stage 5.0 (TID 747). 1219 bytes result sent to driver
15/08/21 08:58:14 INFO TaskSetManager: Starting task 119.0 in stage 5.0 (TID 763, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:14 INFO Executor: Running task 119.0 in stage 5.0 (TID 763)
15/08/21 08:58:14 INFO TaskSetManager: Finished task 103.0 in stage 5.0 (TID 747) in 5560 ms on localhost (104/200)
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:14 INFO Executor: Finished task 101.0 in stage 5.0 (TID 745). 1219 bytes result sent to driver
15/08/21 08:58:14 INFO TaskSetManager: Starting task 120.0 in stage 5.0 (TID 764, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:14 INFO Executor: Running task 120.0 in stage 5.0 (TID 764)
15/08/21 08:58:14 INFO TaskSetManager: Finished task 101.0 in stage 5.0 (TID 745) in 5808 ms on localhost (105/200)
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:14 INFO Executor: Finished task 104.0 in stage 5.0 (TID 748). 1219 bytes result sent to driver
15/08/21 08:58:14 INFO TaskSetManager: Starting task 121.0 in stage 5.0 (TID 765, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:14 INFO Executor: Running task 121.0 in stage 5.0 (TID 765)
15/08/21 08:58:14 INFO TaskSetManager: Finished task 104.0 in stage 5.0 (TID 748) in 5503 ms on localhost (106/200)
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:14 INFO Executor: Finished task 106.0 in stage 5.0 (TID 750). 1219 bytes result sent to driver
15/08/21 08:58:14 INFO TaskSetManager: Starting task 122.0 in stage 5.0 (TID 766, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:14 INFO Executor: Running task 122.0 in stage 5.0 (TID 766)
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:14 INFO TaskSetManager: Finished task 106.0 in stage 5.0 (TID 750) in 5396 ms on localhost (107/200)
15/08/21 08:58:15 INFO Executor: Finished task 107.0 in stage 5.0 (TID 751). 1219 bytes result sent to driver
15/08/21 08:58:15 INFO TaskSetManager: Starting task 123.0 in stage 5.0 (TID 767, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:15 INFO Executor: Running task 123.0 in stage 5.0 (TID 767)
15/08/21 08:58:15 INFO TaskSetManager: Finished task 107.0 in stage 5.0 (TID 751) in 5275 ms on localhost (108/200)
15/08/21 08:58:15 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:15 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:16 INFO Executor: Finished task 109.0 in stage 5.0 (TID 753). 1219 bytes result sent to driver
15/08/21 08:58:16 INFO TaskSetManager: Starting task 124.0 in stage 5.0 (TID 768, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:16 INFO Executor: Running task 124.0 in stage 5.0 (TID 768)
15/08/21 08:58:16 INFO TaskSetManager: Finished task 109.0 in stage 5.0 (TID 753) in 5566 ms on localhost (109/200)
15/08/21 08:58:16 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:16 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:17 INFO Executor: Finished task 110.0 in stage 5.0 (TID 754). 1219 bytes result sent to driver
15/08/21 08:58:17 INFO TaskSetManager: Starting task 125.0 in stage 5.0 (TID 769, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:17 INFO Executor: Running task 125.0 in stage 5.0 (TID 769)
15/08/21 08:58:17 INFO TaskSetManager: Finished task 110.0 in stage 5.0 (TID 754) in 6415 ms on localhost (110/200)
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:17 INFO Executor: Finished task 108.0 in stage 5.0 (TID 752). 1219 bytes result sent to driver
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:17 INFO TaskSetManager: Starting task 126.0 in stage 5.0 (TID 770, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:17 INFO Executor: Running task 126.0 in stage 5.0 (TID 770)
15/08/21 08:58:17 INFO TaskSetManager: Finished task 108.0 in stage 5.0 (TID 752) in 6459 ms on localhost (111/200)
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:17 INFO Executor: Finished task 111.0 in stage 5.0 (TID 755). 1219 bytes result sent to driver
15/08/21 08:58:17 INFO TaskSetManager: Starting task 127.0 in stage 5.0 (TID 771, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:17 INFO Executor: Running task 127.0 in stage 5.0 (TID 771)
15/08/21 08:58:17 INFO TaskSetManager: Finished task 111.0 in stage 5.0 (TID 755) in 6374 ms on localhost (112/200)
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:18 INFO Executor: Finished task 113.0 in stage 5.0 (TID 757). 1219 bytes result sent to driver
15/08/21 08:58:18 INFO TaskSetManager: Starting task 128.0 in stage 5.0 (TID 772, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:18 INFO Executor: Running task 128.0 in stage 5.0 (TID 772)
15/08/21 08:58:18 INFO TaskSetManager: Finished task 113.0 in stage 5.0 (TID 757) in 5899 ms on localhost (113/200)
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:18 INFO Executor: Finished task 112.0 in stage 5.0 (TID 756). 1219 bytes result sent to driver
15/08/21 08:58:18 INFO TaskSetManager: Starting task 129.0 in stage 5.0 (TID 773, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:18 INFO TaskSetManager: Finished task 112.0 in stage 5.0 (TID 756) in 6411 ms on localhost (114/200)
15/08/21 08:58:18 INFO Executor: Running task 129.0 in stage 5.0 (TID 773)
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:18 INFO Executor: Finished task 114.0 in stage 5.0 (TID 758). 1219 bytes result sent to driver
15/08/21 08:58:18 INFO TaskSetManager: Starting task 130.0 in stage 5.0 (TID 774, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:18 INFO Executor: Running task 130.0 in stage 5.0 (TID 774)
15/08/21 08:58:18 INFO TaskSetManager: Finished task 114.0 in stage 5.0 (TID 758) in 6124 ms on localhost (115/200)
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:18 INFO Executor: Finished task 115.0 in stage 5.0 (TID 759). 1219 bytes result sent to driver
15/08/21 08:58:18 INFO TaskSetManager: Starting task 131.0 in stage 5.0 (TID 775, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:18 INFO Executor: Running task 131.0 in stage 5.0 (TID 775)
15/08/21 08:58:18 INFO TaskSetManager: Finished task 115.0 in stage 5.0 (TID 759) in 6279 ms on localhost (116/200)
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:19 INFO Executor: Finished task 116.0 in stage 5.0 (TID 760). 1219 bytes result sent to driver
15/08/21 08:58:19 INFO TaskSetManager: Starting task 132.0 in stage 5.0 (TID 776, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:19 INFO Executor: Running task 132.0 in stage 5.0 (TID 776)
15/08/21 08:58:19 INFO TaskSetManager: Finished task 116.0 in stage 5.0 (TID 760) in 5604 ms on localhost (117/200)
15/08/21 08:58:19 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:19 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:19 INFO Executor: Finished task 117.0 in stage 5.0 (TID 761). 1219 bytes result sent to driver
15/08/21 08:58:19 INFO TaskSetManager: Starting task 133.0 in stage 5.0 (TID 777, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:19 INFO Executor: Running task 133.0 in stage 5.0 (TID 777)
15/08/21 08:58:19 INFO TaskSetManager: Finished task 117.0 in stage 5.0 (TID 761) in 5547 ms on localhost (118/200)
15/08/21 08:58:19 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:19 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:20 INFO Executor: Finished task 118.0 in stage 5.0 (TID 762). 1219 bytes result sent to driver
15/08/21 08:58:20 INFO TaskSetManager: Starting task 134.0 in stage 5.0 (TID 778, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:20 INFO Executor: Running task 134.0 in stage 5.0 (TID 778)
15/08/21 08:58:20 INFO TaskSetManager: Finished task 118.0 in stage 5.0 (TID 762) in 6177 ms on localhost (119/200)
15/08/21 08:58:20 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:20 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:20 INFO Executor: Finished task 119.0 in stage 5.0 (TID 763). 1219 bytes result sent to driver
15/08/21 08:58:20 INFO TaskSetManager: Starting task 135.0 in stage 5.0 (TID 779, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:20 INFO Executor: Running task 135.0 in stage 5.0 (TID 779)
15/08/21 08:58:20 INFO TaskSetManager: Finished task 119.0 in stage 5.0 (TID 763) in 6213 ms on localhost (120/200)
15/08/21 08:58:20 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:20 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:21 INFO Executor: Finished task 121.0 in stage 5.0 (TID 765). 1219 bytes result sent to driver
15/08/21 08:58:21 INFO TaskSetManager: Starting task 136.0 in stage 5.0 (TID 780, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:21 INFO Executor: Running task 136.0 in stage 5.0 (TID 780)
15/08/21 08:58:21 INFO TaskSetManager: Finished task 121.0 in stage 5.0 (TID 765) in 6101 ms on localhost (121/200)
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:21 INFO Executor: Finished task 123.0 in stage 5.0 (TID 767). 1219 bytes result sent to driver
15/08/21 08:58:21 INFO TaskSetManager: Starting task 137.0 in stage 5.0 (TID 781, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:21 INFO Executor: Running task 137.0 in stage 5.0 (TID 781)
15/08/21 08:58:21 INFO TaskSetManager: Finished task 123.0 in stage 5.0 (TID 767) in 5982 ms on localhost (122/200)
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:21 INFO Executor: Finished task 122.0 in stage 5.0 (TID 766). 1219 bytes result sent to driver
15/08/21 08:58:21 INFO TaskSetManager: Starting task 138.0 in stage 5.0 (TID 782, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:21 INFO Executor: Running task 138.0 in stage 5.0 (TID 782)
15/08/21 08:58:21 INFO TaskSetManager: Finished task 122.0 in stage 5.0 (TID 766) in 6629 ms on localhost (123/200)
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:21 INFO Executor: Finished task 120.0 in stage 5.0 (TID 764). 1219 bytes result sent to driver
15/08/21 08:58:21 INFO TaskSetManager: Starting task 139.0 in stage 5.0 (TID 783, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:21 INFO Executor: Running task 139.0 in stage 5.0 (TID 783)
15/08/21 08:58:21 INFO TaskSetManager: Finished task 120.0 in stage 5.0 (TID 764) in 6710 ms on localhost (124/200)
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:22 INFO Executor: Finished task 124.0 in stage 5.0 (TID 768). 1219 bytes result sent to driver
15/08/21 08:58:22 INFO TaskSetManager: Starting task 140.0 in stage 5.0 (TID 784, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:22 INFO Executor: Running task 140.0 in stage 5.0 (TID 784)
15/08/21 08:58:22 INFO TaskSetManager: Finished task 124.0 in stage 5.0 (TID 768) in 6367 ms on localhost (125/200)
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:22 INFO Executor: Finished task 126.0 in stage 5.0 (TID 770). 1219 bytes result sent to driver
15/08/21 08:58:22 INFO TaskSetManager: Starting task 141.0 in stage 5.0 (TID 785, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:22 INFO Executor: Running task 141.0 in stage 5.0 (TID 785)
15/08/21 08:58:22 INFO TaskSetManager: Finished task 126.0 in stage 5.0 (TID 770) in 5620 ms on localhost (126/200)
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:22 INFO Executor: Finished task 125.0 in stage 5.0 (TID 769). 1219 bytes result sent to driver
15/08/21 08:58:22 INFO TaskSetManager: Starting task 142.0 in stage 5.0 (TID 786, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:22 INFO Executor: Running task 142.0 in stage 5.0 (TID 786)
15/08/21 08:58:22 INFO TaskSetManager: Finished task 125.0 in stage 5.0 (TID 769) in 5667 ms on localhost (127/200)
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:22 INFO Executor: Finished task 127.0 in stage 5.0 (TID 771). 1219 bytes result sent to driver
15/08/21 08:58:22 INFO TaskSetManager: Starting task 143.0 in stage 5.0 (TID 787, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:22 INFO Executor: Running task 143.0 in stage 5.0 (TID 787)
15/08/21 08:58:22 INFO TaskSetManager: Finished task 127.0 in stage 5.0 (TID 771) in 5401 ms on localhost (128/200)
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:23 INFO Executor: Finished task 128.0 in stage 5.0 (TID 772). 1219 bytes result sent to driver
15/08/21 08:58:23 INFO TaskSetManager: Starting task 144.0 in stage 5.0 (TID 788, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:23 INFO Executor: Running task 144.0 in stage 5.0 (TID 788)
15/08/21 08:58:23 INFO TaskSetManager: Finished task 128.0 in stage 5.0 (TID 772) in 5821 ms on localhost (129/200)
15/08/21 08:58:23 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:23 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:24 INFO Executor: Finished task 129.0 in stage 5.0 (TID 773). 1219 bytes result sent to driver
15/08/21 08:58:24 INFO TaskSetManager: Starting task 145.0 in stage 5.0 (TID 789, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:24 INFO Executor: Running task 145.0 in stage 5.0 (TID 789)
15/08/21 08:58:24 INFO TaskSetManager: Finished task 129.0 in stage 5.0 (TID 773) in 6008 ms on localhost (130/200)
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:24 INFO Executor: Finished task 130.0 in stage 5.0 (TID 774). 1219 bytes result sent to driver
15/08/21 08:58:24 INFO TaskSetManager: Starting task 146.0 in stage 5.0 (TID 790, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:24 INFO Executor: Running task 146.0 in stage 5.0 (TID 790)
15/08/21 08:58:24 INFO TaskSetManager: Finished task 130.0 in stage 5.0 (TID 774) in 6033 ms on localhost (131/200)
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:24 INFO Executor: Finished task 132.0 in stage 5.0 (TID 776). 1219 bytes result sent to driver
15/08/21 08:58:24 INFO TaskSetManager: Starting task 147.0 in stage 5.0 (TID 791, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:24 INFO Executor: Running task 147.0 in stage 5.0 (TID 791)
15/08/21 08:58:24 INFO TaskSetManager: Finished task 132.0 in stage 5.0 (TID 776) in 5656 ms on localhost (132/200)
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:24 INFO Executor: Finished task 131.0 in stage 5.0 (TID 775). 1219 bytes result sent to driver
15/08/21 08:58:24 INFO TaskSetManager: Starting task 148.0 in stage 5.0 (TID 792, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:24 INFO Executor: Running task 148.0 in stage 5.0 (TID 792)
15/08/21 08:58:24 INFO TaskSetManager: Finished task 131.0 in stage 5.0 (TID 775) in 6345 ms on localhost (133/200)
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:25 INFO Executor: Finished task 133.0 in stage 5.0 (TID 777). 1219 bytes result sent to driver
15/08/21 08:58:25 INFO TaskSetManager: Starting task 149.0 in stage 5.0 (TID 793, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:25 INFO Executor: Running task 149.0 in stage 5.0 (TID 793)
15/08/21 08:58:25 INFO TaskSetManager: Finished task 133.0 in stage 5.0 (TID 777) in 5428 ms on localhost (134/200)
15/08/21 08:58:25 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:25 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:28 INFO Executor: Finished task 134.0 in stage 5.0 (TID 778). 1219 bytes result sent to driver
15/08/21 08:58:28 INFO TaskSetManager: Starting task 150.0 in stage 5.0 (TID 794, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:28 INFO Executor: Running task 150.0 in stage 5.0 (TID 794)
15/08/21 08:58:28 INFO TaskSetManager: Finished task 134.0 in stage 5.0 (TID 778) in 7797 ms on localhost (135/200)
15/08/21 08:58:28 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:28 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:29 INFO Executor: Finished task 135.0 in stage 5.0 (TID 779). 1219 bytes result sent to driver
15/08/21 08:58:29 INFO TaskSetManager: Starting task 151.0 in stage 5.0 (TID 795, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:29 INFO Executor: Running task 151.0 in stage 5.0 (TID 795)
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:29 INFO TaskSetManager: Finished task 135.0 in stage 5.0 (TID 779) in 8273 ms on localhost (136/200)
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:58:29 INFO Executor: Finished task 136.0 in stage 5.0 (TID 780). 1219 bytes result sent to driver
15/08/21 08:58:29 INFO TaskSetManager: Starting task 152.0 in stage 5.0 (TID 796, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:29 INFO Executor: Running task 152.0 in stage 5.0 (TID 796)
15/08/21 08:58:29 INFO TaskSetManager: Finished task 136.0 in stage 5.0 (TID 780) in 8215 ms on localhost (137/200)
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:29 INFO Executor: Finished task 139.0 in stage 5.0 (TID 783). 1219 bytes result sent to driver
15/08/21 08:58:29 INFO TaskSetManager: Starting task 153.0 in stage 5.0 (TID 797, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:29 INFO Executor: Running task 153.0 in stage 5.0 (TID 797)
15/08/21 08:58:29 INFO TaskSetManager: Finished task 139.0 in stage 5.0 (TID 783) in 7646 ms on localhost (138/200)
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:29 INFO Executor: Finished task 137.0 in stage 5.0 (TID 781). 1219 bytes result sent to driver
15/08/21 08:58:29 INFO TaskSetManager: Starting task 154.0 in stage 5.0 (TID 798, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:29 INFO Executor: Running task 154.0 in stage 5.0 (TID 798)
15/08/21 08:58:29 INFO TaskSetManager: Finished task 137.0 in stage 5.0 (TID 781) in 8117 ms on localhost (139/200)
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:29 INFO Executor: Finished task 138.0 in stage 5.0 (TID 782). 1219 bytes result sent to driver
15/08/21 08:58:29 INFO TaskSetManager: Starting task 155.0 in stage 5.0 (TID 799, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:29 INFO Executor: Running task 155.0 in stage 5.0 (TID 799)
15/08/21 08:58:29 INFO TaskSetManager: Finished task 138.0 in stage 5.0 (TID 782) in 7795 ms on localhost (140/200)
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:30 INFO Executor: Finished task 140.0 in stage 5.0 (TID 784). 1219 bytes result sent to driver
15/08/21 08:58:30 INFO TaskSetManager: Starting task 156.0 in stage 5.0 (TID 800, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:30 INFO Executor: Running task 156.0 in stage 5.0 (TID 800)
15/08/21 08:58:30 INFO TaskSetManager: Finished task 140.0 in stage 5.0 (TID 784) in 7623 ms on localhost (141/200)
15/08/21 08:58:30 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 08:58:30 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:30 INFO Executor: Finished task 141.0 in stage 5.0 (TID 785). 1219 bytes result sent to driver
15/08/21 08:58:30 INFO TaskSetManager: Starting task 157.0 in stage 5.0 (TID 801, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:30 INFO Executor: Running task 157.0 in stage 5.0 (TID 801)
15/08/21 08:58:30 INFO TaskSetManager: Finished task 141.0 in stage 5.0 (TID 785) in 7980 ms on localhost (142/200)
15/08/21 08:58:30 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:58:30 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 08:58:31 INFO Executor: Finished task 143.0 in stage 5.0 (TID 787). 1219 bytes result sent to driver
15/08/21 08:58:31 INFO TaskSetManager: Starting task 158.0 in stage 5.0 (TID 802, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:31 INFO Executor: Running task 158.0 in stage 5.0 (TID 802)
15/08/21 08:58:31 INFO TaskSetManager: Finished task 143.0 in stage 5.0 (TID 787) in 8511 ms on localhost (143/200)
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:31 INFO Executor: Finished task 144.0 in stage 5.0 (TID 788). 1219 bytes result sent to driver
15/08/21 08:58:31 INFO Executor: Finished task 142.0 in stage 5.0 (TID 786). 1219 bytes result sent to driver
15/08/21 08:58:31 INFO TaskSetManager: Starting task 159.0 in stage 5.0 (TID 803, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:31 INFO Executor: Running task 159.0 in stage 5.0 (TID 803)
15/08/21 08:58:31 INFO TaskSetManager: Starting task 160.0 in stage 5.0 (TID 804, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:31 INFO TaskSetManager: Finished task 144.0 in stage 5.0 (TID 788) in 7640 ms on localhost (144/200)
15/08/21 08:58:31 INFO TaskSetManager: Finished task 142.0 in stage 5.0 (TID 786) in 8902 ms on localhost (145/200)
15/08/21 08:58:31 INFO Executor: Running task 160.0 in stage 5.0 (TID 804)
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:31 INFO Executor: Finished task 145.0 in stage 5.0 (TID 789). 1219 bytes result sent to driver
15/08/21 08:58:31 INFO TaskSetManager: Starting task 161.0 in stage 5.0 (TID 805, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:31 INFO Executor: Running task 161.0 in stage 5.0 (TID 805)
15/08/21 08:58:31 INFO TaskSetManager: Finished task 145.0 in stage 5.0 (TID 789) in 7212 ms on localhost (146/200)
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:31 INFO Executor: Finished task 146.0 in stage 5.0 (TID 790). 1219 bytes result sent to driver
15/08/21 08:58:31 INFO TaskSetManager: Starting task 162.0 in stage 5.0 (TID 806, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:31 INFO Executor: Running task 162.0 in stage 5.0 (TID 806)
15/08/21 08:58:31 INFO TaskSetManager: Finished task 146.0 in stage 5.0 (TID 790) in 7428 ms on localhost (147/200)
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:34 INFO Executor: Finished task 148.0 in stage 5.0 (TID 792). 1219 bytes result sent to driver
15/08/21 08:58:34 INFO TaskSetManager: Starting task 163.0 in stage 5.0 (TID 807, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:34 INFO Executor: Running task 163.0 in stage 5.0 (TID 807)
15/08/21 08:58:34 INFO TaskSetManager: Finished task 148.0 in stage 5.0 (TID 792) in 9846 ms on localhost (148/200)
15/08/21 08:58:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:34 INFO Executor: Finished task 149.0 in stage 5.0 (TID 793). 1219 bytes result sent to driver
15/08/21 08:58:34 INFO TaskSetManager: Starting task 164.0 in stage 5.0 (TID 808, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:34 INFO TaskSetManager: Finished task 149.0 in stage 5.0 (TID 793) in 9657 ms on localhost (149/200)
15/08/21 08:58:34 INFO Executor: Running task 164.0 in stage 5.0 (TID 808)
15/08/21 08:58:34 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:34 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:35 INFO Executor: Finished task 147.0 in stage 5.0 (TID 791). 1219 bytes result sent to driver
15/08/21 08:58:35 INFO TaskSetManager: Starting task 165.0 in stage 5.0 (TID 809, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:35 INFO Executor: Running task 165.0 in stage 5.0 (TID 809)
15/08/21 08:58:35 INFO TaskSetManager: Finished task 147.0 in stage 5.0 (TID 791) in 10281 ms on localhost (150/200)
15/08/21 08:58:35 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:35 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:35 INFO Executor: Finished task 150.0 in stage 5.0 (TID 794). 1219 bytes result sent to driver
15/08/21 08:58:35 INFO TaskSetManager: Starting task 166.0 in stage 5.0 (TID 810, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:35 INFO Executor: Running task 166.0 in stage 5.0 (TID 810)
15/08/21 08:58:35 INFO TaskSetManager: Finished task 150.0 in stage 5.0 (TID 794) in 7032 ms on localhost (151/200)
15/08/21 08:58:35 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:35 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:37 INFO Executor: Finished task 151.0 in stage 5.0 (TID 795). 1219 bytes result sent to driver
15/08/21 08:58:37 INFO TaskSetManager: Starting task 167.0 in stage 5.0 (TID 811, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:37 INFO TaskSetManager: Finished task 151.0 in stage 5.0 (TID 795) in 8581 ms on localhost (152/200)
15/08/21 08:58:37 INFO Executor: Running task 167.0 in stage 5.0 (TID 811)
15/08/21 08:58:37 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:37 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:38 INFO Executor: Finished task 155.0 in stage 5.0 (TID 799). 1219 bytes result sent to driver
15/08/21 08:58:38 INFO TaskSetManager: Starting task 168.0 in stage 5.0 (TID 812, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:38 INFO Executor: Running task 168.0 in stage 5.0 (TID 812)
15/08/21 08:58:38 INFO TaskSetManager: Finished task 155.0 in stage 5.0 (TID 799) in 9374 ms on localhost (153/200)
15/08/21 08:58:38 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:38 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:38 INFO Executor: Finished task 152.0 in stage 5.0 (TID 796). 1219 bytes result sent to driver
15/08/21 08:58:38 INFO TaskSetManager: Starting task 169.0 in stage 5.0 (TID 813, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:38 INFO Executor: Running task 169.0 in stage 5.0 (TID 813)
15/08/21 08:58:38 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:38 INFO TaskSetManager: Finished task 152.0 in stage 5.0 (TID 796) in 9657 ms on localhost (154/200)
15/08/21 08:58:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:38 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO Executor: Finished task 154.0 in stage 5.0 (TID 798). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO TaskSetManager: Starting task 170.0 in stage 5.0 (TID 814, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 170.0 in stage 5.0 (TID 814)
15/08/21 08:58:39 INFO TaskSetManager: Finished task 154.0 in stage 5.0 (TID 798) in 9794 ms on localhost (155/200)
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO Executor: Finished task 153.0 in stage 5.0 (TID 797). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO TaskSetManager: Starting task 171.0 in stage 5.0 (TID 815, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 171.0 in stage 5.0 (TID 815)
15/08/21 08:58:39 INFO Executor: Finished task 156.0 in stage 5.0 (TID 800). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO TaskSetManager: Starting task 172.0 in stage 5.0 (TID 816, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 172.0 in stage 5.0 (TID 816)
15/08/21 08:58:39 INFO TaskSetManager: Finished task 153.0 in stage 5.0 (TID 797) in 9872 ms on localhost (156/200)
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO TaskSetManager: Finished task 156.0 in stage 5.0 (TID 800) in 8980 ms on localhost (157/200)
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:39 INFO Executor: Finished task 157.0 in stage 5.0 (TID 801). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO TaskSetManager: Starting task 173.0 in stage 5.0 (TID 817, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 173.0 in stage 5.0 (TID 817)
15/08/21 08:58:39 INFO TaskSetManager: Finished task 157.0 in stage 5.0 (TID 801) in 8611 ms on localhost (158/200)
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:39 INFO Executor: Finished task 158.0 in stage 5.0 (TID 802). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO TaskSetManager: Starting task 174.0 in stage 5.0 (TID 818, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 174.0 in stage 5.0 (TID 818)
15/08/21 08:58:39 INFO TaskSetManager: Finished task 158.0 in stage 5.0 (TID 802) in 8218 ms on localhost (159/200)
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO Executor: Finished task 159.0 in stage 5.0 (TID 803). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO TaskSetManager: Starting task 175.0 in stage 5.0 (TID 819, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 175.0 in stage 5.0 (TID 819)
15/08/21 08:58:39 INFO TaskSetManager: Finished task 159.0 in stage 5.0 (TID 803) in 8115 ms on localhost (160/200)
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:39 INFO Executor: Finished task 161.0 in stage 5.0 (TID 805). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO Executor: Finished task 160.0 in stage 5.0 (TID 804). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO TaskSetManager: Starting task 176.0 in stage 5.0 (TID 820, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 176.0 in stage 5.0 (TID 820)
15/08/21 08:58:39 INFO TaskSetManager: Starting task 177.0 in stage 5.0 (TID 821, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 177.0 in stage 5.0 (TID 821)
15/08/21 08:58:39 INFO TaskSetManager: Finished task 160.0 in stage 5.0 (TID 804) in 8233 ms on localhost (161/200)
15/08/21 08:58:39 INFO TaskSetManager: Finished task 161.0 in stage 5.0 (TID 805) in 8218 ms on localhost (162/200)
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 08:58:39 INFO Executor: Finished task 162.0 in stage 5.0 (TID 806). 1219 bytes result sent to driver
15/08/21 08:58:39 INFO TaskSetManager: Starting task 178.0 in stage 5.0 (TID 822, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:39 INFO Executor: Running task 178.0 in stage 5.0 (TID 822)
15/08/21 08:58:39 INFO TaskSetManager: Finished task 162.0 in stage 5.0 (TID 806) in 8100 ms on localhost (163/200)
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:42 INFO Executor: Finished task 163.0 in stage 5.0 (TID 807). 1219 bytes result sent to driver
15/08/21 08:58:42 INFO TaskSetManager: Starting task 179.0 in stage 5.0 (TID 823, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:42 INFO Executor: Running task 179.0 in stage 5.0 (TID 823)
15/08/21 08:58:42 INFO TaskSetManager: Finished task 163.0 in stage 5.0 (TID 807) in 7765 ms on localhost (164/200)
15/08/21 08:58:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:42 INFO Executor: Finished task 164.0 in stage 5.0 (TID 808). 1219 bytes result sent to driver
15/08/21 08:58:42 INFO TaskSetManager: Starting task 180.0 in stage 5.0 (TID 824, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:42 INFO Executor: Running task 180.0 in stage 5.0 (TID 824)
15/08/21 08:58:42 INFO TaskSetManager: Finished task 164.0 in stage 5.0 (TID 808) in 7987 ms on localhost (165/200)
15/08/21 08:58:42 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:42 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:43 INFO Executor: Finished task 165.0 in stage 5.0 (TID 809). 1219 bytes result sent to driver
15/08/21 08:58:43 INFO TaskSetManager: Starting task 181.0 in stage 5.0 (TID 825, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:43 INFO Executor: Running task 181.0 in stage 5.0 (TID 825)
15/08/21 08:58:43 INFO TaskSetManager: Finished task 165.0 in stage 5.0 (TID 809) in 8255 ms on localhost (166/200)
15/08/21 08:58:43 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:43 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:43 INFO Executor: Finished task 166.0 in stage 5.0 (TID 810). 1219 bytes result sent to driver
15/08/21 08:58:43 INFO TaskSetManager: Starting task 182.0 in stage 5.0 (TID 826, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:43 INFO Executor: Running task 182.0 in stage 5.0 (TID 826)
15/08/21 08:58:43 INFO TaskSetManager: Finished task 166.0 in stage 5.0 (TID 810) in 8088 ms on localhost (167/200)
15/08/21 08:58:43 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:43 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:45 INFO Executor: Finished task 167.0 in stage 5.0 (TID 811). 1219 bytes result sent to driver
15/08/21 08:58:45 INFO TaskSetManager: Starting task 183.0 in stage 5.0 (TID 827, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:45 INFO Executor: Running task 183.0 in stage 5.0 (TID 827)
15/08/21 08:58:45 INFO TaskSetManager: Finished task 167.0 in stage 5.0 (TID 811) in 7689 ms on localhost (168/200)
15/08/21 08:58:45 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:45 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:46 INFO Executor: Finished task 168.0 in stage 5.0 (TID 812). 1219 bytes result sent to driver
15/08/21 08:58:46 INFO TaskSetManager: Starting task 184.0 in stage 5.0 (TID 828, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:46 INFO Executor: Running task 184.0 in stage 5.0 (TID 828)
15/08/21 08:58:46 INFO TaskSetManager: Finished task 168.0 in stage 5.0 (TID 812) in 7360 ms on localhost (169/200)
15/08/21 08:58:46 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:46 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:47 INFO Executor: Finished task 177.0 in stage 5.0 (TID 821). 1219 bytes result sent to driver
15/08/21 08:58:47 INFO TaskSetManager: Starting task 185.0 in stage 5.0 (TID 829, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:47 INFO Executor: Running task 185.0 in stage 5.0 (TID 829)
15/08/21 08:58:47 INFO TaskSetManager: Finished task 177.0 in stage 5.0 (TID 821) in 8004 ms on localhost (170/200)
15/08/21 08:58:47 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:47 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO Executor: Finished task 175.0 in stage 5.0 (TID 819). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO TaskSetManager: Starting task 186.0 in stage 5.0 (TID 830, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO Executor: Running task 186.0 in stage 5.0 (TID 830)
15/08/21 08:58:48 INFO TaskSetManager: Finished task 175.0 in stage 5.0 (TID 819) in 8378 ms on localhost (171/200)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO Executor: Finished task 169.0 in stage 5.0 (TID 813). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO TaskSetManager: Starting task 187.0 in stage 5.0 (TID 831, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO Executor: Running task 187.0 in stage 5.0 (TID 831)
15/08/21 08:58:48 INFO TaskSetManager: Finished task 169.0 in stage 5.0 (TID 813) in 9339 ms on localhost (172/200)
15/08/21 08:58:48 INFO Executor: Finished task 172.0 in stage 5.0 (TID 816). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:48 INFO TaskSetManager: Starting task 188.0 in stage 5.0 (TID 832, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO Executor: Running task 188.0 in stage 5.0 (TID 832)
15/08/21 08:58:48 INFO TaskSetManager: Finished task 172.0 in stage 5.0 (TID 816) in 9109 ms on localhost (173/200)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO Executor: Finished task 176.0 in stage 5.0 (TID 820). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO TaskSetManager: Starting task 189.0 in stage 5.0 (TID 833, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO Executor: Running task 189.0 in stage 5.0 (TID 833)
15/08/21 08:58:48 INFO TaskSetManager: Finished task 176.0 in stage 5.0 (TID 820) in 8418 ms on localhost (174/200)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO Executor: Finished task 174.0 in stage 5.0 (TID 818). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO TaskSetManager: Starting task 190.0 in stage 5.0 (TID 834, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO Executor: Running task 190.0 in stage 5.0 (TID 834)
15/08/21 08:58:48 INFO TaskSetManager: Finished task 174.0 in stage 5.0 (TID 818) in 8911 ms on localhost (175/200)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO Executor: Finished task 171.0 in stage 5.0 (TID 815). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO TaskSetManager: Starting task 191.0 in stage 5.0 (TID 835, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO Executor: Running task 191.0 in stage 5.0 (TID 835)
15/08/21 08:58:48 INFO TaskSetManager: Finished task 171.0 in stage 5.0 (TID 815) in 9338 ms on localhost (176/200)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO Executor: Finished task 178.0 in stage 5.0 (TID 822). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO TaskSetManager: Starting task 192.0 in stage 5.0 (TID 836, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO Executor: Running task 192.0 in stage 5.0 (TID 836)
15/08/21 08:58:48 INFO TaskSetManager: Finished task 178.0 in stage 5.0 (TID 822) in 8555 ms on localhost (177/200)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 08:58:48 INFO Executor: Finished task 170.0 in stage 5.0 (TID 814). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO TaskSetManager: Starting task 193.0 in stage 5.0 (TID 837, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO Executor: Running task 193.0 in stage 5.0 (TID 837)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO TaskSetManager: Finished task 170.0 in stage 5.0 (TID 814) in 9491 ms on localhost (178/200)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO Executor: Finished task 173.0 in stage 5.0 (TID 817). 1219 bytes result sent to driver
15/08/21 08:58:48 INFO TaskSetManager: Starting task 194.0 in stage 5.0 (TID 838, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:48 INFO Executor: Running task 194.0 in stage 5.0 (TID 838)
15/08/21 08:58:48 INFO TaskSetManager: Finished task 173.0 in stage 5.0 (TID 817) in 9374 ms on localhost (179/200)
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:49 INFO Executor: Finished task 179.0 in stage 5.0 (TID 823). 1219 bytes result sent to driver
15/08/21 08:58:49 INFO TaskSetManager: Starting task 195.0 in stage 5.0 (TID 839, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:49 INFO Executor: Running task 195.0 in stage 5.0 (TID 839)
15/08/21 08:58:49 INFO TaskSetManager: Finished task 179.0 in stage 5.0 (TID 823) in 7091 ms on localhost (180/200)
15/08/21 08:58:49 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:49 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:50 INFO Executor: Finished task 180.0 in stage 5.0 (TID 824). 1219 bytes result sent to driver
15/08/21 08:58:50 INFO TaskSetManager: Starting task 196.0 in stage 5.0 (TID 840, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:50 INFO Executor: Running task 196.0 in stage 5.0 (TID 840)
15/08/21 08:58:50 INFO TaskSetManager: Finished task 180.0 in stage 5.0 (TID 824) in 7408 ms on localhost (181/200)
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:50 INFO Executor: Finished task 181.0 in stage 5.0 (TID 825). 1219 bytes result sent to driver
15/08/21 08:58:50 INFO TaskSetManager: Starting task 197.0 in stage 5.0 (TID 841, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:50 INFO Executor: Running task 197.0 in stage 5.0 (TID 841)
15/08/21 08:58:50 INFO TaskSetManager: Finished task 181.0 in stage 5.0 (TID 825) in 7407 ms on localhost (182/200)
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:50 INFO Executor: Finished task 182.0 in stage 5.0 (TID 826). 1219 bytes result sent to driver
15/08/21 08:58:50 INFO TaskSetManager: Starting task 198.0 in stage 5.0 (TID 842, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:50 INFO TaskSetManager: Finished task 182.0 in stage 5.0 (TID 826) in 7131 ms on localhost (183/200)
15/08/21 08:58:50 INFO Executor: Running task 198.0 in stage 5.0 (TID 842)
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:51 INFO Executor: Finished task 183.0 in stage 5.0 (TID 827). 1219 bytes result sent to driver
15/08/21 08:58:51 INFO TaskSetManager: Starting task 199.0 in stage 5.0 (TID 843, localhost, PROCESS_LOCAL, 1571 bytes)
15/08/21 08:58:51 INFO Executor: Running task 199.0 in stage 5.0 (TID 843)
15/08/21 08:58:51 INFO TaskSetManager: Finished task 183.0 in stage 5.0 (TID 827) in 6054 ms on localhost (184/200)
15/08/21 08:58:51 INFO ShuffleBlockFetcherIterator: Getting 19 non-empty blocks out of 19 blocks
15/08/21 08:58:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:51 INFO ShuffleBlockFetcherIterator: Getting 57 non-empty blocks out of 57 blocks
15/08/21 08:58:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 08:58:53 INFO Executor: Finished task 184.0 in stage 5.0 (TID 828). 1219 bytes result sent to driver
15/08/21 08:58:53 INFO TaskSetManager: Starting task 197.0 in stage 6.0 (TID 844, localhost, ANY, 1693 bytes)
15/08/21 08:58:53 INFO Executor: Running task 197.0 in stage 6.0 (TID 844)
15/08/21 08:58:53 INFO TaskSetManager: Finished task 184.0 in stage 5.0 (TID 828) in 7137 ms on localhost (185/200)
15/08/21 08:58:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00065-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3500529 length: 3500529 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:58:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:53 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 750000
15/08/21 08:58:53 INFO Executor: Finished task 185.0 in stage 5.0 (TID 829). 1219 bytes result sent to driver
15/08/21 08:58:53 INFO TaskSetManager: Starting task 198.0 in stage 6.0 (TID 845, localhost, ANY, 1691 bytes)
15/08/21 08:58:53 INFO Executor: Running task 198.0 in stage 6.0 (TID 845)
15/08/21 08:58:53 INFO TaskSetManager: Finished task 185.0 in stage 5.0 (TID 829) in 5515 ms on localhost (186/200)
15/08/21 08:58:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00090-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3496809 length: 3496809 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:58:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:53 INFO InternalParquetRecordReader: block read in memory in 265 ms. row count = 750000
15/08/21 08:58:53 INFO Executor: Finished task 197.0 in stage 6.0 (TID 844). 2125 bytes result sent to driver
15/08/21 08:58:53 INFO TaskSetManager: Starting task 199.0 in stage 6.0 (TID 846, localhost, ANY, 1691 bytes)
15/08/21 08:58:53 INFO Executor: Running task 199.0 in stage 6.0 (TID 846)
15/08/21 08:58:53 INFO TaskSetManager: Finished task 197.0 in stage 6.0 (TID 844) in 584 ms on localhost (198/200)
15/08/21 08:58:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_tmp_par/part-r-00085-8ed02ffd-c575-49e0-a41c-d19a3dfefbad.gz.parquet start: 0 end: 3504735 length: 3504735 hosts: [] requestedSchema: message root {
  optional int32 l_orderkey;
  optional double t_sum_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_sum_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 750000 records.
15/08/21 08:58:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:53 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 750000
15/08/21 08:58:54 INFO Executor: Finished task 186.0 in stage 5.0 (TID 830). 1219 bytes result sent to driver
15/08/21 08:58:54 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 847, localhost, ANY, 1762 bytes)
15/08/21 08:58:54 INFO Executor: Running task 0.0 in stage 7.0 (TID 847)
15/08/21 08:58:54 INFO TaskSetManager: Finished task 186.0 in stage 5.0 (TID 830) in 6031 ms on localhost (187/200)
15/08/21 08:58:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:54 INFO Executor: Finished task 198.0 in stage 6.0 (TID 845). 2125 bytes result sent to driver
15/08/21 08:58:54 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 848, localhost, ANY, 1773 bytes)
15/08/21 08:58:54 INFO Executor: Running task 1.0 in stage 7.0 (TID 848)
15/08/21 08:58:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000049_0 start: 134217728 end: 257568535 length: 123350807 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:54 INFO TaskSetManager: Finished task 198.0 in stage 6.0 (TID 845) in 791 ms on localhost (199/200)
15/08/21 08:58:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
15/08/21 08:58:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572843 records.
15/08/21 08:58:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:54 INFO Executor: Finished task 187.0 in stage 5.0 (TID 831). 1219 bytes result sent to driver
15/08/21 08:58:54 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 849, localhost, ANY, 1761 bytes)
15/08/21 08:58:54 INFO Executor: Running task 2.0 in stage 7.0 (TID 849)
15/08/21 08:58:54 INFO TaskSetManager: Finished task 187.0 in stage 5.0 (TID 831) in 6088 ms on localhost (188/200)
15/08/21 08:58:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501150 records.
15/08/21 08:58:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:54 INFO InternalParquetRecordReader: block read in memory in 176 ms. row count = 3501191
15/08/21 08:58:54 INFO InternalParquetRecordReader: block read in memory in 238 ms. row count = 3500939
15/08/21 08:58:54 INFO Executor: Finished task 190.0 in stage 5.0 (TID 834). 1219 bytes result sent to driver
15/08/21 08:58:54 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 850, localhost, ANY, 1772 bytes)
15/08/21 08:58:54 INFO Executor: Running task 3.0 in stage 7.0 (TID 850)
15/08/21 08:58:54 INFO TaskSetManager: Finished task 190.0 in stage 5.0 (TID 834) in 6178 ms on localhost (189/200)
15/08/21 08:58:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000032_0 start: 134217728 end: 257171772 length: 122954044 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572933 records.
15/08/21 08:58:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:54 INFO InternalParquetRecordReader: block read in memory in 389 ms. row count = 3501150
15/08/21 08:58:54 INFO Executor: Finished task 199.0 in stage 6.0 (TID 846). 2125 bytes result sent to driver
15/08/21 08:58:54 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 851, localhost, ANY, 1761 bytes)
15/08/21 08:58:54 INFO Executor: Running task 4.0 in stage 7.0 (TID 851)
15/08/21 08:58:54 INFO TaskSetManager: Finished task 199.0 in stage 6.0 (TID 846) in 968 ms on localhost (200/200)
15/08/21 08:58:54 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/21 08:58:54 INFO DAGScheduler: ShuffleMapStage 6 (processCmd at CliDriver.java:423) finished in 148.609 s
15/08/21 08:58:54 INFO DAGScheduler: looking for newly runnable stages
15/08/21 08:58:54 INFO DAGScheduler: running: Set(ShuffleMapStage 5, ShuffleMapStage 7)
15/08/21 08:58:54 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/21 08:58:54 INFO DAGScheduler: failed: Set()
15/08/21 08:58:54 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@40bb4625
15/08/21 08:58:54 INFO InternalParquetRecordReader: block read in memory in 106 ms. row count = 3502678
15/08/21 08:58:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:54 INFO StatsReportListener: task runtime:(count: 212, mean: 6284.424528, stdev: 2246.400557, max: 10281.000000, min: 584.000000)
15/08/21 08:58:54 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:54 INFO StatsReportListener: 	584.0 ms	808.0 ms	1.0 s	5.6 s	6.5 s	7.8 s	8.6 s	9.3 s	10.3 s
15/08/21 08:58:54 INFO StatsReportListener: shuffle bytes written:(count: 212, mean: 21203479.867925, stdev: 7395672.316297, max: 23872475.000000, min: 3216.000000)
15/08/21 08:58:54 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:54 INFO StatsReportListener: 	3.1 KB	3.2 KB	3.2 KB	22.7 MB	22.7 MB	22.7 MB	22.7 MB	22.7 MB	22.8 MB
15/08/21 08:58:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:54 INFO StatsReportListener: fetch wait time:(count: 189, mean: 0.571429, stdev: 1.191428, max: 10.000000, min: 0.000000)
15/08/21 08:58:54 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:54 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	3.0 ms	10.0 ms
15/08/21 08:58:54 INFO StatsReportListener: remote bytes read:(count: 189, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 08:58:54 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:54 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 5, ShuffleMapStage 7)
15/08/21 08:58:54 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 08:58:54 INFO StatsReportListener: task result size:(count: 212, mean: 1317.292453, stdev: 281.765072, max: 2125.000000, min: 1219.000000)
15/08/21 08:58:54 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:54 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	2.1 KB	2.1 KB	2.1 KB
15/08/21 08:58:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501187 records.
15/08/21 08:58:54 INFO StatsReportListener: executor (non-fetch) time pct: (count: 212, mean: 99.127202, stdev: 1.630233, max: 99.868314, min: 89.467005)
15/08/21 08:58:54 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:54 INFO StatsReportListener: 	89 %	95 %	98 %	99 %	100 %	100 %	100 %	100 %	100 %
15/08/21 08:58:54 INFO StatsReportListener: fetch wait time pct: (count: 189, mean: 0.008314, stdev: 0.017636, max: 0.149031, min: 0.000000)
15/08/21 08:58:54 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:54 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 08:58:54 INFO StatsReportListener: other time pct: (count: 212, mean: 0.865386, stdev: 1.631324, max: 10.532995, min: 0.111426)
15/08/21 08:58:54 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:54 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 2 %	 5 %	11 %
15/08/21 08:58:54 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:54 INFO InternalParquetRecordReader: block read in memory in 134 ms. row count = 3501187
15/08/21 08:58:55 INFO Executor: Finished task 188.0 in stage 5.0 (TID 832). 1219 bytes result sent to driver
15/08/21 08:58:55 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 852, localhost, ANY, 1775 bytes)
15/08/21 08:58:55 INFO TaskSetManager: Finished task 188.0 in stage 5.0 (TID 832) in 6878 ms on localhost (190/200)
15/08/21 08:58:55 INFO Executor: Running task 5.0 in stage 7.0 (TID 852)
15/08/21 08:58:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000063_0 start: 134217728 end: 257477064 length: 123259336 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573045 records.
15/08/21 08:58:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:55 INFO InternalParquetRecordReader: block read in memory in 120 ms. row count = 3500100
15/08/21 08:58:55 INFO Executor: Finished task 189.0 in stage 5.0 (TID 833). 1219 bytes result sent to driver
15/08/21 08:58:55 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 853, localhost, ANY, 1762 bytes)
15/08/21 08:58:55 INFO Executor: Running task 6.0 in stage 7.0 (TID 853)
15/08/21 08:58:55 INFO TaskSetManager: Finished task 189.0 in stage 5.0 (TID 833) in 7163 ms on localhost (191/200)
15/08/21 08:58:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:58:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:55 INFO InternalParquetRecordReader: block read in memory in 112 ms. row count = 3500100
15/08/21 08:58:55 INFO Executor: Finished task 191.0 in stage 5.0 (TID 835). 1219 bytes result sent to driver
15/08/21 08:58:55 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 854, localhost, ANY, 1777 bytes)
15/08/21 08:58:55 INFO Executor: Running task 7.0 in stage 7.0 (TID 854)
15/08/21 08:58:55 INFO TaskSetManager: Finished task 191.0 in stage 5.0 (TID 835) in 7143 ms on localhost (192/200)
15/08/21 08:58:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000074_0 start: 134217728 end: 257329816 length: 123112088 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573983 records.
15/08/21 08:58:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:55 INFO Executor: Finished task 193.0 in stage 5.0 (TID 837). 1219 bytes result sent to driver
15/08/21 08:58:55 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 855, localhost, ANY, 1762 bytes)
15/08/21 08:58:55 INFO Executor: Running task 8.0 in stage 7.0 (TID 855)
15/08/21 08:58:55 INFO TaskSetManager: Finished task 193.0 in stage 5.0 (TID 837) in 7171 ms on localhost (193/200)
15/08/21 08:58:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501583 records.
15/08/21 08:58:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:55 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500100
15/08/21 08:58:55 INFO InternalParquetRecordReader: block read in memory in 206 ms. row count = 3501583
15/08/21 08:58:57 INFO Executor: Finished task 192.0 in stage 5.0 (TID 836). 1219 bytes result sent to driver
15/08/21 08:58:57 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 856, localhost, ANY, 1774 bytes)
15/08/21 08:58:57 INFO Executor: Running task 9.0 in stage 7.0 (TID 856)
15/08/21 08:58:57 INFO TaskSetManager: Finished task 192.0 in stage 5.0 (TID 836) in 8519 ms on localhost (194/200)
15/08/21 08:58:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000065_0 start: 134217728 end: 257425918 length: 123208190 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572784 records.
15/08/21 08:58:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:57 INFO Executor: Finished task 194.0 in stage 5.0 (TID 838). 1219 bytes result sent to driver
15/08/21 08:58:57 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 857, localhost, ANY, 1760 bytes)
15/08/21 08:58:57 INFO Executor: Running task 10.0 in stage 7.0 (TID 857)
15/08/21 08:58:57 INFO TaskSetManager: Finished task 194.0 in stage 5.0 (TID 838) in 8566 ms on localhost (195/200)
15/08/21 08:58:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:58:57 INFO Executor: Finished task 195.0 in stage 5.0 (TID 839). 1219 bytes result sent to driver
15/08/21 08:58:57 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 858, localhost, ANY, 1771 bytes)
15/08/21 08:58:57 INFO Executor: Running task 11.0 in stage 7.0 (TID 858)
15/08/21 08:58:57 INFO TaskSetManager: Finished task 195.0 in stage 5.0 (TID 839) in 7582 ms on localhost (196/200)
15/08/21 08:58:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000009_0 start: 134217728 end: 259746640 length: 125528912 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627997 records.
15/08/21 08:58:57 INFO InternalParquetRecordReader: block read in memory in 146 ms. row count = 3500100
15/08/21 08:58:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:57 INFO InternalParquetRecordReader: block read in memory in 94 ms. row count = 3500100
15/08/21 08:58:57 INFO InternalParquetRecordReader: block read in memory in 112 ms. row count = 3500100
15/08/21 08:58:58 INFO Executor: Finished task 196.0 in stage 5.0 (TID 840). 1219 bytes result sent to driver
15/08/21 08:58:58 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 859, localhost, ANY, 1762 bytes)
15/08/21 08:58:58 INFO Executor: Running task 12.0 in stage 7.0 (TID 859)
15/08/21 08:58:58 INFO TaskSetManager: Finished task 196.0 in stage 5.0 (TID 840) in 7955 ms on localhost (197/200)
15/08/21 08:58:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:58:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:58 INFO Executor: Finished task 198.0 in stage 5.0 (TID 842). 1219 bytes result sent to driver
15/08/21 08:58:58 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 860, localhost, ANY, 1775 bytes)
15/08/21 08:58:58 INFO Executor: Running task 13.0 in stage 7.0 (TID 860)
15/08/21 08:58:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000078_0 start: 134217728 end: 257420451 length: 123202723 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:58 INFO TaskSetManager: Finished task 198.0 in stage 5.0 (TID 842) in 7715 ms on localhost (198/200)
15/08/21 08:58:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:58 INFO Executor: Finished task 197.0 in stage 5.0 (TID 841). 1219 bytes result sent to driver
15/08/21 08:58:58 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 861, localhost, ANY, 1762 bytes)
15/08/21 08:58:58 INFO TaskSetManager: Finished task 197.0 in stage 5.0 (TID 841) in 7752 ms on localhost (199/200)
15/08/21 08:58:58 INFO Executor: Running task 14.0 in stage 7.0 (TID 861)
15/08/21 08:58:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573892 records.
15/08/21 08:58:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:58:58 INFO Executor: Finished task 199.0 in stage 5.0 (TID 843). 1219 bytes result sent to driver
15/08/21 08:58:58 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 862, localhost, ANY, 1773 bytes)
15/08/21 08:58:58 INFO Executor: Running task 15.0 in stage 7.0 (TID 862)
15/08/21 08:58:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000075_0 start: 134217728 end: 257463677 length: 123245949 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:58:58 INFO DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:423) finished in 90.129 s
15/08/21 08:58:58 INFO DAGScheduler: looking for newly runnable stages
15/08/21 08:58:58 INFO DAGScheduler: running: Set(ShuffleMapStage 7)
15/08/21 08:58:58 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/21 08:58:58 INFO DAGScheduler: failed: Set()
15/08/21 08:58:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:58:58 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@419ef543
15/08/21 08:58:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:58 INFO StatsReportListener: task runtime:(count: 11, mean: 7602.000000, stdev: 541.343279, max: 8566.000000, min: 6878.000000)
15/08/21 08:58:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:58 INFO StatsReportListener: 	6.9 s	6.9 s	7.1 s	7.2 s	7.6 s	8.0 s	8.5 s	8.6 s	8.6 s
15/08/21 08:58:58 INFO StatsReportListener: shuffle bytes written:(count: 11, mean: 23808027.454545, stdev: 37803.301139, max: 23860071.000000, min: 23722150.000000)
15/08/21 08:58:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:58 INFO StatsReportListener: 	22.6 MB	22.6 MB	22.7 MB	22.7 MB	22.7 MB	22.7 MB	22.7 MB	22.8 MB	22.8 MB
15/08/21 08:58:58 INFO TaskSetManager: Finished task 199.0 in stage 5.0 (TID 843) in 7178 ms on localhost (200/200)
15/08/21 08:58:58 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/21 08:58:58 INFO StatsReportListener: fetch wait time:(count: 11, mean: 0.909091, stdev: 2.274545, max: 8.000000, min: 0.000000)
15/08/21 08:58:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:58 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	8.0 ms	8.0 ms
15/08/21 08:58:58 INFO StatsReportListener: remote bytes read:(count: 11, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 08:58:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:58 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 08:58:58 INFO DAGScheduler: Missing parents for ResultStage 8: List(ShuffleMapStage 7)
15/08/21 08:58:58 INFO StatsReportListener: task result size:(count: 11, mean: 1219.000000, stdev: 0.000000, max: 1219.000000, min: 1219.000000)
15/08/21 08:58:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:58 INFO StatsReportListener: 	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B	1219.0 B
15/08/21 08:58:58 INFO InternalParquetRecordReader: block read in memory in 174 ms. row count = 3500100
15/08/21 08:58:58 INFO StatsReportListener: executor (non-fetch) time pct: (count: 11, mean: 99.686166, stdev: 0.105339, max: 99.793602, min: 99.429682)
15/08/21 08:58:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:58 INFO StatsReportListener: 	99 %	99 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 08:58:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574066 records.
15/08/21 08:58:58 INFO StatsReportListener: fetch wait time pct: (count: 11, mean: 0.012180, stdev: 0.030010, max: 0.105513, min: 0.000000)
15/08/21 08:58:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:58 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 08:58:58 INFO StatsReportListener: other time pct: (count: 11, mean: 0.301654, stdev: 0.112839, max: 0.570318, min: 0.171459)
15/08/21 08:58:58 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 08:58:58 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %
15/08/21 08:58:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:58:58 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3503008
15/08/21 08:58:58 INFO InternalParquetRecordReader: block read in memory in 135 ms. row count = 3500100
15/08/21 08:58:58 INFO InternalParquetRecordReader: block read in memory in 111 ms. row count = 3500780
15/08/21 08:58:59 INFO InternalParquetRecordReader: Assembled and processed 3500939 records from 2 columns in 5260 ms: 665.57776 rec/ms, 1331.1555 cell/ms
15/08/21 08:58:59 INFO InternalParquetRecordReader: time spent so far 4% reading (238 ms) and 95% processing (5260 ms)
15/08/21 08:58:59 INFO InternalParquetRecordReader: at row 3500939. reading next block
15/08/21 08:58:59 INFO InternalParquetRecordReader: block read in memory in 54 ms. row count = 71904
15/08/21 08:59:00 INFO Executor: Finished task 0.0 in stage 7.0 (TID 847). 2125 bytes result sent to driver
15/08/21 08:59:00 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 863, localhost, ANY, 1759 bytes)
15/08/21 08:59:00 INFO Executor: Running task 16.0 in stage 7.0 (TID 863)
15/08/21 08:59:00 INFO InternalParquetRecordReader: Assembled and processed 3502678 records from 2 columns in 5577 ms: 628.05774 rec/ms, 1256.1155 cell/ms
15/08/21 08:59:00 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 847) in 6254 ms on localhost (1/170)
15/08/21 08:59:00 INFO InternalParquetRecordReader: time spent so far 1% reading (106 ms) and 98% processing (5577 ms)
15/08/21 08:59:00 INFO InternalParquetRecordReader: at row 3502678. reading next block
15/08/21 08:59:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:00 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 70255
15/08/21 08:59:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501183 records.
15/08/21 08:59:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:00 INFO InternalParquetRecordReader: block read in memory in 45 ms. row count = 3501183
15/08/21 08:59:00 INFO Executor: Finished task 1.0 in stage 7.0 (TID 848). 2125 bytes result sent to driver
15/08/21 08:59:00 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 864, localhost, ANY, 1773 bytes)
15/08/21 08:59:00 INFO Executor: Running task 17.0 in stage 7.0 (TID 864)
15/08/21 08:59:00 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 848) in 6358 ms on localhost (2/170)
15/08/21 08:59:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 134217728 end: 261799262 length: 127581534 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3689969 records.
15/08/21 08:59:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:00 INFO InternalParquetRecordReader: block read in memory in 60 ms. row count = 3501351
15/08/21 08:59:00 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5048 ms: 693.3637 rec/ms, 1386.7274 cell/ms
15/08/21 08:59:00 INFO InternalParquetRecordReader: time spent so far 1% reading (101 ms) and 98% processing (5048 ms)
15/08/21 08:59:00 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:00 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 73883
15/08/21 08:59:00 INFO Executor: Finished task 3.0 in stage 7.0 (TID 850). 2125 bytes result sent to driver
15/08/21 08:59:00 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 865, localhost, ANY, 1762 bytes)
15/08/21 08:59:00 INFO Executor: Running task 18.0 in stage 7.0 (TID 865)
15/08/21 08:59:00 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 850) in 6310 ms on localhost (3/170)
15/08/21 08:59:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:00 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5631 ms: 621.57697 rec/ms, 1243.1539 cell/ms
15/08/21 08:59:00 INFO InternalParquetRecordReader: time spent so far 2% reading (120 ms) and 97% processing (5631 ms)
15/08/21 08:59:00 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:00 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 72945
15/08/21 08:59:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:00 INFO Executor: Finished task 2.0 in stage 7.0 (TID 849). 2125 bytes result sent to driver
15/08/21 08:59:00 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 866, localhost, ANY, 1773 bytes)
15/08/21 08:59:00 INFO Executor: Running task 19.0 in stage 7.0 (TID 866)
15/08/21 08:59:00 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 849) in 6703 ms on localhost (4/170)
15/08/21 08:59:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000066_0 start: 134217728 end: 257435404 length: 123217676 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573923 records.
15/08/21 08:59:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:01 INFO InternalParquetRecordReader: block read in memory in 156 ms. row count = 3500100
15/08/21 08:59:01 INFO InternalParquetRecordReader: block read in memory in 146 ms. row count = 3501239
15/08/21 08:59:01 INFO Executor: Finished task 6.0 in stage 7.0 (TID 853). 2125 bytes result sent to driver
15/08/21 08:59:01 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 867, localhost, ANY, 1762 bytes)
15/08/21 08:59:01 INFO Executor: Running task 20.0 in stage 7.0 (TID 867)
15/08/21 08:59:01 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 853) in 5853 ms on localhost (5/170)
15/08/21 08:59:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:01 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 3500100
15/08/21 08:59:01 INFO Executor: Finished task 5.0 in stage 7.0 (TID 852). 2125 bytes result sent to driver
15/08/21 08:59:01 INFO Executor: Finished task 7.0 in stage 7.0 (TID 854). 2125 bytes result sent to driver
15/08/21 08:59:01 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 868, localhost, ANY, 1775 bytes)
15/08/21 08:59:01 INFO Executor: Running task 21.0 in stage 7.0 (TID 868)
15/08/21 08:59:01 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 869, localhost, ANY, 1762 bytes)
15/08/21 08:59:01 INFO Executor: Running task 22.0 in stage 7.0 (TID 869)
15/08/21 08:59:01 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 852) in 6600 ms on localhost (6/170)
15/08/21 08:59:01 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 854) in 6105 ms on localhost (7/170)
15/08/21 08:59:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000047_0 start: 134217728 end: 257347114 length: 123129386 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501423 records.
15/08/21 08:59:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
15/08/21 08:59:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:01 INFO Executor: Finished task 4.0 in stage 7.0 (TID 851). 2125 bytes result sent to driver
15/08/21 08:59:01 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 870, localhost, ANY, 1772 bytes)
15/08/21 08:59:01 INFO Executor: Running task 23.0 in stage 7.0 (TID 870)
15/08/21 08:59:01 INFO Executor: Finished task 8.0 in stage 7.0 (TID 855). 2125 bytes result sent to driver
15/08/21 08:59:01 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 851) in 7004 ms on localhost (8/170)
15/08/21 08:59:01 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 871, localhost, ANY, 1762 bytes)
15/08/21 08:59:01 INFO Executor: Running task 24.0 in stage 7.0 (TID 871)
15/08/21 08:59:01 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 855) in 6068 ms on localhost (9/170)
15/08/21 08:59:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:01 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000046_0 start: 134217728 end: 257844810 length: 123627082 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:01 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:01 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4532 ms: 772.30804 rec/ms, 1544.6161 cell/ms
15/08/21 08:59:01 INFO InternalParquetRecordReader: time spent so far 3% reading (146 ms) and 96% processing (4532 ms)
15/08/21 08:59:01 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:01 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573035 records.
15/08/21 08:59:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:01 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:01 INFO InternalParquetRecordReader: block read in memory in 111 ms. row count = 3501423
15/08/21 08:59:01 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 72684
15/08/21 08:59:01 INFO InternalParquetRecordReader: block read in memory in 227 ms. row count = 3500100
15/08/21 08:59:01 INFO InternalParquetRecordReader: block read in memory in 160 ms. row count = 3500100
15/08/21 08:59:02 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4663 ms: 750.6112 rec/ms, 1501.2224 cell/ms
15/08/21 08:59:02 INFO InternalParquetRecordReader: time spent so far 2% reading (112 ms) and 97% processing (4663 ms)
15/08/21 08:59:02 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:02 INFO InternalParquetRecordReader: block read in memory in 212 ms. row count = 3500100
15/08/21 08:59:02 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 127897
15/08/21 08:59:02 INFO Executor: Finished task 10.0 in stage 7.0 (TID 857). 2125 bytes result sent to driver
15/08/21 08:59:02 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 872, localhost, ANY, 1771 bytes)
15/08/21 08:59:02 INFO Executor: Running task 25.0 in stage 7.0 (TID 872)
15/08/21 08:59:02 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 857) in 5038 ms on localhost (10/170)
15/08/21 08:59:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000024_0 start: 134217728 end: 257827380 length: 123609652 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573921 records.
15/08/21 08:59:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:02 INFO InternalParquetRecordReader: block read in memory in 94 ms. row count = 3501305
15/08/21 08:59:02 INFO Executor: Finished task 9.0 in stage 7.0 (TID 856). 2125 bytes result sent to driver
15/08/21 08:59:02 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 873, localhost, ANY, 1762 bytes)
15/08/21 08:59:02 INFO Executor: Running task 26.0 in stage 7.0 (TID 873)
15/08/21 08:59:02 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 856) in 5398 ms on localhost (11/170)
15/08/21 08:59:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501667 records.
15/08/21 08:59:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:02 INFO Executor: Finished task 11.0 in stage 7.0 (TID 858). 2125 bytes result sent to driver
15/08/21 08:59:02 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 874, localhost, ANY, 1773 bytes)
15/08/21 08:59:02 INFO Executor: Running task 27.0 in stage 7.0 (TID 874)
15/08/21 08:59:02 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 858) in 5376 ms on localhost (12/170)
15/08/21 08:59:02 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000045_0 start: 134217728 end: 257458294 length: 123240566 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:02 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:02 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572747 records.
15/08/21 08:59:02 INFO InternalParquetRecordReader: block read in memory in 117 ms. row count = 3501667
15/08/21 08:59:02 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:02 INFO InternalParquetRecordReader: block read in memory in 370 ms. row count = 3500100
15/08/21 08:59:03 INFO InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 4934 ms: 709.97327 rec/ms, 1419.9465 cell/ms
15/08/21 08:59:03 INFO InternalParquetRecordReader: time spent so far 2% reading (101 ms) and 97% processing (4934 ms)
15/08/21 08:59:03 INFO InternalParquetRecordReader: at row 3503008. reading next block
15/08/21 08:59:03 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 70884
15/08/21 08:59:03 INFO InternalParquetRecordReader: Assembled and processed 3500780 records from 2 columns in 5063 ms: 691.4438 rec/ms, 1382.8876 cell/ms
15/08/21 08:59:03 INFO InternalParquetRecordReader: time spent so far 2% reading (111 ms) and 97% processing (5063 ms)
15/08/21 08:59:03 INFO InternalParquetRecordReader: at row 3500780. reading next block
15/08/21 08:59:03 INFO InternalParquetRecordReader: block read in memory in 17 ms. row count = 73286
15/08/21 08:59:04 INFO Executor: Finished task 14.0 in stage 7.0 (TID 861). 2125 bytes result sent to driver
15/08/21 08:59:04 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 875, localhost, ANY, 1762 bytes)
15/08/21 08:59:04 INFO Executor: Running task 28.0 in stage 7.0 (TID 875)
15/08/21 08:59:04 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 861) in 5535 ms on localhost (13/170)
15/08/21 08:59:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:04 INFO InternalParquetRecordReader: block read in memory in 87 ms. row count = 3500100
15/08/21 08:59:04 INFO Executor: Finished task 15.0 in stage 7.0 (TID 862). 2125 bytes result sent to driver
15/08/21 08:59:04 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 876, localhost, ANY, 1773 bytes)
15/08/21 08:59:04 INFO Executor: Running task 29.0 in stage 7.0 (TID 876)
15/08/21 08:59:04 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 862) in 5933 ms on localhost (14/170)
15/08/21 08:59:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000079_0 start: 134217728 end: 257369456 length: 123151728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:04 INFO Executor: Finished task 12.0 in stage 7.0 (TID 859). 2125 bytes result sent to driver
15/08/21 08:59:04 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 877, localhost, ANY, 1762 bytes)
15/08/21 08:59:04 INFO Executor: Running task 30.0 in stage 7.0 (TID 877)
15/08/21 08:59:04 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 859) in 6173 ms on localhost (15/170)
15/08/21 08:59:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
15/08/21 08:59:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:04 INFO Executor: Finished task 13.0 in stage 7.0 (TID 860). 2125 bytes result sent to driver
15/08/21 08:59:04 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 878, localhost, ANY, 1776 bytes)
15/08/21 08:59:04 INFO Executor: Running task 31.0 in stage 7.0 (TID 878)
15/08/21 08:59:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:04 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 860) in 6081 ms on localhost (16/170)
15/08/21 08:59:04 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000040_0 start: 134217728 end: 257340601 length: 123122873 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:04 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:04 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574247 records.
15/08/21 08:59:04 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:04 INFO InternalParquetRecordReader: block read in memory in 104 ms. row count = 3500100
15/08/21 08:59:04 INFO InternalParquetRecordReader: block read in memory in 178 ms. row count = 3500100
15/08/21 08:59:04 INFO InternalParquetRecordReader: block read in memory in 226 ms. row count = 3500100
15/08/21 08:59:05 INFO InternalParquetRecordReader: Assembled and processed 3501351 records from 2 columns in 4575 ms: 765.32263 rec/ms, 1530.6453 cell/ms
15/08/21 08:59:05 INFO InternalParquetRecordReader: time spent so far 1% reading (60 ms) and 98% processing (4575 ms)
15/08/21 08:59:05 INFO InternalParquetRecordReader: at row 3501351. reading next block
15/08/21 08:59:05 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 188618
15/08/21 08:59:05 INFO Executor: Finished task 16.0 in stage 7.0 (TID 863). 2125 bytes result sent to driver
15/08/21 08:59:05 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 879, localhost, ANY, 1761 bytes)
15/08/21 08:59:05 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 863) in 4952 ms on localhost (17/170)
15/08/21 08:59:05 INFO Executor: Running task 32.0 in stage 7.0 (TID 879)
15/08/21 08:59:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:05 INFO InternalParquetRecordReader: block read in memory in 78 ms. row count = 3500100
15/08/21 08:59:05 INFO Executor: Finished task 17.0 in stage 7.0 (TID 864). 2125 bytes result sent to driver
15/08/21 08:59:05 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 880, localhost, ANY, 1771 bytes)
15/08/21 08:59:05 INFO Executor: Running task 33.0 in stage 7.0 (TID 880)
15/08/21 08:59:05 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 864) in 5293 ms on localhost (18/170)
15/08/21 08:59:05 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000012_0 start: 134217728 end: 259459156 length: 125241428 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627517 records.
15/08/21 08:59:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:05 INFO InternalParquetRecordReader: block read in memory in 121 ms. row count = 3500968
15/08/21 08:59:06 INFO InternalParquetRecordReader: Assembled and processed 3501239 records from 2 columns in 4930 ms: 710.1905 rec/ms, 1420.381 cell/ms
15/08/21 08:59:06 INFO InternalParquetRecordReader: time spent so far 2% reading (146 ms) and 97% processing (4930 ms)
15/08/21 08:59:06 INFO InternalParquetRecordReader: at row 3501239. reading next block
15/08/21 08:59:06 INFO Executor: Finished task 18.0 in stage 7.0 (TID 865). 2125 bytes result sent to driver
15/08/21 08:59:06 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 881, localhost, ANY, 1762 bytes)
15/08/21 08:59:06 INFO Executor: Running task 34.0 in stage 7.0 (TID 881)
15/08/21 08:59:06 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 72684
15/08/21 08:59:06 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 865) in 5279 ms on localhost (19/170)
15/08/21 08:59:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501180 records.
15/08/21 08:59:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:06 INFO Executor: Finished task 20.0 in stage 7.0 (TID 867). 2125 bytes result sent to driver
15/08/21 08:59:06 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 882, localhost, ANY, 1772 bytes)
15/08/21 08:59:06 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 867) in 5075 ms on localhost (20/170)
15/08/21 08:59:06 INFO Executor: Running task 35.0 in stage 7.0 (TID 882)
15/08/21 08:59:06 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000067_0 start: 134217728 end: 257580347 length: 123362619 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:06 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:06 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573027 records.
15/08/21 08:59:06 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:06 INFO InternalParquetRecordReader: block read in memory in 201 ms. row count = 3501180
15/08/21 08:59:06 INFO InternalParquetRecordReader: block read in memory in 176 ms. row count = 3500932
15/08/21 08:59:06 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4677 ms: 748.3643 rec/ms, 1496.7286 cell/ms
15/08/21 08:59:06 INFO InternalParquetRecordReader: time spent so far 4% reading (212 ms) and 95% processing (4677 ms)
15/08/21 08:59:06 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:06 INFO InternalParquetRecordReader: block read in memory in 46 ms. row count = 72935
15/08/21 08:59:06 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4832 ms: 724.35846 rec/ms, 1448.7169 cell/ms
15/08/21 08:59:06 INFO InternalParquetRecordReader: time spent so far 4% reading (227 ms) and 95% processing (4832 ms)
15/08/21 08:59:06 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:06 INFO InternalParquetRecordReader: block read in memory in 51 ms. row count = 74238
15/08/21 08:59:07 INFO InternalParquetRecordReader: Assembled and processed 3501305 records from 2 columns in 4586 ms: 763.47687 rec/ms, 1526.9537 cell/ms
15/08/21 08:59:07 INFO InternalParquetRecordReader: time spent so far 2% reading (94 ms) and 97% processing (4586 ms)
15/08/21 08:59:07 INFO InternalParquetRecordReader: at row 3501305. reading next block
15/08/21 08:59:07 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 72616
15/08/21 08:59:07 INFO Executor: Finished task 19.0 in stage 7.0 (TID 866). 2125 bytes result sent to driver
15/08/21 08:59:07 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 883, localhost, ANY, 1761 bytes)
15/08/21 08:59:07 INFO Executor: Running task 36.0 in stage 7.0 (TID 883)
15/08/21 08:59:07 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 866) in 6218 ms on localhost (21/170)
15/08/21 08:59:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:07 INFO InternalParquetRecordReader: block read in memory in 99 ms. row count = 3500100
15/08/21 08:59:07 INFO Executor: Finished task 23.0 in stage 7.0 (TID 870). 2125 bytes result sent to driver
15/08/21 08:59:07 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4704 ms: 744.06885 rec/ms, 1488.1377 cell/ms
15/08/21 08:59:07 INFO InternalParquetRecordReader: time spent so far 7% reading (370 ms) and 92% processing (4704 ms)
15/08/21 08:59:07 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:07 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 884, localhost, ANY, 1773 bytes)
15/08/21 08:59:07 INFO Executor: Running task 37.0 in stage 7.0 (TID 884)
15/08/21 08:59:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000034_0 start: 134217728 end: 257857124 length: 123639396 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:07 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 870) in 5952 ms on localhost (22/170)
15/08/21 08:59:07 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 72647
15/08/21 08:59:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574216 records.
15/08/21 08:59:07 INFO Executor: Finished task 22.0 in stage 7.0 (TID 869). 2125 bytes result sent to driver
15/08/21 08:59:07 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 885, localhost, ANY, 1761 bytes)
15/08/21 08:59:07 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 869) in 6045 ms on localhost (23/170)
15/08/21 08:59:07 INFO Executor: Running task 38.0 in stage 7.0 (TID 885)
15/08/21 08:59:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:07 INFO Executor: Finished task 24.0 in stage 7.0 (TID 871). 2125 bytes result sent to driver
15/08/21 08:59:07 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 886, localhost, ANY, 1773 bytes)
15/08/21 08:59:07 INFO Executor: Running task 39.0 in stage 7.0 (TID 886)
15/08/21 08:59:07 INFO Executor: Finished task 21.0 in stage 7.0 (TID 868). 2125 bytes result sent to driver
15/08/21 08:59:07 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 887, localhost, ANY, 1761 bytes)
15/08/21 08:59:07 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 871) in 5998 ms on localhost (24/170)
15/08/21 08:59:07 INFO Executor: Running task 40.0 in stage 7.0 (TID 887)
15/08/21 08:59:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 134217728 end: 259199417 length: 124981689 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:07 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 868) in 6075 ms on localhost (25/170)
15/08/21 08:59:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
15/08/21 08:59:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626725 records.
15/08/21 08:59:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501407 records.
15/08/21 08:59:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:07 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 3501165
15/08/21 08:59:07 INFO Executor: Finished task 25.0 in stage 7.0 (TID 872). 2125 bytes result sent to driver
15/08/21 08:59:07 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 888, localhost, ANY, 1773 bytes)
15/08/21 08:59:07 INFO Executor: Running task 41.0 in stage 7.0 (TID 888)
15/08/21 08:59:07 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000028_0 start: 134217728 end: 257564901 length: 123347173 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:07 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 872) in 5646 ms on localhost (26/170)
15/08/21 08:59:07 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:07 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572791 records.
15/08/21 08:59:07 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:07 INFO InternalParquetRecordReader: block read in memory in 37 ms. row count = 3500841
15/08/21 08:59:08 INFO InternalParquetRecordReader: block read in memory in 172 ms. row count = 3501407
15/08/21 08:59:08 INFO InternalParquetRecordReader: block read in memory in 184 ms. row count = 3501121
15/08/21 08:59:08 INFO InternalParquetRecordReader: block read in memory in 216 ms. row count = 3502670
15/08/21 08:59:08 INFO Executor: Finished task 27.0 in stage 7.0 (TID 874). 2125 bytes result sent to driver
15/08/21 08:59:08 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 889, localhost, ANY, 1761 bytes)
15/08/21 08:59:08 INFO Executor: Running task 42.0 in stage 7.0 (TID 889)
15/08/21 08:59:08 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 874) in 5626 ms on localhost (27/170)
15/08/21 08:59:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:08 INFO Executor: Finished task 26.0 in stage 7.0 (TID 873). 2125 bytes result sent to driver
15/08/21 08:59:08 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 890, localhost, ANY, 1772 bytes)
15/08/21 08:59:08 INFO Executor: Running task 43.0 in stage 7.0 (TID 890)
15/08/21 08:59:08 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 873) in 5775 ms on localhost (28/170)
15/08/21 08:59:08 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000023_0 start: 134217728 end: 257460000 length: 123242272 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:08 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501143 records.
15/08/21 08:59:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:08 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573047 records.
15/08/21 08:59:08 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:08 INFO InternalParquetRecordReader: block read in memory in 162 ms. row count = 3501143
15/08/21 08:59:08 INFO InternalParquetRecordReader: block read in memory in 154 ms. row count = 3500100
15/08/21 08:59:09 INFO Executor: Finished task 28.0 in stage 7.0 (TID 875). 2125 bytes result sent to driver
15/08/21 08:59:09 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 891, localhost, ANY, 1762 bytes)
15/08/21 08:59:09 INFO Executor: Running task 44.0 in stage 7.0 (TID 891)
15/08/21 08:59:09 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 875) in 5233 ms on localhost (29/170)
15/08/21 08:59:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501260 records.
15/08/21 08:59:09 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:09 INFO InternalParquetRecordReader: block read in memory in 145 ms. row count = 3501260
15/08/21 08:59:09 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4877 ms: 717.6748 rec/ms, 1435.3496 cell/ms
15/08/21 08:59:09 INFO InternalParquetRecordReader: time spent so far 2% reading (104 ms) and 97% processing (4877 ms)
15/08/21 08:59:09 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:09 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 74147
15/08/21 08:59:09 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5169 ms: 677.13293 rec/ms, 1354.2659 cell/ms
15/08/21 08:59:09 INFO InternalParquetRecordReader: time spent so far 4% reading (226 ms) and 95% processing (5169 ms)
15/08/21 08:59:09 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:09 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 74054
15/08/21 08:59:09 INFO Executor: Finished task 31.0 in stage 7.0 (TID 878). 2125 bytes result sent to driver
15/08/21 08:59:09 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 892, localhost, ANY, 1772 bytes)
15/08/21 08:59:09 INFO Executor: Running task 45.0 in stage 7.0 (TID 892)
15/08/21 08:59:09 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 878) in 5424 ms on localhost (30/170)
15/08/21 08:59:09 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000050_0 start: 134217728 end: 257842743 length: 123625015 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:09 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:09 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573207 records.
15/08/21 08:59:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:10 INFO InternalParquetRecordReader: block read in memory in 98 ms. row count = 3500100
15/08/21 08:59:10 INFO Executor: Finished task 30.0 in stage 7.0 (TID 877). 2125 bytes result sent to driver
15/08/21 08:59:10 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 893, localhost, ANY, 1762 bytes)
15/08/21 08:59:10 INFO Executor: Running task 46.0 in stage 7.0 (TID 893)
15/08/21 08:59:10 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 877) in 5660 ms on localhost (31/170)
15/08/21 08:59:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501217 records.
15/08/21 08:59:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:10 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 3501217
15/08/21 08:59:10 INFO Executor: Finished task 29.0 in stage 7.0 (TID 876). 2125 bytes result sent to driver
15/08/21 08:59:10 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 894, localhost, ANY, 1773 bytes)
15/08/21 08:59:10 INFO Executor: Running task 47.0 in stage 7.0 (TID 894)
15/08/21 08:59:10 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 876) in 5859 ms on localhost (32/170)
15/08/21 08:59:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000042_0 start: 134217728 end: 257576472 length: 123358744 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573258 records.
15/08/21 08:59:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:10 INFO InternalParquetRecordReader: block read in memory in 77 ms. row count = 3501508
15/08/21 08:59:10 INFO InternalParquetRecordReader: Assembled and processed 3500968 records from 2 columns in 4614 ms: 758.7707 rec/ms, 1517.5414 cell/ms
15/08/21 08:59:10 INFO InternalParquetRecordReader: time spent so far 2% reading (121 ms) and 97% processing (4614 ms)
15/08/21 08:59:10 INFO InternalParquetRecordReader: at row 3500968. reading next block
15/08/21 08:59:10 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 126549
15/08/21 08:59:10 INFO Executor: Finished task 32.0 in stage 7.0 (TID 879). 2125 bytes result sent to driver
15/08/21 08:59:10 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 895, localhost, ANY, 1762 bytes)
15/08/21 08:59:10 INFO Executor: Running task 48.0 in stage 7.0 (TID 895)
15/08/21 08:59:10 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 879) in 5325 ms on localhost (33/170)
15/08/21 08:59:10 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:10 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:10 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503050 records.
15/08/21 08:59:10 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:10 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 3503050
15/08/21 08:59:11 INFO Executor: Finished task 33.0 in stage 7.0 (TID 880). 2125 bytes result sent to driver
15/08/21 08:59:11 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 896, localhost, ANY, 1773 bytes)
15/08/21 08:59:11 INFO Executor: Running task 49.0 in stage 7.0 (TID 896)
15/08/21 08:59:11 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 880) in 5293 ms on localhost (34/170)
15/08/21 08:59:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000076_0 start: 134217728 end: 256733155 length: 122515427 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3570986 records.
15/08/21 08:59:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:11 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3503008
15/08/21 08:59:11 INFO InternalParquetRecordReader: Assembled and processed 3500932 records from 2 columns in 4892 ms: 715.6443 rec/ms, 1431.2886 cell/ms
15/08/21 08:59:11 INFO InternalParquetRecordReader: time spent so far 3% reading (176 ms) and 96% processing (4892 ms)
15/08/21 08:59:11 INFO InternalParquetRecordReader: at row 3500932. reading next block
15/08/21 08:59:11 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 72095
15/08/21 08:59:11 INFO Executor: Finished task 35.0 in stage 7.0 (TID 882). 2125 bytes result sent to driver
15/08/21 08:59:11 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 897, localhost, ANY, 1761 bytes)
15/08/21 08:59:11 INFO Executor: Running task 50.0 in stage 7.0 (TID 897)
15/08/21 08:59:11 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 882) in 5604 ms on localhost (35/170)
15/08/21 08:59:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501200 records.
15/08/21 08:59:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:12 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 3501200
15/08/21 08:59:12 INFO Executor: Finished task 34.0 in stage 7.0 (TID 881). 2125 bytes result sent to driver
15/08/21 08:59:12 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 898, localhost, ANY, 1771 bytes)
15/08/21 08:59:12 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 881) in 5864 ms on localhost (36/170)
15/08/21 08:59:12 INFO Executor: Running task 51.0 in stage 7.0 (TID 898)
15/08/21 08:59:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000073_0 start: 134217728 end: 257568635 length: 123350907 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573168 records.
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:12 INFO Executor: Finished task 36.0 in stage 7.0 (TID 883). 2125 bytes result sent to driver
15/08/21 08:59:12 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 899, localhost, ANY, 1760 bytes)
15/08/21 08:59:12 INFO Executor: Running task 52.0 in stage 7.0 (TID 899)
15/08/21 08:59:12 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 883) in 4964 ms on localhost (37/170)
15/08/21 08:59:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:12 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 3501341
15/08/21 08:59:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500823 records.
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:12 INFO InternalParquetRecordReader: Assembled and processed 3501165 records from 2 columns in 4403 ms: 795.1771 rec/ms, 1590.3542 cell/ms
15/08/21 08:59:12 INFO InternalParquetRecordReader: time spent so far 2% reading (98 ms) and 97% processing (4403 ms)
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 3501165. reading next block
15/08/21 08:59:12 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 73051
15/08/21 08:59:12 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500823
15/08/21 08:59:12 INFO InternalParquetRecordReader: Assembled and processed 3502670 records from 2 columns in 4380 ms: 799.69635 rec/ms, 1599.3927 cell/ms
15/08/21 08:59:12 INFO InternalParquetRecordReader: time spent so far 4% reading (216 ms) and 95% processing (4380 ms)
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 3502670. reading next block
15/08/21 08:59:12 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 124055
15/08/21 08:59:12 INFO Executor: Finished task 37.0 in stage 7.0 (TID 884). 2125 bytes result sent to driver
15/08/21 08:59:12 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 900, localhost, ANY, 1771 bytes)
15/08/21 08:59:12 INFO Executor: Running task 53.0 in stage 7.0 (TID 900)
15/08/21 08:59:12 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 884) in 5003 ms on localhost (38/170)
15/08/21 08:59:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000030_0 start: 134217728 end: 257584480 length: 123366752 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573143 records.
15/08/21 08:59:12 INFO Executor: Finished task 40.0 in stage 7.0 (TID 887). 2125 bytes result sent to driver
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:12 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 901, localhost, ANY, 1760 bytes)
15/08/21 08:59:12 INFO Executor: Running task 54.0 in stage 7.0 (TID 901)
15/08/21 08:59:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:12 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 887) in 4987 ms on localhost (39/170)
15/08/21 08:59:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503144 records.
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:12 INFO InternalParquetRecordReader: block read in memory in 70 ms. row count = 3501035
15/08/21 08:59:12 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 3503144
15/08/21 08:59:12 INFO Executor: Finished task 38.0 in stage 7.0 (TID 885). 2125 bytes result sent to driver
15/08/21 08:59:12 INFO Executor: Finished task 39.0 in stage 7.0 (TID 886). 2125 bytes result sent to driver
15/08/21 08:59:12 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 902, localhost, ANY, 1773 bytes)
15/08/21 08:59:12 INFO Executor: Running task 55.0 in stage 7.0 (TID 902)
15/08/21 08:59:12 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 903, localhost, ANY, 1761 bytes)
15/08/21 08:59:12 INFO Executor: Running task 56.0 in stage 7.0 (TID 903)
15/08/21 08:59:12 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 885) in 5203 ms on localhost (40/170)
15/08/21 08:59:12 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 886) in 5189 ms on localhost (41/170)
15/08/21 08:59:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 134217728 end: 259050527 length: 124832799 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3624571 records.
15/08/21 08:59:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:12 INFO InternalParquetRecordReader: Assembled and processed 3500841 records from 2 columns in 5046 ms: 693.7854 rec/ms, 1387.5708 cell/ms
15/08/21 08:59:12 INFO InternalParquetRecordReader: time spent so far 0% reading (37 ms) and 99% processing (5046 ms)
15/08/21 08:59:12 INFO InternalParquetRecordReader: at row 3500841. reading next block
15/08/21 08:59:13 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
15/08/21 08:59:13 INFO InternalParquetRecordReader: block read in memory in 49 ms. row count = 71950
15/08/21 08:59:13 INFO InternalParquetRecordReader: block read in memory in 128 ms. row count = 3500100
15/08/21 08:59:13 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4549 ms: 769.4219 rec/ms, 1538.8438 cell/ms
15/08/21 08:59:13 INFO InternalParquetRecordReader: time spent so far 3% reading (154 ms) and 96% processing (4549 ms)
15/08/21 08:59:13 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:13 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 72947
15/08/21 08:59:13 INFO Executor: Finished task 42.0 in stage 7.0 (TID 889). 2125 bytes result sent to driver
15/08/21 08:59:13 INFO Executor: Finished task 41.0 in stage 7.0 (TID 888). 2125 bytes result sent to driver
15/08/21 08:59:13 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 904, localhost, ANY, 1775 bytes)
15/08/21 08:59:13 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 905, localhost, ANY, 1762 bytes)
15/08/21 08:59:13 INFO Executor: Running task 58.0 in stage 7.0 (TID 905)
15/08/21 08:59:13 INFO Executor: Running task 57.0 in stage 7.0 (TID 904)
15/08/21 08:59:13 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 889) in 5706 ms on localhost (42/170)
15/08/21 08:59:13 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 888) in 6045 ms on localhost (43/170)
15/08/21 08:59:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000021_0 start: 134217728 end: 257461890 length: 123244162 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573988 records.
15/08/21 08:59:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:13 INFO Executor: Finished task 43.0 in stage 7.0 (TID 890). 2125 bytes result sent to driver
15/08/21 08:59:13 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 906, localhost, ANY, 1777 bytes)
15/08/21 08:59:13 INFO Executor: Running task 59.0 in stage 7.0 (TID 906)
15/08/21 08:59:13 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 890) in 5757 ms on localhost (44/170)
15/08/21 08:59:13 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000069_0 start: 134217728 end: 257416343 length: 123198615 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:13 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:13 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574223 records.
15/08/21 08:59:13 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:14 INFO InternalParquetRecordReader: block read in memory in 106 ms. row count = 3501614
15/08/21 08:59:14 INFO InternalParquetRecordReader: block read in memory in 181 ms. row count = 3500100
15/08/21 08:59:14 INFO InternalParquetRecordReader: block read in memory in 233 ms. row count = 3500824
15/08/21 08:59:14 INFO Executor: Finished task 44.0 in stage 7.0 (TID 891). 2125 bytes result sent to driver
15/08/21 08:59:14 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 907, localhost, ANY, 1761 bytes)
15/08/21 08:59:14 INFO Executor: Running task 60.0 in stage 7.0 (TID 907)
15/08/21 08:59:14 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 891) in 4969 ms on localhost (45/170)
15/08/21 08:59:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501191 records.
15/08/21 08:59:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:14 INFO InternalParquetRecordReader: block read in memory in 109 ms. row count = 3501191
15/08/21 08:59:14 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4453 ms: 786.00946 rec/ms, 1572.0189 cell/ms
15/08/21 08:59:14 INFO InternalParquetRecordReader: time spent so far 2% reading (98 ms) and 97% processing (4453 ms)
15/08/21 08:59:14 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:14 INFO InternalParquetRecordReader: block read in memory in 41 ms. row count = 73107
15/08/21 08:59:14 INFO InternalParquetRecordReader: Assembled and processed 3501508 records from 2 columns in 4391 ms: 797.4284 rec/ms, 1594.8568 cell/ms
15/08/21 08:59:14 INFO InternalParquetRecordReader: time spent so far 1% reading (77 ms) and 98% processing (4391 ms)
15/08/21 08:59:14 INFO InternalParquetRecordReader: at row 3501508. reading next block
15/08/21 08:59:14 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 71750
15/08/21 08:59:15 INFO Executor: Finished task 46.0 in stage 7.0 (TID 893). 2125 bytes result sent to driver
15/08/21 08:59:15 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 908, localhost, ANY, 1773 bytes)
15/08/21 08:59:15 INFO Executor: Running task 61.0 in stage 7.0 (TID 908)
15/08/21 08:59:15 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 893) in 5015 ms on localhost (46/170)
15/08/21 08:59:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000017_0 start: 134217728 end: 257883468 length: 123665740 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573362 records.
15/08/21 08:59:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:15 INFO Executor: Finished task 45.0 in stage 7.0 (TID 892). 2125 bytes result sent to driver
15/08/21 08:59:15 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 909, localhost, ANY, 1761 bytes)
15/08/21 08:59:15 INFO Executor: Running task 62.0 in stage 7.0 (TID 909)
15/08/21 08:59:15 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 892) in 5367 ms on localhost (47/170)
15/08/21 08:59:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:15 INFO InternalParquetRecordReader: block read in memory in 157 ms. row count = 3500100
15/08/21 08:59:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501059 records.
15/08/21 08:59:15 INFO Executor: Finished task 47.0 in stage 7.0 (TID 894). 2125 bytes result sent to driver
15/08/21 08:59:15 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 910, localhost, ANY, 1775 bytes)
15/08/21 08:59:15 INFO Executor: Running task 63.0 in stage 7.0 (TID 910)
15/08/21 08:59:15 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 894) in 5094 ms on localhost (48/170)
15/08/21 08:59:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000081_0 start: 134217728 end: 257592803 length: 123375075 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573291 records.
15/08/21 08:59:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:15 INFO InternalParquetRecordReader: block read in memory in 104 ms. row count = 3501059
15/08/21 08:59:15 INFO Executor: Finished task 48.0 in stage 7.0 (TID 895). 2125 bytes result sent to driver
15/08/21 08:59:15 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 911, localhost, ANY, 1761 bytes)
15/08/21 08:59:15 INFO Executor: Running task 64.0 in stage 7.0 (TID 911)
15/08/21 08:59:15 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 895) in 4985 ms on localhost (49/170)
15/08/21 08:59:15 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:15 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:15 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:15 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:15 INFO InternalParquetRecordReader: block read in memory in 246 ms. row count = 3501221
15/08/21 08:59:15 INFO InternalParquetRecordReader: block read in memory in 136 ms. row count = 3500100
15/08/21 08:59:16 INFO InternalParquetRecordReader: Assembled and processed 3503008 records from 2 columns in 5004 ms: 700.04156 rec/ms, 1400.0831 cell/ms
15/08/21 08:59:16 INFO InternalParquetRecordReader: time spent so far 1% reading (70 ms) and 98% processing (5004 ms)
15/08/21 08:59:16 INFO InternalParquetRecordReader: at row 3503008. reading next block
15/08/21 08:59:16 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 67978
15/08/21 08:59:16 INFO Executor: Finished task 49.0 in stage 7.0 (TID 896). 2125 bytes result sent to driver
15/08/21 08:59:16 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 912, localhost, ANY, 1773 bytes)
15/08/21 08:59:16 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 896) in 5694 ms on localhost (50/170)
15/08/21 08:59:16 INFO Executor: Running task 65.0 in stage 7.0 (TID 912)
15/08/21 08:59:16 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000019_0 start: 134217728 end: 258145407 length: 123927679 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:16 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:16 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574053 records.
15/08/21 08:59:16 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:16 INFO InternalParquetRecordReader: Assembled and processed 3501341 records from 2 columns in 4629 ms: 756.3925 rec/ms, 1512.785 cell/ms
15/08/21 08:59:16 INFO InternalParquetRecordReader: time spent so far 1% reading (86 ms) and 98% processing (4629 ms)
15/08/21 08:59:16 INFO InternalParquetRecordReader: at row 3501341. reading next block
15/08/21 08:59:16 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 71827
15/08/21 08:59:16 INFO InternalParquetRecordReader: block read in memory in 90 ms. row count = 3500100
15/08/21 08:59:17 INFO Executor: Finished task 50.0 in stage 7.0 (TID 897). 2125 bytes result sent to driver
15/08/21 08:59:17 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 913, localhost, ANY, 1760 bytes)
15/08/21 08:59:17 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 897) in 5440 ms on localhost (51/170)
15/08/21 08:59:17 INFO Executor: Running task 66.0 in stage 7.0 (TID 913)
15/08/21 08:59:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:17 INFO Executor: Finished task 52.0 in stage 7.0 (TID 899). 2125 bytes result sent to driver
15/08/21 08:59:17 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 914, localhost, ANY, 1773 bytes)
15/08/21 08:59:17 INFO Executor: Running task 67.0 in stage 7.0 (TID 914)
15/08/21 08:59:17 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 899) in 5245 ms on localhost (52/170)
15/08/21 08:59:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 134217728 end: 259344407 length: 125126679 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:17 INFO Executor: Finished task 51.0 in stage 7.0 (TID 898). 2125 bytes result sent to driver
15/08/21 08:59:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627630 records.
15/08/21 08:59:17 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 915, localhost, ANY, 1762 bytes)
15/08/21 08:59:17 INFO Executor: Running task 68.0 in stage 7.0 (TID 915)
15/08/21 08:59:17 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 898) in 5404 ms on localhost (53/170)
15/08/21 08:59:17 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:17 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:17 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:17 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:17 INFO InternalParquetRecordReader: block read in memory in 101 ms. row count = 3500100
15/08/21 08:59:17 INFO InternalParquetRecordReader: Assembled and processed 3501035 records from 2 columns in 4699 ms: 745.0596 rec/ms, 1490.1191 cell/ms
15/08/21 08:59:17 INFO InternalParquetRecordReader: time spent so far 1% reading (70 ms) and 98% processing (4699 ms)
15/08/21 08:59:17 INFO InternalParquetRecordReader: at row 3501035. reading next block
15/08/21 08:59:17 INFO InternalParquetRecordReader: block read in memory in 48 ms. row count = 72108
15/08/21 08:59:17 INFO InternalParquetRecordReader: block read in memory in 103 ms. row count = 3500100
15/08/21 08:59:17 INFO InternalParquetRecordReader: block read in memory in 175 ms. row count = 3500100
15/08/21 08:59:18 INFO Executor: Finished task 54.0 in stage 7.0 (TID 901). 2125 bytes result sent to driver
15/08/21 08:59:18 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 916, localhost, ANY, 1773 bytes)
15/08/21 08:59:18 INFO Executor: Running task 69.0 in stage 7.0 (TID 916)
15/08/21 08:59:18 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 901) in 5278 ms on localhost (54/170)
15/08/21 08:59:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000072_0 start: 134217728 end: 257768338 length: 123550610 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:18 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4969 ms: 704.3872 rec/ms, 1408.7744 cell/ms
15/08/21 08:59:18 INFO InternalParquetRecordReader: time spent so far 1% reading (63 ms) and 98% processing (4969 ms)
15/08/21 08:59:18 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:18 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 124471
15/08/21 08:59:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574075 records.
15/08/21 08:59:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:18 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500100
15/08/21 08:59:18 INFO Executor: Finished task 53.0 in stage 7.0 (TID 900). 2125 bytes result sent to driver
15/08/21 08:59:18 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 917, localhost, ANY, 1761 bytes)
15/08/21 08:59:18 INFO Executor: Running task 70.0 in stage 7.0 (TID 917)
15/08/21 08:59:18 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 900) in 5471 ms on localhost (55/170)
15/08/21 08:59:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502727 records.
15/08/21 08:59:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:18 INFO Executor: Finished task 56.0 in stage 7.0 (TID 903). 2125 bytes result sent to driver
15/08/21 08:59:18 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 918, localhost, ANY, 1775 bytes)
15/08/21 08:59:18 INFO Executor: Running task 71.0 in stage 7.0 (TID 918)
15/08/21 08:59:18 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 903) in 5290 ms on localhost (56/170)
15/08/21 08:59:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000071_0 start: 134217728 end: 257102581 length: 122884853 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:18 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 3502727
15/08/21 08:59:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571405 records.
15/08/21 08:59:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:18 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 3500100
15/08/21 08:59:18 INFO Executor: Finished task 55.0 in stage 7.0 (TID 902). 2125 bytes result sent to driver
15/08/21 08:59:18 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 919, localhost, ANY, 1762 bytes)
15/08/21 08:59:18 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 902) in 5547 ms on localhost (57/170)
15/08/21 08:59:18 INFO Executor: Running task 72.0 in stage 7.0 (TID 919)
15/08/21 08:59:18 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:18 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:18 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501229 records.
15/08/21 08:59:18 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:18 INFO InternalParquetRecordReader: Assembled and processed 3501614 records from 2 columns in 4442 ms: 788.2967 rec/ms, 1576.5934 cell/ms
15/08/21 08:59:18 INFO InternalParquetRecordReader: time spent so far 2% reading (106 ms) and 97% processing (4442 ms)
15/08/21 08:59:18 INFO InternalParquetRecordReader: at row 3501614. reading next block
15/08/21 08:59:18 INFO InternalParquetRecordReader: block read in memory in 128 ms. row count = 3501229
15/08/21 08:59:18 INFO InternalParquetRecordReader: block read in memory in 111 ms. row count = 72609
15/08/21 08:59:18 INFO InternalParquetRecordReader: Assembled and processed 3500824 records from 2 columns in 4666 ms: 750.28375 rec/ms, 1500.5675 cell/ms
15/08/21 08:59:18 INFO InternalParquetRecordReader: time spent so far 4% reading (233 ms) and 95% processing (4666 ms)
15/08/21 08:59:18 INFO InternalParquetRecordReader: at row 3500824. reading next block
15/08/21 08:59:18 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 73164
15/08/21 08:59:19 INFO Executor: Finished task 59.0 in stage 7.0 (TID 906). 2125 bytes result sent to driver
15/08/21 08:59:19 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 920, localhost, ANY, 1775 bytes)
15/08/21 08:59:19 INFO Executor: Running task 73.0 in stage 7.0 (TID 920)
15/08/21 08:59:19 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 906) in 5200 ms on localhost (58/170)
15/08/21 08:59:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000055_0 start: 134217728 end: 257873629 length: 123655901 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572739 records.
15/08/21 08:59:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:19 INFO InternalParquetRecordReader: block read in memory in 89 ms. row count = 3500100
15/08/21 08:59:19 INFO Executor: Finished task 57.0 in stage 7.0 (TID 904). 2125 bytes result sent to driver
15/08/21 08:59:19 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 921, localhost, ANY, 1760 bytes)
15/08/21 08:59:19 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 904) in 5771 ms on localhost (59/170)
15/08/21 08:59:19 INFO Executor: Running task 74.0 in stage 7.0 (TID 921)
15/08/21 08:59:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:19 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3500100
15/08/21 08:59:19 INFO Executor: Finished task 58.0 in stage 7.0 (TID 905). 2125 bytes result sent to driver
15/08/21 08:59:19 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 922, localhost, ANY, 1770 bytes)
15/08/21 08:59:19 INFO Executor: Running task 75.0 in stage 7.0 (TID 922)
15/08/21 08:59:19 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 905) in 6020 ms on localhost (60/170)
15/08/21 08:59:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 134217728 end: 259470450 length: 125252722 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:19 INFO Executor: Finished task 60.0 in stage 7.0 (TID 907). 2125 bytes result sent to driver
15/08/21 08:59:19 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 923, localhost, ANY, 1761 bytes)
15/08/21 08:59:19 INFO Executor: Running task 76.0 in stage 7.0 (TID 923)
15/08/21 08:59:19 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 907) in 5725 ms on localhost (61/170)
15/08/21 08:59:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627768 records.
15/08/21 08:59:19 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:19 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:19 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:19 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:20 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500100
15/08/21 08:59:20 INFO InternalParquetRecordReader: block read in memory in 141 ms. row count = 3501462
15/08/21 08:59:20 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4691 ms: 746.13086 rec/ms, 1492.2617 cell/ms
15/08/21 08:59:20 INFO InternalParquetRecordReader: time spent so far 3% reading (157 ms) and 96% processing (4691 ms)
15/08/21 08:59:20 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:20 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 73262
15/08/21 08:59:20 INFO InternalParquetRecordReader: Assembled and processed 3501221 records from 2 columns in 4709 ms: 743.5169 rec/ms, 1487.0338 cell/ms
15/08/21 08:59:20 INFO InternalParquetRecordReader: time spent so far 4% reading (246 ms) and 95% processing (4709 ms)
15/08/21 08:59:20 INFO InternalParquetRecordReader: at row 3501221. reading next block
15/08/21 08:59:20 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 72070
15/08/21 08:59:20 INFO Executor: Finished task 61.0 in stage 7.0 (TID 908). 2125 bytes result sent to driver
15/08/21 08:59:20 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 924, localhost, ANY, 1774 bytes)
15/08/21 08:59:20 INFO Executor: Running task 77.0 in stage 7.0 (TID 924)
15/08/21 08:59:20 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 908) in 5383 ms on localhost (62/170)
15/08/21 08:59:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000010_0 start: 134217728 end: 259842205 length: 125624477 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627712 records.
15/08/21 08:59:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:20 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 3501302
15/08/21 08:59:20 INFO Executor: Finished task 64.0 in stage 7.0 (TID 911). 2125 bytes result sent to driver
15/08/21 08:59:20 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 925, localhost, ANY, 1761 bytes)
15/08/21 08:59:20 INFO Executor: Running task 78.0 in stage 7.0 (TID 925)
15/08/21 08:59:20 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 911) in 5185 ms on localhost (63/170)
15/08/21 08:59:20 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:20 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:20 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503221 records.
15/08/21 08:59:20 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:20 INFO InternalParquetRecordReader: block read in memory in 65 ms. row count = 3503221
15/08/21 08:59:21 INFO Executor: Finished task 62.0 in stage 7.0 (TID 909). 2125 bytes result sent to driver
15/08/21 08:59:21 INFO Executor: Finished task 63.0 in stage 7.0 (TID 910). 2125 bytes result sent to driver
15/08/21 08:59:21 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 926, localhost, ANY, 1776 bytes)
15/08/21 08:59:21 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 927, localhost, ANY, 1762 bytes)
15/08/21 08:59:21 INFO Executor: Running task 80.0 in stage 7.0 (TID 927)
15/08/21 08:59:21 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 909) in 5744 ms on localhost (64/170)
15/08/21 08:59:21 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 910) in 5638 ms on localhost (65/170)
15/08/21 08:59:21 INFO Executor: Running task 79.0 in stage 7.0 (TID 926)
15/08/21 08:59:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000014_0 start: 134217728 end: 257071082 length: 122853354 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:21 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:21 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572089 records.
15/08/21 08:59:21 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:21 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:21 INFO InternalParquetRecordReader: block read in memory in 197 ms. row count = 3500100
15/08/21 08:59:21 INFO InternalParquetRecordReader: block read in memory in 202 ms. row count = 3500100
15/08/21 08:59:21 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4865 ms: 719.445 rec/ms, 1438.89 cell/ms
15/08/21 08:59:21 INFO InternalParquetRecordReader: time spent so far 1% reading (90 ms) and 98% processing (4865 ms)
15/08/21 08:59:21 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:21 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 73953
15/08/21 08:59:22 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4483 ms: 780.7495 rec/ms, 1561.499 cell/ms
15/08/21 08:59:22 INFO InternalParquetRecordReader: time spent so far 3% reading (175 ms) and 96% processing (4483 ms)
15/08/21 08:59:22 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:22 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 127530
15/08/21 08:59:22 INFO Executor: Finished task 65.0 in stage 7.0 (TID 912). 2125 bytes result sent to driver
15/08/21 08:59:22 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 928, localhost, ANY, 1774 bytes)
15/08/21 08:59:22 INFO Executor: Running task 81.0 in stage 7.0 (TID 928)
15/08/21 08:59:22 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 912) in 5713 ms on localhost (66/170)
15/08/21 08:59:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000044_0 start: 134217728 end: 257075001 length: 122857273 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574581 records.
15/08/21 08:59:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:22 INFO Executor: Finished task 68.0 in stage 7.0 (TID 915). 2125 bytes result sent to driver
15/08/21 08:59:22 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 929, localhost, ANY, 1761 bytes)
15/08/21 08:59:22 INFO Executor: Running task 82.0 in stage 7.0 (TID 929)
15/08/21 08:59:22 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 915) in 5110 ms on localhost (67/170)
15/08/21 08:59:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:22 INFO Executor: Finished task 66.0 in stage 7.0 (TID 913). 2125 bytes result sent to driver
15/08/21 08:59:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:22 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 930, localhost, ANY, 1773 bytes)
15/08/21 08:59:22 INFO Executor: Running task 83.0 in stage 7.0 (TID 930)
15/08/21 08:59:22 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 913) in 5264 ms on localhost (68/170)
15/08/21 08:59:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000062_0 start: 134217728 end: 257427527 length: 123209799 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574316 records.
15/08/21 08:59:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:22 INFO Executor: Finished task 67.0 in stage 7.0 (TID 914). 2125 bytes result sent to driver
15/08/21 08:59:22 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 931, localhost, ANY, 1761 bytes)
15/08/21 08:59:22 INFO Executor: Running task 84.0 in stage 7.0 (TID 931)
15/08/21 08:59:22 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 914) in 5278 ms on localhost (69/170)
15/08/21 08:59:22 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:22 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:22 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:22 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:22 INFO InternalParquetRecordReader: block read in memory in 118 ms. row count = 3500100
15/08/21 08:59:22 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 3500100
15/08/21 08:59:22 INFO InternalParquetRecordReader: block read in memory in 310 ms. row count = 3503231
15/08/21 08:59:22 INFO InternalParquetRecordReader: block read in memory in 220 ms. row count = 3503274
15/08/21 08:59:22 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4453 ms: 786.00946 rec/ms, 1572.0189 cell/ms
15/08/21 08:59:22 INFO InternalParquetRecordReader: time spent so far 2% reading (100 ms) and 97% processing (4453 ms)
15/08/21 08:59:22 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:22 INFO InternalParquetRecordReader: block read in memory in 56 ms. row count = 71305
15/08/21 08:59:22 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4812 ms: 727.3691 rec/ms, 1454.7382 cell/ms
15/08/21 08:59:22 INFO InternalParquetRecordReader: time spent so far 1% reading (91 ms) and 98% processing (4812 ms)
15/08/21 08:59:22 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:23 INFO InternalParquetRecordReader: block read in memory in 58 ms. row count = 73975
15/08/21 08:59:23 INFO Executor: Finished task 71.0 in stage 7.0 (TID 918). 2125 bytes result sent to driver
15/08/21 08:59:23 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 932, localhost, ANY, 1774 bytes)
15/08/21 08:59:23 INFO Executor: Running task 85.0 in stage 7.0 (TID 932)
15/08/21 08:59:23 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 918) in 5409 ms on localhost (70/170)
15/08/21 08:59:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000035_0 start: 134217728 end: 257349334 length: 123131606 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574256 records.
15/08/21 08:59:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:23 INFO InternalParquetRecordReader: block read in memory in 82 ms. row count = 3500100
15/08/21 08:59:23 INFO Executor: Finished task 70.0 in stage 7.0 (TID 917). 2125 bytes result sent to driver
15/08/21 08:59:23 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 933, localhost, ANY, 1761 bytes)
15/08/21 08:59:23 INFO Executor: Running task 86.0 in stage 7.0 (TID 933)
15/08/21 08:59:23 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 917) in 5665 ms on localhost (71/170)
15/08/21 08:59:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:23 INFO Executor: Finished task 72.0 in stage 7.0 (TID 919). 2125 bytes result sent to driver
15/08/21 08:59:23 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 934, localhost, ANY, 1773 bytes)
15/08/21 08:59:23 INFO Executor: Running task 87.0 in stage 7.0 (TID 934)
15/08/21 08:59:23 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 919) in 5410 ms on localhost (72/170)
15/08/21 08:59:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000018_0 start: 134217728 end: 257709471 length: 123491743 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:23 INFO Executor: Finished task 69.0 in stage 7.0 (TID 916). 2125 bytes result sent to driver
15/08/21 08:59:23 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 935, localhost, ANY, 1760 bytes)
15/08/21 08:59:23 INFO Executor: Running task 88.0 in stage 7.0 (TID 935)
15/08/21 08:59:23 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 916) in 5893 ms on localhost (73/170)
15/08/21 08:59:23 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:23 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574154 records.
15/08/21 08:59:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:23 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:23 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:24 INFO InternalParquetRecordReader: block read in memory in 163 ms. row count = 3500100
15/08/21 08:59:24 INFO InternalParquetRecordReader: block read in memory in 90 ms. row count = 3500100
15/08/21 08:59:24 INFO InternalParquetRecordReader: block read in memory in 119 ms. row count = 3500100
15/08/21 08:59:24 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4663 ms: 750.6112 rec/ms, 1501.2224 cell/ms
15/08/21 08:59:24 INFO InternalParquetRecordReader: time spent so far 1% reading (89 ms) and 98% processing (4663 ms)
15/08/21 08:59:24 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:24 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 72639
15/08/21 08:59:24 INFO InternalParquetRecordReader: Assembled and processed 3501462 records from 2 columns in 4444 ms: 787.9077 rec/ms, 1575.8154 cell/ms
15/08/21 08:59:24 INFO InternalParquetRecordReader: time spent so far 3% reading (141 ms) and 96% processing (4444 ms)
15/08/21 08:59:24 INFO InternalParquetRecordReader: at row 3501462. reading next block
15/08/21 08:59:24 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 126306
15/08/21 08:59:24 INFO Executor: Finished task 73.0 in stage 7.0 (TID 920). 2125 bytes result sent to driver
15/08/21 08:59:25 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 936, localhost, ANY, 1772 bytes)
15/08/21 08:59:25 INFO Executor: Running task 89.0 in stage 7.0 (TID 936)
15/08/21 08:59:25 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 920) in 5864 ms on localhost (74/170)
15/08/21 08:59:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 134217728 end: 259741355 length: 125523627 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627697 records.
15/08/21 08:59:25 INFO Executor: Finished task 76.0 in stage 7.0 (TID 923). 2125 bytes result sent to driver
15/08/21 08:59:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:25 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 937, localhost, ANY, 1761 bytes)
15/08/21 08:59:25 INFO Executor: Running task 90.0 in stage 7.0 (TID 937)
15/08/21 08:59:25 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 923) in 5148 ms on localhost (75/170)
15/08/21 08:59:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:25 INFO InternalParquetRecordReader: Assembled and processed 3501302 records from 2 columns in 4415 ms: 793.0469 rec/ms, 1586.0938 cell/ms
15/08/21 08:59:25 INFO InternalParquetRecordReader: time spent so far 2% reading (100 ms) and 97% processing (4415 ms)
15/08/21 08:59:25 INFO InternalParquetRecordReader: at row 3501302. reading next block
15/08/21 08:59:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:25 INFO InternalParquetRecordReader: block read in memory in 35 ms. row count = 126410
15/08/21 08:59:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:25 INFO Executor: Finished task 74.0 in stage 7.0 (TID 921). 2125 bytes result sent to driver
15/08/21 08:59:25 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 938, localhost, ANY, 1776 bytes)
15/08/21 08:59:25 INFO Executor: Running task 91.0 in stage 7.0 (TID 938)
15/08/21 08:59:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000051_0 start: 134217728 end: 257333395 length: 123115667 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:25 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 921) in 5767 ms on localhost (76/170)
15/08/21 08:59:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:25 INFO InternalParquetRecordReader: block read in memory in 369 ms. row count = 3500100
15/08/21 08:59:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573967 records.
15/08/21 08:59:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:25 INFO Executor: Finished task 75.0 in stage 7.0 (TID 922). 2125 bytes result sent to driver
15/08/21 08:59:25 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 939, localhost, ANY, 1761 bytes)
15/08/21 08:59:25 INFO Executor: Running task 92.0 in stage 7.0 (TID 939)
15/08/21 08:59:25 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 922) in 5594 ms on localhost (77/170)
15/08/21 08:59:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:25 INFO InternalParquetRecordReader: block read in memory in 386 ms. row count = 3500100
15/08/21 08:59:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:25 INFO InternalParquetRecordReader: block read in memory in 93 ms. row count = 3500100
15/08/21 08:59:25 INFO InternalParquetRecordReader: block read in memory in 202 ms. row count = 3500100
15/08/21 08:59:25 INFO Executor: Finished task 78.0 in stage 7.0 (TID 925). 2125 bytes result sent to driver
15/08/21 08:59:25 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 940, localhost, ANY, 1776 bytes)
15/08/21 08:59:25 INFO Executor: Running task 93.0 in stage 7.0 (TID 940)
15/08/21 08:59:25 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 925) in 5088 ms on localhost (78/170)
15/08/21 08:59:25 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000041_0 start: 134217728 end: 257330329 length: 123112601 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574008 records.
15/08/21 08:59:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:26 INFO Executor: Finished task 77.0 in stage 7.0 (TID 924). 2125 bytes result sent to driver
15/08/21 08:59:26 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 941, localhost, ANY, 1762 bytes)
15/08/21 08:59:26 INFO Executor: Running task 94.0 in stage 7.0 (TID 941)
15/08/21 08:59:26 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 924) in 5515 ms on localhost (79/170)
15/08/21 08:59:26 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4719 ms: 741.70374 rec/ms, 1483.4075 cell/ms
15/08/21 08:59:26 INFO InternalParquetRecordReader: time spent so far 4% reading (202 ms) and 95% processing (4719 ms)
15/08/21 08:59:26 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:26 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 71989
15/08/21 08:59:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:26 INFO InternalParquetRecordReader: block read in memory in 124 ms. row count = 3500100
15/08/21 08:59:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:26 INFO InternalParquetRecordReader: block read in memory in 68 ms. row count = 3500100
15/08/21 08:59:26 INFO Executor: Finished task 79.0 in stage 7.0 (TID 926). 2125 bytes result sent to driver
15/08/21 08:59:26 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 942, localhost, ANY, 1772 bytes)
15/08/21 08:59:26 INFO Executor: Running task 95.0 in stage 7.0 (TID 942)
15/08/21 08:59:26 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 926) in 5402 ms on localhost (80/170)
15/08/21 08:59:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000070_0 start: 134217728 end: 257756022 length: 123538294 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574308 records.
15/08/21 08:59:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:26 INFO InternalParquetRecordReader: block read in memory in 85 ms. row count = 3500100
15/08/21 08:59:26 INFO Executor: Finished task 80.0 in stage 7.0 (TID 927). 2125 bytes result sent to driver
15/08/21 08:59:26 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 943, localhost, ANY, 1761 bytes)
15/08/21 08:59:26 INFO Executor: Running task 96.0 in stage 7.0 (TID 943)
15/08/21 08:59:26 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 927) in 5544 ms on localhost (81/170)
15/08/21 08:59:26 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:26 INFO InternalParquetRecordReader: block read in memory in 69 ms. row count = 3500100
15/08/21 08:59:27 INFO InternalParquetRecordReader: Assembled and processed 3503274 records from 2 columns in 4585 ms: 764.0729 rec/ms, 1528.1458 cell/ms
15/08/21 08:59:27 INFO InternalParquetRecordReader: time spent so far 4% reading (220 ms) and 95% processing (4585 ms)
15/08/21 08:59:27 INFO InternalParquetRecordReader: at row 3503274. reading next block
15/08/21 08:59:27 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 71042
15/08/21 08:59:27 INFO InternalParquetRecordReader: Assembled and processed 3503231 records from 2 columns in 4710 ms: 743.78577 rec/ms, 1487.5715 cell/ms
15/08/21 08:59:27 INFO InternalParquetRecordReader: time spent so far 6% reading (310 ms) and 93% processing (4710 ms)
15/08/21 08:59:27 INFO InternalParquetRecordReader: at row 3503231. reading next block
15/08/21 08:59:27 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 71350
15/08/21 08:59:28 INFO Executor: Finished task 82.0 in stage 7.0 (TID 929). 2125 bytes result sent to driver
15/08/21 08:59:28 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 944, localhost, ANY, 1774 bytes)
15/08/21 08:59:28 INFO Executor: Running task 97.0 in stage 7.0 (TID 944)
15/08/21 08:59:28 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 929) in 5472 ms on localhost (82/170)
15/08/21 08:59:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000036_0 start: 134217728 end: 257832393 length: 123614665 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573690 records.
15/08/21 08:59:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:28 INFO InternalParquetRecordReader: block read in memory in 102 ms. row count = 3500612
15/08/21 08:59:28 INFO Executor: Finished task 83.0 in stage 7.0 (TID 930). 2125 bytes result sent to driver
15/08/21 08:59:28 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 945, localhost, ANY, 1762 bytes)
15/08/21 08:59:28 INFO Executor: Running task 98.0 in stage 7.0 (TID 945)
15/08/21 08:59:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:28 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 930) in 5815 ms on localhost (83/170)
15/08/21 08:59:28 INFO Executor: Finished task 84.0 in stage 7.0 (TID 931). 2125 bytes result sent to driver
15/08/21 08:59:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501124 records.
15/08/21 08:59:28 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 946, localhost, ANY, 1775 bytes)
15/08/21 08:59:28 INFO Executor: Running task 99.0 in stage 7.0 (TID 946)
15/08/21 08:59:28 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 931) in 5794 ms on localhost (84/170)
15/08/21 08:59:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000077_0 start: 134217728 end: 257486081 length: 123268353 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573157 records.
15/08/21 08:59:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:28 INFO Executor: Finished task 81.0 in stage 7.0 (TID 928). 2125 bytes result sent to driver
15/08/21 08:59:28 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 947, localhost, ANY, 1761 bytes)
15/08/21 08:59:28 INFO Executor: Running task 100.0 in stage 7.0 (TID 947)
15/08/21 08:59:28 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 928) in 6136 ms on localhost (85/170)
15/08/21 08:59:28 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:28 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4848 ms: 721.96783 rec/ms, 1443.9357 cell/ms
15/08/21 08:59:28 INFO InternalParquetRecordReader: time spent so far 1% reading (82 ms) and 98% processing (4848 ms)
15/08/21 08:59:28 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:28 INFO InternalParquetRecordReader: block read in memory in 71 ms. row count = 74156
15/08/21 08:59:28 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4656 ms: 751.7397 rec/ms, 1503.4794 cell/ms
15/08/21 08:59:28 INFO InternalParquetRecordReader: time spent so far 2% reading (119 ms) and 97% processing (4656 ms)
15/08/21 08:59:28 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:28 INFO InternalParquetRecordReader: block read in memory in 34 ms. row count = 74054
15/08/21 08:59:28 INFO InternalParquetRecordReader: block read in memory in 312 ms. row count = 3501124
15/08/21 08:59:28 INFO InternalParquetRecordReader: block read in memory in 340 ms. row count = 3500100
15/08/21 08:59:28 INFO InternalParquetRecordReader: block read in memory in 323 ms. row count = 3500100
15/08/21 08:59:29 INFO Executor: Finished task 86.0 in stage 7.0 (TID 933). 2125 bytes result sent to driver
15/08/21 08:59:29 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 948, localhost, ANY, 1773 bytes)
15/08/21 08:59:29 INFO Executor: Running task 101.0 in stage 7.0 (TID 948)
15/08/21 08:59:29 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 933) in 5763 ms on localhost (86/170)
15/08/21 08:59:29 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000083_0 start: 134217728 end: 257368738 length: 123151010 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
15/08/21 08:59:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:29 INFO InternalParquetRecordReader: block read in memory in 227 ms. row count = 3500100
15/08/21 08:59:29 INFO Executor: Finished task 87.0 in stage 7.0 (TID 934). 2125 bytes result sent to driver
15/08/21 08:59:29 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 949, localhost, ANY, 1762 bytes)
15/08/21 08:59:29 INFO Executor: Running task 102.0 in stage 7.0 (TID 949)
15/08/21 08:59:29 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 934) in 6116 ms on localhost (87/170)
15/08/21 08:59:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:30 INFO Executor: Finished task 88.0 in stage 7.0 (TID 935). 2125 bytes result sent to driver
15/08/21 08:59:30 INFO Executor: Finished task 85.0 in stage 7.0 (TID 932). 2125 bytes result sent to driver
15/08/21 08:59:30 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 950, localhost, ANY, 1774 bytes)
15/08/21 08:59:30 INFO Executor: Running task 103.0 in stage 7.0 (TID 950)
15/08/21 08:59:30 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 951, localhost, ANY, 1761 bytes)
15/08/21 08:59:30 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 932) in 6510 ms on localhost (88/170)
15/08/21 08:59:30 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 935) in 6236 ms on localhost (89/170)
15/08/21 08:59:30 INFO Executor: Running task 104.0 in stage 7.0 (TID 951)
15/08/21 08:59:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000059_0 start: 134217728 end: 257317174 length: 123099446 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500786 records.
15/08/21 08:59:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573826 records.
15/08/21 08:59:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:30 INFO InternalParquetRecordReader: block read in memory in 259 ms. row count = 3500100
15/08/21 08:59:30 INFO InternalParquetRecordReader: block read in memory in 195 ms. row count = 3500786
15/08/21 08:59:30 INFO InternalParquetRecordReader: block read in memory in 213 ms. row count = 3500100
15/08/21 08:59:30 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5161 ms: 678.1825 rec/ms, 1356.365 cell/ms
15/08/21 08:59:30 INFO InternalParquetRecordReader: time spent so far 3% reading (202 ms) and 96% processing (5161 ms)
15/08/21 08:59:30 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:30 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 73867
15/08/21 08:59:30 INFO Executor: Finished task 90.0 in stage 7.0 (TID 937). 2125 bytes result sent to driver
15/08/21 08:59:30 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 952, localhost, ANY, 1773 bytes)
15/08/21 08:59:30 INFO Executor: Running task 105.0 in stage 7.0 (TID 952)
15/08/21 08:59:30 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 937) in 5904 ms on localhost (90/170)
15/08/21 08:59:30 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000029_0 start: 134217728 end: 257566042 length: 123348314 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573221 records.
15/08/21 08:59:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:31 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5014 ms: 698.0654 rec/ms, 1396.1309 cell/ms
15/08/21 08:59:31 INFO InternalParquetRecordReader: time spent so far 2% reading (124 ms) and 97% processing (5014 ms)
15/08/21 08:59:31 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:31 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 73908
15/08/21 08:59:31 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5692 ms: 614.91565 rec/ms, 1229.8313 cell/ms
15/08/21 08:59:31 INFO InternalParquetRecordReader: time spent so far 6% reading (369 ms) and 93% processing (5692 ms)
15/08/21 08:59:31 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:31 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 127597
15/08/21 08:59:31 INFO InternalParquetRecordReader: block read in memory in 134 ms. row count = 3501171
15/08/21 08:59:31 INFO Executor: Finished task 91.0 in stage 7.0 (TID 938). 2125 bytes result sent to driver
15/08/21 08:59:31 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 953, localhost, ANY, 1762 bytes)
15/08/21 08:59:31 INFO Executor: Running task 106.0 in stage 7.0 (TID 953)
15/08/21 08:59:31 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 938) in 6439 ms on localhost (91/170)
15/08/21 08:59:31 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502806 records.
15/08/21 08:59:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:31 INFO InternalParquetRecordReader: block read in memory in 93 ms. row count = 3502806
15/08/21 08:59:32 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5463 ms: 640.69196 rec/ms, 1281.3839 cell/ms
15/08/21 08:59:32 INFO InternalParquetRecordReader: time spent so far 1% reading (85 ms) and 98% processing (5463 ms)
15/08/21 08:59:32 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:32 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 74208
15/08/21 08:59:32 INFO Executor: Finished task 89.0 in stage 7.0 (TID 936). 2125 bytes result sent to driver
15/08/21 08:59:32 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 954, localhost, ANY, 1775 bytes)
15/08/21 08:59:32 INFO Executor: Running task 107.0 in stage 7.0 (TID 954)
15/08/21 08:59:32 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 936) in 7650 ms on localhost (92/170)
15/08/21 08:59:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000057_0 start: 134217728 end: 257458240 length: 123240512 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:32 INFO Executor: Finished task 94.0 in stage 7.0 (TID 941). 2125 bytes result sent to driver
15/08/21 08:59:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:32 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 955, localhost, ANY, 1762 bytes)
15/08/21 08:59:32 INFO Executor: Running task 108.0 in stage 7.0 (TID 955)
15/08/21 08:59:32 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 941) in 6664 ms on localhost (93/170)
15/08/21 08:59:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571385 records.
15/08/21 08:59:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:32 INFO Executor: Finished task 93.0 in stage 7.0 (TID 940). 2125 bytes result sent to driver
15/08/21 08:59:32 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 956, localhost, ANY, 1773 bytes)
15/08/21 08:59:32 INFO Executor: Finished task 92.0 in stage 7.0 (TID 939). 2125 bytes result sent to driver
15/08/21 08:59:32 INFO Executor: Running task 109.0 in stage 7.0 (TID 956)
15/08/21 08:59:32 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 957, localhost, ANY, 1762 bytes)
15/08/21 08:59:32 INFO Executor: Running task 110.0 in stage 7.0 (TID 957)
15/08/21 08:59:32 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 940) in 6876 ms on localhost (94/170)
15/08/21 08:59:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000060_0 start: 134217728 end: 257458739 length: 123241011 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:32 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 939) in 7254 ms on localhost (95/170)
15/08/21 08:59:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501448 records.
15/08/21 08:59:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574219 records.
15/08/21 08:59:32 INFO InternalParquetRecordReader: block read in memory in 83 ms. row count = 3500100
15/08/21 08:59:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:32 INFO InternalParquetRecordReader: block read in memory in 145 ms. row count = 3500100
15/08/21 08:59:32 INFO Executor: Finished task 95.0 in stage 7.0 (TID 942). 2125 bytes result sent to driver
15/08/21 08:59:32 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 958, localhost, ANY, 1774 bytes)
15/08/21 08:59:32 INFO Executor: Running task 111.0 in stage 7.0 (TID 958)
15/08/21 08:59:32 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 942) in 6468 ms on localhost (96/170)
15/08/21 08:59:32 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000048_0 start: 134217728 end: 257439181 length: 123221453 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572926 records.
15/08/21 08:59:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:33 INFO InternalParquetRecordReader: block read in memory in 88 ms. row count = 3500100
15/08/21 08:59:33 INFO Executor: Finished task 96.0 in stage 7.0 (TID 943). 2125 bytes result sent to driver
15/08/21 08:59:33 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 959, localhost, ANY, 1762 bytes)
15/08/21 08:59:33 INFO Executor: Running task 112.0 in stage 7.0 (TID 959)
15/08/21 08:59:33 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 943) in 6529 ms on localhost (97/170)
15/08/21 08:59:33 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:33 INFO InternalParquetRecordReader: block read in memory in 296 ms. row count = 3501364
15/08/21 08:59:33 INFO InternalParquetRecordReader: block read in memory in 334 ms. row count = 3501448
15/08/21 08:59:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:33 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 3500100
15/08/21 08:59:33 INFO InternalParquetRecordReader: Assembled and processed 3500612 records from 2 columns in 5388 ms: 649.70526 rec/ms, 1299.4105 cell/ms
15/08/21 08:59:33 INFO InternalParquetRecordReader: time spent so far 1% reading (102 ms) and 98% processing (5388 ms)
15/08/21 08:59:33 INFO InternalParquetRecordReader: at row 3500612. reading next block
15/08/21 08:59:33 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 73078
15/08/21 08:59:34 INFO Executor: Finished task 97.0 in stage 7.0 (TID 944). 2125 bytes result sent to driver
15/08/21 08:59:34 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 960, localhost, ANY, 1773 bytes)
15/08/21 08:59:34 INFO Executor: Running task 113.0 in stage 7.0 (TID 960)
15/08/21 08:59:34 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 944) in 6350 ms on localhost (98/170)
15/08/21 08:59:34 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000084_0 start: 134217728 end: 181459518 length: 47241790 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1466882 records.
15/08/21 08:59:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:34 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5590 ms: 626.136 rec/ms, 1252.272 cell/ms
15/08/21 08:59:34 INFO InternalParquetRecordReader: time spent so far 5% reading (340 ms) and 94% processing (5590 ms)
15/08/21 08:59:34 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:34 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 73057
15/08/21 08:59:34 INFO InternalParquetRecordReader: block read in memory in 96 ms. row count = 1466882
15/08/21 08:59:35 INFO Executor: Finished task 99.0 in stage 7.0 (TID 946). 2125 bytes result sent to driver
15/08/21 08:59:35 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 961, localhost, ANY, 1761 bytes)
15/08/21 08:59:35 INFO Executor: Running task 114.0 in stage 7.0 (TID 961)
15/08/21 08:59:35 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 946) in 6633 ms on localhost (99/170)
15/08/21 08:59:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:35 INFO Executor: Finished task 98.0 in stage 7.0 (TID 945). 2125 bytes result sent to driver
15/08/21 08:59:35 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 962, localhost, ANY, 1772 bytes)
15/08/21 08:59:35 INFO Executor: Running task 115.0 in stage 7.0 (TID 962)
15/08/21 08:59:35 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 945) in 6829 ms on localhost (100/170)
15/08/21 08:59:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 134217728 end: 259458210 length: 125240482 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627682 records.
15/08/21 08:59:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:35 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 3500100
15/08/21 08:59:35 INFO InternalParquetRecordReader: block read in memory in 120 ms. row count = 3503132
15/08/21 08:59:35 INFO Executor: Finished task 100.0 in stage 7.0 (TID 947). 2125 bytes result sent to driver
15/08/21 08:59:35 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 963, localhost, ANY, 1761 bytes)
15/08/21 08:59:35 INFO Executor: Running task 116.0 in stage 7.0 (TID 963)
15/08/21 08:59:35 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 947) in 7074 ms on localhost (101/170)
15/08/21 08:59:35 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:35 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 3500100
15/08/21 08:59:35 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5999 ms: 583.44727 rec/ms, 1166.8945 cell/ms
15/08/21 08:59:35 INFO InternalParquetRecordReader: time spent so far 3% reading (227 ms) and 96% processing (5999 ms)
15/08/21 08:59:35 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:35 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 73871
15/08/21 08:59:36 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6025 ms: 580.92944 rec/ms, 1161.8589 cell/ms
15/08/21 08:59:36 INFO InternalParquetRecordReader: time spent so far 3% reading (213 ms) and 96% processing (6025 ms)
15/08/21 08:59:36 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:36 INFO InternalParquetRecordReader: block read in memory in 13 ms. row count = 73726
15/08/21 08:59:36 INFO Executor: Finished task 101.0 in stage 7.0 (TID 948). 2125 bytes result sent to driver
15/08/21 08:59:36 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 964, localhost, ANY, 1772 bytes)
15/08/21 08:59:36 INFO Executor: Running task 117.0 in stage 7.0 (TID 964)
15/08/21 08:59:36 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 948) in 7043 ms on localhost (102/170)
15/08/21 08:59:36 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000033_0 start: 134217728 end: 257467186 length: 123249458 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574192 records.
15/08/21 08:59:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:36 INFO InternalParquetRecordReader: block read in memory in 162 ms. row count = 3501130
15/08/21 08:59:36 INFO InternalParquetRecordReader: Assembled and processed 3501171 records from 2 columns in 5762 ms: 607.6312 rec/ms, 1215.2625 cell/ms
15/08/21 08:59:36 INFO InternalParquetRecordReader: time spent so far 2% reading (134 ms) and 97% processing (5762 ms)
15/08/21 08:59:36 INFO InternalParquetRecordReader: at row 3501171. reading next block
15/08/21 08:59:37 INFO InternalParquetRecordReader: block read in memory in 38 ms. row count = 72050
15/08/21 08:59:37 INFO Executor: Finished task 113.0 in stage 7.0 (TID 960). 2125 bytes result sent to driver
15/08/21 08:59:37 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 965, localhost, ANY, 1760 bytes)
15/08/21 08:59:37 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 960) in 3575 ms on localhost (103/170)
15/08/21 08:59:37 INFO Executor: Running task 118.0 in stage 7.0 (TID 965)
15/08/21 08:59:37 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:37 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500631 records.
15/08/21 08:59:38 INFO Executor: Finished task 102.0 in stage 7.0 (TID 949). 2125 bytes result sent to driver
15/08/21 08:59:38 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 966, localhost, ANY, 1772 bytes)
15/08/21 08:59:38 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 949) in 8045 ms on localhost (104/170)
15/08/21 08:59:38 INFO Executor: Running task 119.0 in stage 7.0 (TID 966)
15/08/21 08:59:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 134217728 end: 260141700 length: 125923972 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3648307 records.
15/08/21 08:59:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:38 INFO Executor: Finished task 104.0 in stage 7.0 (TID 951). 2125 bytes result sent to driver
15/08/21 08:59:38 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 967, localhost, ANY, 1760 bytes)
15/08/21 08:59:38 INFO Executor: Running task 120.0 in stage 7.0 (TID 967)
15/08/21 08:59:38 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 951) in 8003 ms on localhost (105/170)
15/08/21 08:59:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:38 INFO Executor: Finished task 105.0 in stage 7.0 (TID 952). 2125 bytes result sent to driver
15/08/21 08:59:38 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 968, localhost, ANY, 1772 bytes)
15/08/21 08:59:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501115 records.
15/08/21 08:59:38 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 952) in 7215 ms on localhost (106/170)
15/08/21 08:59:38 INFO Executor: Finished task 103.0 in stage 7.0 (TID 950). 2125 bytes result sent to driver
15/08/21 08:59:38 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 969, localhost, ANY, 1761 bytes)
15/08/21 08:59:38 INFO Executor: Running task 122.0 in stage 7.0 (TID 969)
15/08/21 08:59:38 INFO Executor: Running task 121.0 in stage 7.0 (TID 968)
15/08/21 08:59:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:38 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 950) in 8069 ms on localhost (107/170)
15/08/21 08:59:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000008_0 start: 134217728 end: 259582440 length: 125364712 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:38 INFO InternalParquetRecordReader: block read in memory in 146 ms. row count = 3500631
15/08/21 08:59:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626900 records.
15/08/21 08:59:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:38 INFO InternalParquetRecordReader: block read in memory in 167 ms. row count = 3501115
15/08/21 08:59:38 INFO InternalParquetRecordReader: block read in memory in 203 ms. row count = 3500100
15/08/21 08:59:38 INFO InternalParquetRecordReader: block read in memory in 402 ms. row count = 3500100
15/08/21 08:59:38 INFO InternalParquetRecordReader: block read in memory in 369 ms. row count = 3501235
15/08/21 08:59:38 INFO Executor: Finished task 106.0 in stage 7.0 (TID 953). 2125 bytes result sent to driver
15/08/21 08:59:38 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 970, localhost, ANY, 1774 bytes)
15/08/21 08:59:38 INFO Executor: Running task 123.0 in stage 7.0 (TID 970)
15/08/21 08:59:38 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 953) in 6952 ms on localhost (108/170)
15/08/21 08:59:38 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000025_0 start: 134217728 end: 257838232 length: 123620504 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:38 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:38 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574036 records.
15/08/21 08:59:38 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:39 INFO InternalParquetRecordReader: block read in memory in 128 ms. row count = 3501487
15/08/21 08:59:39 INFO InternalParquetRecordReader: Assembled and processed 3501364 records from 2 columns in 5959 ms: 587.57574 rec/ms, 1175.1515 cell/ms
15/08/21 08:59:39 INFO InternalParquetRecordReader: time spent so far 4% reading (296 ms) and 95% processing (5959 ms)
15/08/21 08:59:39 INFO InternalParquetRecordReader: at row 3501364. reading next block
15/08/21 08:59:39 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6328 ms: 553.11316 rec/ms, 1106.2263 cell/ms
15/08/21 08:59:39 INFO InternalParquetRecordReader: time spent so far 1% reading (83 ms) and 98% processing (6328 ms)
15/08/21 08:59:39 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:39 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 72855
15/08/21 08:59:39 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 71285
15/08/21 08:59:39 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6559 ms: 533.6332 rec/ms, 1067.2664 cell/ms
15/08/21 08:59:39 INFO InternalParquetRecordReader: time spent so far 1% reading (88 ms) and 98% processing (6559 ms)
15/08/21 08:59:39 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:39 INFO InternalParquetRecordReader: block read in memory in 31 ms. row count = 72826
15/08/21 08:59:39 INFO Executor: Finished task 107.0 in stage 7.0 (TID 954). 2125 bytes result sent to driver
15/08/21 08:59:39 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 971, localhost, ANY, 1761 bytes)
15/08/21 08:59:39 INFO Executor: Running task 124.0 in stage 7.0 (TID 971)
15/08/21 08:59:39 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 954) in 7130 ms on localhost (109/170)
15/08/21 08:59:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501235 records.
15/08/21 08:59:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:39 INFO Executor: Finished task 109.0 in stage 7.0 (TID 956). 2125 bytes result sent to driver
15/08/21 08:59:39 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 972, localhost, ANY, 1773 bytes)
15/08/21 08:59:39 INFO Executor: Running task 125.0 in stage 7.0 (TID 972)
15/08/21 08:59:39 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 956) in 7090 ms on localhost (110/170)
15/08/21 08:59:39 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000039_0 start: 134217728 end: 257849235 length: 123631507 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:39 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:39 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573259 records.
15/08/21 08:59:39 INFO InternalParquetRecordReader: block read in memory in 75 ms. row count = 3501235
15/08/21 08:59:39 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:40 INFO Executor: Finished task 111.0 in stage 7.0 (TID 958). 2125 bytes result sent to driver
15/08/21 08:59:40 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 973, localhost, ANY, 1761 bytes)
15/08/21 08:59:40 INFO Executor: Running task 126.0 in stage 7.0 (TID 973)
15/08/21 08:59:40 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 958) in 7155 ms on localhost (111/170)
15/08/21 08:59:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:40 INFO InternalParquetRecordReader: block read in memory in 171 ms. row count = 3500100
15/08/21 08:59:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502933 records.
15/08/21 08:59:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:40 INFO InternalParquetRecordReader: block read in memory in 97 ms. row count = 3502933
15/08/21 08:59:40 INFO Executor: Finished task 110.0 in stage 7.0 (TID 957). 2125 bytes result sent to driver
15/08/21 08:59:40 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 974, localhost, ANY, 1774 bytes)
15/08/21 08:59:40 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 957) in 7466 ms on localhost (112/170)
15/08/21 08:59:40 INFO Executor: Running task 127.0 in stage 7.0 (TID 974)
15/08/21 08:59:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000061_0 start: 134217728 end: 257181348 length: 122963620 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571471 records.
15/08/21 08:59:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:40 INFO InternalParquetRecordReader: block read in memory in 100 ms. row count = 3501332
15/08/21 08:59:40 INFO Executor: Finished task 112.0 in stage 7.0 (TID 959). 2125 bytes result sent to driver
15/08/21 08:59:40 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 975, localhost, ANY, 1760 bytes)
15/08/21 08:59:40 INFO Executor: Running task 128.0 in stage 7.0 (TID 975)
15/08/21 08:59:40 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 959) in 7470 ms on localhost (113/170)
15/08/21 08:59:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500833 records.
15/08/21 08:59:40 INFO Executor: Finished task 108.0 in stage 7.0 (TID 955). 2125 bytes result sent to driver
15/08/21 08:59:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:40 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 976, localhost, ANY, 1772 bytes)
15/08/21 08:59:40 INFO Executor: Running task 129.0 in stage 7.0 (TID 976)
15/08/21 08:59:40 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 955) in 7949 ms on localhost (114/170)
15/08/21 08:59:40 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000031_0 start: 134217728 end: 257473792 length: 123256064 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:40 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:40 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573329 records.
15/08/21 08:59:40 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:41 INFO InternalParquetRecordReader: block read in memory in 536 ms. row count = 3500833
15/08/21 08:59:41 INFO InternalParquetRecordReader: block read in memory in 511 ms. row count = 3500100
15/08/21 08:59:41 INFO InternalParquetRecordReader: Assembled and processed 3503132 records from 2 columns in 5904 ms: 593.34894 rec/ms, 1186.6979 cell/ms
15/08/21 08:59:41 INFO InternalParquetRecordReader: time spent so far 1% reading (120 ms) and 98% processing (5904 ms)
15/08/21 08:59:41 INFO InternalParquetRecordReader: at row 3503132. reading next block
15/08/21 08:59:41 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 124550
15/08/21 08:59:42 INFO Executor: Finished task 115.0 in stage 7.0 (TID 962). 2125 bytes result sent to driver
15/08/21 08:59:42 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 977, localhost, ANY, 1761 bytes)
15/08/21 08:59:42 INFO Executor: Running task 130.0 in stage 7.0 (TID 977)
15/08/21 08:59:42 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 962) in 6799 ms on localhost (115/170)
15/08/21 08:59:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3502559 records.
15/08/21 08:59:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:42 INFO InternalParquetRecordReader: block read in memory in 103 ms. row count = 3502559
15/08/21 08:59:42 INFO Executor: Finished task 116.0 in stage 7.0 (TID 963). 2125 bytes result sent to driver
15/08/21 08:59:42 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 978, localhost, ANY, 1774 bytes)
15/08/21 08:59:42 INFO Executor: Running task 131.0 in stage 7.0 (TID 978)
15/08/21 08:59:42 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 963) in 6627 ms on localhost (116/170)
15/08/21 08:59:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000038_0 start: 134217728 end: 257455806 length: 123238078 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571559 records.
15/08/21 08:59:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:42 INFO InternalParquetRecordReader: block read in memory in 116 ms. row count = 3500100
15/08/21 08:59:42 INFO Executor: Finished task 114.0 in stage 7.0 (TID 961). 2125 bytes result sent to driver
15/08/21 08:59:42 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 979, localhost, ANY, 1762 bytes)
15/08/21 08:59:42 INFO Executor: Running task 132.0 in stage 7.0 (TID 979)
15/08/21 08:59:42 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 961) in 7376 ms on localhost (117/170)
15/08/21 08:59:42 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:42 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:42 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501043 records.
15/08/21 08:59:42 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:42 INFO InternalParquetRecordReader: block read in memory in 131 ms. row count = 3501043
15/08/21 08:59:43 INFO InternalParquetRecordReader: Assembled and processed 3501130 records from 2 columns in 6905 ms: 507.04272 rec/ms, 1014.08545 cell/ms
15/08/21 08:59:43 INFO InternalParquetRecordReader: time spent so far 2% reading (162 ms) and 97% processing (6905 ms)
15/08/21 08:59:43 INFO InternalParquetRecordReader: at row 3501130. reading next block
15/08/21 08:59:43 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 73062
15/08/21 08:59:44 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5475 ms: 639.28766 rec/ms, 1278.5753 cell/ms
15/08/21 08:59:44 INFO InternalParquetRecordReader: time spent so far 6% reading (402 ms) and 93% processing (5475 ms)
15/08/21 08:59:44 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:44 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 148207
15/08/21 08:59:44 INFO Executor: Finished task 117.0 in stage 7.0 (TID 964). 2125 bytes result sent to driver
15/08/21 08:59:44 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 980, localhost, ANY, 1776 bytes)
15/08/21 08:59:44 INFO Executor: Running task 133.0 in stage 7.0 (TID 980)
15/08/21 08:59:44 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 964) in 7618 ms on localhost (118/170)
15/08/21 08:59:44 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000064_0 start: 134217728 end: 257547934 length: 123330206 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:44 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:44 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573366 records.
15/08/21 08:59:44 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:44 INFO InternalParquetRecordReader: block read in memory in 88 ms. row count = 3501786
15/08/21 08:59:44 INFO InternalParquetRecordReader: Assembled and processed 3501487 records from 2 columns in 5600 ms: 625.26556 rec/ms, 1250.5311 cell/ms
15/08/21 08:59:44 INFO InternalParquetRecordReader: time spent so far 2% reading (128 ms) and 97% processing (5600 ms)
15/08/21 08:59:44 INFO InternalParquetRecordReader: at row 3501487. reading next block
15/08/21 08:59:44 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 72549
15/08/21 08:59:44 INFO InternalParquetRecordReader: Assembled and processed 3501235 records from 2 columns in 6196 ms: 565.0799 rec/ms, 1130.1598 cell/ms
15/08/21 08:59:44 INFO InternalParquetRecordReader: time spent so far 5% reading (369 ms) and 94% processing (6196 ms)
15/08/21 08:59:44 INFO InternalParquetRecordReader: at row 3501235. reading next block
15/08/21 08:59:44 INFO InternalParquetRecordReader: block read in memory in 67 ms. row count = 125665
15/08/21 08:59:45 INFO Executor: Finished task 118.0 in stage 7.0 (TID 965). 2125 bytes result sent to driver
15/08/21 08:59:45 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 981, localhost, ANY, 1762 bytes)
15/08/21 08:59:45 INFO Executor: Running task 134.0 in stage 7.0 (TID 981)
15/08/21 08:59:45 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 965) in 7186 ms on localhost (119/170)
15/08/21 08:59:45 INFO Executor: Finished task 119.0 in stage 7.0 (TID 966). 2125 bytes result sent to driver
15/08/21 08:59:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:45 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 982, localhost, ANY, 1773 bytes)
15/08/21 08:59:45 INFO Executor: Running task 135.0 in stage 7.0 (TID 982)
15/08/21 08:59:45 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 966) in 7086 ms on localhost (120/170)
15/08/21 08:59:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000027_0 start: 134217728 end: 257790576 length: 123572848 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574145 records.
15/08/21 08:59:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:45 INFO InternalParquetRecordReader: block read in memory in 93 ms. row count = 3500100
15/08/21 08:59:45 INFO Executor: Finished task 122.0 in stage 7.0 (TID 969). 2125 bytes result sent to driver
15/08/21 08:59:45 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 983, localhost, ANY, 1761 bytes)
15/08/21 08:59:45 INFO Executor: Running task 136.0 in stage 7.0 (TID 983)
15/08/21 08:59:45 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 969) in 7106 ms on localhost (121/170)
15/08/21 08:59:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501169 records.
15/08/21 08:59:45 INFO Executor: Finished task 123.0 in stage 7.0 (TID 970). 2125 bytes result sent to driver
15/08/21 08:59:45 INFO InternalParquetRecordReader: block read in memory in 165 ms. row count = 3500100
15/08/21 08:59:45 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 984, localhost, ANY, 1773 bytes)
15/08/21 08:59:45 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 970) in 6537 ms on localhost (122/170)
15/08/21 08:59:45 INFO Executor: Running task 137.0 in stage 7.0 (TID 984)
15/08/21 08:59:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000043_0 start: 134217728 end: 257571649 length: 123353921 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573313 records.
15/08/21 08:59:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:45 INFO InternalParquetRecordReader: block read in memory in 365 ms. row count = 3501169
15/08/21 08:59:45 INFO InternalParquetRecordReader: block read in memory in 355 ms. row count = 3501584
15/08/21 08:59:45 INFO Executor: Finished task 121.0 in stage 7.0 (TID 968). 2125 bytes result sent to driver
15/08/21 08:59:45 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 985, localhost, ANY, 1761 bytes)
15/08/21 08:59:45 INFO Executor: Running task 138.0 in stage 7.0 (TID 985)
15/08/21 08:59:45 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 968) in 7633 ms on localhost (123/170)
15/08/21 08:59:45 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:45 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:45 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501195 records.
15/08/21 08:59:45 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:46 INFO Executor: Finished task 126.0 in stage 7.0 (TID 973). 2125 bytes result sent to driver
15/08/21 08:59:46 INFO InternalParquetRecordReader: block read in memory in 185 ms. row count = 3501195
15/08/21 08:59:46 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 986, localhost, ANY, 1774 bytes)
15/08/21 08:59:46 INFO Executor: Running task 139.0 in stage 7.0 (TID 986)
15/08/21 08:59:46 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 973) in 6045 ms on localhost (124/170)
15/08/21 08:59:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000015_0 start: 134217728 end: 257573201 length: 123355473 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572913 records.
15/08/21 08:59:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:46 INFO Executor: Finished task 120.0 in stage 7.0 (TID 967). 2125 bytes result sent to driver
15/08/21 08:59:46 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 987, localhost, ANY, 1761 bytes)
15/08/21 08:59:46 INFO Executor: Running task 140.0 in stage 7.0 (TID 987)
15/08/21 08:59:46 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 967) in 8041 ms on localhost (125/170)
15/08/21 08:59:46 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:46 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:46 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501339 records.
15/08/21 08:59:46 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:46 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6248 ms: 560.19525 rec/ms, 1120.3905 cell/ms
15/08/21 08:59:46 INFO InternalParquetRecordReader: Assembled and processed 3501332 records from 2 columns in 5897 ms: 593.748 rec/ms, 1187.496 cell/ms
15/08/21 08:59:46 INFO InternalParquetRecordReader: time spent so far 2% reading (171 ms) and 97% processing (6248 ms)
15/08/21 08:59:46 INFO InternalParquetRecordReader: time spent so far 1% reading (100 ms) and 98% processing (5897 ms)
15/08/21 08:59:46 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:46 INFO InternalParquetRecordReader: at row 3501332. reading next block
15/08/21 08:59:46 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 73159
15/08/21 08:59:46 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 70139
15/08/21 08:59:46 INFO InternalParquetRecordReader: block read in memory in 149 ms. row count = 3501339
15/08/21 08:59:46 INFO InternalParquetRecordReader: block read in memory in 288 ms. row count = 3500728
15/08/21 08:59:47 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6265 ms: 558.6752 rec/ms, 1117.3503 cell/ms
15/08/21 08:59:47 INFO InternalParquetRecordReader: time spent so far 7% reading (511 ms) and 92% processing (6265 ms)
15/08/21 08:59:47 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:47 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 73229
15/08/21 08:59:48 INFO Executor: Finished task 124.0 in stage 7.0 (TID 971). 2125 bytes result sent to driver
15/08/21 08:59:48 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 988, localhost, ANY, 1773 bytes)
15/08/21 08:59:48 INFO Executor: Running task 141.0 in stage 7.0 (TID 988)
15/08/21 08:59:48 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 971) in 8536 ms on localhost (126/170)
15/08/21 08:59:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000020_0 start: 134217728 end: 257466118 length: 123248390 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572985 records.
15/08/21 08:59:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:48 INFO Executor: Finished task 127.0 in stage 7.0 (TID 974). 2125 bytes result sent to driver
15/08/21 08:59:48 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 989, localhost, ANY, 1761 bytes)
15/08/21 08:59:48 INFO Executor: Running task 142.0 in stage 7.0 (TID 989)
15/08/21 08:59:48 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 974) in 8202 ms on localhost (127/170)
15/08/21 08:59:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:48 INFO InternalParquetRecordReader: block read in memory in 61 ms. row count = 3500100
15/08/21 08:59:48 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5936 ms: 589.63947 rec/ms, 1179.2789 cell/ms
15/08/21 08:59:48 INFO InternalParquetRecordReader: time spent so far 1% reading (116 ms) and 98% processing (5936 ms)
15/08/21 08:59:48 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:48 INFO Executor: Finished task 125.0 in stage 7.0 (TID 972). 2125 bytes result sent to driver
15/08/21 08:59:48 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 990, localhost, ANY, 1773 bytes)
15/08/21 08:59:48 INFO Executor: Running task 143.0 in stage 7.0 (TID 990)
15/08/21 08:59:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:48 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 972) in 8630 ms on localhost (128/170)
15/08/21 08:59:48 INFO InternalParquetRecordReader: block read in memory in 33 ms. row count = 71459
15/08/21 08:59:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000026_0 start: 134217728 end: 257888240 length: 123670512 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574338 records.
15/08/21 08:59:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:48 INFO InternalParquetRecordReader: block read in memory in 151 ms. row count = 3500100
15/08/21 08:59:48 INFO Executor: Finished task 128.0 in stage 7.0 (TID 975). 2125 bytes result sent to driver
15/08/21 08:59:48 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 991, localhost, ANY, 1761 bytes)
15/08/21 08:59:48 INFO Executor: Running task 144.0 in stage 7.0 (TID 991)
15/08/21 08:59:48 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 975) in 8111 ms on localhost (129/170)
15/08/21 08:59:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:48 INFO InternalParquetRecordReader: block read in memory in 159 ms. row count = 3501076
15/08/21 08:59:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:48 INFO Executor: Finished task 130.0 in stage 7.0 (TID 977). 2125 bytes result sent to driver
15/08/21 08:59:48 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 992, localhost, ANY, 1772 bytes)
15/08/21 08:59:48 INFO Executor: Finished task 129.0 in stage 7.0 (TID 976). 2125 bytes result sent to driver
15/08/21 08:59:48 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 977) in 6828 ms on localhost (130/170)
15/08/21 08:59:48 INFO Executor: Running task 145.0 in stage 7.0 (TID 992)
15/08/21 08:59:48 INFO InternalParquetRecordReader: block read in memory in 136 ms. row count = 3500100
15/08/21 08:59:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000053_0 start: 134217728 end: 258178393 length: 123960665 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:48 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 993, localhost, ANY, 1762 bytes)
15/08/21 08:59:48 INFO Executor: Running task 146.0 in stage 7.0 (TID 993)
15/08/21 08:59:48 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 976) in 8238 ms on localhost (131/170)
15/08/21 08:59:48 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:48 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573846 records.
15/08/21 08:59:48 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501395 records.
15/08/21 08:59:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:48 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:49 INFO InternalParquetRecordReader: block read in memory in 106 ms. row count = 3501395
15/08/21 08:59:49 INFO Executor: Finished task 131.0 in stage 7.0 (TID 978). 2125 bytes result sent to driver
15/08/21 08:59:49 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 994, localhost, ANY, 1775 bytes)
15/08/21 08:59:49 INFO Executor: Running task 147.0 in stage 7.0 (TID 994)
15/08/21 08:59:49 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 978) in 6758 ms on localhost (132/170)
15/08/21 08:59:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000054_0 start: 134217728 end: 257798680 length: 123580952 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572381 records.
15/08/21 08:59:49 INFO InternalParquetRecordReader: block read in memory in 226 ms. row count = 3500100
15/08/21 08:59:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:49 INFO InternalParquetRecordReader: block read in memory in 507 ms. row count = 3500100
15/08/21 08:59:49 INFO Executor: Finished task 132.0 in stage 7.0 (TID 979). 2125 bytes result sent to driver
15/08/21 08:59:49 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 995, localhost, ANY, 1761 bytes)
15/08/21 08:59:49 INFO Executor: Running task 148.0 in stage 7.0 (TID 995)
15/08/21 08:59:49 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 979) in 7177 ms on localhost (133/170)
15/08/21 08:59:49 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:49 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:49 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501121 records.
15/08/21 08:59:49 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:49 INFO InternalParquetRecordReader: block read in memory in 113 ms. row count = 3501121
15/08/21 08:59:51 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5796 ms: 603.88196 rec/ms, 1207.7639 cell/ms
15/08/21 08:59:51 INFO InternalParquetRecordReader: time spent so far 2% reading (165 ms) and 97% processing (5796 ms)
15/08/21 08:59:51 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:51 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 74045
15/08/21 08:59:51 INFO Executor: Finished task 134.0 in stage 7.0 (TID 981). 2125 bytes result sent to driver
15/08/21 08:59:51 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 996, localhost, ANY, 1774 bytes)
15/08/21 08:59:51 INFO Executor: Running task 149.0 in stage 7.0 (TID 996)
15/08/21 08:59:51 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 981) in 6313 ms on localhost (134/170)
15/08/21 08:59:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000080_0 start: 134217728 end: 257837778 length: 123620050 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573120 records.
15/08/21 08:59:51 INFO InternalParquetRecordReader: Assembled and processed 3501786 records from 2 columns in 7028 ms: 498.2621 rec/ms, 996.5242 cell/ms
15/08/21 08:59:51 INFO InternalParquetRecordReader: time spent so far 1% reading (88 ms) and 98% processing (7028 ms)
15/08/21 08:59:51 INFO InternalParquetRecordReader: at row 3501786. reading next block
15/08/21 08:59:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:51 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 71580
15/08/21 08:59:51 INFO InternalParquetRecordReader: block read in memory in 133 ms. row count = 3500100
15/08/21 08:59:51 INFO Executor: Finished task 135.0 in stage 7.0 (TID 982). 2125 bytes result sent to driver
15/08/21 08:59:51 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 997, localhost, ANY, 1762 bytes)
15/08/21 08:59:51 INFO Executor: Running task 150.0 in stage 7.0 (TID 997)
15/08/21 08:59:51 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 982) in 6688 ms on localhost (135/170)
15/08/21 08:59:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:51 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:51 INFO InternalParquetRecordReader: Assembled and processed 3501584 records from 2 columns in 6143 ms: 570.012 rec/ms, 1140.024 cell/ms
15/08/21 08:59:51 INFO InternalParquetRecordReader: time spent so far 5% reading (355 ms) and 94% processing (6143 ms)
15/08/21 08:59:51 INFO InternalParquetRecordReader: at row 3501584. reading next block
15/08/21 08:59:51 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:51 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 71729
15/08/21 08:59:51 INFO Executor: Finished task 133.0 in stage 7.0 (TID 980). 2125 bytes result sent to driver
15/08/21 08:59:51 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 998, localhost, ANY, 1775 bytes)
15/08/21 08:59:51 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 980) in 7785 ms on localhost (136/170)
15/08/21 08:59:51 INFO Executor: Running task 151.0 in stage 7.0 (TID 998)
15/08/21 08:59:51 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000068_0 start: 134217728 end: 257748250 length: 123530522 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:51 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573993 records.
15/08/21 08:59:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:52 INFO InternalParquetRecordReader: block read in memory in 150 ms. row count = 3500100
15/08/21 08:59:52 INFO InternalParquetRecordReader: block read in memory in 118 ms. row count = 3500100
15/08/21 08:59:52 INFO Executor: Finished task 138.0 in stage 7.0 (TID 985). 2125 bytes result sent to driver
15/08/21 08:59:52 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 999, localhost, ANY, 1761 bytes)
15/08/21 08:59:52 INFO Executor: Running task 152.0 in stage 7.0 (TID 999)
15/08/21 08:59:52 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 985) in 6714 ms on localhost (137/170)
15/08/21 08:59:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:52 INFO InternalParquetRecordReader: block read in memory in 116 ms. row count = 3500100
15/08/21 08:59:52 INFO Executor: Finished task 137.0 in stage 7.0 (TID 984). 2125 bytes result sent to driver
15/08/21 08:59:52 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 1000, localhost, ANY, 1776 bytes)
15/08/21 08:59:52 INFO Executor: Running task 153.0 in stage 7.0 (TID 1000)
15/08/21 08:59:52 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 984) in 7493 ms on localhost (138/170)
15/08/21 08:59:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000037_0 start: 134217728 end: 257331238 length: 123113510 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573971 records.
15/08/21 08:59:52 INFO Executor: Finished task 136.0 in stage 7.0 (TID 983). 2125 bytes result sent to driver
15/08/21 08:59:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:52 INFO InternalParquetRecordReader: Assembled and processed 3500728 records from 2 columns in 6393 ms: 547.58765 rec/ms, 1095.1753 cell/ms
15/08/21 08:59:52 INFO InternalParquetRecordReader: time spent so far 4% reading (288 ms) and 95% processing (6393 ms)
15/08/21 08:59:52 INFO InternalParquetRecordReader: at row 3500728. reading next block
15/08/21 08:59:52 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 72185
15/08/21 08:59:52 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 1001, localhost, ANY, 1762 bytes)
15/08/21 08:59:52 INFO Executor: Running task 154.0 in stage 7.0 (TID 1001)
15/08/21 08:59:52 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 983) in 7606 ms on localhost (139/170)
15/08/21 08:59:52 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:52 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:52 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3503161 records.
15/08/21 08:59:52 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:52 INFO InternalParquetRecordReader: block read in memory in 97 ms. row count = 3500100
15/08/21 08:59:53 INFO InternalParquetRecordReader: block read in memory in 141 ms. row count = 3503161
15/08/21 08:59:53 INFO Executor: Finished task 139.0 in stage 7.0 (TID 986). 2125 bytes result sent to driver
15/08/21 08:59:53 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 1002, localhost, ANY, 1775 bytes)
15/08/21 08:59:53 INFO Executor: Running task 155.0 in stage 7.0 (TID 1002)
15/08/21 08:59:53 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 986) in 7387 ms on localhost (140/170)
15/08/21 08:59:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000056_0 start: 134217728 end: 257176539 length: 122958811 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3571357 records.
15/08/21 08:59:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:53 INFO InternalParquetRecordReader: block read in memory in 131 ms. row count = 3501317
15/08/21 08:59:53 INFO Executor: Finished task 140.0 in stage 7.0 (TID 987). 2125 bytes result sent to driver
15/08/21 08:59:53 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 1003, localhost, ANY, 1761 bytes)
15/08/21 08:59:53 INFO Executor: Running task 156.0 in stage 7.0 (TID 1003)
15/08/21 08:59:53 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 987) in 7662 ms on localhost (141/170)
15/08/21 08:59:53 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:53 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:53 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501689 records.
15/08/21 08:59:53 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:54 INFO InternalParquetRecordReader: block read in memory in 149 ms. row count = 3501689
15/08/21 08:59:54 INFO InternalParquetRecordReader: Assembled and processed 3501076 records from 2 columns in 5631 ms: 621.7503 rec/ms, 1243.5006 cell/ms
15/08/21 08:59:54 INFO InternalParquetRecordReader: time spent so far 2% reading (159 ms) and 97% processing (5631 ms)
15/08/21 08:59:54 INFO InternalParquetRecordReader: at row 3501076. reading next block
15/08/21 08:59:54 INFO InternalParquetRecordReader: block read in memory in 16 ms. row count = 73262
15/08/21 08:59:54 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5971 ms: 586.1832 rec/ms, 1172.3665 cell/ms
15/08/21 08:59:54 INFO InternalParquetRecordReader: time spent so far 1% reading (61 ms) and 98% processing (5971 ms)
15/08/21 08:59:54 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:54 INFO InternalParquetRecordReader: block read in memory in 84 ms. row count = 72885
15/08/21 08:59:54 INFO Executor: Finished task 143.0 in stage 7.0 (TID 990). 2125 bytes result sent to driver
15/08/21 08:59:54 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 1004, localhost, ANY, 1774 bytes)
15/08/21 08:59:54 INFO Executor: Running task 157.0 in stage 7.0 (TID 1004)
15/08/21 08:59:54 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 990) in 6483 ms on localhost (142/170)
15/08/21 08:59:54 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000052_0 start: 134217728 end: 257446174 length: 123228446 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:54 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3572541 records.
15/08/21 08:59:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:55 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5741 ms: 609.6673 rec/ms, 1219.3346 cell/ms
15/08/21 08:59:55 INFO InternalParquetRecordReader: time spent so far 3% reading (226 ms) and 96% processing (5741 ms)
15/08/21 08:59:55 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:55 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 73746
15/08/21 08:59:55 INFO InternalParquetRecordReader: block read in memory in 306 ms. row count = 3500100
15/08/21 08:59:55 INFO Executor: Finished task 141.0 in stage 7.0 (TID 988). 2125 bytes result sent to driver
15/08/21 08:59:55 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 1005, localhost, ANY, 1760 bytes)
15/08/21 08:59:55 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 988) in 7138 ms on localhost (143/170)
15/08/21 08:59:55 INFO Executor: Running task 158.0 in stage 7.0 (TID 1005)
15/08/21 08:59:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:55 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:55 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501046 records.
15/08/21 08:59:55 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:55 INFO InternalParquetRecordReader: block read in memory in 391 ms. row count = 3501046
15/08/21 08:59:55 INFO Executor: Finished task 142.0 in stage 7.0 (TID 989). 2125 bytes result sent to driver
15/08/21 08:59:55 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 1006, localhost, ANY, 1772 bytes)
15/08/21 08:59:55 INFO Executor: Running task 159.0 in stage 7.0 (TID 1006)
15/08/21 08:59:55 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 989) in 7564 ms on localhost (144/170)
15/08/21 08:59:55 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000011_0 start: 134217728 end: 259884384 length: 125666656 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3626733 records.
15/08/21 08:59:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:56 INFO Executor: Finished task 146.0 in stage 7.0 (TID 993). 2125 bytes result sent to driver
15/08/21 08:59:56 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 1007, localhost, ANY, 1760 bytes)
15/08/21 08:59:56 INFO Executor: Running task 160.0 in stage 7.0 (TID 1007)
15/08/21 08:59:56 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 993) in 7265 ms on localhost (145/170)
15/08/21 08:59:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501221 records.
15/08/21 08:59:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:56 INFO Executor: Finished task 145.0 in stage 7.0 (TID 992). 2125 bytes result sent to driver
15/08/21 08:59:56 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 1008, localhost, ANY, 1774 bytes)
15/08/21 08:59:56 INFO Executor: Running task 161.0 in stage 7.0 (TID 1008)
15/08/21 08:59:56 INFO InternalParquetRecordReader: block read in memory in 113 ms. row count = 3501221
15/08/21 08:59:56 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 992) in 7471 ms on localhost (146/170)
15/08/21 08:59:56 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000013_0 start: 134217728 end: 259183553 length: 124965825 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:56 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:56 INFO InternalParquetRecordReader: block read in memory in 256 ms. row count = 3500100
15/08/21 08:59:56 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3627071 records.
15/08/21 08:59:56 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:56 INFO InternalParquetRecordReader: block read in memory in 125 ms. row count = 3503161
15/08/21 08:59:57 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 7538 ms: 464.3274 rec/ms, 928.6548 cell/ms
15/08/21 08:59:57 INFO InternalParquetRecordReader: time spent so far 6% reading (507 ms) and 93% processing (7538 ms)
15/08/21 08:59:57 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:57 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 72281
15/08/21 08:59:57 INFO Executor: Finished task 144.0 in stage 7.0 (TID 991). 2125 bytes result sent to driver
15/08/21 08:59:57 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 1009, localhost, ANY, 1761 bytes)
15/08/21 08:59:57 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 991) in 8847 ms on localhost (147/170)
15/08/21 08:59:57 INFO Executor: Running task 162.0 in stage 7.0 (TID 1009)
15/08/21 08:59:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:57 INFO Executor: Finished task 148.0 in stage 7.0 (TID 995). 2125 bytes result sent to driver
15/08/21 08:59:57 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 1010, localhost, ANY, 1774 bytes)
15/08/21 08:59:57 INFO Executor: Running task 163.0 in stage 7.0 (TID 1010)
15/08/21 08:59:57 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 995) in 7978 ms on localhost (148/170)
15/08/21 08:59:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000022_0 start: 134217728 end: 257504450 length: 123286722 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573920 records.
15/08/21 08:59:57 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 3500100
15/08/21 08:59:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:57 INFO Executor: Finished task 147.0 in stage 7.0 (TID 994). 2125 bytes result sent to driver
15/08/21 08:59:57 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 1011, localhost, ANY, 1761 bytes)
15/08/21 08:59:57 INFO Executor: Running task 164.0 in stage 7.0 (TID 1011)
15/08/21 08:59:57 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 994) in 8670 ms on localhost (149/170)
15/08/21 08:59:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:57 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:57 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3501110 records.
15/08/21 08:59:57 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500779
15/08/21 08:59:57 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:57 INFO InternalParquetRecordReader: block read in memory in 95 ms. row count = 3501110
15/08/21 08:59:58 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6545 ms: 534.77466 rec/ms, 1069.5493 cell/ms
15/08/21 08:59:58 INFO InternalParquetRecordReader: time spent so far 1% reading (133 ms) and 98% processing (6545 ms)
15/08/21 08:59:58 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:58 INFO InternalParquetRecordReader: block read in memory in 21 ms. row count = 73020
15/08/21 08:59:58 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 6614 ms: 529.1956 rec/ms, 1058.3912 cell/ms
15/08/21 08:59:58 INFO InternalParquetRecordReader: time spent so far 1% reading (118 ms) and 98% processing (6614 ms)
15/08/21 08:59:58 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 08:59:58 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 73893
15/08/21 08:59:58 INFO Executor: Finished task 149.0 in stage 7.0 (TID 996). 2125 bytes result sent to driver
15/08/21 08:59:58 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 1012, localhost, ANY, 1773 bytes)
15/08/21 08:59:58 INFO Executor: Running task 165.0 in stage 7.0 (TID 1012)
15/08/21 08:59:58 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 996) in 7531 ms on localhost (150/170)
15/08/21 08:59:58 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000082_0 start: 134217728 end: 257173847 length: 122956119 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:58 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:58 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573142 records.
15/08/21 08:59:58 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:59 INFO InternalParquetRecordReader: block read in memory in 76 ms. row count = 3502917
15/08/21 08:59:59 INFO Executor: Finished task 151.0 in stage 7.0 (TID 998). 2125 bytes result sent to driver
15/08/21 08:59:59 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 1013, localhost, ANY, 1761 bytes)
15/08/21 08:59:59 INFO Executor: Running task 166.0 in stage 7.0 (TID 1013)
15/08/21 08:59:59 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 998) in 7236 ms on localhost (151/170)
15/08/21 08:59:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500519 records.
15/08/21 08:59:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:59 INFO InternalParquetRecordReader: block read in memory in 91 ms. row count = 3500519
15/08/21 08:59:59 INFO Executor: Finished task 150.0 in stage 7.0 (TID 997). 2125 bytes result sent to driver
15/08/21 08:59:59 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 1014, localhost, ANY, 1774 bytes)
15/08/21 08:59:59 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 997) in 7592 ms on localhost (152/170)
15/08/21 08:59:59 INFO Executor: Running task 167.0 in stage 7.0 (TID 1014)
15/08/21 08:59:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000016_0 start: 134217728 end: 257494956 length: 123277228 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3573577 records.
15/08/21 08:59:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:59 INFO InternalParquetRecordReader: block read in memory in 79 ms. row count = 3500100
15/08/21 08:59:59 INFO Executor: Finished task 152.0 in stage 7.0 (TID 999). 2125 bytes result sent to driver
15/08/21 08:59:59 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 1015, localhost, ANY, 1762 bytes)
15/08/21 08:59:59 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 999) in 7186 ms on localhost (153/170)
15/08/21 08:59:59 INFO Executor: Running task 168.0 in stage 7.0 (TID 1015)
15/08/21 08:59:59 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 0 end: 134217728 length: 134217728 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 08:59:59 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 08:59:59 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3500100 records.
15/08/21 08:59:59 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 08:59:59 INFO InternalParquetRecordReader: block read in memory in 81 ms. row count = 3500100
15/08/21 09:00:00 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 7058 ms: 495.90536 rec/ms, 991.8107 cell/ms
15/08/21 09:00:00 INFO InternalParquetRecordReader: time spent so far 1% reading (97 ms) and 98% processing (7058 ms)
15/08/21 09:00:00 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 09:00:00 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 73871
15/08/21 09:00:00 INFO Executor: Finished task 154.0 in stage 7.0 (TID 1001). 2125 bytes result sent to driver
15/08/21 09:00:00 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 1016, localhost, ANY, 1775 bytes)
15/08/21 09:00:00 INFO Executor: Running task 169.0 in stage 7.0 (TID 1016)
15/08/21 09:00:00 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 1001) in 7271 ms on localhost (154/170)
15/08/21 09:00:00 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000058_0 start: 134217728 end: 257035718 length: 122817990 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_orderkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/21 09:00:00 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/21 09:00:00 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 3574089 records.
15/08/21 09:00:00 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/21 09:00:00 INFO InternalParquetRecordReader: block read in memory in 139 ms. row count = 3503378
15/08/21 09:00:00 INFO Executor: Finished task 153.0 in stage 7.0 (TID 1000). 2125 bytes result sent to driver
15/08/21 09:00:00 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 1000) in 7826 ms on localhost (155/170)
15/08/21 09:00:00 INFO Executor: Finished task 156.0 in stage 7.0 (TID 1003). 2125 bytes result sent to driver
15/08/21 09:00:00 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 1003) in 6992 ms on localhost (156/170)
15/08/21 09:00:00 INFO InternalParquetRecordReader: Assembled and processed 3501317 records from 2 columns in 7131 ms: 490.99945 rec/ms, 981.9989 cell/ms
15/08/21 09:00:00 INFO InternalParquetRecordReader: time spent so far 1% reading (131 ms) and 98% processing (7131 ms)
15/08/21 09:00:00 INFO InternalParquetRecordReader: at row 3501317. reading next block
15/08/21 09:00:00 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 70040
15/08/21 09:00:01 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5973 ms: 585.98694 rec/ms, 1171.9739 cell/ms
15/08/21 09:00:01 INFO InternalParquetRecordReader: time spent so far 4% reading (306 ms) and 95% processing (5973 ms)
15/08/21 09:00:01 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 09:00:01 INFO Executor: Finished task 155.0 in stage 7.0 (TID 1002). 2125 bytes result sent to driver
15/08/21 09:00:01 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 72441
15/08/21 09:00:01 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 1002) in 7923 ms on localhost (157/170)
15/08/21 09:00:01 INFO Executor: Finished task 157.0 in stage 7.0 (TID 1004). 2125 bytes result sent to driver
15/08/21 09:00:01 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 1004) in 6861 ms on localhost (158/170)
15/08/21 09:00:01 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 5444 ms: 642.928 rec/ms, 1285.856 cell/ms
15/08/21 09:00:01 INFO InternalParquetRecordReader: time spent so far 4% reading (256 ms) and 95% processing (5444 ms)
15/08/21 09:00:01 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 09:00:01 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 126633
15/08/21 09:00:01 INFO Executor: Finished task 158.0 in stage 7.0 (TID 1005). 2125 bytes result sent to driver
15/08/21 09:00:01 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 1005) in 6403 ms on localhost (159/170)
15/08/21 09:00:01 INFO InternalParquetRecordReader: Assembled and processed 3503161 records from 2 columns in 5389 ms: 650.05774 rec/ms, 1300.1155 cell/ms
15/08/21 09:00:01 INFO InternalParquetRecordReader: time spent so far 2% reading (125 ms) and 97% processing (5389 ms)
15/08/21 09:00:01 INFO InternalParquetRecordReader: at row 3503161. reading next block
15/08/21 09:00:01 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 123910
15/08/21 09:00:02 INFO Executor: Finished task 160.0 in stage 7.0 (TID 1007). 2125 bytes result sent to driver
15/08/21 09:00:02 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 1007) in 6195 ms on localhost (160/170)
15/08/21 09:00:02 INFO Executor: Finished task 159.0 in stage 7.0 (TID 1006). 2125 bytes result sent to driver
15/08/21 09:00:02 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 1006) in 6423 ms on localhost (161/170)
15/08/21 09:00:02 INFO Executor: Finished task 161.0 in stage 7.0 (TID 1008). 2125 bytes result sent to driver
15/08/21 09:00:02 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 1008) in 6155 ms on localhost (162/170)
15/08/21 09:00:02 INFO InternalParquetRecordReader: Assembled and processed 3500779 records from 2 columns in 5069 ms: 690.6252 rec/ms, 1381.2504 cell/ms
15/08/21 09:00:02 INFO InternalParquetRecordReader: time spent so far 1% reading (91 ms) and 98% processing (5069 ms)
15/08/21 09:00:02 INFO InternalParquetRecordReader: at row 3500779. reading next block
15/08/21 09:00:02 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 73141
15/08/21 09:00:02 INFO Executor: Finished task 162.0 in stage 7.0 (TID 1009). 2125 bytes result sent to driver
15/08/21 09:00:02 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 1009) in 5458 ms on localhost (163/170)
15/08/21 09:00:03 INFO Executor: Finished task 164.0 in stage 7.0 (TID 1011). 2125 bytes result sent to driver
15/08/21 09:00:03 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 1011) in 5496 ms on localhost (164/170)
15/08/21 09:00:03 INFO Executor: Finished task 163.0 in stage 7.0 (TID 1010). 2125 bytes result sent to driver
15/08/21 09:00:03 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 1010) in 5787 ms on localhost (165/170)
15/08/21 09:00:03 INFO InternalParquetRecordReader: Assembled and processed 3502917 records from 2 columns in 4391 ms: 797.74927 rec/ms, 1595.4985 cell/ms
15/08/21 09:00:03 INFO InternalParquetRecordReader: time spent so far 1% reading (76 ms) and 98% processing (4391 ms)
15/08/21 09:00:03 INFO InternalParquetRecordReader: at row 3502917. reading next block
15/08/21 09:00:03 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 70225
15/08/21 09:00:03 INFO Executor: Finished task 165.0 in stage 7.0 (TID 1012). 2125 bytes result sent to driver
15/08/21 09:00:03 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 1012) in 4846 ms on localhost (166/170)
15/08/21 09:00:03 INFO InternalParquetRecordReader: Assembled and processed 3500100 records from 2 columns in 4332 ms: 807.964 rec/ms, 1615.928 cell/ms
15/08/21 09:00:03 INFO InternalParquetRecordReader: time spent so far 1% reading (79 ms) and 98% processing (4332 ms)
15/08/21 09:00:03 INFO InternalParquetRecordReader: at row 3500100. reading next block
15/08/21 09:00:03 INFO InternalParquetRecordReader: block read in memory in 42 ms. row count = 73477
15/08/21 09:00:04 INFO Executor: Finished task 166.0 in stage 7.0 (TID 1013). 2125 bytes result sent to driver
15/08/21 09:00:04 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 1013) in 4887 ms on localhost (167/170)
15/08/21 09:00:04 INFO Executor: Finished task 167.0 in stage 7.0 (TID 1014). 2125 bytes result sent to driver
15/08/21 09:00:04 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 1014) in 4821 ms on localhost (168/170)
15/08/21 09:00:04 INFO InternalParquetRecordReader: Assembled and processed 3503378 records from 2 columns in 4152 ms: 843.7808 rec/ms, 1687.5616 cell/ms
15/08/21 09:00:04 INFO InternalParquetRecordReader: time spent so far 3% reading (139 ms) and 96% processing (4152 ms)
15/08/21 09:00:04 INFO InternalParquetRecordReader: at row 3503378. reading next block
15/08/21 09:00:04 INFO Executor: Finished task 168.0 in stage 7.0 (TID 1015). 2125 bytes result sent to driver
15/08/21 09:00:04 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 1015) in 4815 ms on localhost (169/170)
15/08/21 09:00:04 INFO InternalParquetRecordReader: block read in memory in 51 ms. row count = 70711
15/08/21 09:00:04 INFO Executor: Finished task 169.0 in stage 7.0 (TID 1016). 2125 bytes result sent to driver
15/08/21 09:00:04 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 1016) in 4740 ms on localhost (170/170)
15/08/21 09:00:04 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/21 09:00:04 INFO DAGScheduler: ShuffleMapStage 7 (processCmd at CliDriver.java:423) finished in 218.660 s
15/08/21 09:00:04 INFO DAGScheduler: looking for newly runnable stages
15/08/21 09:00:04 INFO DAGScheduler: running: Set()
15/08/21 09:00:04 INFO DAGScheduler: waiting: Set(ResultStage 8)
15/08/21 09:00:04 INFO DAGScheduler: failed: Set()
15/08/21 09:00:04 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@7e24476c
15/08/21 09:00:04 INFO StatsReportListener: task runtime:(count: 170, mean: 6274.611765, stdev: 1007.287804, max: 8847.000000, min: 3575.000000)
15/08/21 09:00:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:00:04 INFO StatsReportListener: 	3.6 s	5.0 s	5.1 s	5.4 s	6.1 s	7.1 s	7.7 s	8.0 s	8.8 s
15/08/21 09:00:04 INFO StatsReportListener: shuffle bytes written:(count: 170, mean: 22160815.064706, stdev: 1033869.959364, max: 23161542.000000, min: 9212204.000000)
15/08/21 09:00:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:00:04 INFO StatsReportListener: 	8.8 MB	20.9 MB	20.9 MB	21.0 MB	21.0 MB	21.4 MB	21.4 MB	21.7 MB	22.1 MB
15/08/21 09:00:04 INFO StatsReportListener: task result size:(count: 170, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/21 09:00:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:00:04 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/21 09:00:04 INFO StatsReportListener: executor (non-fetch) time pct: (count: 170, mean: 99.539301, stdev: 0.411633, max: 99.852665, min: 94.763308)
15/08/21 09:00:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:00:04 INFO StatsReportListener: 	95 %	99 %	99 %	99 %	100 %	100 %	100 %	100 %	100 %
15/08/21 09:00:04 INFO StatsReportListener: other time pct: (count: 170, mean: 0.460699, stdev: 0.411633, max: 5.236692, min: 0.147335)
15/08/21 09:00:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:00:04 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 1 %	 5 %
15/08/21 09:00:04 INFO DAGScheduler: Missing parents for ResultStage 8: List()
15/08/21 09:00:04 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423), which is now runnable
15/08/21 09:00:04 INFO MemoryStore: ensureFreeSpace(16720) called with curMem=1444586, maxMem=22226833244
15/08/21 09:00:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 16.3 KB, free 20.7 GB)
15/08/21 09:00:04 INFO MemoryStore: ensureFreeSpace(7754) called with curMem=1461306, maxMem=22226833244
15/08/21 09:00:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.6 KB, free 20.7 GB)
15/08/21 09:00:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:51693 (size: 7.6 KB, free: 20.7 GB)
15/08/21 09:00:04 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874
15/08/21 09:00:04 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 8 (MapPartitionsRDD[44] at processCmd at CliDriver.java:423)
15/08/21 09:00:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/21 09:00:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 1017, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 1018, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 1019, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 1020, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 1021, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 1022, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 1023, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 1024, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 1025, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 1026, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 1027, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 1028, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 1029, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 1030, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:04 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 1031, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:05 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 1032, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:05 INFO Executor: Running task 0.0 in stage 8.0 (TID 1017)
15/08/21 09:00:05 INFO Executor: Running task 1.0 in stage 8.0 (TID 1018)
15/08/21 09:00:05 INFO Executor: Running task 2.0 in stage 8.0 (TID 1019)
15/08/21 09:00:05 INFO Executor: Running task 5.0 in stage 8.0 (TID 1022)
15/08/21 09:00:05 INFO Executor: Running task 4.0 in stage 8.0 (TID 1021)
15/08/21 09:00:05 INFO Executor: Running task 3.0 in stage 8.0 (TID 1020)
15/08/21 09:00:05 INFO Executor: Running task 6.0 in stage 8.0 (TID 1023)
15/08/21 09:00:05 INFO Executor: Running task 8.0 in stage 8.0 (TID 1025)
15/08/21 09:00:05 INFO Executor: Running task 12.0 in stage 8.0 (TID 1029)
15/08/21 09:00:05 INFO Executor: Running task 15.0 in stage 8.0 (TID 1032)
15/08/21 09:00:05 INFO Executor: Running task 11.0 in stage 8.0 (TID 1028)
15/08/21 09:00:05 INFO Executor: Running task 7.0 in stage 8.0 (TID 1024)
15/08/21 09:00:05 INFO Executor: Running task 10.0 in stage 8.0 (TID 1027)
15/08/21 09:00:05 INFO Executor: Running task 13.0 in stage 8.0 (TID 1030)
15/08/21 09:00:05 INFO Executor: Running task 9.0 in stage 8.0 (TID 1026)
15/08/21 09:00:05 INFO Executor: Running task 14.0 in stage 8.0 (TID 1031)
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO Executor: Finished task 5.0 in stage 8.0 (TID 1022). 1806 bytes result sent to driver
15/08/21 09:00:26 INFO Executor: Finished task 4.0 in stage 8.0 (TID 1021). 1602 bytes result sent to driver
15/08/21 09:00:26 INFO Executor: Finished task 6.0 in stage 8.0 (TID 1023). 1802 bytes result sent to driver
15/08/21 09:00:26 INFO Executor: Finished task 0.0 in stage 8.0 (TID 1017). 1805 bytes result sent to driver
15/08/21 09:00:26 INFO Executor: Finished task 12.0 in stage 8.0 (TID 1029). 1737 bytes result sent to driver
15/08/21 09:00:26 INFO Executor: Finished task 7.0 in stage 8.0 (TID 1024). 1805 bytes result sent to driver
15/08/21 09:00:26 INFO Executor: Finished task 8.0 in stage 8.0 (TID 1025). 1941 bytes result sent to driver
15/08/21 09:00:26 INFO Executor: Finished task 3.0 in stage 8.0 (TID 1020). 1668 bytes result sent to driver
15/08/21 09:00:26 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 1033, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 16.0 in stage 8.0 (TID 1033)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 1034, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 17.0 in stage 8.0 (TID 1034)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 1035, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 18.0 in stage 8.0 (TID 1035)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 1036, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 19.0 in stage 8.0 (TID 1036)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 1037, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 20.0 in stage 8.0 (TID 1037)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 1038, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 21.0 in stage 8.0 (TID 1038)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 1039, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 22.0 in stage 8.0 (TID 1039)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 1040, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 23.0 in stage 8.0 (TID 1040)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO Executor: Finished task 2.0 in stage 8.0 (TID 1019). 1738 bytes result sent to driver
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 1041, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 24.0 in stage 8.0 (TID 1041)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO Executor: Finished task 9.0 in stage 8.0 (TID 1026). 1738 bytes result sent to driver
15/08/21 09:00:26 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 1042, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 25.0 in stage 8.0 (TID 1042)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 1017) in 21869 ms on localhost (1/200)
15/08/21 09:00:26 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 1021) in 21872 ms on localhost (2/200)
15/08/21 09:00:26 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 1022) in 21873 ms on localhost (3/200)
15/08/21 09:00:26 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 1023) in 21874 ms on localhost (4/200)
15/08/21 09:00:26 INFO Executor: Finished task 13.0 in stage 8.0 (TID 1030). 1737 bytes result sent to driver
15/08/21 09:00:26 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 1043, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 26.0 in stage 8.0 (TID 1043)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 1024) in 21881 ms on localhost (5/200)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 1025) in 21883 ms on localhost (6/200)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 09:00:26 INFO Executor: Finished task 1.0 in stage 8.0 (TID 1018). 1602 bytes result sent to driver
15/08/21 09:00:26 INFO Executor: Finished task 14.0 in stage 8.0 (TID 1031). 1874 bytes result sent to driver
15/08/21 09:00:26 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 1019) in 21894 ms on localhost (7/200)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO Executor: Finished task 10.0 in stage 8.0 (TID 1027). 1943 bytes result sent to driver
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/21 09:00:26 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 1044, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 27.0 in stage 8.0 (TID 1044)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 1045, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 1046, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:26 INFO Executor: Running task 28.0 in stage 8.0 (TID 1045)
15/08/21 09:00:26 INFO Executor: Running task 29.0 in stage 8.0 (TID 1046)
15/08/21 09:00:26 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 1029) in 21910 ms on localhost (8/200)
15/08/21 09:00:26 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 1020) in 21914 ms on localhost (9/200)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 1026) in 21945 ms on localhost (10/200)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:26 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 1018) in 21952 ms on localhost (11/200)
15/08/21 09:00:26 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 1031) in 21955 ms on localhost (12/200)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:26 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 1027) in 21961 ms on localhost (13/200)
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:27 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 1030) in 22001 ms on localhost (14/200)
15/08/21 09:00:27 INFO Executor: Finished task 11.0 in stage 8.0 (TID 1028). 1737 bytes result sent to driver
15/08/21 09:00:27 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 1047, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:27 INFO Executor: Running task 30.0 in stage 8.0 (TID 1047)
15/08/21 09:00:27 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 1028) in 22037 ms on localhost (15/200)
15/08/21 09:00:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:27 INFO Executor: Finished task 15.0 in stage 8.0 (TID 1032). 1738 bytes result sent to driver
15/08/21 09:00:27 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 1048, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:27 INFO Executor: Running task 31.0 in stage 8.0 (TID 1048)
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:28 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 1032) in 23391 ms on localhost (16/200)
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:49 INFO Executor: Finished task 17.0 in stage 8.0 (TID 1034). 1426 bytes result sent to driver
15/08/21 09:00:49 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 1049, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:49 INFO Executor: Running task 32.0 in stage 8.0 (TID 1049)
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:49 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 1034) in 22437 ms on localhost (17/200)
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:49 INFO Executor: Finished task 16.0 in stage 8.0 (TID 1033). 2077 bytes result sent to driver
15/08/21 09:00:49 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 1050, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:49 INFO Executor: Running task 33.0 in stage 8.0 (TID 1050)
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:49 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 1033) in 22673 ms on localhost (18/200)
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:49 INFO Executor: Finished task 18.0 in stage 8.0 (TID 1035). 1806 bytes result sent to driver
15/08/21 09:00:49 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 1051, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:49 INFO Executor: Running task 34.0 in stage 8.0 (TID 1051)
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:49 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 1035) in 22879 ms on localhost (19/200)
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:50 INFO Executor: Finished task 19.0 in stage 8.0 (TID 1036). 1670 bytes result sent to driver
15/08/21 09:00:50 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 1052, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:50 INFO Executor: Running task 35.0 in stage 8.0 (TID 1052)
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:50 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 1036) in 23384 ms on localhost (20/200)
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:50 INFO Executor: Finished task 20.0 in stage 8.0 (TID 1037). 1602 bytes result sent to driver
15/08/21 09:00:50 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 1053, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:50 INFO Executor: Running task 36.0 in stage 8.0 (TID 1053)
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:50 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 1037) in 23471 ms on localhost (21/200)
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:50 INFO Executor: Finished task 21.0 in stage 8.0 (TID 1038). 1426 bytes result sent to driver
15/08/21 09:00:50 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 1054, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:50 INFO Executor: Running task 37.0 in stage 8.0 (TID 1054)
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:50 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 1038) in 23798 ms on localhost (22/200)
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 09:00:51 INFO Executor: Finished task 22.0 in stage 8.0 (TID 1039). 1602 bytes result sent to driver
15/08/21 09:00:51 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 1055, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:51 INFO Executor: Running task 38.0 in stage 8.0 (TID 1055)
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:51 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 1039) in 24366 ms on localhost (23/200)
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:51 INFO Executor: Finished task 23.0 in stage 8.0 (TID 1040). 1669 bytes result sent to driver
15/08/21 09:00:51 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 1056, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:51 INFO Executor: Running task 39.0 in stage 8.0 (TID 1056)
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:51 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 1040) in 24599 ms on localhost (24/200)
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:51 INFO Executor: Finished task 24.0 in stage 8.0 (TID 1041). 1738 bytes result sent to driver
15/08/21 09:00:51 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 1057, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:51 INFO Executor: Running task 40.0 in stage 8.0 (TID 1057)
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:51 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 1041) in 24676 ms on localhost (25/200)
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:51 INFO Executor: Finished task 25.0 in stage 8.0 (TID 1042). 1426 bytes result sent to driver
15/08/21 09:00:51 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 1058, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:51 INFO Executor: Running task 41.0 in stage 8.0 (TID 1058)
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:51 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 1042) in 24700 ms on localhost (26/200)
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:53 INFO Executor: Finished task 26.0 in stage 8.0 (TID 1043). 1874 bytes result sent to driver
15/08/21 09:00:53 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 1059, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:53 INFO Executor: Running task 42.0 in stage 8.0 (TID 1059)
15/08/21 09:00:53 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 1043) in 27074 ms on localhost (27/200)
15/08/21 09:00:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO Executor: Finished task 27.0 in stage 8.0 (TID 1044). 1601 bytes result sent to driver
15/08/21 09:00:54 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 1060, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:54 INFO Executor: Running task 43.0 in stage 8.0 (TID 1060)
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 1044) in 27262 ms on localhost (28/200)
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO Executor: Finished task 28.0 in stage 8.0 (TID 1045). 1875 bytes result sent to driver
15/08/21 09:00:54 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 1061, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:54 INFO Executor: Running task 44.0 in stage 8.0 (TID 1061)
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:00:54 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 1045) in 27628 ms on localhost (29/200)
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO Executor: Finished task 29.0 in stage 8.0 (TID 1046). 1806 bytes result sent to driver
15/08/21 09:00:54 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 1062, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:54 INFO Executor: Running task 45.0 in stage 8.0 (TID 1062)
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 1046) in 27765 ms on localhost (30/200)
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO Executor: Finished task 30.0 in stage 8.0 (TID 1047). 1601 bytes result sent to driver
15/08/21 09:00:54 INFO Executor: Finished task 31.0 in stage 8.0 (TID 1048). 1601 bytes result sent to driver
15/08/21 09:00:54 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 1063, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:54 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 1064, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:00:54 INFO Executor: Running task 46.0 in stage 8.0 (TID 1063)
15/08/21 09:00:54 INFO Executor: Running task 47.0 in stage 8.0 (TID 1064)
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:00:54 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 1047) in 27685 ms on localhost (31/200)
15/08/21 09:00:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:00:54 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 1048) in 27673 ms on localhost (32/200)
15/08/21 09:01:09 INFO Executor: Finished task 32.0 in stage 8.0 (TID 1049). 1670 bytes result sent to driver
15/08/21 09:01:09 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 1065, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:09 INFO Executor: Running task 48.0 in stage 8.0 (TID 1065)
15/08/21 09:01:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:09 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 1049) in 20177 ms on localhost (33/200)
15/08/21 09:01:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:10 INFO Executor: Finished task 33.0 in stage 8.0 (TID 1050). 1874 bytes result sent to driver
15/08/21 09:01:10 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 1066, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:10 INFO Executor: Running task 49.0 in stage 8.0 (TID 1066)
15/08/21 09:01:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:01:10 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 1050) in 21383 ms on localhost (34/200)
15/08/21 09:01:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO Executor: Finished task 34.0 in stage 8.0 (TID 1051). 1738 bytes result sent to driver
15/08/21 09:01:11 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 1067, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:11 INFO Executor: Running task 50.0 in stage 8.0 (TID 1067)
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 1051) in 21497 ms on localhost (35/200)
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO Executor: Finished task 35.0 in stage 8.0 (TID 1052). 1873 bytes result sent to driver
15/08/21 09:01:11 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 1068, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:11 INFO Executor: Running task 51.0 in stage 8.0 (TID 1068)
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 1052) in 21222 ms on localhost (36/200)
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO Executor: Finished task 36.0 in stage 8.0 (TID 1053). 1804 bytes result sent to driver
15/08/21 09:01:11 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 1069, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:11 INFO Executor: Running task 52.0 in stage 8.0 (TID 1069)
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 1053) in 21626 ms on localhost (37/200)
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO Executor: Finished task 37.0 in stage 8.0 (TID 1054). 1602 bytes result sent to driver
15/08/21 09:01:12 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 1070, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:12 INFO Executor: Running task 53.0 in stage 8.0 (TID 1070)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 1054) in 21588 ms on localhost (38/200)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:12 INFO Executor: Finished task 38.0 in stage 8.0 (TID 1055). 1670 bytes result sent to driver
15/08/21 09:01:12 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 1071, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:12 INFO Executor: Running task 54.0 in stage 8.0 (TID 1071)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 1055) in 21175 ms on localhost (39/200)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO Executor: Finished task 39.0 in stage 8.0 (TID 1056). 1736 bytes result sent to driver
15/08/21 09:01:12 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 1072, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:12 INFO Executor: Running task 55.0 in stage 8.0 (TID 1072)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 1056) in 21016 ms on localhost (40/200)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO Executor: Finished task 40.0 in stage 8.0 (TID 1057). 1737 bytes result sent to driver
15/08/21 09:01:12 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 1073, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:12 INFO Executor: Running task 56.0 in stage 8.0 (TID 1073)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 1057) in 21065 ms on localhost (41/200)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:12 INFO Executor: Finished task 41.0 in stage 8.0 (TID 1058). 1873 bytes result sent to driver
15/08/21 09:01:12 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 1074, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:12 INFO Executor: Running task 57.0 in stage 8.0 (TID 1074)
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 1058) in 22868 ms on localhost (42/200)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:14 INFO Executor: Finished task 42.0 in stage 8.0 (TID 1059). 1670 bytes result sent to driver
15/08/21 09:01:14 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 1075, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:14 INFO Executor: Running task 58.0 in stage 8.0 (TID 1075)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 1059) in 20490 ms on localhost (43/200)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:14 INFO Executor: Finished task 43.0 in stage 8.0 (TID 1060). 1806 bytes result sent to driver
15/08/21 09:01:14 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 1076, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:14 INFO Executor: Running task 59.0 in stage 8.0 (TID 1076)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:14 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 1060) in 20316 ms on localhost (44/200)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:14 INFO Executor: Finished task 44.0 in stage 8.0 (TID 1061). 1943 bytes result sent to driver
15/08/21 09:01:14 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 1077, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:14 INFO Executor: Running task 60.0 in stage 8.0 (TID 1077)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:14 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 1061) in 20346 ms on localhost (45/200)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:14 INFO Executor: Finished task 45.0 in stage 8.0 (TID 1062). 1738 bytes result sent to driver
15/08/21 09:01:14 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 1078, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:14 INFO Executor: Running task 61.0 in stage 8.0 (TID 1078)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:14 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 1062) in 20269 ms on localhost (46/200)
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:15 INFO Executor: Finished task 46.0 in stage 8.0 (TID 1063). 1602 bytes result sent to driver
15/08/21 09:01:15 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 1079, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:15 INFO Executor: Running task 62.0 in stage 8.0 (TID 1079)
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:15 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 1063) in 20383 ms on localhost (47/200)
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/21 09:01:15 INFO Executor: Finished task 47.0 in stage 8.0 (TID 1064). 1806 bytes result sent to driver
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 09:01:15 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 1080, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:15 INFO Executor: Running task 63.0 in stage 8.0 (TID 1080)
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:15 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 1064) in 20438 ms on localhost (48/200)
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:18 INFO Executor: Finished task 48.0 in stage 8.0 (TID 1065). 1943 bytes result sent to driver
15/08/21 09:01:18 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 1081, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:18 INFO Executor: Running task 64.0 in stage 8.0 (TID 1081)
15/08/21 09:01:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:18 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 1065) in 8886 ms on localhost (49/200)
15/08/21 09:01:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:23 INFO Executor: Finished task 49.0 in stage 8.0 (TID 1066). 1602 bytes result sent to driver
15/08/21 09:01:23 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 1082, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:23 INFO Executor: Running task 65.0 in stage 8.0 (TID 1082)
15/08/21 09:01:23 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 1066) in 12641 ms on localhost (50/200)
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:23 INFO Executor: Finished task 50.0 in stage 8.0 (TID 1067). 1670 bytes result sent to driver
15/08/21 09:01:23 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 1083, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:23 INFO Executor: Running task 66.0 in stage 8.0 (TID 1083)
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:23 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 1067) in 12496 ms on localhost (51/200)
15/08/21 09:01:30 INFO Executor: Finished task 51.0 in stage 8.0 (TID 1068). 1669 bytes result sent to driver
15/08/21 09:01:30 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 1084, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:30 INFO Executor: Running task 67.0 in stage 8.0 (TID 1084)
15/08/21 09:01:30 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 1068) in 19186 ms on localhost (52/200)
15/08/21 09:01:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:30 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:32 INFO Executor: Finished task 52.0 in stage 8.0 (TID 1069). 1805 bytes result sent to driver
15/08/21 09:01:32 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 1085, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:32 INFO Executor: Running task 68.0 in stage 8.0 (TID 1085)
15/08/21 09:01:32 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:01:32 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 1069) in 20510 ms on localhost (53/200)
15/08/21 09:01:32 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:33 INFO Executor: Finished task 53.0 in stage 8.0 (TID 1070). 1805 bytes result sent to driver
15/08/21 09:01:33 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 1086, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:33 INFO Executor: Running task 69.0 in stage 8.0 (TID 1086)
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:33 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 1070) in 20942 ms on localhost (54/200)
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:33 INFO Executor: Finished task 54.0 in stage 8.0 (TID 1071). 1669 bytes result sent to driver
15/08/21 09:01:33 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 1087, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:33 INFO Executor: Running task 70.0 in stage 8.0 (TID 1087)
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:33 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 1071) in 21114 ms on localhost (55/200)
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:33 INFO Executor: Finished task 55.0 in stage 8.0 (TID 1072). 1804 bytes result sent to driver
15/08/21 09:01:33 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 1088, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:33 INFO Executor: Running task 71.0 in stage 8.0 (TID 1088)
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:33 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 1072) in 21564 ms on localhost (56/200)
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:51 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:51693 in memory (size: 4.9 KB, free: 20.7 GB)
15/08/21 09:01:51 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:51693 in memory (size: 3.7 KB, free: 20.7 GB)
15/08/21 09:01:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:51693 in memory (size: 3.8 KB, free: 20.7 GB)
15/08/21 09:01:51 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:51693 in memory (size: 3.6 KB, free: 20.7 GB)
15/08/21 09:01:51 INFO Executor: Finished task 56.0 in stage 8.0 (TID 1073). 1805 bytes result sent to driver
15/08/21 09:01:51 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 1089, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:51 INFO Executor: Running task 72.0 in stage 8.0 (TID 1089)
15/08/21 09:01:51 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:51 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 1073) in 39507 ms on localhost (57/200)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:52 INFO Executor: Finished task 57.0 in stage 8.0 (TID 1074). 1737 bytes result sent to driver
15/08/21 09:01:52 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 1090, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:52 INFO Executor: Running task 73.0 in stage 8.0 (TID 1090)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 1074) in 39829 ms on localhost (58/200)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO Executor: Finished task 58.0 in stage 8.0 (TID 1075). 1668 bytes result sent to driver
15/08/21 09:01:52 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 1091, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:52 INFO Executor: Running task 74.0 in stage 8.0 (TID 1091)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 1075) in 38171 ms on localhost (59/200)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO Executor: Finished task 59.0 in stage 8.0 (TID 1076). 1875 bytes result sent to driver
15/08/21 09:01:52 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 1092, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:52 INFO Executor: Running task 75.0 in stage 8.0 (TID 1092)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 1076) in 38303 ms on localhost (60/200)
15/08/21 09:01:52 INFO Executor: Finished task 60.0 in stage 8.0 (TID 1077). 1805 bytes result sent to driver
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 1093, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:52 INFO Executor: Running task 76.0 in stage 8.0 (TID 1093)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 1077) in 37940 ms on localhost (61/200)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:52 INFO Executor: Finished task 61.0 in stage 8.0 (TID 1078). 2009 bytes result sent to driver
15/08/21 09:01:52 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 1094, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:52 INFO Executor: Running task 77.0 in stage 8.0 (TID 1094)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 09:01:52 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 1078) in 38001 ms on localhost (62/200)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:52 INFO Executor: Finished task 62.0 in stage 8.0 (TID 1079). 1737 bytes result sent to driver
15/08/21 09:01:52 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 1095, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:52 INFO Executor: Running task 78.0 in stage 8.0 (TID 1095)
15/08/21 09:01:52 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 1079) in 37901 ms on localhost (63/200)
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:53 INFO Executor: Finished task 63.0 in stage 8.0 (TID 1080). 1804 bytes result sent to driver
15/08/21 09:01:53 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 1096, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:53 INFO Executor: Running task 79.0 in stage 8.0 (TID 1096)
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:53 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 1080) in 38059 ms on localhost (64/200)
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:01:53 INFO Executor: Finished task 64.0 in stage 8.0 (TID 1081). 1873 bytes result sent to driver
15/08/21 09:01:53 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 1097, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:53 INFO Executor: Running task 80.0 in stage 8.0 (TID 1097)
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:01:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:01:53 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 1081) in 35054 ms on localhost (65/200)
15/08/21 09:01:56 INFO Executor: Finished task 65.0 in stage 8.0 (TID 1082). 1803 bytes result sent to driver
15/08/21 09:01:56 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 1098, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:01:56 INFO Executor: Running task 81.0 in stage 8.0 (TID 1098)
15/08/21 09:01:56 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:01:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 09:02:00 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 1082) in 36824 ms on localhost (66/200)
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:00 INFO Executor: Finished task 66.0 in stage 8.0 (TID 1083). 1876 bytes result sent to driver
15/08/21 09:02:00 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 1099, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:00 INFO Executor: Running task 82.0 in stage 8.0 (TID 1099)
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:00 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 1083) in 37139 ms on localhost (67/200)
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:01 INFO Executor: Finished task 67.0 in stage 8.0 (TID 1084). 2078 bytes result sent to driver
15/08/21 09:02:01 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 1100, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:01 INFO Executor: Running task 83.0 in stage 8.0 (TID 1100)
15/08/21 09:02:01 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 1084) in 30858 ms on localhost (68/200)
15/08/21 09:02:01 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:01 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:01 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:05 INFO Executor: Finished task 68.0 in stage 8.0 (TID 1085). 1670 bytes result sent to driver
15/08/21 09:02:05 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 1101, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:05 INFO Executor: Running task 84.0 in stage 8.0 (TID 1101)
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:05 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 1085) in 33132 ms on localhost (69/200)
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:05 INFO Executor: Finished task 69.0 in stage 8.0 (TID 1086). 1426 bytes result sent to driver
15/08/21 09:02:05 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 1102, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:05 INFO Executor: Running task 85.0 in stage 8.0 (TID 1102)
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:05 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 1086) in 32611 ms on localhost (70/200)
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:07 INFO Executor: Finished task 70.0 in stage 8.0 (TID 1087). 1426 bytes result sent to driver
15/08/21 09:02:07 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 1103, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:07 INFO Executor: Running task 86.0 in stage 8.0 (TID 1103)
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:07 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 1087) in 33713 ms on localhost (71/200)
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:07 INFO Executor: Finished task 71.0 in stage 8.0 (TID 1088). 1805 bytes result sent to driver
15/08/21 09:02:07 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 1104, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:07 INFO Executor: Running task 87.0 in stage 8.0 (TID 1104)
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:02:07 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 1088) in 33353 ms on localhost (72/200)
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:12 INFO Executor: Finished task 72.0 in stage 8.0 (TID 1089). 1426 bytes result sent to driver
15/08/21 09:02:12 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 1105, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:12 INFO Executor: Running task 88.0 in stage 8.0 (TID 1105)
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:12 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 1089) in 20648 ms on localhost (73/200)
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:12 INFO Executor: Finished task 73.0 in stage 8.0 (TID 1090). 1942 bytes result sent to driver
15/08/21 09:02:12 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 1106, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:12 INFO Executor: Running task 89.0 in stage 8.0 (TID 1106)
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:12 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 1090) in 20443 ms on localhost (74/200)
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:13 INFO Executor: Finished task 74.0 in stage 8.0 (TID 1091). 2150 bytes result sent to driver
15/08/21 09:02:13 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 1107, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:13 INFO Executor: Running task 90.0 in stage 8.0 (TID 1107)
15/08/21 09:02:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:13 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 1091) in 20472 ms on localhost (75/200)
15/08/21 09:02:13 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO Executor: Finished task 77.0 in stage 8.0 (TID 1094). 2010 bytes result sent to driver
15/08/21 09:02:18 INFO Executor: Finished task 75.0 in stage 8.0 (TID 1092). 1876 bytes result sent to driver
15/08/21 09:02:18 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 1108, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:18 INFO Executor: Running task 91.0 in stage 8.0 (TID 1108)
15/08/21 09:02:18 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 1109, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:18 INFO Executor: Running task 92.0 in stage 8.0 (TID 1109)
15/08/21 09:02:18 INFO Executor: Finished task 76.0 in stage 8.0 (TID 1093). 1805 bytes result sent to driver
15/08/21 09:02:18 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 1110, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:18 INFO Executor: Running task 93.0 in stage 8.0 (TID 1110)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:18 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 1092) in 25724 ms on localhost (76/200)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 1093) in 25709 ms on localhost (77/200)
15/08/21 09:02:18 INFO Executor: Finished task 78.0 in stage 8.0 (TID 1095). 1669 bytes result sent to driver
15/08/21 09:02:18 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 1094) in 25587 ms on localhost (78/200)
15/08/21 09:02:18 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 1111, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:18 INFO Executor: Running task 94.0 in stage 8.0 (TID 1111)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO Executor: Finished task 79.0 in stage 8.0 (TID 1096). 1939 bytes result sent to driver
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:18 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 1112, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO Executor: Running task 95.0 in stage 8.0 (TID 1112)
15/08/21 09:02:18 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 1095) in 25542 ms on localhost (79/200)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 1096) in 25362 ms on localhost (80/200)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO Executor: Finished task 80.0 in stage 8.0 (TID 1097). 1738 bytes result sent to driver
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 1113, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:18 INFO Executor: Running task 96.0 in stage 8.0 (TID 1113)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 1097) in 25313 ms on localhost (81/200)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO Executor: Finished task 81.0 in stage 8.0 (TID 1098). 1736 bytes result sent to driver
15/08/21 09:02:18 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 1114, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:18 INFO Executor: Running task 97.0 in stage 8.0 (TID 1114)
15/08/21 09:02:18 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 1098) in 21937 ms on localhost (82/200)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO Executor: Finished task 82.0 in stage 8.0 (TID 1099). 1804 bytes result sent to driver
15/08/21 09:02:18 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 1115, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:18 INFO Executor: Running task 98.0 in stage 8.0 (TID 1115)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:18 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 1099) in 18295 ms on localhost (83/200)
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:19 INFO Executor: Finished task 83.0 in stage 8.0 (TID 1100). 1426 bytes result sent to driver
15/08/21 09:02:19 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 1116, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:19 INFO Executor: Running task 99.0 in stage 8.0 (TID 1116)
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:19 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 1100) in 17758 ms on localhost (84/200)
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:19 INFO Executor: Finished task 84.0 in stage 8.0 (TID 1101). 1738 bytes result sent to driver
15/08/21 09:02:19 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 1117, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:19 INFO Executor: Running task 100.0 in stage 8.0 (TID 1117)
15/08/21 09:02:19 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 1101) in 14510 ms on localhost (85/200)
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:20 INFO Executor: Finished task 85.0 in stage 8.0 (TID 1102). 1602 bytes result sent to driver
15/08/21 09:02:20 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 1118, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:20 INFO Executor: Running task 101.0 in stage 8.0 (TID 1118)
15/08/21 09:02:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3510 ms
15/08/21 09:02:23 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:23 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 1102) in 18026 ms on localhost (86/200)
15/08/21 09:02:23 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:29 INFO Executor: Finished task 86.0 in stage 8.0 (TID 1103). 1805 bytes result sent to driver
15/08/21 09:02:29 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 1119, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:29 INFO Executor: Running task 102.0 in stage 8.0 (TID 1119)
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:29 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 1103) in 22263 ms on localhost (87/200)
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:29 INFO Executor: Finished task 88.0 in stage 8.0 (TID 1105). 2009 bytes result sent to driver
15/08/21 09:02:29 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 1120, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:29 INFO Executor: Running task 103.0 in stage 8.0 (TID 1120)
15/08/21 09:02:29 INFO Executor: Finished task 87.0 in stage 8.0 (TID 1104). 1669 bytes result sent to driver
15/08/21 09:02:29 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 1121, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:29 INFO Executor: Running task 104.0 in stage 8.0 (TID 1121)
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:29 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 1104) in 22285 ms on localhost (88/200)
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/21 09:02:29 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 1105) in 16901 ms on localhost (89/200)
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:29 INFO Executor: Finished task 89.0 in stage 8.0 (TID 1106). 1670 bytes result sent to driver
15/08/21 09:02:29 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 1122, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:29 INFO Executor: Running task 105.0 in stage 8.0 (TID 1122)
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:29 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 1106) in 16941 ms on localhost (90/200)
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:30 INFO Executor: Finished task 90.0 in stage 8.0 (TID 1107). 1941 bytes result sent to driver
15/08/21 09:02:30 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 1123, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:30 INFO Executor: Running task 106.0 in stage 8.0 (TID 1123)
15/08/21 09:02:30 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 1107) in 17528 ms on localhost (91/200)
15/08/21 09:02:30 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:30 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:35 INFO Executor: Finished task 91.0 in stage 8.0 (TID 1108). 2009 bytes result sent to driver
15/08/21 09:02:35 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 1124, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:35 INFO Executor: Running task 107.0 in stage 8.0 (TID 1124)
15/08/21 09:02:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 09:02:35 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 1108) in 17279 ms on localhost (92/200)
15/08/21 09:02:35 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:37 INFO Executor: Finished task 92.0 in stage 8.0 (TID 1109). 1670 bytes result sent to driver
15/08/21 09:02:37 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 1125, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:37 INFO Executor: Running task 108.0 in stage 8.0 (TID 1125)
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:37 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 1109) in 18737 ms on localhost (93/200)
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:37 INFO Executor: Finished task 93.0 in stage 8.0 (TID 1110). 1736 bytes result sent to driver
15/08/21 09:02:37 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 1126, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:37 INFO Executor: Running task 109.0 in stage 8.0 (TID 1126)
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:37 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 1110) in 19316 ms on localhost (94/200)
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:53 INFO Executor: Finished task 94.0 in stage 8.0 (TID 1111). 1738 bytes result sent to driver
15/08/21 09:02:53 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 1127, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:53 INFO Executor: Running task 110.0 in stage 8.0 (TID 1127)
15/08/21 09:02:53 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:53 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 1111) in 35343 ms on localhost (95/200)
15/08/21 09:02:53 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:53 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:54 INFO Executor: Finished task 95.0 in stage 8.0 (TID 1112). 1805 bytes result sent to driver
15/08/21 09:02:54 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 1128, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:54 INFO Executor: Running task 111.0 in stage 8.0 (TID 1128)
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:54 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 1112) in 35791 ms on localhost (96/200)
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:54 INFO Executor: Finished task 96.0 in stage 8.0 (TID 1113). 1738 bytes result sent to driver
15/08/21 09:02:54 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 1129, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:54 INFO Executor: Running task 112.0 in stage 8.0 (TID 1129)
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/21 09:02:54 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 1113) in 36386 ms on localhost (97/200)
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:55 INFO Executor: Finished task 97.0 in stage 8.0 (TID 1114). 1876 bytes result sent to driver
15/08/21 09:02:55 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 1130, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:55 INFO Executor: Running task 113.0 in stage 8.0 (TID 1130)
15/08/21 09:02:55 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:55 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 1114) in 36909 ms on localhost (98/200)
15/08/21 09:02:55 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:55 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:57 INFO Executor: Finished task 98.0 in stage 8.0 (TID 1115). 1668 bytes result sent to driver
15/08/21 09:02:57 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 1131, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:57 INFO Executor: Running task 114.0 in stage 8.0 (TID 1131)
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:57 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 1115) in 38730 ms on localhost (99/200)
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:57 INFO Executor: Finished task 99.0 in stage 8.0 (TID 1116). 1737 bytes result sent to driver
15/08/21 09:02:57 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 1132, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:57 INFO Executor: Running task 115.0 in stage 8.0 (TID 1132)
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:57 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 1116) in 38674 ms on localhost (100/200)
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:57 INFO Executor: Finished task 100.0 in stage 8.0 (TID 1117). 1669 bytes result sent to driver
15/08/21 09:02:57 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 1133, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:57 INFO Executor: Running task 116.0 in stage 8.0 (TID 1133)
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:58 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 1117) in 38080 ms on localhost (101/200)
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:58 INFO Executor: Finished task 101.0 in stage 8.0 (TID 1118). 1670 bytes result sent to driver
15/08/21 09:02:58 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 1134, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:02:58 INFO Executor: Running task 117.0 in stage 8.0 (TID 1134)
15/08/21 09:02:58 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 1118) in 38089 ms on localhost (102/200)
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:02:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:03 INFO Executor: Finished task 102.0 in stage 8.0 (TID 1119). 1602 bytes result sent to driver
15/08/21 09:03:03 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 1135, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:03 INFO Executor: Running task 118.0 in stage 8.0 (TID 1135)
15/08/21 09:03:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:03 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 1119) in 34381 ms on localhost (103/200)
15/08/21 09:03:03 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:04 INFO Executor: Finished task 103.0 in stage 8.0 (TID 1120). 1426 bytes result sent to driver
15/08/21 09:03:04 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 1136, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:04 INFO Executor: Running task 119.0 in stage 8.0 (TID 1136)
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:04 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 1120) in 34970 ms on localhost (104/200)
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:04 INFO Executor: Finished task 104.0 in stage 8.0 (TID 1121). 1602 bytes result sent to driver
15/08/21 09:03:04 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 1137, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:04 INFO Executor: Running task 120.0 in stage 8.0 (TID 1137)
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:04 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 1121) in 35178 ms on localhost (105/200)
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:05 INFO Executor: Finished task 105.0 in stage 8.0 (TID 1122). 1426 bytes result sent to driver
15/08/21 09:03:05 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 1138, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:05 INFO Executor: Running task 121.0 in stage 8.0 (TID 1138)
15/08/21 09:03:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:05 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 1122) in 35816 ms on localhost (106/200)
15/08/21 09:03:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO Executor: Finished task 106.0 in stage 8.0 (TID 1123). 1670 bytes result sent to driver
15/08/21 09:03:09 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 1139, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:09 INFO Executor: Running task 122.0 in stage 8.0 (TID 1139)
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 1123) in 38489 ms on localhost (107/200)
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO Executor: Finished task 107.0 in stage 8.0 (TID 1124). 1738 bytes result sent to driver
15/08/21 09:03:09 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 1140, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:09 INFO Executor: Running task 123.0 in stage 8.0 (TID 1140)
15/08/21 09:03:09 INFO Executor: Finished task 108.0 in stage 8.0 (TID 1125). 1806 bytes result sent to driver
15/08/21 09:03:09 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 1141, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:09 INFO Executor: Running task 124.0 in stage 8.0 (TID 1141)
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:09 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 1124) in 33624 ms on localhost (108/200)
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 1125) in 32159 ms on localhost (109/200)
15/08/21 09:03:09 INFO Executor: Finished task 109.0 in stage 8.0 (TID 1126). 1669 bytes result sent to driver
15/08/21 09:03:09 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 1142, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:09 INFO Executor: Running task 125.0 in stage 8.0 (TID 1142)
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:09 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 1126) in 31589 ms on localhost (110/200)
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:10 INFO Executor: Finished task 110.0 in stage 8.0 (TID 1127). 1426 bytes result sent to driver
15/08/21 09:03:10 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 1143, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:10 INFO Executor: Running task 126.0 in stage 8.0 (TID 1143)
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:10 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 1127) in 16305 ms on localhost (111/200)
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:10 INFO Executor: Finished task 111.0 in stage 8.0 (TID 1128). 1738 bytes result sent to driver
15/08/21 09:03:10 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 1144, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:10 INFO Executor: Running task 127.0 in stage 8.0 (TID 1144)
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:10 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 1128) in 16544 ms on localhost (112/200)
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:15 INFO Executor: Finished task 112.0 in stage 8.0 (TID 1129). 1874 bytes result sent to driver
15/08/21 09:03:15 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 1145, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:15 INFO Executor: Running task 128.0 in stage 8.0 (TID 1145)
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:15 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 1129) in 20480 ms on localhost (113/200)
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:15 INFO Executor: Finished task 113.0 in stage 8.0 (TID 1130). 1737 bytes result sent to driver
15/08/21 09:03:15 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 1146, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:15 INFO Executor: Running task 129.0 in stage 8.0 (TID 1146)
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:15 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 1130) in 20245 ms on localhost (114/200)
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:16 INFO Executor: Finished task 114.0 in stage 8.0 (TID 1131). 1806 bytes result sent to driver
15/08/21 09:03:16 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 1147, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:16 INFO Executor: Running task 130.0 in stage 8.0 (TID 1147)
15/08/21 09:03:16 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 1131) in 18381 ms on localhost (115/200)
15/08/21 09:03:16 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:16 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:16 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:20 INFO Executor: Finished task 115.0 in stage 8.0 (TID 1132). 1670 bytes result sent to driver
15/08/21 09:03:20 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 1148, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:20 INFO Executor: Running task 131.0 in stage 8.0 (TID 1148)
15/08/21 09:03:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:20 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 1132) in 22693 ms on localhost (116/200)
15/08/21 09:03:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:03:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:21 INFO Executor: Finished task 116.0 in stage 8.0 (TID 1133). 1737 bytes result sent to driver
15/08/21 09:03:21 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 1149, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:21 INFO Executor: Running task 132.0 in stage 8.0 (TID 1149)
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:21 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 1133) in 23036 ms on localhost (117/200)
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:21 INFO Executor: Finished task 117.0 in stage 8.0 (TID 1134). 1737 bytes result sent to driver
15/08/21 09:03:21 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 1150, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:21 INFO Executor: Running task 133.0 in stage 8.0 (TID 1150)
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:21 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 1134) in 23743 ms on localhost (118/200)
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:25 INFO Executor: Finished task 118.0 in stage 8.0 (TID 1135). 1736 bytes result sent to driver
15/08/21 09:03:25 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 1151, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:25 INFO Executor: Running task 134.0 in stage 8.0 (TID 1151)
15/08/21 09:03:25 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:25 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 1135) in 22240 ms on localhost (119/200)
15/08/21 09:03:25 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:26 INFO Executor: Finished task 119.0 in stage 8.0 (TID 1136). 1738 bytes result sent to driver
15/08/21 09:03:26 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 1152, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:26 INFO Executor: Running task 135.0 in stage 8.0 (TID 1152)
15/08/21 09:03:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:26 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 1136) in 21589 ms on localhost (120/200)
15/08/21 09:03:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:27 INFO Executor: Finished task 120.0 in stage 8.0 (TID 1137). 1670 bytes result sent to driver
15/08/21 09:03:27 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 1153, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:27 INFO Executor: Finished task 121.0 in stage 8.0 (TID 1138). 1737 bytes result sent to driver
15/08/21 09:03:27 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 1154, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:27 INFO Executor: Running task 136.0 in stage 8.0 (TID 1153)
15/08/21 09:03:27 INFO Executor: Running task 137.0 in stage 8.0 (TID 1154)
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:27 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 1138) in 22146 ms on localhost (121/200)
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 09:03:27 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 1137) in 23084 ms on localhost (122/200)
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:28 INFO Executor: Finished task 122.0 in stage 8.0 (TID 1139). 1804 bytes result sent to driver
15/08/21 09:03:28 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 1155, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:28 INFO Executor: Running task 138.0 in stage 8.0 (TID 1155)
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:28 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 1139) in 19266 ms on localhost (123/200)
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:28 INFO Executor: Finished task 123.0 in stage 8.0 (TID 1140). 1737 bytes result sent to driver
15/08/21 09:03:28 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 1156, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:28 INFO Executor: Running task 139.0 in stage 8.0 (TID 1156)
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:28 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 1140) in 19360 ms on localhost (124/200)
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO Executor: Finished task 124.0 in stage 8.0 (TID 1141). 1805 bytes result sent to driver
15/08/21 09:03:34 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 1157, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:34 INFO Executor: Running task 140.0 in stage 8.0 (TID 1157)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/21 09:03:34 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 1141) in 25019 ms on localhost (125/200)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO Executor: Finished task 125.0 in stage 8.0 (TID 1142). 1806 bytes result sent to driver
15/08/21 09:03:34 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 1158, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:34 INFO Executor: Running task 141.0 in stage 8.0 (TID 1158)
15/08/21 09:03:34 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 1142) in 25095 ms on localhost (126/200)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO Executor: Finished task 126.0 in stage 8.0 (TID 1143). 1670 bytes result sent to driver
15/08/21 09:03:34 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 1159, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:34 INFO Executor: Running task 142.0 in stage 8.0 (TID 1159)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:34 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 1143) in 24576 ms on localhost (127/200)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:34 INFO Executor: Finished task 127.0 in stage 8.0 (TID 1144). 1670 bytes result sent to driver
15/08/21 09:03:34 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 1160, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:34 INFO Executor: Running task 143.0 in stage 8.0 (TID 1160)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:34 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 1144) in 24051 ms on localhost (128/200)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO Executor: Finished task 129.0 in stage 8.0 (TID 1146). 1426 bytes result sent to driver
15/08/21 09:03:34 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 1161, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:34 INFO Executor: Running task 144.0 in stage 8.0 (TID 1161)
15/08/21 09:03:34 INFO Executor: Finished task 128.0 in stage 8.0 (TID 1145). 1669 bytes result sent to driver
15/08/21 09:03:34 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 1162, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:34 INFO Executor: Running task 145.0 in stage 8.0 (TID 1162)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 1146) in 19181 ms on localhost (129/200)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 1145) in 19632 ms on localhost (130/200)
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:39 INFO Executor: Finished task 130.0 in stage 8.0 (TID 1147). 1669 bytes result sent to driver
15/08/21 09:03:39 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 1163, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:39 INFO Executor: Running task 146.0 in stage 8.0 (TID 1163)
15/08/21 09:03:39 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:39 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 1147) in 23482 ms on localhost (131/200)
15/08/21 09:03:39 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:40 INFO Executor: Finished task 131.0 in stage 8.0 (TID 1148). 1738 bytes result sent to driver
15/08/21 09:03:40 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 1164, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:40 INFO Executor: Running task 147.0 in stage 8.0 (TID 1164)
15/08/21 09:03:41 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 1148) in 20577 ms on localhost (132/200)
15/08/21 09:03:41 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/21 09:03:41 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:41 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:57 INFO Executor: Finished task 132.0 in stage 8.0 (TID 1149). 1602 bytes result sent to driver
15/08/21 09:03:57 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 1165, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:57 INFO Executor: Running task 148.0 in stage 8.0 (TID 1165)
15/08/21 09:03:57 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:57 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 1149) in 36588 ms on localhost (133/200)
15/08/21 09:03:57 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:57 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:58 INFO Executor: Finished task 133.0 in stage 8.0 (TID 1150). 1806 bytes result sent to driver
15/08/21 09:03:58 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 1166, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:58 INFO Executor: Running task 149.0 in stage 8.0 (TID 1166)
15/08/21 09:03:58 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:58 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 1150) in 36577 ms on localhost (134/200)
15/08/21 09:03:58 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:58 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:59 INFO Executor: Finished task 134.0 in stage 8.0 (TID 1151). 1602 bytes result sent to driver
15/08/21 09:03:59 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 1167, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:59 INFO Executor: Running task 150.0 in stage 8.0 (TID 1167)
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:59 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 1151) in 33470 ms on localhost (135/200)
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:59 INFO Executor: Finished task 135.0 in stage 8.0 (TID 1152). 1670 bytes result sent to driver
15/08/21 09:03:59 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 1168, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:03:59 INFO Executor: Running task 151.0 in stage 8.0 (TID 1168)
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:03:59 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 1152) in 33532 ms on localhost (136/200)
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:03:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:02 INFO Executor: Finished task 136.0 in stage 8.0 (TID 1153). 1874 bytes result sent to driver
15/08/21 09:04:02 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 1169, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:02 INFO Executor: Running task 152.0 in stage 8.0 (TID 1169)
15/08/21 09:04:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:02 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 1153) in 35032 ms on localhost (137/200)
15/08/21 09:04:02 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:03 INFO Executor: Finished task 137.0 in stage 8.0 (TID 1154). 1737 bytes result sent to driver
15/08/21 09:04:03 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 1170, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:03 INFO Executor: Running task 153.0 in stage 8.0 (TID 1170)
15/08/21 09:04:03 INFO Executor: Finished task 138.0 in stage 8.0 (TID 1155). 1738 bytes result sent to driver
15/08/21 09:04:03 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 1171, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:03 INFO Executor: Running task 154.0 in stage 8.0 (TID 1171)
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:03 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 1154) in 35327 ms on localhost (138/200)
15/08/21 09:04:03 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 1155) in 34800 ms on localhost (139/200)
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:03 INFO Executor: Finished task 139.0 in stage 8.0 (TID 1156). 1670 bytes result sent to driver
15/08/21 09:04:03 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 1172, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:03 INFO Executor: Running task 155.0 in stage 8.0 (TID 1172)
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:03 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 1156) in 35088 ms on localhost (140/200)
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:04 INFO Executor: Finished task 140.0 in stage 8.0 (TID 1157). 1737 bytes result sent to driver
15/08/21 09:04:04 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 1173, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:04 INFO Executor: Running task 156.0 in stage 8.0 (TID 1173)
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:04 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 1157) in 30233 ms on localhost (141/200)
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:04 INFO Executor: Finished task 141.0 in stage 8.0 (TID 1158). 1670 bytes result sent to driver
15/08/21 09:04:04 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 1174, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:04 INFO Executor: Running task 157.0 in stage 8.0 (TID 1174)
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:04 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 1158) in 30240 ms on localhost (142/200)
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:05 INFO Executor: Finished task 142.0 in stage 8.0 (TID 1159). 1736 bytes result sent to driver
15/08/21 09:04:05 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 1175, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:05 INFO Executor: Running task 158.0 in stage 8.0 (TID 1175)
15/08/21 09:04:05 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:05 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:05 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 1159) in 31229 ms on localhost (143/200)
15/08/21 09:04:05 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:11 INFO Executor: Finished task 143.0 in stage 8.0 (TID 1160). 1737 bytes result sent to driver
15/08/21 09:04:11 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 1176, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:11 INFO Executor: Running task 159.0 in stage 8.0 (TID 1176)
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:12 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 1160) in 37215 ms on localhost (144/200)
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:12 INFO Executor: Finished task 144.0 in stage 8.0 (TID 1161). 1805 bytes result sent to driver
15/08/21 09:04:12 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 1177, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:12 INFO Executor: Running task 160.0 in stage 8.0 (TID 1177)
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:12 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 1161) in 37273 ms on localhost (145/200)
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:13 INFO Executor: Finished task 145.0 in stage 8.0 (TID 1162). 1874 bytes result sent to driver
15/08/21 09:04:13 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 1178, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:13 INFO Executor: Running task 161.0 in stage 8.0 (TID 1178)
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:13 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 1162) in 38213 ms on localhost (146/200)
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:13 INFO Executor: Finished task 146.0 in stage 8.0 (TID 1163). 1601 bytes result sent to driver
15/08/21 09:04:13 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 1179, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:13 INFO Executor: Running task 162.0 in stage 8.0 (TID 1179)
15/08/21 09:04:13 INFO Executor: Finished task 147.0 in stage 8.0 (TID 1164). 1669 bytes result sent to driver
15/08/21 09:04:13 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 1180, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:13 INFO Executor: Running task 163.0 in stage 8.0 (TID 1180)
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:13 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 1163) in 33751 ms on localhost (147/200)
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:13 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 1164) in 32270 ms on localhost (148/200)
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:13 INFO Executor: Finished task 148.0 in stage 8.0 (TID 1165). 1942 bytes result sent to driver
15/08/21 09:04:13 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 1181, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:13 INFO Executor: Running task 164.0 in stage 8.0 (TID 1181)
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:13 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 1165) in 16261 ms on localhost (149/200)
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:19 INFO Executor: Finished task 149.0 in stage 8.0 (TID 1166). 2010 bytes result sent to driver
15/08/21 09:04:19 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 1182, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:19 INFO Executor: Running task 165.0 in stage 8.0 (TID 1182)
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:19 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 1166) in 20517 ms on localhost (150/200)
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:19 INFO Executor: Finished task 150.0 in stage 8.0 (TID 1167). 1941 bytes result sent to driver
15/08/21 09:04:19 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 1183, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:19 INFO Executor: Running task 166.0 in stage 8.0 (TID 1183)
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:19 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 1167) in 20485 ms on localhost (151/200)
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:20 INFO Executor: Finished task 151.0 in stage 8.0 (TID 1168). 1805 bytes result sent to driver
15/08/21 09:04:20 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 1184, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:20 INFO Executor: Running task 167.0 in stage 8.0 (TID 1184)
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:20 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 1168) in 20631 ms on localhost (152/200)
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:20 INFO Executor: Finished task 152.0 in stage 8.0 (TID 1169). 1738 bytes result sent to driver
15/08/21 09:04:20 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 1185, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:20 INFO Executor: Running task 168.0 in stage 8.0 (TID 1185)
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:20 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 1169) in 18249 ms on localhost (153/200)
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:26 INFO Executor: Finished task 153.0 in stage 8.0 (TID 1170). 1737 bytes result sent to driver
15/08/21 09:04:26 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 1186, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:26 INFO Executor: Running task 169.0 in stage 8.0 (TID 1186)
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:26 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 1170) in 23627 ms on localhost (154/200)
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:26 INFO Executor: Finished task 154.0 in stage 8.0 (TID 1171). 1876 bytes result sent to driver
15/08/21 09:04:26 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 1187, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:26 INFO Executor: Running task 170.0 in stage 8.0 (TID 1187)
15/08/21 09:04:26 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 1171) in 23667 ms on localhost (155/200)
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:27 INFO Executor: Finished task 155.0 in stage 8.0 (TID 1172). 1601 bytes result sent to driver
15/08/21 09:04:27 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 1188, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:27 INFO Executor: Running task 171.0 in stage 8.0 (TID 1188)
15/08/21 09:04:27 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:27 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 1172) in 23891 ms on localhost (156/200)
15/08/21 09:04:27 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:29 INFO Executor: Finished task 156.0 in stage 8.0 (TID 1173). 1737 bytes result sent to driver
15/08/21 09:04:29 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 1189, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:29 INFO Executor: Running task 172.0 in stage 8.0 (TID 1189)
15/08/21 09:04:29 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:04:29 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 1173) in 24960 ms on localhost (157/200)
15/08/21 09:04:29 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:04:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:35 INFO Executor: Finished task 157.0 in stage 8.0 (TID 1174). 1805 bytes result sent to driver
15/08/21 09:04:35 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 1190, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:35 INFO Executor: Running task 173.0 in stage 8.0 (TID 1190)
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:35 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 1174) in 31084 ms on localhost (158/200)
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:35 INFO Executor: Finished task 158.0 in stage 8.0 (TID 1175). 1940 bytes result sent to driver
15/08/21 09:04:35 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 1191, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:35 INFO Executor: Running task 174.0 in stage 8.0 (TID 1191)
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:35 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 1175) in 30086 ms on localhost (159/200)
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:36 INFO Executor: Finished task 159.0 in stage 8.0 (TID 1176). 1670 bytes result sent to driver
15/08/21 09:04:36 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 1192, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:36 INFO Executor: Running task 175.0 in stage 8.0 (TID 1192)
15/08/21 09:04:36 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:36 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 1176) in 24679 ms on localhost (160/200)
15/08/21 09:04:36 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:37 INFO Executor: Finished task 160.0 in stage 8.0 (TID 1177). 1602 bytes result sent to driver
15/08/21 09:04:37 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 1193, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:37 INFO Executor: Running task 176.0 in stage 8.0 (TID 1193)
15/08/21 09:04:37 INFO Executor: Finished task 161.0 in stage 8.0 (TID 1178). 1670 bytes result sent to driver
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:37 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 1194, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:37 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 1177) in 24940 ms on localhost (161/200)
15/08/21 09:04:37 INFO Executor: Running task 177.0 in stage 8.0 (TID 1194)
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:37 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 1178) in 24013 ms on localhost (162/200)
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:37 INFO Executor: Finished task 162.0 in stage 8.0 (TID 1179). 1738 bytes result sent to driver
15/08/21 09:04:37 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 1195, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:37 INFO Executor: Running task 178.0 in stage 8.0 (TID 1195)
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:37 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 1179) in 23943 ms on localhost (163/200)
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:38 INFO Executor: Finished task 163.0 in stage 8.0 (TID 1180). 1670 bytes result sent to driver
15/08/21 09:04:38 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 1196, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:38 INFO Executor: Running task 179.0 in stage 8.0 (TID 1196)
15/08/21 09:04:38 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:38 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 1180) in 25625 ms on localhost (164/200)
15/08/21 09:04:38 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:38 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:39 INFO Executor: Finished task 164.0 in stage 8.0 (TID 1181). 1602 bytes result sent to driver
15/08/21 09:04:39 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 1197, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:39 INFO Executor: Running task 180.0 in stage 8.0 (TID 1197)
15/08/21 09:04:39 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:39 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 1181) in 25358 ms on localhost (165/200)
15/08/21 09:04:39 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:04:39 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:44 INFO Executor: Finished task 165.0 in stage 8.0 (TID 1182). 1669 bytes result sent to driver
15/08/21 09:04:44 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 1198, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:44 INFO Executor: Running task 181.0 in stage 8.0 (TID 1198)
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:44 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 1182) in 25618 ms on localhost (166/200)
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:44 INFO Executor: Finished task 166.0 in stage 8.0 (TID 1183). 1737 bytes result sent to driver
15/08/21 09:04:44 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 1199, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:44 INFO Executor: Running task 182.0 in stage 8.0 (TID 1199)
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:44 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 1183) in 24936 ms on localhost (167/200)
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:44 INFO Executor: Finished task 167.0 in stage 8.0 (TID 1184). 1738 bytes result sent to driver
15/08/21 09:04:44 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 1200, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:44 INFO Executor: Running task 183.0 in stage 8.0 (TID 1200)
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:44 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 1184) in 24686 ms on localhost (168/200)
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:46 INFO Executor: Finished task 168.0 in stage 8.0 (TID 1185). 1668 bytes result sent to driver
15/08/21 09:04:46 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 1201, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:46 INFO Executor: Running task 184.0 in stage 8.0 (TID 1201)
15/08/21 09:04:46 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:46 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 1185) in 25123 ms on localhost (169/200)
15/08/21 09:04:46 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:46 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:47 INFO Executor: Finished task 169.0 in stage 8.0 (TID 1186). 1942 bytes result sent to driver
15/08/21 09:04:47 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 1202, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:47 INFO Executor: Running task 185.0 in stage 8.0 (TID 1202)
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:47 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 1186) in 20384 ms on localhost (170/200)
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:47 INFO Executor: Finished task 170.0 in stage 8.0 (TID 1187). 1736 bytes result sent to driver
15/08/21 09:04:47 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 1203, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:04:47 INFO Executor: Running task 186.0 in stage 8.0 (TID 1203)
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:04:47 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 1187) in 20663 ms on localhost (171/200)
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:04:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:02 INFO Executor: Finished task 171.0 in stage 8.0 (TID 1188). 1804 bytes result sent to driver
15/08/21 09:05:02 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 1204, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:02 INFO Executor: Running task 187.0 in stage 8.0 (TID 1204)
15/08/21 09:05:02 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 1188) in 34629 ms on localhost (172/200)
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:02 INFO Executor: Finished task 172.0 in stage 8.0 (TID 1189). 1736 bytes result sent to driver
15/08/21 09:05:02 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 1205, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:02 INFO Executor: Running task 188.0 in stage 8.0 (TID 1205)
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:02 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 1189) in 33516 ms on localhost (173/200)
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:08 INFO Executor: Finished task 173.0 in stage 8.0 (TID 1190). 1738 bytes result sent to driver
15/08/21 09:05:08 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 1206, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:08 INFO Executor: Running task 189.0 in stage 8.0 (TID 1206)
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:08 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 1190) in 32870 ms on localhost (174/200)
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:08 INFO Executor: Finished task 174.0 in stage 8.0 (TID 1191). 1875 bytes result sent to driver
15/08/21 09:05:08 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 1207, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:08 INFO Executor: Running task 190.0 in stage 8.0 (TID 1207)
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:08 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 1191) in 32905 ms on localhost (175/200)
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:08 INFO Executor: Finished task 175.0 in stage 8.0 (TID 1192). 2011 bytes result sent to driver
15/08/21 09:05:08 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 1208, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:08 INFO Executor: Running task 191.0 in stage 8.0 (TID 1208)
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:08 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 1192) in 32304 ms on localhost (176/200)
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:09 INFO Executor: Finished task 176.0 in stage 8.0 (TID 1193). 1602 bytes result sent to driver
15/08/21 09:05:09 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 1209, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:09 INFO Executor: Running task 192.0 in stage 8.0 (TID 1209)
15/08/21 09:05:09 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:09 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 1193) in 32115 ms on localhost (177/200)
15/08/21 09:05:09 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:09 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:10 INFO Executor: Finished task 177.0 in stage 8.0 (TID 1194). 1670 bytes result sent to driver
15/08/21 09:05:10 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 1210, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:10 INFO Executor: Running task 193.0 in stage 8.0 (TID 1210)
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:10 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 1194) in 32984 ms on localhost (178/200)
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:10 INFO Executor: Finished task 178.0 in stage 8.0 (TID 1195). 1806 bytes result sent to driver
15/08/21 09:05:10 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 1211, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:10 INFO Executor: Running task 194.0 in stage 8.0 (TID 1211)
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:10 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 1195) in 33720 ms on localhost (179/200)
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:10 INFO Executor: Finished task 179.0 in stage 8.0 (TID 1196). 1738 bytes result sent to driver
15/08/21 09:05:10 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 1212, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:10 INFO Executor: Running task 195.0 in stage 8.0 (TID 1212)
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:10 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 1196) in 32090 ms on localhost (180/200)
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:17 INFO Executor: Finished task 180.0 in stage 8.0 (TID 1197). 1737 bytes result sent to driver
15/08/21 09:05:17 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 1213, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:17 INFO Executor: Running task 196.0 in stage 8.0 (TID 1213)
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:17 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 1197) in 38037 ms on localhost (181/200)
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:17 INFO Executor: Finished task 181.0 in stage 8.0 (TID 1198). 1736 bytes result sent to driver
15/08/21 09:05:17 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 1214, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:17 INFO Executor: Running task 197.0 in stage 8.0 (TID 1214)
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:17 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 1198) in 32641 ms on localhost (182/200)
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:18 INFO Executor: Finished task 182.0 in stage 8.0 (TID 1199). 1802 bytes result sent to driver
15/08/21 09:05:18 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 1215, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:18 INFO Executor: Running task 198.0 in stage 8.0 (TID 1215)
15/08/21 09:05:18 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 1199) in 33371 ms on localhost (183/200)
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:18 INFO Executor: Finished task 183.0 in stage 8.0 (TID 1200). 1600 bytes result sent to driver
15/08/21 09:05:18 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 1216, localhost, PROCESS_LOCAL, 1623 bytes)
15/08/21 09:05:18 INFO Executor: Running task 199.0 in stage 8.0 (TID 1216)
15/08/21 09:05:18 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 1200) in 33866 ms on localhost (184/200)
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Getting 170 non-empty blocks out of 170 blocks
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Getting 188 non-empty blocks out of 200 blocks
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/21 09:05:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/21 09:05:18 INFO Executor: Finished task 184.0 in stage 8.0 (TID 1201). 1737 bytes result sent to driver
15/08/21 09:05:18 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 1201) in 32749 ms on localhost (185/200)
15/08/21 09:05:19 INFO Executor: Finished task 185.0 in stage 8.0 (TID 1202). 1602 bytes result sent to driver
15/08/21 09:05:19 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 1202) in 32073 ms on localhost (186/200)
15/08/21 09:05:19 INFO Executor: Finished task 186.0 in stage 8.0 (TID 1203). 1670 bytes result sent to driver
15/08/21 09:05:19 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 1203) in 31798 ms on localhost (187/200)
15/08/21 09:05:19 INFO Executor: Finished task 187.0 in stage 8.0 (TID 1204). 1602 bytes result sent to driver
15/08/21 09:05:19 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 1204) in 17046 ms on localhost (188/200)
15/08/21 09:05:19 INFO Executor: Finished task 188.0 in stage 8.0 (TID 1205). 1601 bytes result sent to driver
15/08/21 09:05:19 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 1205) in 16679 ms on localhost (189/200)
15/08/21 09:05:20 INFO Executor: Finished task 189.0 in stage 8.0 (TID 1206). 1668 bytes result sent to driver
15/08/21 09:05:20 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 1206) in 11594 ms on localhost (190/200)
15/08/21 09:05:24 INFO Executor: Finished task 190.0 in stage 8.0 (TID 1207). 1941 bytes result sent to driver
15/08/21 09:05:24 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 1207) in 16135 ms on localhost (191/200)
15/08/21 09:05:25 INFO Executor: Finished task 191.0 in stage 8.0 (TID 1208). 1669 bytes result sent to driver
15/08/21 09:05:25 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 1208) in 16161 ms on localhost (192/200)
15/08/21 09:05:25 INFO Executor: Finished task 192.0 in stage 8.0 (TID 1209). 1669 bytes result sent to driver
15/08/21 09:05:25 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 1209) in 16585 ms on localhost (193/200)
15/08/21 09:05:26 INFO Executor: Finished task 193.0 in stage 8.0 (TID 1210). 1669 bytes result sent to driver
15/08/21 09:05:26 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 1210) in 16367 ms on localhost (194/200)
15/08/21 09:05:26 INFO Executor: Finished task 194.0 in stage 8.0 (TID 1211). 1426 bytes result sent to driver
15/08/21 09:05:26 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 1211) in 15848 ms on localhost (195/200)
15/08/21 09:05:27 INFO Executor: Finished task 195.0 in stage 8.0 (TID 1212). 1602 bytes result sent to driver
15/08/21 09:05:27 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 1212) in 16238 ms on localhost (196/200)
15/08/21 09:05:28 INFO Executor: Finished task 196.0 in stage 8.0 (TID 1213). 1670 bytes result sent to driver
15/08/21 09:05:28 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 1213) in 10878 ms on localhost (197/200)
15/08/21 09:05:28 INFO Executor: Finished task 198.0 in stage 8.0 (TID 1215). 1601 bytes result sent to driver
15/08/21 09:05:28 INFO Executor: Finished task 199.0 in stage 8.0 (TID 1216). 1874 bytes result sent to driver
15/08/21 09:05:28 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 1215) in 10552 ms on localhost (198/200)
15/08/21 09:05:28 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 1216) in 9953 ms on localhost (199/200)
15/08/21 09:05:28 INFO Executor: Finished task 197.0 in stage 8.0 (TID 1214). 1667 bytes result sent to driver
15/08/21 09:05:28 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 1214) in 11596 ms on localhost (200/200)
15/08/21 09:05:28 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/21 09:05:28 INFO DAGScheduler: ResultStage 8 (processCmd at CliDriver.java:423) finished in 323.862 s
15/08/21 09:05:28 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@6a70d108
15/08/21 09:05:28 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:423, took 543.531829 s
15/08/21 09:05:28 INFO StatsReportListener: task runtime:(count: 200, mean: 25604.740000, stdev: 7342.715996, max: 39829.000000, min: 8886.000000)
15/08/21 09:05:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:28 INFO StatsReportListener: 	8.9 s	16.1 s	16.9 s	20.5 s	23.7 s	32.7 s	36.8 s	38.1 s	39.8 s
15/08/21 09:05:28 INFO StatsReportListener: fetch wait time:(count: 200, mean: 1.330000, stdev: 1.615890, max: 11.000000, min: 0.000000)
15/08/21 09:05:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:28 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	2.0 ms	3.0 ms	4.0 ms	11.0 ms
15/08/21 09:05:28 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/21 09:05:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:28 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/21 09:05:28 INFO StatsReportListener: task result size:(count: 200, mean: 1735.070000, stdev: 135.216179, max: 2150.000000, min: 1426.000000)
15/08/21 09:05:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:28 INFO StatsReportListener: 	1426.0 B	1426.0 B	1602.0 B	1669.0 B	1737.0 B	1805.0 B	1940.0 B	1943.0 B	2.1 KB
15/08/21 09:05:28 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 99.455787, stdev: 1.784064, max: 99.932063, min: 80.383890)
15/08/21 09:05:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:28 INFO StatsReportListener: 	80 %	97 %	100 %	100 %	100 %	100 %	100 %	100 %	100 %
15/08/21 09:05:28 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.005431, stdev: 0.006394, max: 0.041578, min: 0.000000)
15/08/21 09:05:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:28 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/21 09:05:28 INFO StatsReportListener: other time pct: (count: 200, mean: 0.538782, stdev: 1.784307, max: 19.610563, min: 0.066859)
15/08/21 09:05:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:28 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 4 %	20 %
15/08/21 09:05:28 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/21 09:05:28 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 09:05:28 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 09:05:28 INFO DAGScheduler: Got job 3 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 09:05:28 INFO DAGScheduler: Final stage: ResultStage 9(processCmd at CliDriver.java:423)
15/08/21 09:05:28 INFO DAGScheduler: Parents of final stage: List()
15/08/21 09:05:28 INFO DAGScheduler: Missing parents: List()
15/08/21 09:05:28 INFO DAGScheduler: Submitting ResultStage 9 (ParallelCollectionRDD[45] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 09:05:29 INFO MemoryStore: ensureFreeSpace(71408) called with curMem=1422078, maxMem=22226833244
15/08/21 09:05:29 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 69.7 KB, free 20.7 GB)
15/08/21 09:05:29 INFO MemoryStore: ensureFreeSpace(26157) called with curMem=1493486, maxMem=22226833244
15/08/21 09:05:29 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.5 KB, free 20.7 GB)
15/08/21 09:05:29 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:51693 (size: 25.5 KB, free: 20.7 GB)
15/08/21 09:05:29 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:874
15/08/21 09:05:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (ParallelCollectionRDD[45] at processCmd at CliDriver.java:423)
15/08/21 09:05:29 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/08/21 09:05:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 1217, localhost, PROCESS_LOCAL, 8222 bytes)
15/08/21 09:05:29 INFO Executor: Running task 0.0 in stage 9.0 (TID 1217)
15/08/21 09:05:29 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/21 09:05:29 INFO CodecConfig: Compression: GZIP
15/08/21 09:05:29 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/21 09:05:29 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/21 09:05:29 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/21 09:05:29 INFO ParquetOutputFormat: Dictionary is on
15/08/21 09:05:29 INFO ParquetOutputFormat: Validation is off
15/08/21 09:05:29 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/21 09:05:29 INFO CodecPool: Got brand-new compressor [.gz]
15/08/21 09:05:29 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 40,692,456
15/08/21 09:05:29 INFO ColumnChunkPageWriteStore: written 615B for [c_name] BINARY: 100 values, 2,207B raw, 551B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 09:05:29 INFO ColumnChunkPageWriteStore: written 455B for [c_custkey] INT32: 100 values, 407B raw, 419B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 09:05:29 INFO ColumnChunkPageWriteStore: written 466B for [o_orderkey] INT32: 100 values, 407B raw, 430B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 09:05:29 INFO ColumnChunkPageWriteStore: written 441B for [o_orderdate] BINARY: 100 values, 1,407B raw, 393B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 09:05:29 INFO ColumnChunkPageWriteStore: written 622B for [o_totalprice] DOUBLE: 100 values, 807B raw, 578B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
15/08/21 09:05:29 INFO ColumnChunkPageWriteStore: written 141B for [sum_quantity] DOUBLE: 100 values, 74B raw, 97B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 18 entries, 144B raw, 18B comp}
15/08/21 09:05:29 INFO FileOutputCommitter: Saved output of task 'attempt_201508210905_0009_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_large_volume_customer_par/_temporary/0/task_201508210905_0009_m_000000
15/08/21 09:05:29 INFO SparkHadoopMapRedUtil: attempt_201508210905_0009_m_000000_0: Committed
15/08/21 09:05:29 INFO Executor: Finished task 0.0 in stage 9.0 (TID 1217). 577 bytes result sent to driver
15/08/21 09:05:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 1217) in 241 ms on localhost (1/1)
15/08/21 09:05:29 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/21 09:05:29 INFO DAGScheduler: ResultStage 9 (processCmd at CliDriver.java:423) finished in 0.242 s
15/08/21 09:05:29 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@50a5b3bf
15/08/21 09:05:29 INFO DAGScheduler: Job 3 finished: processCmd at CliDriver.java:423, took 0.313778 s
15/08/21 09:05:29 INFO StatsReportListener: task runtime:(count: 1, mean: 241.000000, stdev: 0.000000, max: 241.000000, min: 241.000000)
15/08/21 09:05:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:29 INFO StatsReportListener: 	241.0 ms	241.0 ms	241.0 ms	241.0 ms	241.0 ms	241.0 ms	241.0 ms	241.0 ms	241.0 ms
15/08/21 09:05:29 INFO StatsReportListener: task result size:(count: 1, mean: 577.000000, stdev: 0.000000, max: 577.000000, min: 577.000000)
15/08/21 09:05:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:29 INFO StatsReportListener: 	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B	577.0 B
15/08/21 09:05:29 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 75.103734, stdev: 0.000000, max: 75.103734, min: 75.103734)
15/08/21 09:05:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:29 INFO StatsReportListener: 	75 %	75 %	75 %	75 %	75 %	75 %	75 %	75 %	75 %
15/08/21 09:05:29 INFO StatsReportListener: other time pct: (count: 1, mean: 24.896266, stdev: 0.000000, max: 24.896266, min: 24.896266)
15/08/21 09:05:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:29 INFO StatsReportListener: 	25 %	25 %	25 %	25 %	25 %	25 %	25 %	25 %	25 %
15/08/21 09:05:29 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 09:05:29 INFO DefaultWriterContainer: Job job_201508210905_0000 committed.
15/08/21 09:05:29 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/21 09:05:29 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q18_large_volume_customer_par/_common_metadata
15/08/21 09:05:29 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/21 09:05:29 INFO DAGScheduler: Got job 4 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/21 09:05:29 INFO DAGScheduler: Final stage: ResultStage 10(processCmd at CliDriver.java:423)
15/08/21 09:05:29 INFO DAGScheduler: Parents of final stage: List()
15/08/21 09:05:29 INFO DAGScheduler: Missing parents: List()
15/08/21 09:05:29 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423), which has no missing parents
15/08/21 09:05:29 INFO MemoryStore: ensureFreeSpace(3104) called with curMem=1519643, maxMem=22226833244
15/08/21 09:05:29 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 3.0 KB, free 20.7 GB)
15/08/21 09:05:29 INFO MemoryStore: ensureFreeSpace(1802) called with curMem=1522747, maxMem=22226833244
15/08/21 09:05:29 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1802.0 B, free 20.7 GB)
15/08/21 09:05:29 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:51693 (size: 1802.0 B, free: 20.7 GB)
15/08/21 09:05:29 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:874
15/08/21 09:05:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at processCmd at CliDriver.java:423)
15/08/21 09:05:29 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
15/08/21 09:05:29 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 1218, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/21 09:05:29 INFO Executor: Running task 0.0 in stage 10.0 (TID 1218)
15/08/21 09:05:29 INFO Executor: Finished task 0.0 in stage 10.0 (TID 1218). 606 bytes result sent to driver
15/08/21 09:05:29 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 1218) in 16 ms on localhost (1/1)
15/08/21 09:05:29 INFO DAGScheduler: ResultStage 10 (processCmd at CliDriver.java:423) finished in 0.016 s
15/08/21 09:05:29 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/21 09:05:29 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@285e3f1e
15/08/21 09:05:29 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:423, took 0.033860 s
15/08/21 09:05:29 INFO StatsReportListener: task runtime:(count: 1, mean: 16.000000, stdev: 0.000000, max: 16.000000, min: 16.000000)
15/08/21 09:05:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:29 INFO StatsReportListener: 	16.0 ms	16.0 ms	16.0 ms	16.0 ms	16.0 ms	16.0 ms	16.0 ms	16.0 ms	16.0 ms
15/08/21 09:05:29 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/21 09:05:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:29 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
Time taken: 545.48 seconds
15/08/21 09:05:29 INFO CliDriver: Time taken: 545.48 seconds
15/08/21 09:05:29 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 6.250000, stdev: 0.000000, max: 6.250000, min: 6.250000)
15/08/21 09:05:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:29 INFO StatsReportListener: 	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %
15/08/21 09:05:29 INFO StatsReportListener: other time pct: (count: 1, mean: 93.750000, stdev: 0.000000, max: 93.750000, min: 93.750000)
15/08/21 09:05:29 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/21 09:05:29 INFO StatsReportListener: 	94 %	94 %	94 %	94 %	94 %	94 %	94 %	94 %	94 %
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/21 09:05:29 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/21 09:05:29 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/21 09:05:29 INFO DAGScheduler: Stopping DAGScheduler
15/08/21 09:05:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/21 09:05:29 INFO Utils: path = /tmp/spark-0c40987a-aced-4d32-8eb2-2707f5ab193e/blockmgr-0d41c176-1df1-4415-8082-ee73248c575d, already present as root for deletion.
15/08/21 09:05:29 INFO MemoryStore: MemoryStore cleared
15/08/21 09:05:29 INFO BlockManager: BlockManager stopped
15/08/21 09:05:29 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/21 09:05:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/21 09:05:29 INFO SparkContext: Successfully stopped SparkContext
15/08/21 09:05:29 INFO Utils: Shutdown hook called
15/08/21 09:05:29 INFO Utils: Deleting directory /tmp/spark-0c40987a-aced-4d32-8eb2-2707f5ab193e
15/08/21 09:05:29 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/21 09:05:29 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/21 09:05:30 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/08/21 09:05:35 INFO Utils: Deleting directory /tmp/spark-8d24afb8-042d-4fe2-b005-489adb258038
15/08/21 09:05:35 INFO Utils: Deleting directory /tmp/spark-b5bc57da-4ba2-4fe7-a354-10b55146260a
