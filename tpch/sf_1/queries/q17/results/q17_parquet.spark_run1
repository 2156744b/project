 -- the query
insert into table lineitem_tmp_par
select 
  l_partkey as t_partkey, 0.2 * avg(l_quantity) as t_avg_quantity
from 
  lineitem_par
group by l_partkey;

insert into table q17_small_quantity_order_revenue_par
select
  sum(l_extendedprice) / 7.0 as avg_yearly
from
  (select l_quantity, l_extendedprice, t_avg_quantity from
   lineitem_tmp_par t join
     (select
        l_quantity, l_partkey, l_extendedprice
      from
        part_par p join lineitem_par l
        on
          p.p_partkey = l.l_partkey
          and p.p_brand = 'Brand#54'
          and p.p_container = 'SM BAG'
      ) l1 on l1.l_partkey = t.t_partkey
   ) a
where l_quantity < t_avg_quantity;
15/08/15 19:21:48 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/15 19:21:48 INFO metastore: Connected to metastore.
15/08/15 19:21:49 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/15 19:21:49 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:49 INFO SparkContext: Running Spark version 1.4.1
15/08/15 19:21:49 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:49 INFO SecurityManager: Changing view acls to: hive
15/08/15 19:21:49 INFO SecurityManager: Changing modify acls to: hive
15/08/15 19:21:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/15 19:21:50 INFO Slf4jLogger: Slf4jLogger started
15/08/15 19:21:50 INFO Remoting: Starting remoting
15/08/15 19:21:50 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.122.56:40591]
15/08/15 19:21:50 INFO Utils: Successfully started service 'sparkDriver' on port 40591.
15/08/15 19:21:50 INFO SparkEnv: Registering MapOutputTracker
15/08/15 19:21:50 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:50 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:50 INFO SparkEnv: Registering BlockManagerMaster
15/08/15 19:21:50 INFO DiskBlockManager: Created local directory at /tmp/spark-350ecfc2-4e22-48a1-8f61-561c2d9257a9/blockmgr-d1896467-952a-4412-ab10-f9c82143f9d8
15/08/15 19:21:50 INFO MemoryStore: MemoryStore started with capacity 3.1 GB
15/08/15 19:21:50 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:50 INFO HttpFileServer: HTTP File server directory is /tmp/spark-350ecfc2-4e22-48a1-8f61-561c2d9257a9/httpd-467e58c1-c53a-4dd0-b3ae-b1aef0b673bd
15/08/15 19:21:50 INFO HttpServer: Starting HTTP Server
15/08/15 19:21:50 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/15 19:21:50 INFO AbstractConnector: Started SocketConnector@0.0.0.0:42640
15/08/15 19:21:50 INFO Utils: Successfully started service 'HTTP file server' on port 42640.
15/08/15 19:21:50 INFO SparkEnv: Registering OutputCommitCoordinator
15/08/15 19:21:51 INFO Server: jetty-8.y.z-SNAPSHOT
15/08/15 19:21:51 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/15 19:21:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/15 19:21:51 INFO SparkUI: Started SparkUI at http://192.168.122.56:4040
15/08/15 19:21:51 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:51 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:51 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:51 INFO Executor: Starting executor ID driver on host localhost
15/08/15 19:21:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50234.
15/08/15 19:21:51 INFO NettyBlockTransferService: Server created on 50234
15/08/15 19:21:51 INFO BlockManagerMaster: Trying to register BlockManager
15/08/15 19:21:51 INFO BlockManagerMasterEndpoint: Registering block manager localhost:50234 with 3.1 GB RAM, BlockManagerId(driver, localhost, 50234)
15/08/15 19:21:51 INFO BlockManagerMaster: Registered BlockManager
15/08/15 19:21:51 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.
15/08/15 19:21:51 INFO HiveContext: Initializing execution hive, version 0.13.1
15/08/15 19:21:51 INFO HiveContext: Initializing HiveMetastoreConnection version 0.13.1 using Spark classes.
15/08/15 19:21:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/15 19:21:52 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/15 19:21:52 INFO metastore: Connected to metastore.
15/08/15 19:21:53 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/15 19:21:53 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
SET spark.sql.hive.version=0.13.1
SET spark.sql.hive.version=0.13.1
15/08/15 19:21:53 INFO ParseDriver: Parsing command: -- the query
insert into table lineitem_tmp_par
select 
  l_partkey as t_partkey, 0.2 * avg(l_quantity) as t_avg_quantity
from 
  lineitem_par
group by l_partkey
15/08/15 19:21:54 INFO ParseDriver: Parse Completed
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/15 19:21:56 INFO MemoryStore: ensureFreeSpace(257472) called with curMem=0, maxMem=3333968363
15/08/15 19:21:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 251.4 KB, free 3.1 GB)
15/08/15 19:21:56 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=257472, maxMem=3333968363
15/08/15 19:21:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/15 19:21:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:50234 (size: 22.3 KB, free: 3.1 GB)
15/08/15 19:21:56 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:423
15/08/15 19:21:56 INFO Exchange: Using SparkSqlSerializer2.
15/08/15 19:21:57 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/15 19:21:57 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/15 19:21:57 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/15 19:21:57 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/15 19:21:57 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/15 19:21:57 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/15 19:21:57 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:21:57 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/15 19:21:57 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/15 19:21:57 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/15 19:21:57 INFO DAGScheduler: Registering RDD 3 (processCmd at CliDriver.java:423)
15/08/15 19:21:57 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:423) with 200 output partitions (allowLocal=false)
15/08/15 19:21:57 INFO DAGScheduler: Final stage: ResultStage 1(processCmd at CliDriver.java:423)
15/08/15 19:21:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
15/08/15 19:21:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
15/08/15 19:21:57 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423), which has no missing parents
15/08/15 19:21:57 INFO MemoryStore: ensureFreeSpace(9560) called with curMem=280265, maxMem=3333968363
15/08/15 19:21:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 3.1 GB)
15/08/15 19:21:57 INFO MemoryStore: ensureFreeSpace(4724) called with curMem=289825, maxMem=3333968363
15/08/15 19:21:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 3.1 GB)
15/08/15 19:21:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:50234 (size: 4.6 KB, free: 3.1 GB)
15/08/15 19:21:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/08/15 19:21:57 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at processCmd at CliDriver.java:423)
15/08/15 19:21:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
15/08/15 19:21:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 1767 bytes)
15/08/15 19:21:57 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, ANY, 1769 bytes)
15/08/15 19:21:57 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, ANY, 1768 bytes)
15/08/15 19:21:57 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, ANY, 1767 bytes)
15/08/15 19:21:57 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, ANY, 1767 bytes)
15/08/15 19:21:57 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, ANY, 1769 bytes)
15/08/15 19:21:57 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, ANY, 1769 bytes)
15/08/15 19:21:57 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, ANY, 1768 bytes)
15/08/15 19:21:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/15 19:21:57 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/08/15 19:21:57 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
15/08/15 19:21:57 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/15 19:21:57 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/08/15 19:21:57 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
15/08/15 19:21:57 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
15/08/15 19:21:57 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 26485016 length: 26485016 hosts: [] requestedSchema: message root {
  optional int32 l_partkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 26576747 length: 26576747 hosts: [] requestedSchema: message root {
  optional int32 l_partkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 26505368 length: 26505368 hosts: [] requestedSchema: message root {
  optional int32 l_partkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 26235204 length: 26235204 hosts: [] requestedSchema: message root {
  optional int32 l_partkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 26210131 length: 26210131 hosts: [] requestedSchema: message root {
  optional int32 l_partkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 26234990 length: 26234990 hosts: [] requestedSchema: message root {
  optional int32 l_partkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 26243215 length: 26243215 hosts: [] requestedSchema: message root {
  optional int32 l_partkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:21:57 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 26536257 length: 26536257 hosts: [] requestedSchema: message root {
  optional int32 l_partkey;
  optional double l_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:21:57 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/15 19:21:57 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/15 19:21:57 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/15 19:21:57 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/15 19:21:57 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/15 19:21:57 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/15 19:21:57 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/15 19:21:57 INFO CodecPool: Got brand-new decompressor [.snappy]
15/08/15 19:22:03 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2125 bytes result sent to driver
15/08/15 19:22:03 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2125 bytes result sent to driver
15/08/15 19:22:03 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 2125 bytes result sent to driver
15/08/15 19:22:03 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2125 bytes result sent to driver
15/08/15 19:22:03 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 5600 ms on localhost (1/8)
15/08/15 19:22:03 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 5598 ms on localhost (2/8)
15/08/15 19:22:03 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5607 ms on localhost (3/8)
15/08/15 19:22:03 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 5607 ms on localhost (4/8)
15/08/15 19:22:03 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2125 bytes result sent to driver
15/08/15 19:22:03 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2125 bytes result sent to driver
15/08/15 19:22:03 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2125 bytes result sent to driver
15/08/15 19:22:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2125 bytes result sent to driver
15/08/15 19:22:03 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 5624 ms on localhost (5/8)
15/08/15 19:22:03 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 5636 ms on localhost (6/8)
15/08/15 19:22:03 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 5635 ms on localhost (7/8)
15/08/15 19:22:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5669 ms on localhost (8/8)
15/08/15 19:22:03 INFO DAGScheduler: ShuffleMapStage 0 (processCmd at CliDriver.java:423) finished in 5.687 s
15/08/15 19:22:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/15 19:22:03 INFO DAGScheduler: looking for newly runnable stages
15/08/15 19:22:03 INFO DAGScheduler: running: Set()
15/08/15 19:22:03 INFO DAGScheduler: waiting: Set(ResultStage 1)
15/08/15 19:22:03 INFO DAGScheduler: failed: Set()
15/08/15 19:22:03 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@69bc933c
15/08/15 19:22:03 INFO DAGScheduler: Missing parents for ResultStage 1: List()
15/08/15 19:22:03 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423), which is now runnable
15/08/15 19:22:03 INFO StatsReportListener: task runtime:(count: 8, mean: 5622.000000, stdev: 22.605309, max: 5669.000000, min: 5598.000000)
15/08/15 19:22:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:03 INFO StatsReportListener: 	5.6 s	5.6 s	5.6 s	5.6 s	5.6 s	5.6 s	5.7 s	5.7 s	5.7 s
15/08/15 19:22:03 INFO StatsReportListener: shuffle bytes written:(count: 8, mean: 2620496.750000, stdev: 1187.857394, max: 2623521.000000, min: 2619567.000000)
15/08/15 19:22:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:03 INFO StatsReportListener: 	2.5 MB	2.5 MB	2.5 MB	2.5 MB	2.5 MB	2.5 MB	2.5 MB	2.5 MB	2.5 MB
15/08/15 19:22:03 INFO StatsReportListener: task result size:(count: 8, mean: 2125.000000, stdev: 0.000000, max: 2125.000000, min: 2125.000000)
15/08/15 19:22:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:03 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/15 19:22:03 INFO StatsReportListener: executor (non-fetch) time pct: (count: 8, mean: 98.452225, stdev: 0.457940, max: 98.862020, min: 97.357143)
15/08/15 19:22:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:03 INFO StatsReportListener: 	97 %	97 %	97 %	98 %	99 %	99 %	99 %	99 %	99 %
15/08/15 19:22:03 INFO StatsReportListener: other time pct: (count: 8, mean: 1.547775, stdev: 0.457940, max: 2.642857, min: 1.137980)
15/08/15 19:22:03 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:03 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 1 %	 2 %	 3 %	 3 %	 3 %
15/08/15 19:22:03 INFO MemoryStore: ensureFreeSpace(80032) called with curMem=294549, maxMem=3333968363
15/08/15 19:22:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 78.2 KB, free 3.1 GB)
15/08/15 19:22:03 INFO MemoryStore: ensureFreeSpace(30629) called with curMem=374581, maxMem=3333968363
15/08/15 19:22:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.9 KB, free 3.1 GB)
15/08/15 19:22:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:50234 (size: 29.9 KB, free: 3.1 GB)
15/08/15 19:22:03 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
15/08/15 19:22:03 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at processCmd at CliDriver.java:423)
15/08/15 19:22:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 200 tasks
15/08/15 19:22:03 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 10, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 11, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 12, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 13, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 14, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 15, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 16, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 17, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 18, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 19, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 20, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 21, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 22, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 23, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:03 INFO Executor: Running task 1.0 in stage 1.0 (TID 9)
15/08/15 19:22:03 INFO Executor: Running task 3.0 in stage 1.0 (TID 11)
15/08/15 19:22:03 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
15/08/15 19:22:03 INFO Executor: Running task 4.0 in stage 1.0 (TID 12)
15/08/15 19:22:03 INFO Executor: Running task 7.0 in stage 1.0 (TID 15)
15/08/15 19:22:03 INFO Executor: Running task 6.0 in stage 1.0 (TID 14)
15/08/15 19:22:03 INFO Executor: Running task 8.0 in stage 1.0 (TID 16)
15/08/15 19:22:03 INFO Executor: Running task 5.0 in stage 1.0 (TID 13)
15/08/15 19:22:03 INFO Executor: Running task 2.0 in stage 1.0 (TID 10)
15/08/15 19:22:03 INFO Executor: Running task 10.0 in stage 1.0 (TID 18)
15/08/15 19:22:03 INFO Executor: Running task 11.0 in stage 1.0 (TID 19)
15/08/15 19:22:03 INFO Executor: Running task 12.0 in stage 1.0 (TID 20)
15/08/15 19:22:03 INFO Executor: Running task 9.0 in stage 1.0 (TID 17)
15/08/15 19:22:03 INFO Executor: Running task 13.0 in stage 1.0 (TID 21)
15/08/15 19:22:03 INFO Executor: Running task 14.0 in stage 1.0 (TID 22)
15/08/15 19:22:03 INFO Executor: Running task 15.0 in stage 1.0 (TID 23)
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15-Aug-2015 19:21:55 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
15-Aug-2015 19:21:55 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
15-Aug-2015 19:21:55 INFO: parquet.hadoop.ParquetFileReader: reading another 8 footers
15-Aug-2015 19:21:55 INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
15-Aug-2015 19:21:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15-Aug-2015 19:21:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15-Aug-2015 19:21:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15-Aug-2015 19:21:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15-Aug-2015 19:21:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15-Aug-2015 19:21:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15-Aug-2015 19:21:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15-Aug-2015 19:21:57 WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749056 records.
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 755812 records.
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749157 records.
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 751036 records.
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749050 records.
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749096 records.
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 748901 records.
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 749107 records.
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 749050
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 751036
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 749096
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 748901
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 749107
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 755812
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 749157
15-Aug-2015 19:21:57 INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 89 ms. row count = 749056
15-Aug-2015 19:22:03 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
15-Aug-2015 19:22:03 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
15-Aug-2015 19:22:03 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
15-Aug-2015 19:22:03 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
15-Aug-2015 19:22:03 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
15-Aug-2015 19:22:03 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
15-Aug-2015 19:22:03 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.codec.CodecConfig: Compression: GZIP
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Dictionary is on
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetOutputFormat: Validation is off
15-Aug-2015 19:22:03 INFO: parquet.hadoop.ParquetO15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:03 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:03 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:03 INFO ZlibFactory: Successfully loaded & initialized native-zlib library
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:03 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,012
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,068
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,148
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,164
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,028
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,172
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,924
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,980
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,180
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,220
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,148
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,276
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,751B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,715B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,749B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,713B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,752B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,716B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,742B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,706B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,916B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,872B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,002B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,958B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,080B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,036B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,982B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,938B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,030B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,986B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,752B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,716B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 831 entries, 6,648B raw, 831B comp}
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,765B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,729B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,751B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,715B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 838 entries, 6,704B raw, 838B comp}
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 842 entries, 6,736B raw, 842B comp}
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,977B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,933B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,955B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,911B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,979B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,935B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,993B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,949B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,969B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,925B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,044B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,000B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,970B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,926B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,027B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,983B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000004_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000004
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000009_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000009
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000013_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000013
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000007_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000007
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000013_0: Committed
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000009_0: Committed
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000000
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000000_0: Committed
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000003_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000003
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000003_0: Committed
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000010_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000010
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000010_0: Committed
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000004_0: Committed
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000012_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000012
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000002_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000002
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000006_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000006
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000002_0: Committed
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000012_0: Committed
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000014_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000014
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000014_0: Committed
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000007_0: Committed
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000006_0: Committed
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000011_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000011
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000015_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000015
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000011_0: Committed
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000015_0: Committed
15/08/15 19:22:04 INFO Executor: Finished task 13.0 in stage 1.0 (TID 21). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 4.0 in stage 1.0 (TID 12). 843 bytes result sent to driver
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000001_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000001
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000001_0: Committed
15/08/15 19:22:04 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 24, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 16.0 in stage 1.0 (TID 24)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 25, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 8). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 15.0 in stage 1.0 (TID 23). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 11.0 in stage 1.0 (TID 19). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 26, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 18.0 in stage 1.0 (TID 26)
15/08/15 19:22:04 INFO Executor: Finished task 6.0 in stage 1.0 (TID 14). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Running task 17.0 in stage 1.0 (TID 25)
15/08/15 19:22:04 INFO Executor: Finished task 7.0 in stage 1.0 (TID 15). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 10.0 in stage 1.0 (TID 18). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 21) in 971 ms on localhost (1/200)
15/08/15 19:22:04 INFO Executor: Finished task 9.0 in stage 1.0 (TID 17). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 14.0 in stage 1.0 (TID 22). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 12.0 in stage 1.0 (TID 20). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 2.0 in stage 1.0 (TID 10). 843 bytes result sent to driver
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000008_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000008
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000005_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000005
15/08/15 19:22:04 INFO Executor: Finished task 3.0 in stage 1.0 (TID 11). 843 bytes result sent to driver
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000005_0: Committed
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000008_0: Committed
15/08/15 19:22:04 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 12) in 981 ms on localhost (2/200)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 27, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 19.0 in stage 1.0 (TID 27)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 28, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Finished task 1.0 in stage 1.0 (TID 9). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Running task 20.0 in stage 1.0 (TID 28)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 989 ms on localhost (3/200)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 29, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 21.0 in stage 1.0 (TID 29)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 30, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 22.0 in stage 1.0 (TID 30)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 19) in 983 ms on localhost (4/200)
15/08/15 19:22:04 INFO Executor: Finished task 8.0 in stage 1.0 (TID 16). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 5.0 in stage 1.0 (TID 13). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 31, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 23.0 in stage 1.0 (TID 31)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 23) in 983 ms on localhost (5/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 14) in 990 ms on localhost (6/200)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 32, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 24.0 in stage 1.0 (TID 32)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 33, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 25.0 in stage 1.0 (TID 33)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 15) in 995 ms on localhost (7/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 18) in 996 ms on localhost (8/200)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 34, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 26.0 in stage 1.0 (TID 34)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 35, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 27.0 in stage 1.0 (TID 35)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 36, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO Executor: Running task 28.0 in stage 1.0 (TID 36)
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 37, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 29.0 in stage 1.0 (TID 37)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 22) in 1001 ms on localhost (9/200)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 38, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 30.0 in stage 1.0 (TID 38)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 39, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 31.0 in stage 1.0 (TID 39)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 10) in 1017 ms on localhost (10/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 11) in 1017 ms on localhost (11/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 20) in 1012 ms on localhost (12/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 17) in 1016 ms on localhost (13/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 9) in 1027 ms on localhost (14/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 13) in 1029 ms on localhost (15/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 16) in 1028 ms on localhost (16/200)
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,990B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,946B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000016_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000016
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000016_0: Committed
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,172
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,065B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,021B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO Executor: Finished task 16.0 in stage 1.0 (TID 24). 843 bytes result sent to driver
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 40, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,036
15/08/15 19:22:04 INFO Executor: Running task 32.0 in stage 1.0 (TID 40)
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,929B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,885B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 24) in 572 ms on localhost (17/200)
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000025_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000025
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000025_0: Committed
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000018_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000018
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000018_0: Committed
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,196
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,060B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,016B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:04 INFO Executor: Finished task 18.0 in stage 1.0 (TID 26). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 33.0 in stage 1.0 (TID 41, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:04 INFO Executor: Running task 33.0 in stage 1.0 (TID 41)
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,116
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,068
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,749B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,713B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,050B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,006B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,918B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,874B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,945B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,901B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,024B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,980B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:04 INFO Executor: Finished task 25.0 in stage 1.0 (TID 33). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 34.0 in stage 1.0 (TID 42, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:50234 in memory (size: 4.6 KB, free: 3.1 GB)
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO Executor: Running task 34.0 in stage 1.0 (TID 42)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 26) in 629 ms on localhost (18/200)
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,076
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,204
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,974B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,930B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,975B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,931B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 33) in 625 ms on localhost (19/200)
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,028B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,984B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,188
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000021_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000021
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000021_0: Committed
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,018B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,974B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 5,096B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,052B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,751B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,715B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000031_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000031
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000031_0: Committed
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,985B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,941B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000024_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000024
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000024_0: Committed
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,076
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO Executor: Finished task 21.0 in stage 1.0 (TID 29). 843 bytes result sent to driver
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,924B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,880B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO Executor: Finished task 31.0 in stage 1.0 (TID 39). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 35.0 in stage 1.0 (TID 43, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 35.0 in stage 1.0 (TID 43)
15/08/15 19:22:04 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:04 INFO Executor: Finished task 24.0 in stage 1.0 (TID 32). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 36.0 in stage 1.0 (TID 44, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000023_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000023
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000023_0: Committed
15/08/15 19:22:04 INFO Executor: Running task 36.0 in stage 1.0 (TID 44)
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000020_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000020
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000020_0: Committed
15/08/15 19:22:04 INFO TaskSetManager: Starting task 37.0 in stage 1.0 (TID 45, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 37.0 in stage 1.0 (TID 45)
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO ColumnChunkPageWriteStore: written 4,975B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,931B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:04 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 39) in 657 ms on localhost (20/200)
15/08/15 19:22:04 INFO Executor: Finished task 20.0 in stage 1.0 (TID 28). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 38.0 in stage 1.0 (TID 46, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 38.0 in stage 1.0 (TID 46)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 32) in 677 ms on localhost (21/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 29) in 686 ms on localhost (22/200)
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000027_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000027
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000027_0: Committed
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000029_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000029
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000029_0: Committed
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000030_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000030
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000030_0: Committed
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000026_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000026
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000026_0: Committed
15/08/15 19:22:04 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 28) in 697 ms on localhost (23/200)
15/08/15 19:22:04 INFO Executor: Finished task 27.0 in stage 1.0 (TID 35). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 23.0 in stage 1.0 (TID 31). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 39.0 in stage 1.0 (TID 47, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 39.0 in stage 1.0 (TID 47)
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000022_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000022
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000022_0: Committed
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO TaskSetManager: Starting task 40.0 in stage 1.0 (TID 48, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/15 19:22:04 INFO Executor: Running task 40.0 in stage 1.0 (TID 48)
15/08/15 19:22:04 INFO Executor: Finished task 30.0 in stage 1.0 (TID 38). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Finished task 29.0 in stage 1.0 (TID 37). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 41.0 in stage 1.0 (TID 49, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Finished task 26.0 in stage 1.0 (TID 34). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Running task 41.0 in stage 1.0 (TID 49)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 42.0 in stage 1.0 (TID 50, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 42.0 in stage 1.0 (TID 50)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 43.0 in stage 1.0 (TID 51, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Finished task 22.0 in stage 1.0 (TID 30). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Starting task 44.0 in stage 1.0 (TID 52, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 44.0 in stage 1.0 (TID 52)
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000017_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000017
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000017_0: Committed
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 31) in 712 ms on localhost (24/200)
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 38) in 694 ms on localhost (25/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 35) in 702 ms on localhost (26/200)
15/08/15 19:22:04 INFO Executor: Running task 43.0 in stage 1.0 (TID 51)
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000019_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000019
15/08/15 19:22:04 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000028_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000028
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000019_0: Committed
15/08/15 19:22:04 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000028_0: Committed
15/08/15 19:22:04 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 37) in 701 ms on localhost (27/200)
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:04 INFO Executor: Finished task 17.0 in stage 1.0 (TID 25). 843 bytes result sent to driver
15/08/15 19:22:04 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 34) in 716 ms on localhost (28/200)
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO TaskSetManager: Starting task 45.0 in stage 1.0 (TID 53, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Finished task 19.0 in stage 1.0 (TID 27). 843 bytes result sent to driver
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO Executor: Finished task 28.0 in stage 1.0 (TID 36). 843 bytes result sent to driver
15/08/15 19:22:04 INFO Executor: Running task 45.0 in stage 1.0 (TID 53)
15/08/15 19:22:04 INFO TaskSetManager: Starting task 46.0 in stage 1.0 (TID 54, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:04 INFO Executor: Running task 46.0 in stage 1.0 (TID 54)
15/08/15 19:22:04 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:04 INFO TaskSetManager: Starting task 47.0 in stage 1.0 (TID 55, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:04 INFO Executor: Running task 47.0 in stage 1.0 (TID 55)
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:04 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:04 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:04 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:04 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 25) in 760 ms on localhost (29/200)
15/08/15 19:22:04 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 30) in 744 ms on localhost (30/200)
15/08/15 19:22:04 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 27) in 761 ms on localhost (31/200)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 36) in 748 ms on localhost (32/200)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,052
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,244
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,975B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,931B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,042B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,998B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,002B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,958B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,180
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000032_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000032
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000032_0: Committed
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000034_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000034
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000034_0: Committed
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,752B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,716B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,002B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,958B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO Executor: Finished task 34.0 in stage 1.0 (TID 42). 843 bytes result sent to driver
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO Executor: Finished task 32.0 in stage 1.0 (TID 40). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Starting task 48.0 in stage 1.0 (TID 56, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 48.0 in stage 1.0 (TID 56)
15/08/15 19:22:05 INFO TaskSetManager: Starting task 49.0 in stage 1.0 (TID 57, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,028
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO Executor: Running task 49.0 in stage 1.0 (TID 57)
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,750B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,714B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 40) in 694 ms on localhost (33/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,959B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,915B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 34.0 in stage 1.0 (TID 42) in 269 ms on localhost (34/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,920B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,876B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000037_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000037
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000037_0: Committed
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,116
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,844
15/08/15 19:22:05 INFO Executor: Finished task 37.0 in stage 1.0 (TID 45). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,767B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,731B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,011B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,967B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,004B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,960B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 50.0 in stage 1.0 (TID 58, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 821 entries, 6,568B raw, 821B comp}
15/08/15 19:22:05 INFO Executor: Running task 50.0 in stage 1.0 (TID 58)
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000035_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000035
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000035_0: Committed
15/08/15 19:22:05 INFO Executor: Finished task 35.0 in stage 1.0 (TID 43). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Starting task 51.0 in stage 1.0 (TID 59, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 51.0 in stage 1.0 (TID 59)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 37.0 in stage 1.0 (TID 45) in 280 ms on localhost (35/200)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 35.0 in stage 1.0 (TID 43) in 287 ms on localhost (36/200)
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000038_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000038
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000038_0: Committed
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000033_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000033
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000033_0: Committed
15/08/15 19:22:05 INFO Executor: Finished task 38.0 in stage 1.0 (TID 46). 843 bytes result sent to driver
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000042_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000042
15/08/15 19:22:05 INFO TaskSetManager: Starting task 52.0 in stage 1.0 (TID 60, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000042_0: Committed
15/08/15 19:22:05 INFO Executor: Finished task 33.0 in stage 1.0 (TID 41). 843 bytes result sent to driver
15/08/15 19:22:05 INFO Executor: Running task 52.0 in stage 1.0 (TID 60)
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000036_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000036
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000036_0: Committed
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000044_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000044
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000044_0: Committed
15/08/15 19:22:05 INFO TaskSetManager: Starting task 53.0 in stage 1.0 (TID 61, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Finished task 42.0 in stage 1.0 (TID 50). 843 bytes result sent to driver
15/08/15 19:22:05 INFO Executor: Running task 53.0 in stage 1.0 (TID 61)
15/08/15 19:22:05 INFO TaskSetManager: Starting task 54.0 in stage 1.0 (TID 62, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 54.0 in stage 1.0 (TID 62)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 38.0 in stage 1.0 (TID 46) in 287 ms on localhost (37/200)
15/08/15 19:22:05 INFO Executor: Finished task 36.0 in stage 1.0 (TID 44). 843 bytes result sent to driver
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,068
15/08/15 19:22:05 INFO Executor: Finished task 44.0 in stage 1.0 (TID 52). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,766B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,730B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,903B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,859B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 55.0 in stage 1.0 (TID 63, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 55.0 in stage 1.0 (TID 63)
15/08/15 19:22:05 INFO TaskSetManager: Starting task 56.0 in stage 1.0 (TID 64, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,044
15/08/15 19:22:05 INFO Executor: Running task 56.0 in stage 1.0 (TID 64)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,956B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,912B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO TaskSetManager: Finished task 33.0 in stage 1.0 (TID 41) in 390 ms on localhost (38/200)
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO TaskSetManager: Finished task 42.0 in stage 1.0 (TID 50) in 267 ms on localhost (39/200)
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,765B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,729B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,030B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,986B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,973B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,929B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,076
15/08/15 19:22:05 INFO TaskSetManager: Finished task 44.0 in stage 1.0 (TID 52) in 274 ms on localhost (40/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,749B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,713B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 36.0 in stage 1.0 (TID 44) in 324 ms on localhost (41/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,986B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,942B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,060
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,753B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,717B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,766B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,730B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,906B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,862B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,884B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,840B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000047_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000047
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000047_0: Committed
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000045_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000045
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000045_0: Committed
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000041_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000041
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000041_0: Committed
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000043_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000043
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000043_0: Committed
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000040_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000040
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000040_0: Committed
15/08/15 19:22:05 INFO Executor: Finished task 47.0 in stage 1.0 (TID 55). 843 bytes result sent to driver
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,052
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000039_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000039
15/08/15 19:22:05 INFO TaskSetManager: Starting task 57.0 in stage 1.0 (TID 65, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000039_0: Committed
15/08/15 19:22:05 INFO Executor: Running task 57.0 in stage 1.0 (TID 65)
15/08/15 19:22:05 INFO Executor: Finished task 45.0 in stage 1.0 (TID 53). 843 bytes result sent to driver
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000046_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000046
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000046_0: Committed
15/08/15 19:22:05 INFO TaskSetManager: Starting task 58.0 in stage 1.0 (TID 66, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 58.0 in stage 1.0 (TID 66)
15/08/15 19:22:05 INFO Executor: Finished task 41.0 in stage 1.0 (TID 49). 843 bytes result sent to driver
15/08/15 19:22:05 INFO Executor: Finished task 43.0 in stage 1.0 (TID 51). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO Executor: Finished task 40.0 in stage 1.0 (TID 48). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Finished task 47.0 in stage 1.0 (TID 55) in 297 ms on localhost (42/200)
15/08/15 19:22:05 INFO Executor: Finished task 39.0 in stage 1.0 (TID 47). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO TaskSetManager: Starting task 59.0 in stage 1.0 (TID 67, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 59.0 in stage 1.0 (TID 67)
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO Executor: Finished task 46.0 in stage 1.0 (TID 54). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO TaskSetManager: Starting task 60.0 in stage 1.0 (TID 68, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,971B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,927B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 61.0 in stage 1.0 (TID 69, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 60.0 in stage 1.0 (TID 68)
15/08/15 19:22:05 INFO Executor: Running task 61.0 in stage 1.0 (TID 69)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 45.0 in stage 1.0 (TID 53) in 317 ms on localhost (43/200)
15/08/15 19:22:05 INFO TaskSetManager: Starting task 62.0 in stage 1.0 (TID 70, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 62.0 in stage 1.0 (TID 70)
15/08/15 19:22:05 INFO TaskSetManager: Starting task 63.0 in stage 1.0 (TID 71, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 63.0 in stage 1.0 (TID 71)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 43.0 in stage 1.0 (TID 51) in 342 ms on localhost (44/200)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO TaskSetManager: Finished task 41.0 in stage 1.0 (TID 49) in 347 ms on localhost (45/200)
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 40.0 in stage 1.0 (TID 48) in 367 ms on localhost (46/200)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO TaskSetManager: Finished task 46.0 in stage 1.0 (TID 54) in 334 ms on localhost (47/200)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 39.0 in stage 1.0 (TID 47) in 374 ms on localhost (48/200)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,212
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,188
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,973B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,929B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,992B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,948B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,212
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,097B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,053B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,020
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,767B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,731B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 843 entries, 6,744B raw, 843B comp}
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000048_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000048
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000048_0: Committed
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000050_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000050
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000050_0: Committed
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,284
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000052_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000052
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000052_0: Committed
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO Executor: Finished task 50.0 in stage 1.0 (TID 58). 843 bytes result sent to driver
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO Executor: Finished task 48.0 in stage 1.0 (TID 56). 843 bytes result sent to driver
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO TaskSetManager: Starting task 64.0 in stage 1.0 (TID 72, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 64.0 in stage 1.0 (TID 72)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000051_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000051
15/08/15 19:22:05 INFO TaskSetManager: Starting task 65.0 in stage 1.0 (TID 73, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,140B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,096B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO Executor: Running task 65.0 in stage 1.0 (TID 73)
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000051_0: Committed
15/08/15 19:22:05 INFO Executor: Finished task 52.0 in stage 1.0 (TID 60). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO Executor: Finished task 51.0 in stage 1.0 (TID 59). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Starting task 66.0 in stage 1.0 (TID 74, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,769B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,733B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 67.0 in stage 1.0 (TID 75, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,025B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,981B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Running task 66.0 in stage 1.0 (TID 74)
15/08/15 19:22:05 INFO Executor: Running task 67.0 in stage 1.0 (TID 75)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 48.0 in stage 1.0 (TID 56) in 583 ms on localhost (49/200)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 50.0 in stage 1.0 (TID 58) in 554 ms on localhost (50/200)
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:05 INFO TaskSetManager: Finished task 52.0 in stage 1.0 (TID 60) in 498 ms on localhost (51/200)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 51.0 in stage 1.0 (TID 59) in 517 ms on localhost (52/200)
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,975B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,931B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,989B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,945B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000055_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000055
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000055_0: Committed
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000049_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000049
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000049_0: Committed
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,100
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000054_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000054
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000054_0: Committed
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,060
15/08/15 19:22:05 INFO Executor: Finished task 49.0 in stage 1.0 (TID 57). 843 bytes result sent to driver
15/08/15 19:22:05 INFO Executor: Finished task 55.0 in stage 1.0 (TID 63). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 68.0 in stage 1.0 (TID 76, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 68.0 in stage 1.0 (TID 76)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 69.0 in stage 1.0 (TID 77, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,046B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,002B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 49.0 in stage 1.0 (TID 57) in 623 ms on localhost (53/200)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000053_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000053
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO Executor: Finished task 54.0 in stage 1.0 (TID 62). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,988B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,944B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Running task 69.0 in stage 1.0 (TID 77)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO TaskSetManager: Starting task 70.0 in stage 1.0 (TID 78, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 70.0 in stage 1.0 (TID 78)
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,212
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,028
15/08/15 19:22:05 INFO TaskSetManager: Finished task 55.0 in stage 1.0 (TID 63) in 528 ms on localhost (54/200)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000053_0: Committed
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000056_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000056
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000056_0: Committed
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,031B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,987B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Finished task 53.0 in stage 1.0 (TID 61). 843 bytes result sent to driver
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Finished task 56.0 in stage 1.0 (TID 64). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,947B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,903B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:05 INFO TaskSetManager: Starting task 71.0 in stage 1.0 (TID 79, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Running task 71.0 in stage 1.0 (TID 79)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 54.0 in stage 1.0 (TID 62) in 546 ms on localhost (55/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,005B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,961B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000061_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000061
15/08/15 19:22:05 INFO TaskSetManager: Starting task 72.0 in stage 1.0 (TID 80, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Running task 72.0 in stage 1.0 (TID 80)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,002B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,958B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,748B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,712B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000061_0: Committed
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,010B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,966B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 53.0 in stage 1.0 (TID 61) in 555 ms on localhost (56/200)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO TaskSetManager: Finished task 56.0 in stage 1.0 (TID 64) in 551 ms on localhost (57/200)
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO Executor: Finished task 61.0 in stage 1.0 (TID 69). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000057_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000057
15/08/15 19:22:05 INFO TaskSetManager: Starting task 73.0 in stage 1.0 (TID 81, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000057_0: Committed
15/08/15 19:22:05 INFO Executor: Running task 73.0 in stage 1.0 (TID 81)
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 61.0 in stage 1.0 (TID 69) in 500 ms on localhost (58/200)
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000062_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000062
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000059_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000059
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000059_0: Committed
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000062_0: Committed
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,212
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO Executor: Finished task 57.0 in stage 1.0 (TID 65). 843 bytes result sent to driver
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000063_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000063
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000060_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000060
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000063_0: Committed
15/08/15 19:22:05 INFO TaskSetManager: Starting task 74.0 in stage 1.0 (TID 82, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000060_0: Committed
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Running task 74.0 in stage 1.0 (TID 82)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,046B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,002B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Finished task 59.0 in stage 1.0 (TID 67). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Starting task 75.0 in stage 1.0 (TID 83, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 75.0 in stage 1.0 (TID 83)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO Executor: Finished task 62.0 in stage 1.0 (TID 70). 843 bytes result sent to driver
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:05 INFO TaskSetManager: Starting task 76.0 in stage 1.0 (TID 84, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 59.0 in stage 1.0 (TID 67) in 521 ms on localhost (59/200)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO Executor: Running task 76.0 in stage 1.0 (TID 84)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO TaskSetManager: Finished task 57.0 in stage 1.0 (TID 65) in 534 ms on localhost (60/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,752B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,716B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Finished task 60.0 in stage 1.0 (TID 68). 843 bytes result sent to driver
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,902B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,858B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 77.0 in stage 1.0 (TID 85, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Finished task 63.0 in stage 1.0 (TID 71). 843 bytes result sent to driver
15/08/15 19:22:05 INFO Executor: Running task 77.0 in stage 1.0 (TID 85)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,951B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,907B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 62.0 in stage 1.0 (TID 70) in 515 ms on localhost (61/200)
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO TaskSetManager: Starting task 78.0 in stage 1.0 (TID 86, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 78.0 in stage 1.0 (TID 86)
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,988
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,020
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 839 entries, 6,712B raw, 839B comp}
15/08/15 19:22:05 INFO TaskSetManager: Finished task 60.0 in stage 1.0 (TID 68) in 538 ms on localhost (62/200)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,750B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,714B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 843 entries, 6,744B raw, 843B comp}
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO TaskSetManager: Finished task 63.0 in stage 1.0 (TID 71) in 530 ms on localhost (63/200)
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,020
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000064_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000064
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000064_0: Committed
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 843 entries, 6,744B raw, 843B comp}
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000065_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000065
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000065_0: Committed
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000066_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000066
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000066_0: Committed
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO Executor: Finished task 64.0 in stage 1.0 (TID 72). 843 bytes result sent to driver
15/08/15 19:22:05 INFO Executor: Finished task 65.0 in stage 1.0 (TID 73). 843 bytes result sent to driver
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000067_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000067
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000067_0: Committed
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000068_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000068
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000068_0: Committed
15/08/15 19:22:05 INFO TaskSetManager: Starting task 79.0 in stage 1.0 (TID 87, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Finished task 66.0 in stage 1.0 (TID 74). 843 bytes result sent to driver
15/08/15 19:22:05 INFO Executor: Running task 79.0 in stage 1.0 (TID 87)
15/08/15 19:22:05 INFO TaskSetManager: Starting task 80.0 in stage 1.0 (TID 88, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO TaskSetManager: Starting task 81.0 in stage 1.0 (TID 89, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,132
15/08/15 19:22:05 INFO Executor: Running task 81.0 in stage 1.0 (TID 89)
15/08/15 19:22:05 INFO Executor: Running task 80.0 in stage 1.0 (TID 88)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 5,027B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,983B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Finished task 65.0 in stage 1.0 (TID 73) in 184 ms on localhost (64/200)
15/08/15 19:22:05 INFO Executor: Finished task 67.0 in stage 1.0 (TID 75). 843 bytes result sent to driver
15/08/15 19:22:05 INFO Executor: Finished task 68.0 in stage 1.0 (TID 76). 843 bytes result sent to driver
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:05 INFO TaskSetManager: Starting task 82.0 in stage 1.0 (TID 90, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 82.0 in stage 1.0 (TID 90)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 64.0 in stage 1.0 (TID 72) in 190 ms on localhost (65/200)
15/08/15 19:22:05 INFO TaskSetManager: Finished task 66.0 in stage 1.0 (TID 74) in 181 ms on localhost (66/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 83.0 in stage 1.0 (TID 91, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,925B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,881B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Running task 83.0 in stage 1.0 (TID 91)
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:05 INFO TaskSetManager: Finished task 67.0 in stage 1.0 (TID 75) in 181 ms on localhost (67/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,765B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,729B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,986B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,942B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO TaskSetManager: Finished task 68.0 in stage 1.0 (TID 76) in 156 ms on localhost (68/200)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000070_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000070
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000070_0: Committed
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO Executor: Finished task 70.0 in stage 1.0 (TID 78). 843 bytes result sent to driver
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000069_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000069
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000072_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000072
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000072_0: Committed
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000069_0: Committed
15/08/15 19:22:05 INFO TaskSetManager: Starting task 84.0 in stage 1.0 (TID 92, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO Executor: Running task 84.0 in stage 1.0 (TID 92)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO Executor: Finished task 72.0 in stage 1.0 (TID 80). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Starting task 85.0 in stage 1.0 (TID 93, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 85.0 in stage 1.0 (TID 93)
15/08/15 19:22:05 INFO Executor: Finished task 69.0 in stage 1.0 (TID 77). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Finished task 70.0 in stage 1.0 (TID 78) in 166 ms on localhost (69/200)
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 86.0 in stage 1.0 (TID 94, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO Executor: Running task 86.0 in stage 1.0 (TID 94)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO TaskSetManager: Finished task 72.0 in stage 1.0 (TID 80) in 150 ms on localhost (70/200)
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO TaskSetManager: Finished task 69.0 in stage 1.0 (TID 77) in 185 ms on localhost (71/200)
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,172
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000071_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000071
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000071_0: Committed
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,748B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,712B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,989B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,945B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:05 INFO Executor: Finished task 71.0 in stage 1.0 (TID 79). 843 bytes result sent to driver
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 87.0 in stage 1.0 (TID 95, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 87.0 in stage 1.0 (TID 95)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,918B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,874B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO TaskSetManager: Finished task 71.0 in stage 1.0 (TID 79) in 183 ms on localhost (72/200)
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,972
15/08/15 19:22:05 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:05 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000073_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000073
15/08/15 19:22:05 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:05 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000073_0: Committed
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 837 entries, 6,696B raw, 837B comp}
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000075_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000075
15/08/15 19:22:05 INFO Executor: Finished task 73.0 in stage 1.0 (TID 81). 843 bytes result sent to driver
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000075_0: Committed
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,068
15/08/15 19:22:05 INFO TaskSetManager: Starting task 88.0 in stage 1.0 (TID 96, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 88.0 in stage 1.0 (TID 96)
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,932
15/08/15 19:22:05 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,945B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,901B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO Executor: Finished task 75.0 in stage 1.0 (TID 83). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Finished task 73.0 in stage 1.0 (TID 81) in 180 ms on localhost (73/200)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO TaskSetManager: Starting task 89.0 in stage 1.0 (TID 97, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 89.0 in stage 1.0 (TID 97)
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 4,958B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,914B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:05 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 832 entries, 6,656B raw, 832B comp}
15/08/15 19:22:05 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000074_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000074
15/08/15 19:22:05 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000074_0: Committed
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO Executor: Finished task 74.0 in stage 1.0 (TID 82). 843 bytes result sent to driver
15/08/15 19:22:05 INFO TaskSetManager: Starting task 90.0 in stage 1.0 (TID 98, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:05 INFO Executor: Running task 90.0 in stage 1.0 (TID 98)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:05 INFO TaskSetManager: Finished task 75.0 in stage 1.0 (TID 83) in 188 ms on localhost (74/200)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:05 INFO TaskSetManager: Finished task 74.0 in stage 1.0 (TID 82) in 199 ms on localhost (75/200)
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000076_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000076
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000076_0: Committed
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000078_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000078
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000077_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000077
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000078_0: Committed
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000077_0: Committed
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO Executor: Finished task 76.0 in stage 1.0 (TID 84). 843 bytes result sent to driver
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO Executor: Finished task 78.0 in stage 1.0 (TID 86). 843 bytes result sent to driver
15/08/15 19:22:06 INFO Executor: Finished task 77.0 in stage 1.0 (TID 85). 843 bytes result sent to driver
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO TaskSetManager: Starting task 91.0 in stage 1.0 (TID 99, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO Executor: Running task 91.0 in stage 1.0 (TID 99)
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO TaskSetManager: Starting task 92.0 in stage 1.0 (TID 100, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO Executor: Running task 92.0 in stage 1.0 (TID 100)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO TaskSetManager: Starting task 93.0 in stage 1.0 (TID 101, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO TaskSetManager: Finished task 76.0 in stage 1.0 (TID 84) in 215 ms on localhost (76/200)
15/08/15 19:22:06 INFO Executor: Running task 93.0 in stage 1.0 (TID 101)
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,100
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO TaskSetManager: Finished task 78.0 in stage 1.0 (TID 86) in 206 ms on localhost (77/200)
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,747B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,711B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,937B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,893B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO TaskSetManager: Finished task 77.0 in stage 1.0 (TID 85) in 219 ms on localhost (78/200)
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,188
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,750B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,714B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 5,014B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,970B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,036
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,750B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,714B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,228
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,967B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,923B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,148
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,841B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,797B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 5,012B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,968B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 5,022B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,978B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,148
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000081_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000081
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000081_0: Committed
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,979B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,935B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO Executor: Finished task 81.0 in stage 1.0 (TID 89). 843 bytes result sent to driver
15/08/15 19:22:06 INFO TaskSetManager: Starting task 94.0 in stage 1.0 (TID 102, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 94.0 in stage 1.0 (TID 102)
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000079_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000079
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000079_0: Committed
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,180
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000085_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000085
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000085_0: Committed
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000082_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000082
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000083_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000083
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000082_0: Committed
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 5,008B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,964B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO TaskSetManager: Finished task 81.0 in stage 1.0 (TID 89) in 223 ms on localhost (79/200)
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000083_0: Committed
15/08/15 19:22:06 INFO Executor: Finished task 79.0 in stage 1.0 (TID 87). 843 bytes result sent to driver
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000080_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000080
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000080_0: Committed
15/08/15 19:22:06 INFO TaskSetManager: Starting task 95.0 in stage 1.0 (TID 103, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 95.0 in stage 1.0 (TID 103)
15/08/15 19:22:06 INFO TaskSetManager: Finished task 79.0 in stage 1.0 (TID 87) in 433 ms on localhost (80/200)
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,188
15/08/15 19:22:06 INFO Executor: Finished task 83.0 in stage 1.0 (TID 91). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,753B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,717B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,888B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,844B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO TaskSetManager: Starting task 96.0 in stage 1.0 (TID 104, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Finished task 85.0 in stage 1.0 (TID 93). 843 bytes result sent to driver
15/08/15 19:22:06 INFO Executor: Running task 96.0 in stage 1.0 (TID 104)
15/08/15 19:22:06 INFO TaskSetManager: Starting task 97.0 in stage 1.0 (TID 105, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Finished task 80.0 in stage 1.0 (TID 88). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO TaskSetManager: Starting task 98.0 in stage 1.0 (TID 106, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 98.0 in stage 1.0 (TID 106)
15/08/15 19:22:06 INFO Executor: Running task 97.0 in stage 1.0 (TID 105)
15/08/15 19:22:06 INFO Executor: Finished task 82.0 in stage 1.0 (TID 90). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO TaskSetManager: Finished task 85.0 in stage 1.0 (TID 93) in 392 ms on localhost (81/200)
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000058_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000058
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000058_0: Committed
15/08/15 19:22:06 INFO TaskSetManager: Starting task 99.0 in stage 1.0 (TID 107, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO TaskSetManager: Finished task 83.0 in stage 1.0 (TID 91) in 437 ms on localhost (82/200)
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000084_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000084
15/08/15 19:22:06 INFO TaskSetManager: Finished task 80.0 in stage 1.0 (TID 88) in 455 ms on localhost (83/200)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000084_0: Committed
15/08/15 19:22:06 INFO Executor: Running task 99.0 in stage 1.0 (TID 107)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000086_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000086
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000086_0: Committed
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO Executor: Finished task 84.0 in stage 1.0 (TID 92). 843 bytes result sent to driver
15/08/15 19:22:06 INFO Executor: Finished task 58.0 in stage 1.0 (TID 66). 843 bytes result sent to driver
15/08/15 19:22:06 INFO TaskSetManager: Finished task 82.0 in stage 1.0 (TID 90) in 443 ms on localhost (84/200)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO TaskSetManager: Starting task 100.0 in stage 1.0 (TID 108, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO Executor: Running task 100.0 in stage 1.0 (TID 108)
15/08/15 19:22:06 INFO TaskSetManager: Starting task 101.0 in stage 1.0 (TID 109, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Finished task 86.0 in stage 1.0 (TID 94). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO Executor: Running task 101.0 in stage 1.0 (TID 109)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO TaskSetManager: Finished task 84.0 in stage 1.0 (TID 92) in 428 ms on localhost (85/200)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO TaskSetManager: Finished task 58.0 in stage 1.0 (TID 66) in 1049 ms on localhost (86/200)
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO TaskSetManager: Starting task 102.0 in stage 1.0 (TID 110, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 102.0 in stage 1.0 (TID 110)
15/08/15 19:22:06 INFO TaskSetManager: Finished task 86.0 in stage 1.0 (TID 94) in 432 ms on localhost (87/200)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,148
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,994B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,950B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,044
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,752B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,716B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000087_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000087
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,745B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,709B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000087_0: Committed
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,980B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,936B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,244
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 5,020B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,976B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,204
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,956
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,991B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,947B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO Executor: Finished task 87.0 in stage 1.0 (TID 95). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO TaskSetManager: Starting task 103.0 in stage 1.0 (TID 111, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,997B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,953B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO Executor: Running task 103.0 in stage 1.0 (TID 111)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,753B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,717B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 835 entries, 6,680B raw, 835B comp}
15/08/15 19:22:06 INFO TaskSetManager: Finished task 87.0 in stage 1.0 (TID 95) in 460 ms on localhost (88/200)
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000088_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000088
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000088_0: Committed
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000091_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000091
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000091_0: Committed
15/08/15 19:22:06 INFO Executor: Finished task 88.0 in stage 1.0 (TID 96). 843 bytes result sent to driver
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000089_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000089
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000089_0: Committed
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000090_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000090
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000090_0: Committed
15/08/15 19:22:06 INFO TaskSetManager: Starting task 104.0 in stage 1.0 (TID 112, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 104.0 in stage 1.0 (TID 112)
15/08/15 19:22:06 INFO Executor: Finished task 89.0 in stage 1.0 (TID 97). 843 bytes result sent to driver
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000093_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000093
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO Executor: Finished task 90.0 in stage 1.0 (TID 98). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000092_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000092
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO Executor: Finished task 91.0 in stage 1.0 (TID 99). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000092_0: Committed
15/08/15 19:22:06 INFO TaskSetManager: Starting task 105.0 in stage 1.0 (TID 113, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000093_0: Committed
15/08/15 19:22:06 INFO TaskSetManager: Finished task 88.0 in stage 1.0 (TID 96) in 470 ms on localhost (89/200)
15/08/15 19:22:06 INFO TaskSetManager: Starting task 106.0 in stage 1.0 (TID 114, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 105.0 in stage 1.0 (TID 113)
15/08/15 19:22:06 INFO Executor: Running task 106.0 in stage 1.0 (TID 114)
15/08/15 19:22:06 INFO TaskSetManager: Starting task 107.0 in stage 1.0 (TID 115, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO Executor: Running task 107.0 in stage 1.0 (TID 115)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO Executor: Finished task 93.0 in stage 1.0 (TID 101). 843 bytes result sent to driver
15/08/15 19:22:06 INFO TaskSetManager: Finished task 91.0 in stage 1.0 (TID 99) in 423 ms on localhost (90/200)
15/08/15 19:22:06 INFO TaskSetManager: Starting task 108.0 in stage 1.0 (TID 116, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 108.0 in stage 1.0 (TID 116)
15/08/15 19:22:06 INFO TaskSetManager: Finished task 90.0 in stage 1.0 (TID 98) in 463 ms on localhost (91/200)
15/08/15 19:22:06 INFO TaskSetManager: Finished task 89.0 in stage 1.0 (TID 97) in 475 ms on localhost (92/200)
15/08/15 19:22:06 INFO Executor: Finished task 92.0 in stage 1.0 (TID 100). 843 bytes result sent to driver
15/08/15 19:22:06 INFO TaskSetManager: Starting task 109.0 in stage 1.0 (TID 117, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO TaskSetManager: Finished task 93.0 in stage 1.0 (TID 101) in 426 ms on localhost (93/200)
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO TaskSetManager: Finished task 92.0 in stage 1.0 (TID 100) in 430 ms on localhost (94/200)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO Executor: Running task 109.0 in stage 1.0 (TID 117)
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,052
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,927B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,883B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,036
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,076
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,977B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,933B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 5,020B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,976B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,060
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000100_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000100
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000100_0: Committed
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,920B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,876B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000096_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000096
15/08/15 19:22:06 INFO Executor: Finished task 100.0 in stage 1.0 (TID 108). 843 bytes result sent to driver
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000096_0: Committed
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO TaskSetManager: Starting task 110.0 in stage 1.0 (TID 118, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 110.0 in stage 1.0 (TID 118)
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,068
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,260
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,753B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,717B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 5,034B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,990B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,978B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,934B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,068
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO Executor: Finished task 96.0 in stage 1.0 (TID 104). 843 bytes result sent to driver
15/08/15 19:22:06 INFO TaskSetManager: Finished task 100.0 in stage 1.0 (TID 108) in 208 ms on localhost (95/200)
15/08/15 19:22:06 INFO TaskSetManager: Starting task 111.0 in stage 1.0 (TID 119, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,962B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,918B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO Executor: Running task 111.0 in stage 1.0 (TID 119)
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO TaskSetManager: Finished task 96.0 in stage 1.0 (TID 104) in 247 ms on localhost (96/200)
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,068
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 5,022B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,978B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000098_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000098
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000098_0: Committed
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO Executor: Finished task 98.0 in stage 1.0 (TID 106). 843 bytes result sent to driver
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO TaskSetManager: Starting task 112.0 in stage 1.0 (TID 120, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO Executor: Running task 112.0 in stage 1.0 (TID 120)
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 271 ms
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,940
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO TaskSetManager: Finished task 98.0 in stage 1.0 (TID 106) in 563 ms on localhost (97/200)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 833 entries, 6,664B raw, 833B comp}
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000095_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000095
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000095_0: Committed
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,044
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000097_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000097
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000094_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000094
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,963B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,919B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO Executor: Finished task 95.0 in stage 1.0 (TID 103). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO TaskSetManager: Starting task 113.0 in stage 1.0 (TID 121, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 113.0 in stage 1.0 (TID 121)
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000097_0: Committed
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000094_0: Committed
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000099_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000099
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000099_0: Committed
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO TaskSetManager: Finished task 95.0 in stage 1.0 (TID 103) in 792 ms on localhost (98/200)
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,052
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000102_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000102
15/08/15 19:22:06 INFO Executor: Finished task 97.0 in stage 1.0 (TID 105). 843 bytes result sent to driver
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000102_0: Committed
15/08/15 19:22:06 INFO Executor: Finished task 94.0 in stage 1.0 (TID 102). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,752B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,716B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,904B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,860B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO Executor: Finished task 99.0 in stage 1.0 (TID 107). 843 bytes result sent to driver
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO TaskSetManager: Starting task 114.0 in stage 1.0 (TID 122, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO Executor: Finished task 102.0 in stage 1.0 (TID 110). 843 bytes result sent to driver
15/08/15 19:22:06 INFO TaskSetManager: Starting task 115.0 in stage 1.0 (TID 123, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO Executor: Running task 114.0 in stage 1.0 (TID 122)
15/08/15 19:22:06 INFO TaskSetManager: Starting task 116.0 in stage 1.0 (TID 124, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 115.0 in stage 1.0 (TID 123)
15/08/15 19:22:06 INFO Executor: Running task 116.0 in stage 1.0 (TID 124)
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO TaskSetManager: Starting task 117.0 in stage 1.0 (TID 125, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 117.0 in stage 1.0 (TID 125)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO TaskSetManager: Finished task 97.0 in stage 1.0 (TID 105) in 592 ms on localhost (99/200)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,020
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO TaskSetManager: Finished task 94.0 in stage 1.0 (TID 102) in 815 ms on localhost (100/200)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO TaskSetManager: Finished task 99.0 in stage 1.0 (TID 107) in 579 ms on localhost (101/200)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO TaskSetManager: Finished task 102.0 in stage 1.0 (TID 110) in 554 ms on localhost (102/200)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 843 entries, 6,744B raw, 843B comp}
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000106_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000106
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000106_0: Committed
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO Executor: Finished task 106.0 in stage 1.0 (TID 114). 843 bytes result sent to driver
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO TaskSetManager: Starting task 118.0 in stage 1.0 (TID 126, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 118.0 in stage 1.0 (TID 126)
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000101_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000101
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000101_0: Committed
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO TaskSetManager: Finished task 106.0 in stage 1.0 (TID 114) in 482 ms on localhost (103/200)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,020
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000103_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000103
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000103_0: Committed
15/08/15 19:22:06 INFO Executor: Finished task 101.0 in stage 1.0 (TID 109). 843 bytes result sent to driver
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,148
15/08/15 19:22:06 INFO TaskSetManager: Starting task 119.0 in stage 1.0 (TID 127, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO Executor: Running task 119.0 in stage 1.0 (TID 127)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 843 entries, 6,744B raw, 843B comp}
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,984B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,940B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO TaskSetManager: Finished task 101.0 in stage 1.0 (TID 109) in 599 ms on localhost (104/200)
15/08/15 19:22:06 INFO Executor: Finished task 103.0 in stage 1.0 (TID 111). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000104_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000104
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000104_0: Committed
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO TaskSetManager: Starting task 120.0 in stage 1.0 (TID 128, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,932
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,982B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,938B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,751B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,715B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 832 entries, 6,656B raw, 832B comp}
15/08/15 19:22:06 INFO Executor: Finished task 104.0 in stage 1.0 (TID 112). 843 bytes result sent to driver
15/08/15 19:22:06 INFO Executor: Running task 120.0 in stage 1.0 (TID 128)
15/08/15 19:22:06 INFO TaskSetManager: Finished task 103.0 in stage 1.0 (TID 111) in 544 ms on localhost (105/200)
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO TaskSetManager: Starting task 121.0 in stage 1.0 (TID 129, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 121.0 in stage 1.0 (TID 129)
15/08/15 19:22:06 INFO TaskSetManager: Finished task 104.0 in stage 1.0 (TID 112) in 528 ms on localhost (106/200)
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,148
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,993B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,949B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000107_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000107
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000107_0: Committed
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000109_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000109
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000109_0: Committed
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000108_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000108
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000108_0: Committed
15/08/15 19:22:06 INFO Executor: Finished task 107.0 in stage 1.0 (TID 115). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO TaskSetManager: Starting task 122.0 in stage 1.0 (TID 130, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Finished task 108.0 in stage 1.0 (TID 116). 843 bytes result sent to driver
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:06 INFO TaskSetManager: Starting task 123.0 in stage 1.0 (TID 131, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO Executor: Running task 123.0 in stage 1.0 (TID 131)
15/08/15 19:22:06 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000105_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000105
15/08/15 19:22:06 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000105_0: Committed
15/08/15 19:22:06 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,132
15/08/15 19:22:06 INFO Executor: Finished task 109.0 in stage 1.0 (TID 117). 843 bytes result sent to driver
15/08/15 19:22:06 INFO TaskSetManager: Starting task 124.0 in stage 1.0 (TID 132, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO TaskSetManager: Finished task 108.0 in stage 1.0 (TID 116) in 538 ms on localhost (107/200)
15/08/15 19:22:06 INFO Executor: Running task 124.0 in stage 1.0 (TID 132)
15/08/15 19:22:06 INFO Executor: Finished task 105.0 in stage 1.0 (TID 113). 843 bytes result sent to driver
15/08/15 19:22:06 INFO Executor: Running task 122.0 in stage 1.0 (TID 130)
15/08/15 19:22:06 INFO TaskSetManager: Starting task 125.0 in stage 1.0 (TID 133, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO Executor: Running task 125.0 in stage 1.0 (TID 133)
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO TaskSetManager: Finished task 107.0 in stage 1.0 (TID 115) in 552 ms on localhost (108/200)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO TaskSetManager: Finished task 109.0 in stage 1.0 (TID 117) in 538 ms on localhost (109/200)
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO TaskSetManager: Finished task 105.0 in stage 1.0 (TID 113) in 566 ms on localhost (110/200)
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ColumnChunkPageWriteStore: written 4,952B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,908B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:06 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:06 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:06 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:06 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:06 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000111_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000111
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000111_0: Committed
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO Executor: Finished task 111.0 in stage 1.0 (TID 119). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 126.0 in stage 1.0 (TID 134, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:07 INFO Executor: Running task 126.0 in stage 1.0 (TID 134)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,899B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,855B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 111.0 in stage 1.0 (TID 119) in 483 ms on localhost (111/200)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000110_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000110
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000110_0: Committed
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,116
15/08/15 19:22:07 INFO Executor: Finished task 110.0 in stage 1.0 (TID 118). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 127.0 in stage 1.0 (TID 135, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 127.0 in stage 1.0 (TID 135)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,990B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,946B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,100
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO TaskSetManager: Finished task 110.0 in stage 1.0 (TID 118) in 530 ms on localhost (112/200)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,939B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,895B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,939B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,895B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,204
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,972
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,995B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,951B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000112_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000112
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000112_0: Committed
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,767B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,731B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 837 entries, 6,696B raw, 837B comp}
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO Executor: Finished task 112.0 in stage 1.0 (TID 120). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO TaskSetManager: Starting task 128.0 in stage 1.0 (TID 136, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 128.0 in stage 1.0 (TID 136)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000113_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000113
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000113_0: Committed
15/08/15 19:22:07 INFO TaskSetManager: Finished task 112.0 in stage 1.0 (TID 120) in 700 ms on localhost (113/200)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,236
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO Executor: Finished task 113.0 in stage 1.0 (TID 121). 843 bytes result sent to driver
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:07 INFO TaskSetManager: Starting task 129.0 in stage 1.0 (TID 137, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 129.0 in stage 1.0 (TID 137)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,753B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,717B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,994B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,950B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,922B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,878B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,765B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,729B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 113.0 in stage 1.0 (TID 121) in 407 ms on localhost (114/200)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,004B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,960B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000117_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000117
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,030B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,986B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000117_0: Committed
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000114_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000114
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000114_0: Committed
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,116
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO Executor: Finished task 117.0 in stage 1.0 (TID 125). 843 bytes result sent to driver
15/08/15 19:22:07 INFO Executor: Finished task 114.0 in stage 1.0 (TID 122). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 130.0 in stage 1.0 (TID 138, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,769B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,733B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Running task 130.0 in stage 1.0 (TID 138)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,051B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,007B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Starting task 131.0 in stage 1.0 (TID 139, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 131.0 in stage 1.0 (TID 139)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000116_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000116
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000116_0: Committed
15/08/15 19:22:07 INFO TaskSetManager: Finished task 117.0 in stage 1.0 (TID 125) in 409 ms on localhost (115/200)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000118_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000118
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000118_0: Committed
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,895B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,851B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 114.0 in stage 1.0 (TID 122) in 417 ms on localhost (116/200)
15/08/15 19:22:07 INFO Executor: Finished task 116.0 in stage 1.0 (TID 124). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 132.0 in stage 1.0 (TID 140, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 132.0 in stage 1.0 (TID 140)
15/08/15 19:22:07 INFO Executor: Finished task 118.0 in stage 1.0 (TID 126). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 133.0 in stage 1.0 (TID 141, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO Executor: Running task 133.0 in stage 1.0 (TID 141)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,228
15/08/15 19:22:07 INFO TaskSetManager: Finished task 116.0 in stage 1.0 (TID 124) in 421 ms on localhost (117/200)
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO TaskSetManager: Finished task 118.0 in stage 1.0 (TID 126) in 403 ms on localhost (118/200)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,994B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,950B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000115_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000115
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000115_0: Committed
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,976B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,932B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO Executor: Finished task 115.0 in stage 1.0 (TID 123). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 134.0 in stage 1.0 (TID 142, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 134.0 in stage 1.0 (TID 142)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000121_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000121
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000121_0: Committed
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 115.0 in stage 1.0 (TID 123) in 442 ms on localhost (119/200)
15/08/15 19:22:07 INFO Executor: Finished task 121.0 in stage 1.0 (TID 129). 843 bytes result sent to driver
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Starting task 135.0 in stage 1.0 (TID 143, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 135.0 in stage 1.0 (TID 143)
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO TaskSetManager: Finished task 121.0 in stage 1.0 (TID 129) in 405 ms on localhost (120/200)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000119_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000119
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000119_0: Committed
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,116
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,220
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO Executor: Finished task 119.0 in stage 1.0 (TID 127). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000125_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000125
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000125_0: Committed
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,954B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,910B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,949B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,905B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO Executor: Finished task 125.0 in stage 1.0 (TID 133). 843 bytes result sent to driver
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000123_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000123
15/08/15 19:22:07 INFO TaskSetManager: Starting task 136.0 in stage 1.0 (TID 144, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000123_0: Committed
15/08/15 19:22:07 INFO Executor: Running task 136.0 in stage 1.0 (TID 144)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO TaskSetManager: Finished task 119.0 in stage 1.0 (TID 127) in 444 ms on localhost (121/200)
15/08/15 19:22:07 INFO Executor: Finished task 123.0 in stage 1.0 (TID 131). 843 bytes result sent to driver
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Starting task 137.0 in stage 1.0 (TID 145, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 137.0 in stage 1.0 (TID 145)
15/08/15 19:22:07 INFO TaskSetManager: Starting task 138.0 in stage 1.0 (TID 146, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000120_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000120
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000120_0: Committed
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO TaskSetManager: Finished task 125.0 in stage 1.0 (TID 133) in 387 ms on localhost (122/200)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO TaskSetManager: Finished task 123.0 in stage 1.0 (TID 131) in 394 ms on localhost (123/200)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000122_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000122
15/08/15 19:22:07 INFO Executor: Running task 138.0 in stage 1.0 (TID 146)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000122_0: Committed
15/08/15 19:22:07 INFO Executor: Finished task 120.0 in stage 1.0 (TID 128). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 139.0 in stage 1.0 (TID 147, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 139.0 in stage 1.0 (TID 147)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000124_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000124
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000124_0: Committed
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,964
15/08/15 19:22:07 INFO Executor: Finished task 122.0 in stage 1.0 (TID 130). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO TaskSetManager: Starting task 140.0 in stage 1.0 (TID 148, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,012
15/08/15 19:22:07 INFO TaskSetManager: Finished task 120.0 in stage 1.0 (TID 128) in 452 ms on localhost (124/200)
15/08/15 19:22:07 INFO Executor: Running task 140.0 in stage 1.0 (TID 148)
15/08/15 19:22:07 INFO Executor: Finished task 124.0 in stage 1.0 (TID 132). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 836 entries, 6,688B raw, 836B comp}
15/08/15 19:22:07 INFO TaskSetManager: Starting task 141.0 in stage 1.0 (TID 149, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 141.0 in stage 1.0 (TID 149)
15/08/15 19:22:07 INFO TaskSetManager: Finished task 122.0 in stage 1.0 (TID 130) in 407 ms on localhost (125/200)
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO TaskSetManager: Finished task 124.0 in stage 1.0 (TID 132) in 403 ms on localhost (126/200)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 842 entries, 6,736B raw, 842B comp}
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000126_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000126
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000126_0: Committed
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,076
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO Executor: Finished task 126.0 in stage 1.0 (TID 134). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,030B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,986B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,004
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000129_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000129
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000127_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000127
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000129_0: Committed
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000127_0: Committed
15/08/15 19:22:07 INFO TaskSetManager: Starting task 142.0 in stage 1.0 (TID 150, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO Executor: Running task 142.0 in stage 1.0 (TID 150)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Finished task 127.0 in stage 1.0 (TID 135). 843 bytes result sent to driver
15/08/15 19:22:07 INFO Executor: Finished task 129.0 in stage 1.0 (TID 137). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Finished task 126.0 in stage 1.0 (TID 134) in 399 ms on localhost (127/200)
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO TaskSetManager: Starting task 143.0 in stage 1.0 (TID 151, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO TaskSetManager: Starting task 144.0 in stage 1.0 (TID 152, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO Executor: Running task 144.0 in stage 1.0 (TID 152)
15/08/15 19:22:07 INFO Executor: Running task 143.0 in stage 1.0 (TID 151)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 841 entries, 6,728B raw, 841B comp}
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 127.0 in stage 1.0 (TID 135) in 384 ms on localhost (128/200)
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO TaskSetManager: Finished task 129.0 in stage 1.0 (TID 137) in 146 ms on localhost (129/200)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000128_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000128
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000128_0: Committed
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,228
15/08/15 19:22:07 INFO Executor: Finished task 128.0 in stage 1.0 (TID 136). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000130_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000130
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Starting task 145.0 in stage 1.0 (TID 153, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000130_0: Committed
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Running task 145.0 in stage 1.0 (TID 153)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,086B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,042B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO Executor: Finished task 130.0 in stage 1.0 (TID 138). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,070B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,026B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO TaskSetManager: Starting task 146.0 in stage 1.0 (TID 154, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 146.0 in stage 1.0 (TID 154)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000132_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000132
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000132_0: Committed
15/08/15 19:22:07 INFO TaskSetManager: Finished task 128.0 in stage 1.0 (TID 136) in 270 ms on localhost (130/200)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,076
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,132
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 130.0 in stage 1.0 (TID 138) in 241 ms on localhost (131/200)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,981B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,937B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,968B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,924B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,999B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,955B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Finished task 132.0 in stage 1.0 (TID 140). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 147.0 in stage 1.0 (TID 155, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 147.0 in stage 1.0 (TID 155)
15/08/15 19:22:07 INFO TaskSetManager: Finished task 132.0 in stage 1.0 (TID 140) in 240 ms on localhost (132/200)
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000133_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000133
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000133_0: Committed
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO Executor: Finished task 133.0 in stage 1.0 (TID 141). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 148.0 in stage 1.0 (TID 156, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO Executor: Running task 148.0 in stage 1.0 (TID 156)
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO TaskSetManager: Finished task 133.0 in stage 1.0 (TID 141) in 258 ms on localhost (133/200)
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,308
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,992B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,948B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000134_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000134
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000134_0: Committed
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000131_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000131
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000131_0: Committed
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000135_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000135
15/08/15 19:22:07 INFO Executor: Finished task 134.0 in stage 1.0 (TID 142). 843 bytes result sent to driver
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO Executor: Finished task 131.0 in stage 1.0 (TID 139). 843 bytes result sent to driver
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO TaskSetManager: Starting task 149.0 in stage 1.0 (TID 157, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000135_0: Committed
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO Executor: Running task 149.0 in stage 1.0 (TID 157)
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO TaskSetManager: Starting task 150.0 in stage 1.0 (TID 158, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 150.0 in stage 1.0 (TID 158)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,228
15/08/15 19:22:07 INFO TaskSetManager: Finished task 131.0 in stage 1.0 (TID 139) in 290 ms on localhost (134/200)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO Executor: Finished task 135.0 in stage 1.0 (TID 143). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/15 19:22:07 INFO TaskSetManager: Finished task 134.0 in stage 1.0 (TID 142) in 266 ms on localhost (135/200)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,043B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,999B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/15 19:22:07 INFO TaskSetManager: Starting task 151.0 in stage 1.0 (TID 159, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 151.0 in stage 1.0 (TID 159)
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:07 INFO TaskSetManager: Finished task 135.0 in stage 1.0 (TID 143) in 266 ms on localhost (136/200)
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,966B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,922B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,076
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,976B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,932B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000136_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000136
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000136_0: Committed
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,982B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,938B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,996
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO Executor: Finished task 136.0 in stage 1.0 (TID 144). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO TaskSetManager: Starting task 152.0 in stage 1.0 (TID 160, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Running task 152.0 in stage 1.0 (TID 160)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 840 entries, 6,720B raw, 840B comp}
15/08/15 19:22:07 INFO TaskSetManager: Finished task 136.0 in stage 1.0 (TID 144) in 288 ms on localhost (137/200)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,972
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000141_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000141
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000141_0: Committed
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 837 entries, 6,696B raw, 837B comp}
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,980
15/08/15 19:22:07 INFO Executor: Finished task 141.0 in stage 1.0 (TID 149). 843 bytes result sent to driver
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,751B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,715B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO TaskSetManager: Starting task 153.0 in stage 1.0 (TID 161, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,055B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,011B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 838 entries, 6,704B raw, 838B comp}
15/08/15 19:22:07 INFO Executor: Running task 153.0 in stage 1.0 (TID 161)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 141.0 in stage 1.0 (TID 149) in 276 ms on localhost (138/200)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000137_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000137
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000137_0: Committed
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,919B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,875B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Finished task 137.0 in stage 1.0 (TID 145). 843 bytes result sent to driver
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO TaskSetManager: Starting task 154.0 in stage 1.0 (TID 162, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000146_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000146
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO Executor: Running task 154.0 in stage 1.0 (TID 162)
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000146_0: Committed
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO TaskSetManager: Finished task 137.0 in stage 1.0 (TID 145) in 304 ms on localhost (139/200)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,212
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO Executor: Finished task 146.0 in stage 1.0 (TID 154). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 155.0 in stage 1.0 (TID 163, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 155.0 in stage 1.0 (TID 163)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000139_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000139
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000139_0: Committed
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,053B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,009B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,244
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000138_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000138
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000138_0: Committed
15/08/15 19:22:07 INFO TaskSetManager: Finished task 146.0 in stage 1.0 (TID 154) in 251 ms on localhost (140/200)
15/08/15 19:22:07 INFO Executor: Finished task 139.0 in stage 1.0 (TID 147). 843 bytes result sent to driver
15/08/15 19:22:07 INFO Executor: Finished task 138.0 in stage 1.0 (TID 146). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 156.0 in stage 1.0 (TID 164, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:07 INFO TaskSetManager: Starting task 157.0 in stage 1.0 (TID 165, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,993B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,949B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Running task 157.0 in stage 1.0 (TID 165)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Running task 156.0 in stage 1.0 (TID 164)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,021B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,977B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 139.0 in stage 1.0 (TID 147) in 415 ms on localhost (141/200)
15/08/15 19:22:07 INFO TaskSetManager: Finished task 138.0 in stage 1.0 (TID 146) in 422 ms on localhost (142/200)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000142_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000142
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000142_0: Committed
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,132
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000144_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000144
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000144_0: Committed
15/08/15 19:22:07 INFO Executor: Finished task 142.0 in stage 1.0 (TID 150). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,927B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,883B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO TaskSetManager: Starting task 158.0 in stage 1.0 (TID 166, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 158.0 in stage 1.0 (TID 166)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,042B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,998B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000145_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000145
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000145_0: Committed
15/08/15 19:22:07 INFO Executor: Finished task 144.0 in stage 1.0 (TID 152). 843 bytes result sent to driver
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000143_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000143
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000143_0: Committed
15/08/15 19:22:07 INFO TaskSetManager: Starting task 159.0 in stage 1.0 (TID 167, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO TaskSetManager: Finished task 142.0 in stage 1.0 (TID 150) in 406 ms on localhost (143/200)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO TaskSetManager: Finished task 144.0 in stage 1.0 (TID 152) in 393 ms on localhost (144/200)
15/08/15 19:22:07 INFO Executor: Finished task 145.0 in stage 1.0 (TID 153). 843 bytes result sent to driver
15/08/15 19:22:07 INFO Executor: Running task 159.0 in stage 1.0 (TID 167)
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO TaskSetManager: Starting task 160.0 in stage 1.0 (TID 168, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 160.0 in stage 1.0 (TID 168)
15/08/15 19:22:07 INFO Executor: Finished task 143.0 in stage 1.0 (TID 151). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 161.0 in stage 1.0 (TID 169, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 161.0 in stage 1.0 (TID 169)
15/08/15 19:22:07 INFO TaskSetManager: Finished task 145.0 in stage 1.0 (TID 153) in 298 ms on localhost (145/200)
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000150_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000150
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000150_0: Committed
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000148_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000148
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000148_0: Committed
15/08/15 19:22:07 INFO TaskSetManager: Finished task 143.0 in stage 1.0 (TID 151) in 405 ms on localhost (146/200)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000147_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000147
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000147_0: Committed
15/08/15 19:22:07 INFO Executor: Finished task 148.0 in stage 1.0 (TID 156). 843 bytes result sent to driver
15/08/15 19:22:07 INFO Executor: Finished task 147.0 in stage 1.0 (TID 155). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO Executor: Finished task 150.0 in stage 1.0 (TID 158). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 162.0 in stage 1.0 (TID 170, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 162.0 in stage 1.0 (TID 170)
15/08/15 19:22:07 INFO TaskSetManager: Starting task 163.0 in stage 1.0 (TID 171, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO Executor: Running task 163.0 in stage 1.0 (TID 171)
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO TaskSetManager: Starting task 164.0 in stage 1.0 (TID 172, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO TaskSetManager: Finished task 148.0 in stage 1.0 (TID 156) in 273 ms on localhost (147/200)
15/08/15 19:22:07 INFO Executor: Running task 164.0 in stage 1.0 (TID 172)
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 147.0 in stage 1.0 (TID 155) in 294 ms on localhost (148/200)
15/08/15 19:22:07 INFO TaskSetManager: Finished task 150.0 in stage 1.0 (TID 158) in 256 ms on localhost (149/200)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000151_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000151
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000151_0: Committed
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000149_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000149
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000149_0: Committed
15/08/15 19:22:07 INFO Executor: Finished task 151.0 in stage 1.0 (TID 159). 843 bytes result sent to driver
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,196
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO Executor: Finished task 149.0 in stage 1.0 (TID 157). 843 bytes result sent to driver
15/08/15 19:22:07 INFO TaskSetManager: Starting task 165.0 in stage 1.0 (TID 173, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 165.0 in stage 1.0 (TID 173)
15/08/15 19:22:07 INFO TaskSetManager: Starting task 166.0 in stage 1.0 (TID 174, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Running task 166.0 in stage 1.0 (TID 174)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,923B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,879B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,116
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,956B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,912B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO TaskSetManager: Finished task 149.0 in stage 1.0 (TID 157) in 288 ms on localhost (150/200)
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO TaskSetManager: Finished task 151.0 in stage 1.0 (TID 159) in 276 ms on localhost (151/200)
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,036
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000152_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000152
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000152_0: Committed
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000153_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000153
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000153_0: Committed
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,978B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,934B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Finished task 152.0 in stage 1.0 (TID 160). 843 bytes result sent to driver
15/08/15 19:22:07 INFO Executor: Finished task 153.0 in stage 1.0 (TID 161). 843 bytes result sent to driver
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,196
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Starting task 167.0 in stage 1.0 (TID 175, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 167.0 in stage 1.0 (TID 175)
15/08/15 19:22:07 INFO TaskSetManager: Starting task 168.0 in stage 1.0 (TID 176, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,196
15/08/15 19:22:07 INFO Executor: Running task 168.0 in stage 1.0 (TID 176)
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,276
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,148
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,992B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,948B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,945B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,901B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 153.0 in stage 1.0 (TID 161) in 259 ms on localhost (152/200)
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,954B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,910B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 4,974B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,930B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO TaskSetManager: Finished task 152.0 in stage 1.0 (TID 160) in 281 ms on localhost (153/200)
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,124
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,768B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,732B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,196
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,012
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,034B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,990B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,765B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,729B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,011B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,967B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000157_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000157
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000157_0: Committed
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 5,005B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,961B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 2,750B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,714B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:07 INFO Executor: Finished task 157.0 in stage 1.0 (TID 165). 843 bytes result sent to driver
15/08/15 19:22:07 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 842 entries, 6,736B raw, 842B comp}
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO TaskSetManager: Starting task 169.0 in stage 1.0 (TID 177, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:07 INFO Executor: Running task 169.0 in stage 1.0 (TID 177)
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:07 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:07 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:07 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:07 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:07 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:07 INFO TaskSetManager: Finished task 157.0 in stage 1.0 (TID 165) in 164 ms on localhost (154/200)
15/08/15 19:22:07 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000155_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000155
15/08/15 19:22:07 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000155_0: Committed
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:07 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:07 INFO Executor: Finished task 155.0 in stage 1.0 (TID 163). 843 bytes result sent to driver
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,164
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,972
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000154_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000154
15/08/15 19:22:08 INFO TaskSetManager: Starting task 170.0 in stage 1.0 (TID 178, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000154_0: Committed
15/08/15 19:22:08 INFO Executor: Running task 170.0 in stage 1.0 (TID 178)
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,236
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,977B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,933B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 837 entries, 6,696B raw, 837B comp}
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO Executor: Finished task 154.0 in stage 1.0 (TID 162). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,014B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,970B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 163) in 418 ms on localhost (155/200)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000158_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000158
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000158_0: Committed
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000159_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000159
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000159_0: Committed
15/08/15 19:22:08 INFO TaskSetManager: Finished task 154.0 in stage 1.0 (TID 162) in 432 ms on localhost (156/200)
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO Executor: Finished task 158.0 in stage 1.0 (TID 166). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO TaskSetManager: Starting task 171.0 in stage 1.0 (TID 179, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 171.0 in stage 1.0 (TID 179)
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,164
15/08/15 19:22:08 INFO Executor: Finished task 159.0 in stage 1.0 (TID 167). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 172.0 in stage 1.0 (TID 180, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 172.0 in stage 1.0 (TID 180)
15/08/15 19:22:08 INFO TaskSetManager: Starting task 173.0 in stage 1.0 (TID 181, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000163_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000163
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000163_0: Committed
15/08/15 19:22:08 INFO Executor: Running task 173.0 in stage 1.0 (TID 181)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 159.0 in stage 1.0 (TID 167) in 297 ms on localhost (157/200)
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 158.0 in stage 1.0 (TID 166) in 302 ms on localhost (158/200)
15/08/15 19:22:08 INFO Executor: Finished task 163.0 in stage 1.0 (TID 171). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000161_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000161
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000161_0: Committed
15/08/15 19:22:08 INFO TaskSetManager: Finished task 163.0 in stage 1.0 (TID 171) in 279 ms on localhost (159/200)
15/08/15 19:22:08 INFO TaskSetManager: Starting task 174.0 in stage 1.0 (TID 182, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,934B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,890B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000140_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000140
15/08/15 19:22:08 INFO Executor: Running task 174.0 in stage 1.0 (TID 182)
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000140_0: Committed
15/08/15 19:22:08 INFO Executor: Finished task 161.0 in stage 1.0 (TID 169). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 175.0 in stage 1.0 (TID 183, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 175.0 in stage 1.0 (TID 183)
15/08/15 19:22:08 INFO Executor: Finished task 140.0 in stage 1.0 (TID 148). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Finished task 161.0 in stage 1.0 (TID 169) in 303 ms on localhost (160/200)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 140.0 in stage 1.0 (TID 148) in 743 ms on localhost (161/200)
15/08/15 19:22:08 INFO TaskSetManager: Starting task 176.0 in stage 1.0 (TID 184, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 176.0 in stage 1.0 (TID 184)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000166_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000166
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000166_0: Committed
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO Executor: Finished task 166.0 in stage 1.0 (TID 174). 843 bytes result sent to driver
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000165_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000165
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000165_0: Committed
15/08/15 19:22:08 INFO TaskSetManager: Starting task 177.0 in stage 1.0 (TID 185, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:08 INFO Executor: Finished task 165.0 in stage 1.0 (TID 173). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Finished task 166.0 in stage 1.0 (TID 174) in 283 ms on localhost (162/200)
15/08/15 19:22:08 INFO Executor: Running task 177.0 in stage 1.0 (TID 185)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO TaskSetManager: Starting task 178.0 in stage 1.0 (TID 186, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 178.0 in stage 1.0 (TID 186)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO TaskSetManager: Finished task 165.0 in stage 1.0 (TID 173) in 288 ms on localhost (163/200)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,985B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,941B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000162_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000162
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000162_0: Committed
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,196
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,031B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,987B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/15 19:22:08 INFO Executor: Finished task 162.0 in stage 1.0 (TID 170). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO TaskSetManager: Starting task 179.0 in stage 1.0 (TID 187, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 179.0 in stage 1.0 (TID 187)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000164_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000164
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000164_0: Committed
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO TaskSetManager: Finished task 162.0 in stage 1.0 (TID 170) in 326 ms on localhost (164/200)
15/08/15 19:22:08 INFO Executor: Finished task 164.0 in stage 1.0 (TID 172). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 180.0 in stage 1.0 (TID 188, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 180.0 in stage 1.0 (TID 188)
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO TaskSetManager: Finished task 164.0 in stage 1.0 (TID 172) in 330 ms on localhost (165/200)
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,180
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,761B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,725B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,961B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,917B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000167_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000167
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000167_0: Committed
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,988
15/08/15 19:22:08 INFO Executor: Finished task 167.0 in stage 1.0 (TID 175). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO TaskSetManager: Starting task 181.0 in stage 1.0 (TID 189, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 181.0 in stage 1.0 (TID 189)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 839 entries, 6,712B raw, 839B comp}
15/08/15 19:22:08 INFO TaskSetManager: Finished task 167.0 in stage 1.0 (TID 175) in 303 ms on localhost (166/200)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,180
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,076
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,983B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,939B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,904B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,860B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,116
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,980
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,768B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,732B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,932B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,888B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 838 entries, 6,704B raw, 838B comp}
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,164
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,908
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,768B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,732B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,008B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,964B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000170_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000170
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000170_0: Committed
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 829 entries, 6,632B raw, 829B comp}
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO Executor: Finished task 170.0 in stage 1.0 (TID 178). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 182.0 in stage 1.0 (TID 190, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 182.0 in stage 1.0 (TID 190)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 170.0 in stage 1.0 (TID 178) in 168 ms on localhost (167/200)
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000172_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000172
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,036
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000172_0: Committed
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,945B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,901B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:08 INFO Executor: Finished task 172.0 in stage 1.0 (TID 180). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 183.0 in stage 1.0 (TID 191, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO Executor: Running task 183.0 in stage 1.0 (TID 191)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,018B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,974B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 172.0 in stage 1.0 (TID 180) in 162 ms on localhost (168/200)
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,052B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,008B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000169_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000169
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000169_0: Committed
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000175_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000175
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000175_0: Committed
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000171_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000171
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000171_0: Committed
15/08/15 19:22:08 INFO Executor: Finished task 175.0 in stage 1.0 (TID 183). 843 bytes result sent to driver
15/08/15 19:22:08 INFO Executor: Finished task 169.0 in stage 1.0 (TID 177). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 184.0 in stage 1.0 (TID 192, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO Executor: Running task 184.0 in stage 1.0 (TID 192)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000173_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000173
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000173_0: Committed
15/08/15 19:22:08 INFO TaskSetManager: Starting task 185.0 in stage 1.0 (TID 193, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Finished task 171.0 in stage 1.0 (TID 179). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 186.0 in stage 1.0 (TID 194, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 185.0 in stage 1.0 (TID 193)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO Executor: Running task 186.0 in stage 1.0 (TID 194)
15/08/15 19:22:08 INFO Executor: Finished task 173.0 in stage 1.0 (TID 181). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Finished task 175.0 in stage 1.0 (TID 183) in 337 ms on localhost (169/200)
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO TaskSetManager: Starting task 187.0 in stage 1.0 (TID 195, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 187.0 in stage 1.0 (TID 195)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 171.0 in stage 1.0 (TID 179) in 358 ms on localhost (170/200)
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:08 INFO TaskSetManager: Finished task 173.0 in stage 1.0 (TID 181) in 356 ms on localhost (171/200)
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000174_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000174
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000174_0: Committed
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000176_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000176
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000176_0: Committed
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,027B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,983B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO Executor: Finished task 174.0 in stage 1.0 (TID 182). 843 bytes result sent to driver
15/08/15 19:22:08 INFO Executor: Finished task 176.0 in stage 1.0 (TID 184). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Finished task 169.0 in stage 1.0 (TID 177) in 520 ms on localhost (172/200)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000156_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000156
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000156_0: Committed
15/08/15 19:22:08 INFO TaskSetManager: Starting task 188.0 in stage 1.0 (TID 196, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 188.0 in stage 1.0 (TID 196)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000179_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000179
15/08/15 19:22:08 INFO TaskSetManager: Starting task 189.0 in stage 1.0 (TID 197, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000179_0: Committed
15/08/15 19:22:08 INFO Executor: Running task 189.0 in stage 1.0 (TID 197)
15/08/15 19:22:08 INFO Executor: Finished task 156.0 in stage 1.0 (TID 164). 843 bytes result sent to driver
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000160_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000160
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000160_0: Committed
15/08/15 19:22:08 INFO TaskSetManager: Finished task 174.0 in stage 1.0 (TID 182) in 356 ms on localhost (173/200)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000178_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000178
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000178_0: Committed
15/08/15 19:22:08 INFO Executor: Finished task 160.0 in stage 1.0 (TID 168). 843 bytes result sent to driver
15/08/15 19:22:08 INFO Executor: Finished task 179.0 in stage 1.0 (TID 187). 843 bytes result sent to driver
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 176.0 in stage 1.0 (TID 184) in 349 ms on localhost (174/200)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO Executor: Finished task 178.0 in stage 1.0 (TID 186). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 190.0 in stage 1.0 (TID 198, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 190.0 in stage 1.0 (TID 198)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000180_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000180
15/08/15 19:22:08 INFO TaskSetManager: Starting task 191.0 in stage 1.0 (TID 199, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000180_0: Committed
15/08/15 19:22:08 INFO Executor: Running task 191.0 in stage 1.0 (TID 199)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 156.0 in stage 1.0 (TID 164) in 694 ms on localhost (175/200)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 160.0 in stage 1.0 (TID 168) in 667 ms on localhost (176/200)
15/08/15 19:22:08 INFO Executor: Finished task 180.0 in stage 1.0 (TID 188). 843 bytes result sent to driver
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,188
15/08/15 19:22:08 INFO TaskSetManager: Starting task 192.0 in stage 1.0 (TID 200, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 192.0 in stage 1.0 (TID 200)
15/08/15 19:22:08 INFO TaskSetManager: Starting task 193.0 in stage 1.0 (TID 201, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO Executor: Running task 193.0 in stage 1.0 (TID 201)
15/08/15 19:22:08 INFO TaskSetManager: Starting task 194.0 in stage 1.0 (TID 202, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 194.0 in stage 1.0 (TID 202)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,034B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,990B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 179.0 in stage 1.0 (TID 187) in 334 ms on localhost (177/200)
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO TaskSetManager: Finished task 180.0 in stage 1.0 (TID 188) in 330 ms on localhost (178/200)
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO TaskSetManager: Finished task 178.0 in stage 1.0 (TID 186) in 355 ms on localhost (179/200)
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000177_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000177
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000177_0: Committed
15/08/15 19:22:08 INFO Executor: Finished task 177.0 in stage 1.0 (TID 185). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 195.0 in stage 1.0 (TID 203, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 195.0 in stage 1.0 (TID 203)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 177.0 in stage 1.0 (TID 185) in 372 ms on localhost (180/200)
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,292
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,097B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,053B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,156
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000181_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000181
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000181_0: Committed
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,100
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,755B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,068B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,024B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO Executor: Finished task 181.0 in stage 1.0 (TID 189). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Starting task 196.0 in stage 1.0 (TID 204, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 196.0 in stage 1.0 (TID 204)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,754B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,718B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,956B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,912B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO TaskSetManager: Finished task 181.0 in stage 1.0 (TID 189) in 329 ms on localhost (181/200)
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,475,924
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 831 entries, 6,648B raw, 831B comp}
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000182_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000182
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000182_0: Committed
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,002B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,958B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000183_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000183
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000183_0: Committed
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:08 INFO Executor: Finished task 182.0 in stage 1.0 (TID 190). 843 bytes result sent to driver
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO Executor: Finished task 183.0 in stage 1.0 (TID 191). 843 bytes result sent to driver
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000185_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000185
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000185_0: Committed
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,759B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Starting task 197.0 in stage 1.0 (TID 205, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,929B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,885B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO Executor: Running task 197.0 in stage 1.0 (TID 205)
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,036
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO TaskSetManager: Starting task 198.0 in stage 1.0 (TID 206, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 198.0 in stage 1.0 (TID 206)
15/08/15 19:22:08 INFO Executor: Finished task 185.0 in stage 1.0 (TID 193). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,760B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,724B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 182.0 in stage 1.0 (TID 190) in 337 ms on localhost (182/200)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,985B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,941B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Starting task 199.0 in stage 1.0 (TID 207, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:08 INFO Executor: Running task 199.0 in stage 1.0 (TID 207)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 183.0 in stage 1.0 (TID 191) in 321 ms on localhost (183/200)
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO TaskSetManager: Finished task 185.0 in stage 1.0 (TID 193) in 139 ms on localhost (184/200)
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,108
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000168_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000168
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000168_0: Committed
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,953B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,909B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO Executor: Finished task 168.0 in stage 1.0 (TID 176). 843 bytes result sent to driver
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000187_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000187
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,026B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,982B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000187_0: Committed
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000186_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000186
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000186_0: Committed
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,084
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,268
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,758B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,722B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,132
15/08/15 19:22:08 INFO Executor: Finished task 187.0 in stage 1.0 (TID 195). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,896B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,852B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 168.0 in stage 1.0 (TID 176) in 814 ms on localhost (185/200)
15/08/15 19:22:08 INFO Executor: Finished task 186.0 in stage 1.0 (TID 194). 843 bytes result sent to driver
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000188_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000188
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000188_0: Committed
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000192_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000192
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000192_0: Committed
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,012
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,762B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,726B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,044
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,070B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 5,026B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,757B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,721B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,004B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,960B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,764B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,728B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 1,327B for [t_avg_quantity] DOUBLE: 1,000 values, 1,260B raw, 1,283B comp, 1 pages, encodings: [RLE, BIT_PACKED, PLAIN_DICTIONARY], dic { 842 entries, 6,736B raw, 842B comp}
15/08/15 19:22:08 INFO TaskSetManager: Finished task 187.0 in stage 1.0 (TID 195) in 277 ms on localhost (186/200)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,756B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,720B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 186.0 in stage 1.0 (TID 194) in 289 ms on localhost (187/200)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,972B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,928B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO Executor: Finished task 192.0 in stage 1.0 (TID 200). 843 bytes result sent to driver
15/08/15 19:22:08 INFO Executor: Finished task 188.0 in stage 1.0 (TID 196). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO TaskSetManager: Finished task 188.0 in stage 1.0 (TID 196) in 275 ms on localhost (188/200)
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 192.0 in stage 1.0 (TID 200) in 258 ms on localhost (189/200)
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000189_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000189
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000189_0: Committed
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:08 INFO Executor: Finished task 189.0 in stage 1.0 (TID 197). 843 bytes result sent to driver
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000191_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000191
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000195_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000195
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,212
15/08/15 19:22:08 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 197) in 285 ms on localhost (190/200)
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000195_0: Committed
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000194_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000194
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000194_0: Committed
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000191_0: Committed
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,763B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 5,008B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,964B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO Executor: Finished task 194.0 in stage 1.0 (TID 202). 843 bytes result sent to driver
15/08/15 19:22:08 INFO Executor: Finished task 191.0 in stage 1.0 (TID 199). 843 bytes result sent to driver
15/08/15 19:22:08 INFO Executor: Finished task 195.0 in stage 1.0 (TID 203). 843 bytes result sent to driver
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000190_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000190
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000190_0: Committed
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000184_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000184
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000184_0: Committed
15/08/15 19:22:08 INFO TaskSetManager: Finished task 194.0 in stage 1.0 (TID 202) in 273 ms on localhost (191/200)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 191.0 in stage 1.0 (TID 199) in 283 ms on localhost (192/200)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 195.0 in stage 1.0 (TID 203) in 261 ms on localhost (193/200)
15/08/15 19:22:08 INFO Executor: Finished task 190.0 in stage 1.0 (TID 198). 843 bytes result sent to driver
15/08/15 19:22:08 INFO Executor: Finished task 184.0 in stage 1.0 (TID 192). 843 bytes result sent to driver
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000193_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000193
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000193_0: Committed
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO TaskSetManager: Finished task 190.0 in stage 1.0 (TID 198) in 288 ms on localhost (194/200)
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 184.0 in stage 1.0 (TID 192) in 485 ms on localhost (195/200)
15/08/15 19:22:08 INFO Executor: Finished task 193.0 in stage 1.0 (TID 201). 843 bytes result sent to driver
15/08/15 19:22:08 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:08 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:08 INFO TaskSetManager: Finished task 193.0 in stage 1.0 (TID 201) in 283 ms on localhost (196/200)
15/08/15 19:22:08 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:08 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:08 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:08 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,092
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000196_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000196
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000196_0: Committed
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,766B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,730B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,926B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,882B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:08 INFO Executor: Finished task 196.0 in stage 1.0 (TID 204). 843 bytes result sent to driver
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,768B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,732B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO TaskSetManager: Finished task 196.0 in stage 1.0 (TID 204) in 259 ms on localhost (197/200)
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,961B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,917B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,476,140
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 2,767B for [t_partkey] INT32: 1,000 values, 4,007B raw, 2,731B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO ColumnChunkPageWriteStore: written 4,930B for [t_avg_quantity] DOUBLE: 1,000 values, 8,007B raw, 4,886B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000199_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000199
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000199_0: Committed
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000197_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000197
15/08/15 19:22:08 INFO Executor: Finished task 199.0 in stage 1.0 (TID 207). 843 bytes result sent to driver
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000197_0: Committed
15/08/15 19:22:08 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0001_m_000198_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_temporary/0/task_201508151922_0001_m_000198
15/08/15 19:22:08 INFO SparkHadoopMapRedUtil: attempt_201508151922_0001_m_000198_0: Committed
15/08/15 19:22:08 INFO Executor: Finished task 197.0 in stage 1.0 (TID 205). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Finished task 199.0 in stage 1.0 (TID 207) in 233 ms on localhost (198/200)
15/08/15 19:22:08 INFO Executor: Finished task 198.0 in stage 1.0 (TID 206). 843 bytes result sent to driver
15/08/15 19:22:08 INFO TaskSetManager: Finished task 197.0 in stage 1.0 (TID 205) in 243 ms on localhost (199/200)
15/08/15 19:22:08 INFO TaskSetManager: Finished task 198.0 in stage 1.0 (TID 206) in 242 ms on localhost (200/200)
15/08/15 19:22:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/15 19:22:08 INFO DAGScheduler: ResultStage 1 (processCmd at CliDriver.java:423) finished in 5.564 s
15/08/15 19:22:08 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1c76500f
15/08/15 19:22:08 INFO StatsReportListener: task runtime:(count: 200, mean: 454.135000, stdev: 230.121265, max: 1049.000000, min: 139.000000)
15/08/15 19:22:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:08 INFO StatsReportListener: 	139.0 ms	181.0 ms	219.0 ms	283.0 ms	405.0 ms	554.0 ms	792.0 ms	996.0 ms	1.0 s
15/08/15 19:22:08 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.105000, stdev: 0.322452, max: 2.000000, min: 0.000000)
15/08/15 19:22:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:08 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/15 19:22:08 INFO DAGScheduler: Job 0 finished: processCmd at CliDriver.java:423, took 11.624458 s
15/08/15 19:22:08 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/15 19:22:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:08 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/15 19:22:08 INFO StatsReportListener: task result size:(count: 200, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/15 19:22:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:08 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/15 19:22:08 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 64.408643, stdev: 20.067240, max: 92.203898, min: 17.843866)
15/08/15 19:22:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:08 INFO StatsReportListener: 	18 %	24 %	29 %	54 %	73 %	80 %	84 %	86 %	92 %
15/08/15 19:22:08 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.027522, stdev: 0.091294, max: 0.531915, min: 0.000000)
15/08/15 19:22:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:08 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/15 19:22:08 INFO StatsReportListener: other time pct: (count: 200, mean: 35.563835, stdev: 20.052738, max: 82.156134, min: 7.796102)
15/08/15 19:22:08 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:08 INFO StatsReportListener: 	 8 %	14 %	16 %	20 %	27 %	48 %	71 %	77 %	82 %
15/08/15 19:22:10 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/15 19:22:10 INFO DefaultWriterContainer: Job job_201508151921_0000 committed.
15/08/15 19:22:10 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/15 19:22:10 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/_common_metadata
15/08/15 19:22:10 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/15 19:22:10 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/15 19:22:10 INFO DAGScheduler: Final stage: ResultStage 2(processCmd at CliDriver.java:423)
15/08/15 19:22:10 INFO DAGScheduler: Parents of final stage: List()
15/08/15 19:22:10 INFO DAGScheduler: Missing parents: List()
15/08/15 19:22:10 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423), which has no missing parents
15/08/15 19:22:10 INFO MemoryStore: ensureFreeSpace(2960) called with curMem=390926, maxMem=3333968363
15/08/15 19:22:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 3.1 GB)
15/08/15 19:22:10 INFO MemoryStore: ensureFreeSpace(1773) called with curMem=393886, maxMem=3333968363
15/08/15 19:22:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1773.0 B, free 3.1 GB)
15/08/15 19:22:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:50234 (size: 1773.0 B, free: 3.1 GB)
15/08/15 19:22:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/08/15 19:22:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:423)
15/08/15 19:22:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/15 19:22:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 208, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/15 19:22:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 208)
15/08/15 19:22:10 INFO Executor: Finished task 0.0 in stage 2.0 (TID 208). 606 bytes result sent to driver
15/08/15 19:22:10 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 208) in 27 ms on localhost (1/1)
15/08/15 19:22:10 INFO DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:423) finished in 0.028 s
15/08/15 19:22:10 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/15 19:22:10 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@27cd2654
15/08/15 19:22:10 INFO DAGScheduler: Job 1 finished: processCmd at CliDriver.java:423, took 0.049415 s
15/08/15 19:22:10 INFO StatsReportListener: task runtime:(count: 1, mean: 27.000000, stdev: 0.000000, max: 27.000000, min: 27.000000)
15/08/15 19:22:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:10 INFO StatsReportListener: 	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms	27.0 ms
15/08/15 19:22:10 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/15 19:22:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:10 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/15 19:22:10 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 14.814815, stdev: 0.000000, max: 14.814815, min: 14.814815)
15/08/15 19:22:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:10 INFO StatsReportListener: 	15 %	15 %	15 %	15 %	15 %	15 %	15 %	15 %	15 %
15/08/15 19:22:10 INFO StatsReportListener: other time pct: (count: 1, mean: 85.185185, stdev: 0.000000, max: 85.185185, min: 85.185185)
15/08/15 19:22:10 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:10 INFO StatsReportListener: 	85 %	85 %	85 %	85 %	85 %	85 %	85 %	85 %	85 %
Time taken: 16.955 seconds
15/08/15 19:22:10 INFO CliDriver: Time taken: 16.955 seconds
15/08/15 19:22:10 INFO ParseDriver: Parsing command: insert into table q17_small_quantity_order_revenue_par
select
  sum(l_extendedprice) / 7.0 as avg_yearly
from
  (select l_quantity, l_extendedprice, t_avg_quantity from
   lineitem_tmp_par t join
     (select
        l_quantity, l_partkey, l_extendedprice
      from
        part_par p join lineitem_par l
        on
          p.p_partkey = l.l_partkey
          and p.p_brand = 'Brand#54'
          and p.p_container = 'SM BAG'
      ) l1 on l1.l_partkey = t.t_partkey
   ) a
where l_quantity < t_avg_quantity
15/08/15 19:22:10 INFO ParseDriver: Parse Completed
15/08/15 19:22:10 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/15 19:22:10 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/15 19:22:10 INFO ParquetFileReader: reading another 2 footers
15/08/15 19:22:10 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=395659, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=653187, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/15 19:22:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:50234 (size: 22.3 KB, free: 3.1 GB)
15/08/15 19:22:11 INFO SparkContext: Created broadcast 4 from processCmd at CliDriver.java:423
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=675980, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=933508, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/15 19:22:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:50234 (size: 22.3 KB, free: 3.1 GB)
15/08/15 19:22:11 INFO SparkContext: Created broadcast 5 from processCmd at CliDriver.java:423
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(257528) called with curMem=956301, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 251.5 KB, free 3.1 GB)
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(22793) called with curMem=1213829, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.3 KB, free 3.1 GB)
15/08/15 19:22:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:50234 (size: 22.3 KB, free: 3.1 GB)
15/08/15 19:22:11 INFO SparkContext: Created broadcast 6 from processCmd at CliDriver.java:423
15/08/15 19:22:11 INFO Exchange: Using SparkSqlSerializer2.
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/15 19:22:11 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1145
15/08/15 19:22:11 INFO DAGScheduler: Got job 2 (run at ThreadPoolExecutor.java:1145) with 2 output partitions (allowLocal=false)
15/08/15 19:22:11 INFO DAGScheduler: Final stage: ResultStage 3(run at ThreadPoolExecutor.java:1145)
15/08/15 19:22:11 INFO DAGScheduler: Parents of final stage: List()
15/08/15 19:22:11 INFO DAGScheduler: Missing parents: List()
15/08/15 19:22:11 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at run at ThreadPoolExecutor.java:1145), which has no missing parents
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(6512) called with curMem=1236622, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 6.4 KB, free 3.1 GB)
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(3457) called with curMem=1243134, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.4 KB, free 3.1 GB)
15/08/15 19:22:11 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:50234 (size: 3.4 KB, free: 3.1 GB)
15/08/15 19:22:11 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:874
15/08/15 19:22:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at run at ThreadPoolExecutor.java:1145)
15/08/15 19:22:11 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
15/08/15 19:22:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 209, localhost, ANY, 1728 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 210, localhost, ANY, 1730 bytes)
15/08/15 19:22:11 INFO Executor: Running task 0.0 in stage 3.0 (TID 209)
15/08/15 19:22:11 INFO Executor: Running task 1.0 in stage 3.0 (TID 210)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/part_par/000001_0 start: 0 end: 1981661 length: 1981661 hosts: [] requestedSchema: message root {
  optional int32 p_partkey;
  optional binary p_brand (UTF8);
  optional binary p_container (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"p_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"p_name","type":"string","nullable":true,"metadata":{}},{"name":"p_mfgr","type":"string","nullable":true,"metadata":{}},{"name":"p_brand","type":"string","nullable":true,"metadata":{}},{"name":"p_type","type":"string","nullable":true,"metadata":{}},{"name":"p_size","type":"integer","nullable":true,"metadata":{}},{"name":"p_container","type":"string","nullable":true,"metadata":{}},{"name":"p_retailprice","type":"double","nullable":true,"metadata":{}},{"name":"p_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"p_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"p_brand","type":"string","nullable":true,"metadata":{}},{"name":"p_container","type":"string","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/part_par/000000_0 start: 0 end: 4475467 length: 4475467 hosts: [] requestedSchema: message root {
  optional int32 p_partkey;
  optional binary p_brand (UTF8);
  optional binary p_container (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"p_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"p_name","type":"string","nullable":true,"metadata":{}},{"name":"p_mfgr","type":"string","nullable":true,"metadata":{}},{"name":"p_brand","type":"string","nullable":true,"metadata":{}},{"name":"p_type","type":"string","nullable":true,"metadata":{}},{"name":"p_size","type":"integer","nullable":true,"metadata":{}},{"name":"p_container","type":"string","nullable":true,"metadata":{}},{"name":"p_retailprice","type":"double","nullable":true,"metadata":{}},{"name":"p_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"p_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"p_brand","type":"string","nullable":true,"metadata":{}},{"name":"p_container","type":"string","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 60668 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139332 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 60668
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 139332
15/08/15 19:22:11 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1145
15/08/15 19:22:11 INFO DAGScheduler: Got job 3 (run at ThreadPoolExecutor.java:1145) with 200 output partitions (allowLocal=false)
15/08/15 19:22:11 INFO DAGScheduler: Final stage: ResultStage 4(run at ThreadPoolExecutor.java:1145)
15/08/15 19:22:11 INFO DAGScheduler: Parents of final stage: List()
15/08/15 19:22:11 INFO DAGScheduler: Missing parents: List()
15/08/15 19:22:11 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at run at ThreadPoolExecutor.java:1145), which has no missing parents
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(4184) called with curMem=1246591, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 4.1 KB, free 3.1 GB)
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(2375) called with curMem=1250775, maxMem=3333968363
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.3 KB, free 3.1 GB)
15/08/15 19:22:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:50234 (size: 2.3 KB, free: 3.1 GB)
15/08/15 19:22:11 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:874
15/08/15 19:22:11 INFO Executor: Finished task 1.0 in stage 3.0 (TID 210). 2401 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 210) in 293 ms on localhost (1/2)
15/08/15 19:22:11 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at run at ThreadPoolExecutor.java:1145)
15/08/15 19:22:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 200 tasks
15/08/15 19:22:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 211, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 212, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 213, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 214, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 215, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 216, localhost, ANY, 1703 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 217, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 218, localhost, ANY, 1701 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 219, localhost, ANY, 1705 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 220, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 221, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 222, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO Executor: Finished task 0.0 in stage 3.0 (TID 209). 3096 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 223, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 224, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 225, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO Executor: Running task 1.0 in stage 4.0 (TID 212)
15/08/15 19:22:11 INFO Executor: Running task 3.0 in stage 4.0 (TID 214)
15/08/15 19:22:11 INFO Executor: Running task 7.0 in stage 4.0 (TID 218)
15/08/15 19:22:11 INFO Executor: Running task 6.0 in stage 4.0 (TID 217)
15/08/15 19:22:11 INFO Executor: Running task 4.0 in stage 4.0 (TID 215)
15/08/15 19:22:11 INFO Executor: Running task 2.0 in stage 4.0 (TID 213)
15/08/15 19:22:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 211)
15/08/15 19:22:11 INFO Executor: Running task 5.0 in stage 4.0 (TID 216)
15/08/15 19:22:11 INFO Executor: Running task 8.0 in stage 4.0 (TID 219)
15/08/15 19:22:11 INFO Executor: Running task 13.0 in stage 4.0 (TID 224)
15/08/15 19:22:11 INFO Executor: Running task 11.0 in stage 4.0 (TID 222)
15/08/15 19:22:11 INFO Executor: Running task 14.0 in stage 4.0 (TID 225)
15/08/15 19:22:11 INFO Executor: Running task 12.0 in stage 4.0 (TID 223)
15/08/15 19:22:11 INFO Executor: Running task 10.0 in stage 4.0 (TID 221)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00126-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8163 length: 8163 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00054-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8349 length: 8349 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00088-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8193 length: 8193 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00148-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8195 length: 8195 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO Executor: Running task 9.0 in stage 4.0 (TID 220)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00104-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9067 length: 9067 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 226, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00144-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8120 length: 8120 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00026-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8296 length: 8296 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00037-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8205 length: 8205 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00124-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8201 length: 8201 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00093-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9028 length: 9028 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00184-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9035 length: 9035 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00078-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8148 length: 8148 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00107-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8186 length: 8186 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00080-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8223 length: 8223 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO Executor: Running task 15.0 in stage 4.0 (TID 226)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00068-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8990 length: 8990 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 209) in 349 ms on localhost (2/2)
15/08/15 19:22:11 INFO DAGScheduler: ResultStage 3 (run at ThreadPoolExecutor.java:1145) finished in 0.349 s
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO DAGScheduler: Job 2 finished: run at ThreadPoolExecutor.java:1145, took 0.382312 s
15/08/15 19:22:11 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@3c34b0d
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00166-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8223 length: 8223 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO StatsReportListener: task runtime:(count: 2, mean: 321.000000, stdev: 28.000000, max: 349.000000, min: 293.000000)
15/08/15 19:22:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:11 INFO StatsReportListener: 	293.0 ms	293.0 ms	293.0 ms	293.0 ms	349.0 ms	349.0 ms	349.0 ms	349.0 ms	349.0 ms
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO StatsReportListener: task result size:(count: 2, mean: 2748.500000, stdev: 347.500000, max: 3096.000000, min: 2401.000000)
15/08/15 19:22:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:11 INFO StatsReportListener: 	2.3 KB	2.3 KB	2.3 KB	2.3 KB	3.0 KB	3.0 KB	3.0 KB	3.0 KB	3.0 KB
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:11 INFO StatsReportListener: executor (non-fetch) time pct: (count: 2, mean: 90.460311, stdev: 1.348563, max: 91.808874, min: 89.111748)
15/08/15 19:22:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(25064) called with curMem=1253150, maxMem=3333968363
15/08/15 19:22:11 INFO StatsReportListener: 	89 %	89 %	89 %	89 %	92 %	92 %	92 %	92 %	92 %
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 24.5 KB, free 3.1 GB)
15/08/15 19:22:11 INFO StatsReportListener: other time pct: (count: 2, mean: 9.539689, stdev: 1.348563, max: 10.888252, min: 8.191126)
15/08/15 19:22:11 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:11 INFO StatsReportListener: 	 8 %	 8 %	 8 %	 8 %	11 %	11 %	11 %	11 %	11 %
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO MemoryStore: ensureFreeSpace(1605) called with curMem=1278214, maxMem=3333968363
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1605.0 B, free 3.1 GB)
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:50234 (size: 1605.0 B, free: 3.1 GB)
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO SparkContext: Created broadcast 9 from run at ThreadPoolExecutor.java:1145
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 1000
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 1000
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 1000
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO CodecPool: Got brand-new decompressor [.gz]
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 15 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 14 ms. row count = 1000
15/08/15 19:22:11 INFO Executor: Finished task 5.0 in stage 4.0 (TID 216). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 227, localhost, ANY, 1705 bytes)
15/08/15 19:22:11 INFO Executor: Running task 16.0 in stage 4.0 (TID 227)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00073-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8182 length: 8182 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO Executor: Finished task 11.0 in stage 4.0 (TID 222). 19896 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 228, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO Executor: Running task 17.0 in stage 4.0 (TID 228)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00087-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8086 length: 8086 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO Executor: Finished task 3.0 in stage 4.0 (TID 214). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 229, localhost, ANY, 1705 bytes)
15/08/15 19:22:11 INFO Executor: Running task 18.0 in stage 4.0 (TID 229)
15/08/15 19:22:11 INFO Executor: Finished task 6.0 in stage 4.0 (TID 217). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00156-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8180 length: 8180 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 230, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO Executor: Running task 19.0 in stage 4.0 (TID 230)
15/08/15 19:22:11 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 216) in 141 ms on localhost (1/200)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00136-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8184 length: 8184 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:11 INFO Executor: Finished task 2.0 in stage 4.0 (TID 213). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 20.0 in stage 4.0 (TID 231, localhost, ANY, 1703 bytes)
15/08/15 19:22:11 INFO Executor: Running task 20.0 in stage 4.0 (TID 231)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00044-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8209 length: 8209 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 222) in 153 ms on localhost (2/200)
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 214) in 170 ms on localhost (3/200)
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:11 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 217) in 185 ms on localhost (4/200)
15/08/15 19:22:11 INFO Executor: Finished task 7.0 in stage 4.0 (TID 218). 19896 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 21.0 in stage 4.0 (TID 232, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO Executor: Running task 21.0 in stage 4.0 (TID 232)
15/08/15 19:22:11 INFO Executor: Finished task 13.0 in stage 4.0 (TID 224). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 22.0 in stage 4.0 (TID 233, localhost, ANY, 1703 bytes)
15/08/15 19:22:11 INFO Executor: Running task 22.0 in stage 4.0 (TID 233)
15/08/15 19:22:11 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 213) in 239 ms on localhost (5/200)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00145-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8251 length: 8251 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO Executor: Finished task 10.0 in stage 4.0 (TID 221). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00186-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9038 length: 9038 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO Executor: Finished task 12.0 in stage 4.0 (TID 223). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO Executor: Finished task 4.0 in stage 4.0 (TID 215). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO Executor: Finished task 8.0 in stage 4.0 (TID 219). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 23.0 in stage 4.0 (TID 234, localhost, ANY, 1705 bytes)
15/08/15 19:22:11 INFO Executor: Running task 23.0 in stage 4.0 (TID 234)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 24.0 in stage 4.0 (TID 235, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO Executor: Running task 24.0 in stage 4.0 (TID 235)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 25.0 in stage 4.0 (TID 236, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO Executor: Running task 25.0 in stage 4.0 (TID 236)
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00123-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8237 length: 8237 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00185-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8155 length: 8155 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00169-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8183 length: 8183 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO TaskSetManager: Starting task 26.0 in stage 4.0 (TID 237, localhost, ANY, 1702 bytes)
15/08/15 19:22:11 INFO Executor: Running task 26.0 in stage 4.0 (TID 237)
15/08/15 19:22:11 INFO Executor: Finished task 1.0 in stage 4.0 (TID 212). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO Executor: Finished task 15.0 in stage 4.0 (TID 226). 19898 bytes result sent to driver
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO TaskSetManager: Starting task 27.0 in stage 4.0 (TID 238, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO Executor: Running task 27.0 in stage 4.0 (TID 238)
15/08/15 19:22:11 INFO TaskSetManager: Starting task 28.0 in stage 4.0 (TID 239, localhost, ANY, 1703 bytes)
15/08/15 19:22:11 INFO Executor: Running task 28.0 in stage 4.0 (TID 239)
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00161-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9076 length: 9076 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00109-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8190 length: 8190 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00033-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8115 length: 8115 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO Executor: Finished task 14.0 in stage 4.0 (TID 225). 19897 bytes result sent to driver
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:11 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 INFO TaskSetManager: Starting task 29.0 in stage 4.0 (TID 240, localhost, ANY, 1704 bytes)
15/08/15 19:22:11 INFO Executor: Running task 29.0 in stage 4.0 (TID 240)
15/08/15 19:22:11 INFO Executor: Finished task 9.0 in stage 4.0 (TID 220). 19897 bytes result sent to driver
15/08/15 19:22:11 INFO TaskSetManager: Starting task 30.0 in stage 4.0 (TID 241, localhost, ANY, 1701 bytes)
15/08/15 19:22:11 INFO Executor: Running task 30.0 in stage 4.0 (TID 241)
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 28 ms. row count = 1000
15/08/15 19:22:11 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 1000
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00002-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8233 length: 8233 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00017-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8181 length: 8181 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:11 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 0.0 in stage 4.0 (TID 211). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 31.0 in stage 4.0 (TID 242, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 31.0 in stage 4.0 (TID 242)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00178-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8144 length: 8144 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 16.0 in stage 4.0 (TID 227). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 18.0 in stage 4.0 (TID 229). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 32.0 in stage 4.0 (TID 243, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 33.0 in stage 4.0 (TID 244, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 33.0 in stage 4.0 (TID 244)
15/08/15 19:22:12 INFO Executor: Running task 32.0 in stage 4.0 (TID 243)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00019-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8179 length: 8179 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00082-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8162 length: 8162 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 19.0 in stage 4.0 (TID 230). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 17.0 in stage 4.0 (TID 228). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 34.0 in stage 4.0 (TID 245, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 34.0 in stage 4.0 (TID 245)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 35.0 in stage 4.0 (TID 246, localhost, ANY, 1701 bytes)
15/08/15 19:22:12 INFO Executor: Running task 35.0 in stage 4.0 (TID 246)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 223) in 385 ms on localhost (6/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 218) in 393 ms on localhost (7/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00040-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8240 length: 8240 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00047-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8114 length: 8114 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 20.0 in stage 4.0 (TID 231). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 36.0 in stage 4.0 (TID 247, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 26.0 in stage 4.0 (TID 237). 19898 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 37.0 in stage 4.0 (TID 248, localhost, ANY, 1701 bytes)
15/08/15 19:22:12 INFO Executor: Running task 37.0 in stage 4.0 (TID 248)
15/08/15 19:22:12 INFO Executor: Running task 36.0 in stage 4.0 (TID 247)
15/08/15 19:22:12 INFO Executor: Finished task 22.0 in stage 4.0 (TID 233). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 38.0 in stage 4.0 (TID 249, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00191-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8228 length: 8228 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 24.0 in stage 4.0 (TID 235). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00005-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8283 length: 8283 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 39.0 in stage 4.0 (TID 250, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 39.0 in stage 4.0 (TID 250)
15/08/15 19:22:12 INFO Executor: Finished task 33.0 in stage 4.0 (TID 244). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 38.0 in stage 4.0 (TID 249)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 40.0 in stage 4.0 (TID 251, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 40.0 in stage 4.0 (TID 251)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 221) in 425 ms on localhost (8/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00152-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8129 length: 8129 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00092-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8198 length: 8198 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 32.0 in stage 4.0 (TID 243). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 27.0 in stage 4.0 (TID 238). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 41.0 in stage 4.0 (TID 252, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 41.0 in stage 4.0 (TID 252)
15/08/15 19:22:12 INFO Executor: Finished task 31.0 in stage 4.0 (TID 242). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 23.0 in stage 4.0 (TID 234). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 28.0 in stage 4.0 (TID 239). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00032-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8241 length: 8241 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 21.0 in stage 4.0 (TID 232). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 25.0 in stage 4.0 (TID 236). 19898 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 35.0 in stage 4.0 (TID 246). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 30.0 in stage 4.0 (TID 241). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 42.0 in stage 4.0 (TID 253, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 34.0 in stage 4.0 (TID 245). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 42.0 in stage 4.0 (TID 253)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00154-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8152 length: 8152 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 29.0 in stage 4.0 (TID 240). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 43.0 in stage 4.0 (TID 254, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 224) in 450 ms on localhost (9/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00028-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8125 length: 8125 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 43.0 in stage 4.0 (TID 254)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Starting task 44.0 in stage 4.0 (TID 255, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 45.0 in stage 4.0 (TID 256, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 45.0 in stage 4.0 (TID 256)
15/08/15 19:22:12 INFO Executor: Running task 44.0 in stage 4.0 (TID 255)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 46.0 in stage 4.0 (TID 257, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 46.0 in stage 4.0 (TID 257)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00031-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8225 length: 8225 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 47.0 in stage 4.0 (TID 258, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00119-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8265 length: 8265 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00036-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8921 length: 8921 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 48.0 in stage 4.0 (TID 259, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00162-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9026 length: 9026 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 47.0 in stage 4.0 (TID 258)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00179-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8219 length: 8219 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 38.0 in stage 4.0 (TID 249). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 48.0 in stage 4.0 (TID 259)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 215) in 472 ms on localhost (10/200)
15/08/15 19:22:12 INFO Executor: Finished task 36.0 in stage 4.0 (TID 247). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 37.0 in stage 4.0 (TID 248). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00189-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8162 length: 8162 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 49.0 in stage 4.0 (TID 260, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 50.0 in stage 4.0 (TID 261, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Running task 49.0 in stage 4.0 (TID 260)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 50.0 in stage 4.0 (TID 261)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 51.0 in stage 4.0 (TID 262, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 51.0 in stage 4.0 (TID 262)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 52.0 in stage 4.0 (TID 263, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00180-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8255 length: 8255 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00120-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8099 length: 8099 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 52.0 in stage 4.0 (TID 263)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 219) in 478 ms on localhost (11/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00163-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8208 length: 8208 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00111-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8200 length: 8200 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 53.0 in stage 4.0 (TID 264, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 53.0 in stage 4.0 (TID 264)
15/08/15 19:22:12 INFO Executor: Finished task 42.0 in stage 4.0 (TID 253). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 40.0 in stage 4.0 (TID 251). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00147-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8257 length: 8257 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 45.0 in stage 4.0 (TID 256). 19898 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 47.0 in stage 4.0 (TID 258). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 39.0 in stage 4.0 (TID 250). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 54.0 in stage 4.0 (TID 265, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 54.0 in stage 4.0 (TID 265)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 55.0 in stage 4.0 (TID 266, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 41.0 in stage 4.0 (TID 252). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 55.0 in stage 4.0 (TID 266)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Starting task 56.0 in stage 4.0 (TID 267, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 56.0 in stage 4.0 (TID 267)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00150-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8225 length: 8225 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00137-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8244 length: 8244 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 44.0 in stage 4.0 (TID 255). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 43.0 in stage 4.0 (TID 254). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 52.0 in stage 4.0 (TID 263). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 57.0 in stage 4.0 (TID 268, localhost, ANY, 1700 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00014-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9084 length: 9084 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 58.0 in stage 4.0 (TID 269, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 58.0 in stage 4.0 (TID 269)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 57.0 in stage 4.0 (TID 268)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 46.0 in stage 4.0 (TID 257). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 59.0 in stage 4.0 (TID 270, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 59.0 in stage 4.0 (TID 270)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 212) in 517 ms on localhost (12/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00008-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8241 length: 8241 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 220) in 510 ms on localhost (13/200)
15/08/15 19:22:12 INFO Executor: Finished task 50.0 in stage 4.0 (TID 261). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00069-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8235 length: 8235 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 48.0 in stage 4.0 (TID 259). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 226) in 499 ms on localhost (14/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00132-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9073 length: 9073 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 51.0 in stage 4.0 (TID 262). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 60.0 in stage 4.0 (TID 271, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Finished task 49.0 in stage 4.0 (TID 260). 19898 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 53.0 in stage 4.0 (TID 264). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Finished task 55.0 in stage 4.0 (TID 266). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 60.0 in stage 4.0 (TID 271)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 61.0 in stage 4.0 (TID 272, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 61.0 in stage 4.0 (TID 272)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 62.0 in stage 4.0 (TID 273, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 62.0 in stage 4.0 (TID 273)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00095-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8232 length: 8232 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 63.0 in stage 4.0 (TID 274, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 63.0 in stage 4.0 (TID 274)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00198-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8142 length: 8142 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 64.0 in stage 4.0 (TID 275, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00177-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8227 length: 8227 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 64.0 in stage 4.0 (TID 275)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 65.0 in stage 4.0 (TID 276, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 65.0 in stage 4.0 (TID 276)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 225) in 522 ms on localhost (15/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00077-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9023 length: 9023 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00091-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8217 length: 8217 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00114-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8147 length: 8147 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 66.0 in stage 4.0 (TID 277, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Finished task 56.0 in stage 4.0 (TID 267). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 66.0 in stage 4.0 (TID 277)
15/08/15 19:22:12 INFO Executor: Finished task 59.0 in stage 4.0 (TID 270). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 54.0 in stage 4.0 (TID 265). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 57.0 in stage 4.0 (TID 268). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00030-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8184 length: 8184 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 67.0 in stage 4.0 (TID 278, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 67.0 in stage 4.0 (TID 278)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00035-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8199 length: 8199 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 58.0 in stage 4.0 (TID 269). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 68.0 in stage 4.0 (TID 279, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 68.0 in stage 4.0 (TID 279)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 227) in 460 ms on localhost (16/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 229) in 427 ms on localhost (17/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 60.0 in stage 4.0 (TID 271). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 211) in 558 ms on localhost (18/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 64.0 in stage 4.0 (TID 275). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00046-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8095 length: 8095 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 61.0 in stage 4.0 (TID 272). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 69.0 in stage 4.0 (TID 280, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 70.0 in stage 4.0 (TID 281, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 66.0 in stage 4.0 (TID 277). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 69.0 in stage 4.0 (TID 280)
15/08/15 19:22:12 INFO Executor: Running task 70.0 in stage 4.0 (TID 281)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 71.0 in stage 4.0 (TID 282, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00143-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9049 length: 9049 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 65.0 in stage 4.0 (TID 276). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 71.0 in stage 4.0 (TID 282)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00140-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9052 length: 9052 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 62.0 in stage 4.0 (TID 273). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 72.0 in stage 4.0 (TID 283, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 72.0 in stage 4.0 (TID 283)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 230) in 438 ms on localhost (19/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 73.0 in stage 4.0 (TID 284, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00079-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8216 length: 8216 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 67.0 in stage 4.0 (TID 278). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 73.0 in stage 4.0 (TID 284)
15/08/15 19:22:12 INFO Executor: Finished task 63.0 in stage 4.0 (TID 274). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00135-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8204 length: 8204 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 74.0 in stage 4.0 (TID 285, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 228) in 476 ms on localhost (20/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 74.0 in stage 4.0 (TID 285)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00115-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8130 length: 8130 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 75.0 in stage 4.0 (TID 286, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 75.0 in stage 4.0 (TID 286)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 26.0 in stage 4.0 (TID 237) in 326 ms on localhost (21/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00011-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9114 length: 9114 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 76.0 in stage 4.0 (TID 287, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00006-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8180 length: 8180 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 76.0 in stage 4.0 (TID 287)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00076-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8157 length: 8157 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 77.0 in stage 4.0 (TID 288, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 77.0 in stage 4.0 (TID 288)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Starting task 78.0 in stage 4.0 (TID 289, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 69.0 in stage 4.0 (TID 280). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 78.0 in stage 4.0 (TID 289)
15/08/15 19:22:12 INFO Executor: Finished task 68.0 in stage 4.0 (TID 279). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00097-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8122 length: 8122 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 71.0 in stage 4.0 (TID 282). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 79.0 in stage 4.0 (TID 290, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 79.0 in stage 4.0 (TID 290)
15/08/15 19:22:12 INFO Executor: Finished task 70.0 in stage 4.0 (TID 281). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00016-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8196 length: 8196 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 80.0 in stage 4.0 (TID 291, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 72.0 in stage 4.0 (TID 283). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 73.0 in stage 4.0 (TID 284). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 81.0 in stage 4.0 (TID 292, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 80.0 in stage 4.0 (TID 291)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00027-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8174 length: 8174 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 20.0 in stage 4.0 (TID 231) in 449 ms on localhost (22/200)
15/08/15 19:22:12 INFO Executor: Running task 81.0 in stage 4.0 (TID 292)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00187-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8206 length: 8206 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 75.0 in stage 4.0 (TID 286). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 22.0 in stage 4.0 (TID 233) in 374 ms on localhost (23/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00084-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8179 length: 8179 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Finished task 24.0 in stage 4.0 (TID 235) in 353 ms on localhost (24/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Starting task 82.0 in stage 4.0 (TID 293, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 82.0 in stage 4.0 (TID 293)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 76.0 in stage 4.0 (TID 287). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00173-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8995 length: 8995 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 83.0 in stage 4.0 (TID 294, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 83.0 in stage 4.0 (TID 294)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 84.0 in stage 4.0 (TID 295, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 84.0 in stage 4.0 (TID 295)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00009-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8103 length: 8103 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 78.0 in stage 4.0 (TID 289). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 77.0 in stage 4.0 (TID 288). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 85.0 in stage 4.0 (TID 296, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 33.0 in stage 4.0 (TID 244) in 271 ms on localhost (25/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00089-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8170 length: 8170 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 85.0 in stage 4.0 (TID 296)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 32.0 in stage 4.0 (TID 243) in 274 ms on localhost (26/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 86.0 in stage 4.0 (TID 297, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 81.0 in stage 4.0 (TID 292). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 86.0 in stage 4.0 (TID 297)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 74.0 in stage 4.0 (TID 285). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00083-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8209 length: 8209 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 87.0 in stage 4.0 (TID 298, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00175-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8145 length: 8145 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 80.0 in stage 4.0 (TID 291). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 31.0 in stage 4.0 (TID 242) in 296 ms on localhost (27/200)
15/08/15 19:22:12 INFO Executor: Running task 87.0 in stage 4.0 (TID 298)
15/08/15 19:22:12 INFO Executor: Finished task 83.0 in stage 4.0 (TID 294). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 27.0 in stage 4.0 (TID 238) in 367 ms on localhost (28/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 79.0 in stage 4.0 (TID 290). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 88.0 in stage 4.0 (TID 299, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 88.0 in stage 4.0 (TID 299)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 89.0 in stage 4.0 (TID 300, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 82.0 in stage 4.0 (TID 293). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 84.0 in stage 4.0 (TID 295). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 89.0 in stage 4.0 (TID 300)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00192-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8133 length: 8133 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00086-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8207 length: 8207 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00059-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8209 length: 8209 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 90.0 in stage 4.0 (TID 301, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 90.0 in stage 4.0 (TID 301)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00074-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9146 length: 9146 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 91.0 in stage 4.0 (TID 302, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 91.0 in stage 4.0 (TID 302)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00045-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8161 length: 8161 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 92.0 in stage 4.0 (TID 303, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 86.0 in stage 4.0 (TID 297). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 23.0 in stage 4.0 (TID 234) in 401 ms on localhost (29/200)
15/08/15 19:22:12 INFO Executor: Running task 92.0 in stage 4.0 (TID 303)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 93.0 in stage 4.0 (TID 304, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 93.0 in stage 4.0 (TID 304)
15/08/15 19:22:12 INFO Executor: Finished task 85.0 in stage 4.0 (TID 296). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00159-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8247 length: 8247 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 94.0 in stage 4.0 (TID 305, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 94.0 in stage 4.0 (TID 305)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 95.0 in stage 4.0 (TID 306, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 95.0 in stage 4.0 (TID 306)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00057-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8245 length: 8245 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 87.0 in stage 4.0 (TID 298). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00116-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8201 length: 8201 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Finished task 89.0 in stage 4.0 (TID 300). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00053-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8178 length: 8178 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 21.0 in stage 4.0 (TID 232) in 439 ms on localhost (30/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 96.0 in stage 4.0 (TID 307, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 88.0 in stage 4.0 (TID 299). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 28.0 in stage 4.0 (TID 239) in 395 ms on localhost (31/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 25.0 in stage 4.0 (TID 236) in 411 ms on localhost (32/200)
15/08/15 19:22:12 INFO Executor: Running task 96.0 in stage 4.0 (TID 307)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 97.0 in stage 4.0 (TID 308, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00101-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9045 length: 9045 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 90.0 in stage 4.0 (TID 301). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Starting task 98.0 in stage 4.0 (TID 309, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 97.0 in stage 4.0 (TID 308)
15/08/15 19:22:12 INFO Executor: Running task 98.0 in stage 4.0 (TID 309)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 99.0 in stage 4.0 (TID 310, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 91.0 in stage 4.0 (TID 302). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 100.0 in stage 4.0 (TID 311, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 99.0 in stage 4.0 (TID 310)
15/08/15 19:22:12 INFO Executor: Running task 100.0 in stage 4.0 (TID 311)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00066-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8158 length: 8158 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00041-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8180 length: 8180 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 92.0 in stage 4.0 (TID 303). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 101.0 in stage 4.0 (TID 312, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 101.0 in stage 4.0 (TID 312)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00100-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8129 length: 8129 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00153-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8155 length: 8155 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 102.0 in stage 4.0 (TID 313, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 102.0 in stage 4.0 (TID 313)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 103.0 in stage 4.0 (TID 314, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 95.0 in stage 4.0 (TID 306). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 103.0 in stage 4.0 (TID 314)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 104.0 in stage 4.0 (TID 315, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00071-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8196 length: 8196 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 105.0 in stage 4.0 (TID 316, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00099-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8223 length: 8223 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 105.0 in stage 4.0 (TID 316)
15/08/15 19:22:12 INFO Executor: Running task 104.0 in stage 4.0 (TID 315)
15/08/15 19:22:12 INFO Executor: Finished task 93.0 in stage 4.0 (TID 304). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 106.0 in stage 4.0 (TID 317, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00023-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8149 length: 8149 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 107.0 in stage 4.0 (TID 318, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00103-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8101 length: 8101 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 94.0 in stage 4.0 (TID 305). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 107.0 in stage 4.0 (TID 318)
15/08/15 19:22:12 INFO Executor: Running task 106.0 in stage 4.0 (TID 317)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 108.0 in stage 4.0 (TID 319, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00193-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8173 length: 8173 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00110-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8157 length: 8157 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 109.0 in stage 4.0 (TID 320, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 109.0 in stage 4.0 (TID 320)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 96.0 in stage 4.0 (TID 307). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 110.0 in stage 4.0 (TID 321, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 110.0 in stage 4.0 (TID 321)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 111.0 in stage 4.0 (TID 322, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00165-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8185 length: 8185 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 112.0 in stage 4.0 (TID 323, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 108.0 in stage 4.0 (TID 319)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00003-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8188 length: 8188 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Running task 111.0 in stage 4.0 (TID 322)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00160-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8162 length: 8162 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00067-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9058 length: 9058 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00050-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8198 length: 8198 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 29.0 in stage 4.0 (TID 240) in 394 ms on localhost (33/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 35.0 in stage 4.0 (TID 246) in 310 ms on localhost (34/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 98.0 in stage 4.0 (TID 309). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Running task 112.0 in stage 4.0 (TID 323)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 113.0 in stage 4.0 (TID 324, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 99.0 in stage 4.0 (TID 310). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 34.0 in stage 4.0 (TID 245) in 315 ms on localhost (35/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00118-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9092 length: 9092 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 113.0 in stage 4.0 (TID 324)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Finished task 107.0 in stage 4.0 (TID 318). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 100.0 in stage 4.0 (TID 311). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00164-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8137 length: 8137 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 109.0 in stage 4.0 (TID 320). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 101.0 in stage 4.0 (TID 312). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 114.0 in stage 4.0 (TID 325, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 114.0 in stage 4.0 (TID 325)
15/08/15 19:22:12 INFO Executor: Finished task 105.0 in stage 4.0 (TID 316). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 115.0 in stage 4.0 (TID 326, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 115.0 in stage 4.0 (TID 326)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00056-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8194 length: 8194 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 116.0 in stage 4.0 (TID 327, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Running task 116.0 in stage 4.0 (TID 327)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 111.0 in stage 4.0 (TID 322). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 117.0 in stage 4.0 (TID 328, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 118.0 in stage 4.0 (TID 329, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00131-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8274 length: 8274 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 118.0 in stage 4.0 (TID 329)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 119.0 in stage 4.0 (TID 330, localhost, ANY, 1701 bytes)
15/08/15 19:22:12 INFO Executor: Running task 119.0 in stage 4.0 (TID 330)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 103.0 in stage 4.0 (TID 314). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 112.0 in stage 4.0 (TID 323). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 117.0 in stage 4.0 (TID 328)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 30.0 in stage 4.0 (TID 241) in 418 ms on localhost (36/200)
15/08/15 19:22:12 INFO Executor: Finished task 102.0 in stage 4.0 (TID 313). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 36.0 in stage 4.0 (TID 247) in 311 ms on localhost (37/200)
15/08/15 19:22:12 INFO Executor: Finished task 104.0 in stage 4.0 (TID 315). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00102-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8164 length: 8164 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 97.0 in stage 4.0 (TID 308). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00020-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8244 length: 8244 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Finished task 38.0 in stage 4.0 (TID 249) in 310 ms on localhost (38/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00183-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8268 length: 8268 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00122-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8185 length: 8185 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 120.0 in stage 4.0 (TID 331, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 120.0 in stage 4.0 (TID 331)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 121.0 in stage 4.0 (TID 332, localhost, ANY, 1701 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 121.0 in stage 4.0 (TID 332)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 122.0 in stage 4.0 (TID 333, localhost, ANY, 1701 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00094-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8177 length: 8177 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 122.0 in stage 4.0 (TID 333)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 123.0 in stage 4.0 (TID 334, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 123.0 in stage 4.0 (TID 334)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00181-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8242 length: 8242 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 106.0 in stage 4.0 (TID 317). 19898 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00146-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8180 length: 8180 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 116.0 in stage 4.0 (TID 327). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 113.0 in stage 4.0 (TID 324). 19898 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00121-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8192 length: 8192 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 110.0 in stage 4.0 (TID 321). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 124.0 in stage 4.0 (TID 335, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 124.0 in stage 4.0 (TID 335)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 125.0 in stage 4.0 (TID 336, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 125.0 in stage 4.0 (TID 336)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 126.0 in stage 4.0 (TID 337, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 127.0 in stage 4.0 (TID 338, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 126.0 in stage 4.0 (TID 337)
15/08/15 19:22:12 INFO Executor: Finished task 108.0 in stage 4.0 (TID 319). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 117.0 in stage 4.0 (TID 328). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 127.0 in stage 4.0 (TID 338)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 120.0 in stage 4.0 (TID 331). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 128.0 in stage 4.0 (TID 339, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 128.0 in stage 4.0 (TID 339)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 129.0 in stage 4.0 (TID 340, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 129.0 in stage 4.0 (TID 340)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 130.0 in stage 4.0 (TID 341, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 37.0 in stage 4.0 (TID 248) in 341 ms on localhost (39/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 42.0 in stage 4.0 (TID 253) in 315 ms on localhost (40/200)
15/08/15 19:22:12 INFO Executor: Finished task 119.0 in stage 4.0 (TID 330). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 114.0 in stage 4.0 (TID 325). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 115.0 in stage 4.0 (TID 326). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 131.0 in stage 4.0 (TID 342, localhost, ANY, 1701 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00106-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8166 length: 8166 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00155-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8198 length: 8198 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 131.0 in stage 4.0 (TID 342)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00142-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9095 length: 9095 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 132.0 in stage 4.0 (TID 343, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00171-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8109 length: 8109 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 122.0 in stage 4.0 (TID 333). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 123.0 in stage 4.0 (TID 334). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00182-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8300 length: 8300 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00195-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8099 length: 8099 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 133.0 in stage 4.0 (TID 344, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 132.0 in stage 4.0 (TID 343)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 133.0 in stage 4.0 (TID 344)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00043-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8182 length: 8182 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 134.0 in stage 4.0 (TID 345, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 118.0 in stage 4.0 (TID 329). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 130.0 in stage 4.0 (TID 341)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00021-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8262 length: 8262 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 134.0 in stage 4.0 (TID 345)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 135.0 in stage 4.0 (TID 346, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 121.0 in stage 4.0 (TID 332). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 135.0 in stage 4.0 (TID 346)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00139-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8168 length: 8168 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00170-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8167 length: 8167 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00001-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8149 length: 8149 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00117-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8147 length: 8147 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 136.0 in stage 4.0 (TID 347, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 127.0 in stage 4.0 (TID 338). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 136.0 in stage 4.0 (TID 347)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 137.0 in stage 4.0 (TID 348, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 137.0 in stage 4.0 (TID 348)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 138.0 in stage 4.0 (TID 349, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 138.0 in stage 4.0 (TID 349)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00199-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8137 length: 8137 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 40.0 in stage 4.0 (TID 251) in 363 ms on localhost (41/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Starting task 139.0 in stage 4.0 (TID 350, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 139.0 in stage 4.0 (TID 350)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00125-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8214 length: 8214 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 140.0 in stage 4.0 (TID 351, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 128.0 in stage 4.0 (TID 339). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 47.0 in stage 4.0 (TID 258) in 337 ms on localhost (42/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 39.0 in stage 4.0 (TID 250) in 372 ms on localhost (43/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 45.0 in stage 4.0 (TID 256) in 340 ms on localhost (44/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 124.0 in stage 4.0 (TID 335). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 141.0 in stage 4.0 (TID 352, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 141.0 in stage 4.0 (TID 352)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 142.0 in stage 4.0 (TID 353, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00105-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9027 length: 9027 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00004-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8231 length: 8231 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00108-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9082 length: 9082 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 142.0 in stage 4.0 (TID 353)
15/08/15 19:22:12 INFO Executor: Running task 140.0 in stage 4.0 (TID 351)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 44.0 in stage 4.0 (TID 255) in 350 ms on localhost (45/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00049-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8176 length: 8176 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00174-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8221 length: 8221 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 125.0 in stage 4.0 (TID 336). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 143.0 in stage 4.0 (TID 354, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Running task 143.0 in stage 4.0 (TID 354)
15/08/15 19:22:12 INFO Executor: Finished task 131.0 in stage 4.0 (TID 342). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 137.0 in stage 4.0 (TID 348). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 144.0 in stage 4.0 (TID 355, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 144.0 in stage 4.0 (TID 355)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00039-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8104 length: 8104 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 145.0 in stage 4.0 (TID 356, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 129.0 in stage 4.0 (TID 340). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 146.0 in stage 4.0 (TID 357, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 146.0 in stage 4.0 (TID 357)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00070-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9090 length: 9090 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 43.0 in stage 4.0 (TID 254) in 360 ms on localhost (46/200)
15/08/15 19:22:12 INFO Executor: Running task 145.0 in stage 4.0 (TID 356)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Finished task 126.0 in stage 4.0 (TID 337). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 133.0 in stage 4.0 (TID 344). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 130.0 in stage 4.0 (TID 341). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00063-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8152 length: 8152 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00051-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8301 length: 8301 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 41.0 in stage 4.0 (TID 252) in 379 ms on localhost (47/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 46.0 in stage 4.0 (TID 257) in 372 ms on localhost (48/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 147.0 in stage 4.0 (TID 358, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 147.0 in stage 4.0 (TID 358)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 134.0 in stage 4.0 (TID 345). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 148.0 in stage 4.0 (TID 359, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00176-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8987 length: 8987 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 148.0 in stage 4.0 (TID 359)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 132.0 in stage 4.0 (TID 343). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00055-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8239 length: 8239 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Finished task 138.0 in stage 4.0 (TID 349). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 140.0 in stage 4.0 (TID 351). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 141.0 in stage 4.0 (TID 352). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 149.0 in stage 4.0 (TID 360, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 139.0 in stage 4.0 (TID 350). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 136.0 in stage 4.0 (TID 347). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 150.0 in stage 4.0 (TID 361, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 150.0 in stage 4.0 (TID 361)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 151.0 in stage 4.0 (TID 362, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 135.0 in stage 4.0 (TID 346). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 149.0 in stage 4.0 (TID 360)
15/08/15 19:22:12 INFO Executor: Running task 151.0 in stage 4.0 (TID 362)
15/08/15 19:22:12 INFO Executor: Finished task 142.0 in stage 4.0 (TID 353). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00010-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8189 length: 8189 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 144.0 in stage 4.0 (TID 355). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00065-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8099 length: 8099 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 146.0 in stage 4.0 (TID 357). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 152.0 in stage 4.0 (TID 363, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 152.0 in stage 4.0 (TID 363)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 145.0 in stage 4.0 (TID 356). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 143.0 in stage 4.0 (TID 354). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 52.0 in stage 4.0 (TID 263) in 382 ms on localhost (49/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00158-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8221 length: 8221 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 153.0 in stage 4.0 (TID 364, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 153.0 in stage 4.0 (TID 364)
15/08/15 19:22:12 INFO Executor: Finished task 147.0 in stage 4.0 (TID 358). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00058-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8203 length: 8203 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 148.0 in stage 4.0 (TID 359). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 50.0 in stage 4.0 (TID 261) in 392 ms on localhost (50/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 48.0 in stage 4.0 (TID 259) in 402 ms on localhost (51/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00042-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8223 length: 8223 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 49.0 in stage 4.0 (TID 260) in 397 ms on localhost (52/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 154.0 in stage 4.0 (TID 365, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 154.0 in stage 4.0 (TID 365)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 155.0 in stage 4.0 (TID 366, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 155.0 in stage 4.0 (TID 366)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 156.0 in stage 4.0 (TID 367, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00007-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8184 length: 8184 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00029-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8232 length: 8232 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Finished task 56.0 in stage 4.0 (TID 267) in 382 ms on localhost (53/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 53.0 in stage 4.0 (TID 264) in 391 ms on localhost (54/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 156.0 in stage 4.0 (TID 367)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 157.0 in stage 4.0 (TID 368, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 150.0 in stage 4.0 (TID 361). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 157.0 in stage 4.0 (TID 368)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00085-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8041 length: 8041 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 149.0 in stage 4.0 (TID 360). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 55.0 in stage 4.0 (TID 266) in 386 ms on localhost (55/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 51.0 in stage 4.0 (TID 262) in 405 ms on localhost (56/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00048-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8181 length: 8181 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 151.0 in stage 4.0 (TID 362). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 158.0 in stage 4.0 (TID 369, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 159.0 in stage 4.0 (TID 370, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 158.0 in stage 4.0 (TID 369)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 57.0 in stage 4.0 (TID 268) in 396 ms on localhost (57/200)
15/08/15 19:22:12 INFO Executor: Running task 159.0 in stage 4.0 (TID 370)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 59.0 in stage 4.0 (TID 270) in 383 ms on localhost (58/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 155.0 in stage 4.0 (TID 366). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00096-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8224 length: 8224 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00157-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8187 length: 8187 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 160.0 in stage 4.0 (TID 371, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 152.0 in stage 4.0 (TID 363). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 161.0 in stage 4.0 (TID 372, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 161.0 in stage 4.0 (TID 372)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 160.0 in stage 4.0 (TID 371)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 54.0 in stage 4.0 (TID 265) in 413 ms on localhost (59/200)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 162.0 in stage 4.0 (TID 373, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00130-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8232 length: 8232 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 162.0 in stage 4.0 (TID 373)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00022-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8221 length: 8221 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 163.0 in stage 4.0 (TID 374, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 64.0 in stage 4.0 (TID 275) in 378 ms on localhost (60/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00018-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8129 length: 8129 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 163.0 in stage 4.0 (TID 374)
15/08/15 19:22:12 INFO Executor: Finished task 157.0 in stage 4.0 (TID 368). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 153.0 in stage 4.0 (TID 364). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 58.0 in stage 4.0 (TID 269) in 401 ms on localhost (61/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00141-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8195 length: 8195 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 156.0 in stage 4.0 (TID 367). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 164.0 in stage 4.0 (TID 375, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 65.0 in stage 4.0 (TID 276) in 386 ms on localhost (62/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 60.0 in stage 4.0 (TID 271) in 399 ms on localhost (63/200)
15/08/15 19:22:12 INFO Executor: Finished task 154.0 in stage 4.0 (TID 365). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 164.0 in stage 4.0 (TID 375)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 165.0 in stage 4.0 (TID 376, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 165.0 in stage 4.0 (TID 376)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 61.0 in stage 4.0 (TID 272) in 398 ms on localhost (64/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 66.0 in stage 4.0 (TID 277) in 384 ms on localhost (65/200)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 166.0 in stage 4.0 (TID 377, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 166.0 in stage 4.0 (TID 377)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00133-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8292 length: 8292 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00127-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8156 length: 8156 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00197-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8174 length: 8174 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 161.0 in stage 4.0 (TID 372). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 163.0 in stage 4.0 (TID 374). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 158.0 in stage 4.0 (TID 369). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 160.0 in stage 4.0 (TID 371). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 159.0 in stage 4.0 (TID 370). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 167.0 in stage 4.0 (TID 378, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 167.0 in stage 4.0 (TID 378)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 168.0 in stage 4.0 (TID 379, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 169.0 in stage 4.0 (TID 380, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 169.0 in stage 4.0 (TID 380)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 170.0 in stage 4.0 (TID 381, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00194-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8277 length: 8277 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 171.0 in stage 4.0 (TID 382, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 171.0 in stage 4.0 (TID 382)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00072-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8129 length: 8129 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00151-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8135 length: 8135 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 168.0 in stage 4.0 (TID 379)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 172.0 in stage 4.0 (TID 383, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 162.0 in stage 4.0 (TID 373). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 170.0 in stage 4.0 (TID 381)
15/08/15 19:22:12 INFO Executor: Running task 172.0 in stage 4.0 (TID 383)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00012-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8199 length: 8199 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 62.0 in stage 4.0 (TID 273) in 421 ms on localhost (66/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 63.0 in stage 4.0 (TID 274) in 421 ms on localhost (67/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 67.0 in stage 4.0 (TID 278) in 403 ms on localhost (68/200)
15/08/15 19:22:12 INFO Executor: Finished task 164.0 in stage 4.0 (TID 375). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 69.0 in stage 4.0 (TID 280) in 391 ms on localhost (69/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00062-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8230 length: 8230 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 173.0 in stage 4.0 (TID 384, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 173.0 in stage 4.0 (TID 384)
15/08/15 19:22:12 INFO Executor: Finished task 165.0 in stage 4.0 (TID 376). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 174.0 in stage 4.0 (TID 385, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00013-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8176 length: 8176 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00024-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8122 length: 8122 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 167.0 in stage 4.0 (TID 378). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Finished task 171.0 in stage 4.0 (TID 382). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 174.0 in stage 4.0 (TID 385)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00113-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8189 length: 8189 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 175.0 in stage 4.0 (TID 386, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Running task 175.0 in stage 4.0 (TID 386)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 166.0 in stage 4.0 (TID 377). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 176.0 in stage 4.0 (TID 387, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00034-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8176 length: 8176 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 176.0 in stage 4.0 (TID 387)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 68.0 in stage 4.0 (TID 279) in 420 ms on localhost (70/200)
15/08/15 19:22:12 INFO Executor: Finished task 172.0 in stage 4.0 (TID 383). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 71.0 in stage 4.0 (TID 282) in 401 ms on localhost (71/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 70.0 in stage 4.0 (TID 281) in 407 ms on localhost (72/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00064-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8254 length: 8254 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 173.0 in stage 4.0 (TID 384). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Starting task 177.0 in stage 4.0 (TID 388, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Running task 177.0 in stage 4.0 (TID 388)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 178.0 in stage 4.0 (TID 389, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 178.0 in stage 4.0 (TID 389)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 72.0 in stage 4.0 (TID 283) in 402 ms on localhost (73/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 75.0 in stage 4.0 (TID 286) in 386 ms on localhost (74/200)
15/08/15 19:22:12 INFO Executor: Finished task 169.0 in stage 4.0 (TID 380). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00172-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9063 length: 9063 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 168.0 in stage 4.0 (TID 379). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Finished task 76.0 in stage 4.0 (TID 287) in 382 ms on localhost (75/200)
15/08/15 19:22:12 INFO Executor: Finished task 170.0 in stage 4.0 (TID 381). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00190-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8206 length: 8206 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 73.0 in stage 4.0 (TID 284) in 402 ms on localhost (76/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 78.0 in stage 4.0 (TID 289) in 376 ms on localhost (77/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 77.0 in stage 4.0 (TID 288) in 385 ms on localhost (78/200)
15/08/15 19:22:12 INFO Executor: Finished task 174.0 in stage 4.0 (TID 385). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 179.0 in stage 4.0 (TID 390, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 81.0 in stage 4.0 (TID 292) in 375 ms on localhost (79/200)
15/08/15 19:22:12 INFO Executor: Finished task 175.0 in stage 4.0 (TID 386). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 74.0 in stage 4.0 (TID 285) in 409 ms on localhost (80/200)
15/08/15 19:22:12 INFO Executor: Running task 179.0 in stage 4.0 (TID 390)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Finished task 80.0 in stage 4.0 (TID 291) in 377 ms on localhost (81/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 176.0 in stage 4.0 (TID 387). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 180.0 in stage 4.0 (TID 391, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 181.0 in stage 4.0 (TID 392, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 180.0 in stage 4.0 (TID 391)
15/08/15 19:22:12 INFO Executor: Running task 181.0 in stage 4.0 (TID 392)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 182.0 in stage 4.0 (TID 393, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00052-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9145 length: 9145 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 182.0 in stage 4.0 (TID 393)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 183.0 in stage 4.0 (TID 394, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00060-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8210 length: 8210 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00167-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8238 length: 8238 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 183.0 in stage 4.0 (TID 394)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00128-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9102 length: 9102 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00112-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8103 length: 8103 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 84.0 in stage 4.0 (TID 295) in 369 ms on localhost (82/200)
15/08/15 19:22:12 INFO Executor: Finished task 177.0 in stage 4.0 (TID 388). 19896 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 79.0 in stage 4.0 (TID 290) in 391 ms on localhost (83/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 82.0 in stage 4.0 (TID 293) in 380 ms on localhost (84/200)
15/08/15 19:22:12 INFO Executor: Finished task 178.0 in stage 4.0 (TID 389). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 83.0 in stage 4.0 (TID 294) in 373 ms on localhost (85/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 184.0 in stage 4.0 (TID 395, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO Executor: Running task 184.0 in stage 4.0 (TID 395)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 85.0 in stage 4.0 (TID 296) in 374 ms on localhost (86/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 86.0 in stage 4.0 (TID 297) in 368 ms on localhost (87/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00025-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8265 length: 8265 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 182.0 in stage 4.0 (TID 393). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 90.0 in stage 4.0 (TID 301) in 360 ms on localhost (88/200)
15/08/15 19:22:12 INFO Executor: Finished task 181.0 in stage 4.0 (TID 392). 19898 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 180.0 in stage 4.0 (TID 391). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 179.0 in stage 4.0 (TID 390). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 87.0 in stage 4.0 (TID 298) in 372 ms on localhost (89/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 88.0 in stage 4.0 (TID 299) in 365 ms on localhost (90/200)
15/08/15 19:22:12 INFO Executor: Finished task 183.0 in stage 4.0 (TID 394). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 185.0 in stage 4.0 (TID 396, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO Executor: Running task 185.0 in stage 4.0 (TID 396)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 89.0 in stage 4.0 (TID 300) in 367 ms on localhost (91/200)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 186.0 in stage 4.0 (TID 397, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 186.0 in stage 4.0 (TID 397)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 91.0 in stage 4.0 (TID 302) in 360 ms on localhost (92/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 95.0 in stage 4.0 (TID 306) in 349 ms on localhost (93/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 92.0 in stage 4.0 (TID 303) in 359 ms on localhost (94/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00149-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8247 length: 8247 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00196-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8216 length: 8216 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 187.0 in stage 4.0 (TID 398, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 187.0 in stage 4.0 (TID 398)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00075-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8119 length: 8119 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 188.0 in stage 4.0 (TID 399, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 188.0 in stage 4.0 (TID 399)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 93.0 in stage 4.0 (TID 304) in 358 ms on localhost (95/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 96.0 in stage 4.0 (TID 307) in 352 ms on localhost (96/200)
15/08/15 19:22:12 INFO Executor: Finished task 184.0 in stage 4.0 (TID 395). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00138-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8187 length: 8187 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Finished task 98.0 in stage 4.0 (TID 309) in 345 ms on localhost (97/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 94.0 in stage 4.0 (TID 305) in 360 ms on localhost (98/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 189.0 in stage 4.0 (TID 400, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 189.0 in stage 4.0 (TID 400)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 190.0 in stage 4.0 (TID 401, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 190.0 in stage 4.0 (TID 401)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Finished task 99.0 in stage 4.0 (TID 310) in 348 ms on localhost (99/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00061-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8195 length: 8195 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Starting task 191.0 in stage 4.0 (TID 402, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO Executor: Running task 191.0 in stage 4.0 (TID 402)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00038-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8161 length: 8161 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO TaskSetManager: Starting task 192.0 in stage 4.0 (TID 403, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Running task 192.0 in stage 4.0 (TID 403)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00015-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8168 length: 8168 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 193.0 in stage 4.0 (TID 404, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Finished task 185.0 in stage 4.0 (TID 396). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Running task 193.0 in stage 4.0 (TID 404)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 100.0 in stage 4.0 (TID 311) in 351 ms on localhost (100/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00168-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8191 length: 8191 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 186.0 in stage 4.0 (TID 397). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 107.0 in stage 4.0 (TID 318) in 343 ms on localhost (101/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00134-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8172 length: 8172 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Finished task 187.0 in stage 4.0 (TID 398). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 188.0 in stage 4.0 (TID 399). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Starting task 194.0 in stage 4.0 (TID 405, localhost, ANY, 1705 bytes)
15/08/15 19:22:12 INFO Executor: Running task 194.0 in stage 4.0 (TID 405)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Starting task 195.0 in stage 4.0 (TID 406, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO Executor: Running task 195.0 in stage 4.0 (TID 406)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 196.0 in stage 4.0 (TID 407, localhost, ANY, 1703 bytes)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00081-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8129 length: 8129 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO Executor: Running task 196.0 in stage 4.0 (TID 407)
15/08/15 19:22:12 INFO TaskSetManager: Starting task 197.0 in stage 4.0 (TID 408, localhost, ANY, 1701 bytes)
15/08/15 19:22:12 INFO Executor: Running task 197.0 in stage 4.0 (TID 408)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00129-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9028 length: 9028 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO TaskSetManager: Starting task 198.0 in stage 4.0 (TID 409, localhost, ANY, 1704 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00188-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8190 length: 8190 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO TaskSetManager: Finished task 109.0 in stage 4.0 (TID 320) in 350 ms on localhost (102/200)
15/08/15 19:22:12 INFO Executor: Running task 198.0 in stage 4.0 (TID 409)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00000-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 9094 length: 9094 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 191.0 in stage 4.0 (TID 402). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO Executor: Finished task 190.0 in stage 4.0 (TID 401). 19897 bytes result sent to driver
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 111.0 in stage 4.0 (TID 322) in 351 ms on localhost (103/200)
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00098-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8182 length: 8182 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO TaskSetManager: Finished task 105.0 in stage 4.0 (TID 316) in 358 ms on localhost (104/200)
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO Executor: Finished task 192.0 in stage 4.0 (TID 403). 19898 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Starting task 199.0 in stage 4.0 (TID 410, localhost, ANY, 1702 bytes)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Running task 199.0 in stage 4.0 (TID 410)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 101.0 in stage 4.0 (TID 312) in 368 ms on localhost (105/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 112.0 in stage 4.0 (TID 323) in 355 ms on localhost (106/200)
15/08/15 19:22:12 INFO Executor: Finished task 193.0 in stage 4.0 (TID 404). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 103.0 in stage 4.0 (TID 314) in 366 ms on localhost (107/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 102.0 in stage 4.0 (TID 313) in 368 ms on localhost (108/200)
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_tmp_par/part-r-00090-c82af9c9-e665-4201-914a-f0bb802bd26a.gz.parquet start: 0 end: 8196 length: 8196 hosts: [] requestedSchema: message root {
  optional int32 t_partkey;
  optional double t_avg_quantity;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"t_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"t_avg_quantity","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 116.0 in stage 4.0 (TID 327) in 337 ms on localhost (109/200)
15/08/15 19:22:12 INFO Executor: Finished task 189.0 in stage 4.0 (TID 400). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 104.0 in stage 4.0 (TID 315) in 372 ms on localhost (110/200)
15/08/15 19:22:12 INFO Executor: Finished task 194.0 in stage 4.0 (TID 405). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO Executor: Finished task 197.0 in stage 4.0 (TID 408). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO Executor: Finished task 198.0 in stage 4.0 (TID 409). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 97.0 in stage 4.0 (TID 308) in 384 ms on localhost (111/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 113.0 in stage 4.0 (TID 324) in 357 ms on localhost (112/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 106.0 in stage 4.0 (TID 317) in 373 ms on localhost (113/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 110.0 in stage 4.0 (TID 321) in 369 ms on localhost (114/200)
15/08/15 19:22:12 INFO Executor: Finished task 195.0 in stage 4.0 (TID 406). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
15/08/15 19:22:12 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:12 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 1000
15/08/15 19:22:12 INFO TaskSetManager: Finished task 108.0 in stage 4.0 (TID 319) in 373 ms on localhost (115/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 117.0 in stage 4.0 (TID 328) in 341 ms on localhost (116/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 119.0 in stage 4.0 (TID 330) in 339 ms on localhost (117/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 120.0 in stage 4.0 (TID 331) in 330 ms on localhost (118/200)
15/08/15 19:22:12 INFO Executor: Finished task 199.0 in stage 4.0 (TID 410). 19897 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 115.0 in stage 4.0 (TID 326) in 352 ms on localhost (119/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 114.0 in stage 4.0 (TID 325) in 357 ms on localhost (120/200)
15/08/15 19:22:12 INFO Executor: Finished task 196.0 in stage 4.0 (TID 407). 19896 bytes result sent to driver
15/08/15 19:22:12 INFO TaskSetManager: Finished task 123.0 in stage 4.0 (TID 334) in 331 ms on localhost (121/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 122.0 in stage 4.0 (TID 333) in 334 ms on localhost (122/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 121.0 in stage 4.0 (TID 332) in 336 ms on localhost (123/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 118.0 in stage 4.0 (TID 329) in 350 ms on localhost (124/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 128.0 in stage 4.0 (TID 339) in 327 ms on localhost (125/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 124.0 in stage 4.0 (TID 335) in 336 ms on localhost (126/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 125.0 in stage 4.0 (TID 336) in 333 ms on localhost (127/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 131.0 in stage 4.0 (TID 342) in 321 ms on localhost (128/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 137.0 in stage 4.0 (TID 348) in 295 ms on localhost (129/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 127.0 in stage 4.0 (TID 338) in 333 ms on localhost (130/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 129.0 in stage 4.0 (TID 340) in 330 ms on localhost (131/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 133.0 in stage 4.0 (TID 344) in 321 ms on localhost (132/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 126.0 in stage 4.0 (TID 337) in 338 ms on localhost (133/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 134.0 in stage 4.0 (TID 345) in 317 ms on localhost (134/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 130.0 in stage 4.0 (TID 341) in 334 ms on localhost (135/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 132.0 in stage 4.0 (TID 343) in 327 ms on localhost (136/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 138.0 in stage 4.0 (TID 349) in 302 ms on localhost (137/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 140.0 in stage 4.0 (TID 351) in 297 ms on localhost (138/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 141.0 in stage 4.0 (TID 352) in 295 ms on localhost (139/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 139.0 in stage 4.0 (TID 350) in 301 ms on localhost (140/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 136.0 in stage 4.0 (TID 347) in 317 ms on localhost (141/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 135.0 in stage 4.0 (TID 346) in 322 ms on localhost (142/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 144.0 in stage 4.0 (TID 355) in 286 ms on localhost (143/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 142.0 in stage 4.0 (TID 353) in 298 ms on localhost (144/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 146.0 in stage 4.0 (TID 357) in 287 ms on localhost (145/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 143.0 in stage 4.0 (TID 354) in 293 ms on localhost (146/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 147.0 in stage 4.0 (TID 358) in 270 ms on localhost (147/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 148.0 in stage 4.0 (TID 359) in 270 ms on localhost (148/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 145.0 in stage 4.0 (TID 356) in 293 ms on localhost (149/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 150.0 in stage 4.0 (TID 361) in 257 ms on localhost (150/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 149.0 in stage 4.0 (TID 360) in 270 ms on localhost (151/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 151.0 in stage 4.0 (TID 362) in 259 ms on localhost (152/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 152.0 in stage 4.0 (TID 363) in 258 ms on localhost (153/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 155.0 in stage 4.0 (TID 366) in 241 ms on localhost (154/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 157.0 in stage 4.0 (TID 368) in 237 ms on localhost (155/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 156.0 in stage 4.0 (TID 367) in 242 ms on localhost (156/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 153.0 in stage 4.0 (TID 364) in 250 ms on localhost (157/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 154.0 in stage 4.0 (TID 365) in 247 ms on localhost (158/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 161.0 in stage 4.0 (TID 372) in 222 ms on localhost (159/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 163.0 in stage 4.0 (TID 374) in 215 ms on localhost (160/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 158.0 in stage 4.0 (TID 369) in 237 ms on localhost (161/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 160.0 in stage 4.0 (TID 371) in 229 ms on localhost (162/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 159.0 in stage 4.0 (TID 370) in 236 ms on localhost (163/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 164.0 in stage 4.0 (TID 375) in 211 ms on localhost (164/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 165.0 in stage 4.0 (TID 376) in 209 ms on localhost (165/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 162.0 in stage 4.0 (TID 373) in 224 ms on localhost (166/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 167.0 in stage 4.0 (TID 378) in 205 ms on localhost (167/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 171.0 in stage 4.0 (TID 382) in 190 ms on localhost (168/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 172.0 in stage 4.0 (TID 383) in 190 ms on localhost (169/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 166.0 in stage 4.0 (TID 377) in 210 ms on localhost (170/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 173.0 in stage 4.0 (TID 384) in 183 ms on localhost (171/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 169.0 in stage 4.0 (TID 380) in 197 ms on localhost (172/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 168.0 in stage 4.0 (TID 379) in 200 ms on localhost (173/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 174.0 in stage 4.0 (TID 385) in 186 ms on localhost (174/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 170.0 in stage 4.0 (TID 381) in 197 ms on localhost (175/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 175.0 in stage 4.0 (TID 386) in 181 ms on localhost (176/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 176.0 in stage 4.0 (TID 387) in 182 ms on localhost (177/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 177.0 in stage 4.0 (TID 388) in 178 ms on localhost (178/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 178.0 in stage 4.0 (TID 389) in 178 ms on localhost (179/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 182.0 in stage 4.0 (TID 393) in 165 ms on localhost (180/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 183.0 in stage 4.0 (TID 394) in 164 ms on localhost (181/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 181.0 in stage 4.0 (TID 392) in 167 ms on localhost (182/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 180.0 in stage 4.0 (TID 391) in 170 ms on localhost (183/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 179.0 in stage 4.0 (TID 390) in 178 ms on localhost (184/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 184.0 in stage 4.0 (TID 395) in 160 ms on localhost (185/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 185.0 in stage 4.0 (TID 396) in 146 ms on localhost (186/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 186.0 in stage 4.0 (TID 397) in 143 ms on localhost (187/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 187.0 in stage 4.0 (TID 398) in 142 ms on localhost (188/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 191.0 in stage 4.0 (TID 402) in 134 ms on localhost (189/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 190.0 in stage 4.0 (TID 401) in 136 ms on localhost (190/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 188.0 in stage 4.0 (TID 399) in 143 ms on localhost (191/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 192.0 in stage 4.0 (TID 403) in 135 ms on localhost (192/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 193.0 in stage 4.0 (TID 404) in 135 ms on localhost (193/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 194.0 in stage 4.0 (TID 405) in 132 ms on localhost (194/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 189.0 in stage 4.0 (TID 400) in 143 ms on localhost (195/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 197.0 in stage 4.0 (TID 408) in 130 ms on localhost (196/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 195.0 in stage 4.0 (TID 406) in 136 ms on localhost (197/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 198.0 in stage 4.0 (TID 409) in 133 ms on localhost (198/200)
15/08/15 19:22:12 INFO TaskSetManager: Finished task 199.0 in stage 4.0 (TID 410) in 128 ms on localhost (199/200)
15/08/15 19:22:12 INFO DAGScheduler: ResultStage 4 (run at ThreadPoolExecutor.java:1145) finished in 1.179 s
15/08/15 19:22:12 INFO TaskSetManager: Finished task 196.0 in stage 4.0 (TID 407) in 138 ms on localhost (200/200)
15/08/15 19:22:12 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@4783e60d
15/08/15 19:22:12 INFO DAGScheduler: Job 3 finished: run at ThreadPoolExecutor.java:1145, took 1.230903 s
15/08/15 19:22:12 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/15 19:22:12 INFO StatsReportListener: task runtime:(count: 200, mean: 317.955000, stdev: 95.264070, max: 558.000000, min: 128.000000)
15/08/15 19:22:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:12 INFO StatsReportListener: 	128.0 ms	141.0 ms	167.0 ms	247.0 ms	343.0 ms	383.0 ms	413.0 ms	450.0 ms	558.0 ms
15/08/15 19:22:12 INFO StatsReportListener: task result size:(count: 200, mean: 19896.730000, stdev: 0.535817, max: 19898.000000, min: 19896.000000)
15/08/15 19:22:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:12 INFO StatsReportListener: 	19.4 KB	19.4 KB	19.4 KB	19.4 KB	19.4 KB	19.4 KB	19.4 KB	19.4 KB	19.4 KB
15/08/15 19:22:12 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 9.073907, stdev: 5.622166, max: 26.241135, min: 2.439024)
15/08/15 19:22:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:12 INFO StatsReportListener: 	 2 %	 3 %	 4 %	 5 %	 7 %	11 %	19 %	22 %	26 %
15/08/15 19:22:12 INFO StatsReportListener: other time pct: (count: 200, mean: 90.926093, stdev: 5.622166, max: 97.560976, min: 73.758865)
15/08/15 19:22:12 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:12 INFO StatsReportListener: 	74 %	79 %	82 %	89 %	93 %	95 %	96 %	97 %	98 %
15/08/15 19:22:13 INFO MemoryStore: ensureFreeSpace(32123408) called with curMem=1279819, maxMem=3333968363
15/08/15 19:22:13 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 30.6 MB, free 3.1 GB)
15/08/15 19:22:14 INFO MemoryStore: ensureFreeSpace(3081671) called with curMem=33403227, maxMem=3333968363
15/08/15 19:22:14 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.9 MB, free 3.1 GB)
15/08/15 19:22:14 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:50234 (size: 2.9 MB, free: 3.1 GB)
15/08/15 19:22:14 INFO SparkContext: Created broadcast 10 from run at ThreadPoolExecutor.java:1145
15/08/15 19:22:14 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:50234 in memory (size: 2.3 KB, free: 3.1 GB)
15/08/15 19:22:14 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:50234 in memory (size: 3.4 KB, free: 3.1 GB)
15/08/15 19:22:14 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:50234 in memory (size: 1773.0 B, free: 3.1 GB)
15/08/15 19:22:14 INFO ParquetRelation2: Using default output committer for Parquet: parquet.hadoop.ParquetOutputCommitter
15/08/15 19:22:14 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:14 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1$$anon$2: Using Task Side Metadata Split Strategy
15/08/15 19:22:14 INFO DAGScheduler: Registering RDD 25 (processCmd at CliDriver.java:423)
15/08/15 19:22:14 INFO DAGScheduler: Got job 4 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/15 19:22:14 INFO DAGScheduler: Final stage: ResultStage 6(processCmd at CliDriver.java:423)
15/08/15 19:22:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
15/08/15 19:22:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
15/08/15 19:22:14 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423), which has no missing parents
15/08/15 19:22:14 INFO MemoryStore: ensureFreeSpace(12712) called with curMem=36463637, maxMem=3333968363
15/08/15 19:22:14 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 12.4 KB, free 3.1 GB)
15/08/15 19:22:14 INFO MemoryStore: ensureFreeSpace(6158) called with curMem=36476349, maxMem=3333968363
15/08/15 19:22:14 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.0 KB, free 3.1 GB)
15/08/15 19:22:14 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:50234 (size: 6.0 KB, free: 3.1 GB)
15/08/15 19:22:14 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:874
15/08/15 19:22:14 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[25] at processCmd at CliDriver.java:423)
15/08/15 19:22:14 INFO TaskSchedulerImpl: Adding task set 5.0 with 8 tasks
15/08/15 19:22:14 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 411, localhost, ANY, 1777 bytes)
15/08/15 19:22:14 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 412, localhost, ANY, 1780 bytes)
15/08/15 19:22:14 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 413, localhost, ANY, 1778 bytes)
15/08/15 19:22:14 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 414, localhost, ANY, 1777 bytes)
15/08/15 19:22:14 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 415, localhost, ANY, 1777 bytes)
15/08/15 19:22:14 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 416, localhost, ANY, 1779 bytes)
15/08/15 19:22:14 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 417, localhost, ANY, 1779 bytes)
15/08/15 19:22:14 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 418, localhost, ANY, 1778 bytes)
15/08/15 19:22:14 INFO Executor: Running task 0.0 in stage 5.0 (TID 411)
15/08/15 19:22:14 INFO Executor: Running task 1.0 in stage 5.0 (TID 412)
15/08/15 19:22:14 INFO Executor: Running task 2.0 in stage 5.0 (TID 413)
15/08/15 19:22:14 INFO Executor: Running task 3.0 in stage 5.0 (TID 414)
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000007_0 start: 0 end: 26536257 length: 26536257 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_partkey;
  optional double l_extendedprice;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000000_0 start: 0 end: 26485016 length: 26485016 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_partkey;
  optional double l_extendedprice;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000005_0 start: 0 end: 26505368 length: 26505368 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_partkey;
  optional double l_extendedprice;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:14 INFO Executor: Running task 4.0 in stage 5.0 (TID 415)
15/08/15 19:22:14 INFO Executor: Running task 5.0 in stage 5.0 (TID 416)
15/08/15 19:22:14 INFO Executor: Running task 6.0 in stage 5.0 (TID 417)
15/08/15 19:22:14 INFO Executor: Running task 7.0 in stage 5.0 (TID 418)
15/08/15 19:22:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000006_0 start: 0 end: 26243215 length: 26243215 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_partkey;
  optional double l_extendedprice;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000001_0 start: 0 end: 26576747 length: 26576747 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_partkey;
  optional double l_extendedprice;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000004_0 start: 0 end: 26235204 length: 26235204 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_partkey;
  optional double l_extendedprice;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000002_0 start: 0 end: 26234990 length: 26234990 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_partkey;
  optional double l_extendedprice;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:14 INFO ParquetRelation2$$anonfun$buildScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/lineitem_par/000003_0 start: 0 end: 26210131 length: 26210131 hosts: [] requestedSchema: message root {
  optional double l_quantity;
  optional int32 l_partkey;
  optional double l_extendedprice;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"l_orderkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_linenumber","type":"integer","nullable":true,"metadata":{}},{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}},{"name":"l_discount","type":"double","nullable":true,"metadata":{}},{"name":"l_tax","type":"double","nullable":true,"metadata":{}},{"name":"l_returnflag","type":"string","nullable":true,"metadata":{}},{"name":"l_linestatus","type":"string","nullable":true,"metadata":{}},{"name":"l_shipdate","type":"string","nullable":true,"metadata":{}},{"name":"l_commitdate","type":"string","nullable":true,"metadata":{}},{"name":"l_receiptdate","type":"string","nullable":true,"metadata":{}},{"name":"l_shipinstruct","type":"string","nullable":true,"metadata":{}},{"name":"l_shipmode","type":"string","nullable":true,"metadata":{}},{"name":"l_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"l_quantity","type":"double","nullable":true,"metadata":{}},{"name":"l_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"l_extendedprice","type":"double","nullable":true,"metadata":{}}]}}}
15/08/15 19:22:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:14 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/15 19:22:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749050 records.
15/08/15 19:22:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749056 records.
15/08/15 19:22:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749096 records.
15/08/15 19:22:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 751036 records.
15/08/15 19:22:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 755812 records.
15/08/15 19:22:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 748901 records.
15/08/15 19:22:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749157 records.
15/08/15 19:22:14 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 749107 records.
15/08/15 19:22:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:14 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/15 19:22:14 INFO InternalParquetRecordReader: block read in memory in 50 ms. row count = 749050
15/08/15 19:22:14 INFO InternalParquetRecordReader: block read in memory in 57 ms. row count = 749056
15/08/15 19:22:14 INFO InternalParquetRecordReader: block read in memory in 66 ms. row count = 748901
15/08/15 19:22:14 INFO InternalParquetRecordReader: block read in memory in 73 ms. row count = 749107
15/08/15 19:22:14 INFO InternalParquetRecordReader: block read in memory in 72 ms. row count = 749096
15/08/15 19:22:14 INFO InternalParquetRecordReader: block read in memory in 82 ms. row count = 749157
15/08/15 19:22:14 INFO InternalParquetRecordReader: block read in memory in 80 ms. row count = 751036
15/08/15 19:22:14 INFO InternalParquetRecordReader: block read in memory in 86 ms. row count = 755812
15/08/15 19:22:14 INFO Executor: Finished task 4.0 in stage 5.0 (TID 415). 1925 bytes result sent to driver
15/08/15 19:22:14 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 415) in 468 ms on localhost (1/8)
15/08/15 19:22:14 INFO Executor: Finished task 6.0 in stage 5.0 (TID 417). 1925 bytes result sent to driver
15/08/15 19:22:14 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 417) in 478 ms on localhost (2/8)
15/08/15 19:22:14 INFO Executor: Finished task 3.0 in stage 5.0 (TID 414). 1925 bytes result sent to driver
15/08/15 19:22:14 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 414) in 484 ms on localhost (3/8)
15/08/15 19:22:14 INFO Executor: Finished task 5.0 in stage 5.0 (TID 416). 1925 bytes result sent to driver
15/08/15 19:22:14 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 416) in 498 ms on localhost (4/8)
15/08/15 19:22:14 INFO Executor: Finished task 2.0 in stage 5.0 (TID 413). 1925 bytes result sent to driver
15/08/15 19:22:14 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 413) in 522 ms on localhost (5/8)
15/08/15 19:22:15 INFO Executor: Finished task 7.0 in stage 5.0 (TID 418). 1925 bytes result sent to driver
15/08/15 19:22:15 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 418) in 567 ms on localhost (6/8)
15/08/15 19:22:15 INFO Executor: Finished task 0.0 in stage 5.0 (TID 411). 1925 bytes result sent to driver
15/08/15 19:22:15 INFO Executor: Finished task 1.0 in stage 5.0 (TID 412). 1925 bytes result sent to driver
15/08/15 19:22:15 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 412) in 582 ms on localhost (7/8)
15/08/15 19:22:15 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 411) in 585 ms on localhost (8/8)
15/08/15 19:22:15 INFO DAGScheduler: ShuffleMapStage 5 (processCmd at CliDriver.java:423) finished in 0.586 s
15/08/15 19:22:15 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/15 19:22:15 INFO DAGScheduler: looking for newly runnable stages
15/08/15 19:22:15 INFO DAGScheduler: running: Set()
15/08/15 19:22:15 INFO DAGScheduler: waiting: Set(ResultStage 6)
15/08/15 19:22:15 INFO DAGScheduler: failed: Set()
15/08/15 19:22:15 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@183d4252
15/08/15 19:22:15 INFO StatsReportListener: task runtime:(count: 8, mean: 523.000000, stdev: 45.356918, max: 585.000000, min: 468.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	468.0 ms	468.0 ms	468.0 ms	484.0 ms	522.0 ms	582.0 ms	585.0 ms	585.0 ms	585.0 ms
15/08/15 19:22:15 INFO StatsReportListener: shuffle bytes written:(count: 8, mean: 31.000000, stdev: 0.000000, max: 31.000000, min: 31.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B	31.0 B
15/08/15 19:22:15 INFO DAGScheduler: Missing parents for ResultStage 6: List()
15/08/15 19:22:15 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423), which is now runnable
15/08/15 19:22:15 INFO StatsReportListener: task result size:(count: 8, mean: 1925.000000, stdev: 0.000000, max: 1925.000000, min: 1925.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B	1925.0 B
15/08/15 19:22:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 8, mean: 95.769661, stdev: 0.706172, max: 96.735395, min: 94.658120)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	95 %	95 %	95 %	95 %	96 %	96 %	97 %	97 %	97 %
15/08/15 19:22:15 INFO StatsReportListener: other time pct: (count: 8, mean: 4.230339, stdev: 0.706172, max: 5.341880, min: 3.264605)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	 3 %	 3 %	 3 %	 4 %	 4 %	 5 %	 5 %	 5 %	 5 %
15/08/15 19:22:15 INFO MemoryStore: ensureFreeSpace(82160) called with curMem=36482507, maxMem=3333968363
15/08/15 19:22:15 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 80.2 KB, free 3.1 GB)
15/08/15 19:22:15 INFO MemoryStore: ensureFreeSpace(31699) called with curMem=36564667, maxMem=3333968363
15/08/15 19:22:15 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 31.0 KB, free 3.1 GB)
15/08/15 19:22:15 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:50234 (size: 31.0 KB, free: 3.1 GB)
15/08/15 19:22:15 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:874
15/08/15 19:22:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[28] at processCmd at CliDriver.java:423)
15/08/15 19:22:15 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/08/15 19:22:15 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 419, localhost, PROCESS_LOCAL, 1165 bytes)
15/08/15 19:22:15 INFO Executor: Running task 0.0 in stage 6.0 (TID 419)
15/08/15 19:22:15 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks out of 8 blocks
15/08/15 19:22:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/15 19:22:15 INFO DefaultWriterContainer: Using output committer class parquet.hadoop.ParquetOutputCommitter for appending.
15/08/15 19:22:15 INFO CodecConfig: Compression: GZIP
15/08/15 19:22:15 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/15 19:22:15 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/15 19:22:15 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/15 19:22:15 INFO ParquetOutputFormat: Dictionary is on
15/08/15 19:22:15 INFO ParquetOutputFormat: Validation is off
15/08/15 19:22:15 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/15 19:22:15 INFO CodecPool: Got brand-new compressor [.gz]
15/08/15 19:22:15 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 29,150,423
15/08/15 19:22:15 INFO ColumnChunkPageWriteStore: written 75B for [avg_yearly] DOUBLE: 1 values, 14B raw, 34B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/15 19:22:15 INFO FileOutputCommitter: Saved output of task 'attempt_201508151922_0006_m_000000_0' to hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q17_small_quantity_order_revenue_par/_temporary/0/task_201508151922_0006_m_000000
15/08/15 19:22:15 INFO SparkHadoopMapRedUtil: attempt_201508151922_0006_m_000000_0: Committed
15/08/15 19:22:15 INFO Executor: Finished task 0.0 in stage 6.0 (TID 419). 843 bytes result sent to driver
15/08/15 19:22:15 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 419) in 119 ms on localhost (1/1)
15/08/15 19:22:15 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/15 19:22:15 INFO DAGScheduler: ResultStage 6 (processCmd at CliDriver.java:423) finished in 0.119 s
15/08/15 19:22:15 INFO DAGScheduler: Job 4 finished: processCmd at CliDriver.java:423, took 0.791733 s
15/08/15 19:22:15 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@200ebd33
15/08/15 19:22:15 INFO StatsReportListener: task runtime:(count: 1, mean: 119.000000, stdev: 0.000000, max: 119.000000, min: 119.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	119.0 ms	119.0 ms	119.0 ms	119.0 ms	119.0 ms	119.0 ms	119.0 ms	119.0 ms	119.0 ms
15/08/15 19:22:15 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms
15/08/15 19:22:15 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/15 19:22:15 INFO StatsReportListener: task result size:(count: 1, mean: 843.000000, stdev: 0.000000, max: 843.000000, min: 843.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B	843.0 B
15/08/15 19:22:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 59.663866, stdev: 0.000000, max: 59.663866, min: 59.663866)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	60 %	60 %	60 %	60 %	60 %	60 %	60 %	60 %	60 %
15/08/15 19:22:15 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/15 19:22:15 INFO StatsReportListener: other time pct: (count: 1, mean: 40.336134, stdev: 0.000000, max: 40.336134, min: 40.336134)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	40 %	40 %	40 %	40 %	40 %	40 %	40 %	40 %	40 %
15/08/15 19:22:15 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/15 19:22:15 INFO DefaultWriterContainer: Job job_201508151922_0000 committed.
15/08/15 19:22:15 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/15 19:22:15 INFO ParquetFileReader: reading summary file: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q17_small_quantity_order_revenue_par/_common_metadata
15/08/15 19:22:15 INFO SparkContext: Starting job: processCmd at CliDriver.java:423
15/08/15 19:22:15 INFO DAGScheduler: Got job 5 (processCmd at CliDriver.java:423) with 1 output partitions (allowLocal=false)
15/08/15 19:22:15 INFO DAGScheduler: Final stage: ResultStage 7(processCmd at CliDriver.java:423)
15/08/15 19:22:15 INFO DAGScheduler: Parents of final stage: List()
15/08/15 19:22:15 INFO DAGScheduler: Missing parents: List()
15/08/15 19:22:15 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at processCmd at CliDriver.java:423), which has no missing parents
15/08/15 19:22:15 INFO MemoryStore: ensureFreeSpace(2848) called with curMem=36596366, maxMem=3333968363
15/08/15 19:22:15 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 2.8 KB, free 3.1 GB)
15/08/15 19:22:15 INFO MemoryStore: ensureFreeSpace(1722) called with curMem=36599214, maxMem=3333968363
15/08/15 19:22:15 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1722.0 B, free 3.1 GB)
15/08/15 19:22:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:50234 (size: 1722.0 B, free: 3.1 GB)
15/08/15 19:22:15 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:874
15/08/15 19:22:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at processCmd at CliDriver.java:423)
15/08/15 19:22:15 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
15/08/15 19:22:15 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 420, localhost, PROCESS_LOCAL, 1316 bytes)
15/08/15 19:22:15 INFO Executor: Running task 0.0 in stage 7.0 (TID 420)
15/08/15 19:22:15 INFO Executor: Finished task 0.0 in stage 7.0 (TID 420). 606 bytes result sent to driver
15/08/15 19:22:15 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 420) in 15 ms on localhost (1/1)
15/08/15 19:22:15 INFO DAGScheduler: ResultStage 7 (processCmd at CliDriver.java:423) finished in 0.016 s
15/08/15 19:22:15 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/15 19:22:15 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5b8994f
15/08/15 19:22:15 INFO DAGScheduler: Job 5 finished: processCmd at CliDriver.java:423, took 0.031663 s
Time taken: 4.761 seconds
15/08/15 19:22:15 INFO StatsReportListener: task runtime:(count: 1, mean: 15.000000, stdev: 0.000000, max: 15.000000, min: 15.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO CliDriver: Time taken: 4.761 seconds
15/08/15 19:22:15 INFO StatsReportListener: 	15.0 ms	15.0 ms	15.0 ms	15.0 ms	15.0 ms	15.0 ms	15.0 ms	15.0 ms	15.0 ms
15/08/15 19:22:15 INFO StatsReportListener: task result size:(count: 1, mean: 606.000000, stdev: 0.000000, max: 606.000000, min: 606.000000)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B	606.0 B
15/08/15 19:22:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 6.666667, stdev: 0.000000, max: 6.666667, min: 6.666667)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	 7 %	 7 %	 7 %	 7 %	 7 %	 7 %	 7 %	 7 %	 7 %
15/08/15 19:22:15 INFO StatsReportListener: other time pct: (count: 1, mean: 93.333333, stdev: 0.000000, max: 93.333333, min: 93.333333)
15/08/15 19:22:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/15 19:22:15 INFO StatsReportListener: 	93 %	93 %	93 %	93 %	93 %	93 %	93 %	93 %	93 %
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/08/15 19:22:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/08/15 19:22:15 INFO SparkUI: Stopped Spark web UI at http://192.168.122.56:4040
15/08/15 19:22:15 INFO DAGScheduler: Stopping DAGScheduler
15/08/15 19:22:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/15 19:22:15 INFO Utils: path = /tmp/spark-350ecfc2-4e22-48a1-8f61-561c2d9257a9/blockmgr-d1896467-952a-4412-ab10-f9c82143f9d8, already present as root for deletion.
15/08/15 19:22:15 INFO MemoryStore: MemoryStore cleared
15/08/15 19:22:15 INFO BlockManager: BlockManager stopped
15/08/15 19:22:15 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/15 19:22:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/15 19:22:15 INFO SparkContext: Successfully stopped SparkContext
15/08/15 19:22:15 INFO Utils: Shutdown hook called
15/08/15 19:22:15 INFO Utils: Deleting directory /tmp/spark-d872d8a6-11f1-468a-ab7f-37a07f988233
15/08/15 19:22:15 INFO Utils: Deleting directory /tmp/spark-741d7b13-808c-445f-8569-a38650041175
15/08/15 19:22:15 INFO Utils: Deleting directory /tmp/spark-350ecfc2-4e22-48a1-8f61-561c2d9257a9
15/08/15 19:22:15 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/08/15 19:22:15 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/08/15 19:22:15 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
