-- number of partitions when shuffling data for aggregates and joins
--set spark.sql.shuffle.partitions=1024;

DROP TABLE q11_important_stock_par;
DROP TABLE q11_part_tmp_par;

-- create the target table
create table q11_important_stock_par(ps_partkey INT, value DOUBLE) STORED AS parquet;
create table q11_part_tmp_par(ps_partkey int, part_value double) STORED AS parquet;

-- the query
insert into table q11_part_tmp_par
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_par
select ps_partkey, part_value as value
from q11_part_tmp_par
        join (  select sum(part_value) as total_value from q11_part_tmp_par) sum_tmp
where part_value > total_value * 0.0001
order by value desc;

