-- number of partitions when shuffling data for aggregates and joins
--set spark.sql.shuffle.partitions=1024;

DROP TABLE q11_important_stock_spark;
DROP TABLE q11_part_tmp_spark;
DROP TABLE q11_sum_tmp_spark;

-- create the target table
create table q11_important_stock_spark(ps_partkey INT, value DOUBLE) STORED AS parquet;
create table q11_part_tmp_spark(ps_partkey int, part_value double) STORED AS parquet;
create table q11_sum_tmp_spark(total_value double) STORED AS parquet;

-- the query
insert into table q11_part_tmp_spark
select 
  ps_partkey, 
  sum(ps_supplycost * ps_availqty) as part_value 
from
  nation_par n join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
  join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by 
	ps_partkey;

insert into table q11_sum_tmp_spark
select 
  sum(part_value) as total_value
from 
  q11_part_tmp_spark;

insert into table q11_important_stock_spark
select 
  ps_partkey, part_value as value
from
  (
    select ps_partkey, part_value, total_value
    from q11_part_tmp_spark join q11_sum_tmp_spark
  ) a
where part_value > total_value * 0.0001
order by value desc;

