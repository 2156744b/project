-- number of partitions when shuffling data for aggregates and joins
--set spark.sql.shuffle.partitions=1024;

DROP TABLE q11_important_stock_par_spark;
DROP TABLE q11_part_tmp_par_spark;

-- create the target table
create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet;
create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet;

-- the query
insert into table q11_part_tmp_par_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey;

insert into table q11_important_stock_par_spark
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par_spark) sum_tmp
        join q11_part_tmp_par_spark
where part_value > total_value * 0.0001
order by value desc;

Spark assembly has been built with Hive, including Datanucleus jars on classpath
15/08/09 15:26:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/08/09 15:26:53 INFO metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
15/08/09 15:26:53 INFO metastore: Connected to metastore.
15/08/09 15:26:54 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
15/08/09 15:26:54 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/08/09 15:26:54 INFO SecurityManager: Changing view acls to: hive
15/08/09 15:26:54 INFO SecurityManager: Changing modify acls to: hive
15/08/09 15:26:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hive); users with modify permissions: Set(hive)
15/08/09 15:26:55 INFO Slf4jLogger: Slf4jLogger started
15/08/09 15:26:55 INFO Remoting: Starting remoting
15/08/09 15:26:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@sandbox.hortonworks.com:47450]
15/08/09 15:26:55 INFO Utils: Successfully started service 'sparkDriver' on port 47450.
15/08/09 15:26:55 INFO SparkEnv: Registering MapOutputTracker
15/08/09 15:26:55 INFO SparkEnv: Registering BlockManagerMaster
15/08/09 15:26:55 INFO DiskBlockManager: Created local directory at /tmp/spark-e130b153-c0e0-4978-b777-1d35876e805a/spark-e9624889-17b1-49df-b2d3-f8b6cbd30b1b
15/08/09 15:26:55 INFO MemoryStore: MemoryStore started with capacity 3.1 GB
15/08/09 15:26:55 INFO HttpFileServer: HTTP File server directory is /tmp/spark-65455c63-0901-46d2-a77d-56636b71da3f/spark-4b336fce-8b26-4a9d-a4f7-c84601bdf62b
15/08/09 15:26:55 INFO HttpServer: Starting HTTP Server
15/08/09 15:26:55 INFO Utils: Successfully started service 'HTTP file server' on port 33526.
15/08/09 15:26:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/08/09 15:26:56 INFO SparkUI: Started SparkUI at http://sandbox.hortonworks.com:4040
15/08/09 15:26:56 INFO Executor: Starting executor ID <driver> on host localhost
15/08/09 15:26:56 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@sandbox.hortonworks.com:47450/user/HeartbeatReceiver
15/08/09 15:26:56 INFO NettyBlockTransferService: Server created on 44535
15/08/09 15:26:56 INFO BlockManagerMaster: Trying to register BlockManager
15/08/09 15:26:56 INFO BlockManagerMasterActor: Registering block manager localhost:44535 with 3.1 GB RAM, BlockManagerId(<driver>, localhost, 44535)
15/08/09 15:26:56 INFO BlockManagerMaster: Registered BlockManager
SET spark.sql.hive.version=0.13.1
15/08/09 15:26:57 INFO ParseDriver: Parsing command: DROP TABLE q11_important_stock_par_spark
15/08/09 15:26:57 INFO ParseDriver: Parse Completed
15/08/09 15:26:58 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:58 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:58 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/09 15:26:58 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO ParseDriver: Parsing command: DROP TABLE q11_important_stock_par_spark
15/08/09 15:26:59 INFO ParseDriver: Parse Completed
15/08/09 15:26:59 INFO PerfLogger: </PERFLOG method=parse start=1439134019005 end=1439134019006 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO Driver: Semantic Analysis Completed
15/08/09 15:26:59 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1439134019006 end=1439134019184 duration=178 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/09 15:26:59 INFO PerfLogger: </PERFLOG method=compile start=1439134018953 end=1439134019198 duration=245 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO Driver: Starting command: DROP TABLE q11_important_stock_par_spark
15/08/09 15:26:59 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1439134018950 end=1439134019237 duration=287 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: </PERFLOG method=runTasks start=1439134019239 end=1439134019455 duration=216 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: </PERFLOG method=Driver.execute start=1439134019198 end=1439134019455 duration=257 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/09 15:26:59 INFO Driver: OK
15/08/09 15:26:59 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: </PERFLOG method=releaseLocks start=1439134019456 end=1439134019456 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO PerfLogger: </PERFLOG method=Driver.run start=1439134018950 end=1439134019456 duration=506 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:26:59 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/09 15:26:59 INFO DAGScheduler: Got job 0 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/09 15:26:59 INFO DAGScheduler: Final stage: Stage 0(collect at SparkPlan.scala:84)
15/08/09 15:26:59 INFO DAGScheduler: Parents of final stage: List()
15/08/09 15:26:59 INFO DAGScheduler: Missing parents: List()
15/08/09 15:26:59 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[2] at map at SparkPlan.scala:84), which has no missing parents
15/08/09 15:26:59 INFO MemoryStore: ensureFreeSpace(1896) called with curMem=0, maxMem=3333968363
15/08/09 15:26:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1896.0 B, free 3.1 GB)
15/08/09 15:26:59 INFO MemoryStore: ensureFreeSpace(1208) called with curMem=1896, maxMem=3333968363
15/08/09 15:26:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1208.0 B, free 3.1 GB)
15/08/09 15:27:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:44535 (size: 1208.0 B, free: 3.1 GB)
15/08/09 15:27:00 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/08/09 15:27:00 INFO DefaultExecutionContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[2] at map at SparkPlan.scala:84)
15/08/09 15:27:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/08/09 15:27:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/09 15:27:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/09 15:27:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 618 bytes result sent to driver
15/08/09 15:27:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 81 ms on localhost (1/1)
15/08/09 15:27:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/08/09 15:27:00 INFO DAGScheduler: Stage 0 (collect at SparkPlan.scala:84) finished in 0.102 s
15/08/09 15:27:00 INFO DAGScheduler: Job 0 finished: collect at SparkPlan.scala:84, took 0.554946 s
Time taken: 3.125 seconds
15/08/09 15:27:00 INFO CliDriver: Time taken: 3.125 seconds
15/08/09 15:27:00 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@5eede331
15/08/09 15:27:00 INFO StatsReportListener: task runtime:(count: 1, mean: 81.000000, stdev: 0.000000, max: 81.000000, min: 81.000000)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	81.0 ms	81.0 ms	81.0 ms	81.0 ms	81.0 ms	81.0 ms	81.0 ms	81.0 ms	81.0 ms
15/08/09 15:27:00 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/09 15:27:00 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 14.814815, stdev: 0.000000, max: 14.814815, min: 14.814815)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	15 %	15 %	15 %	15 %	15 %	15 %	15 %	15 %	15 %
15/08/09 15:27:00 INFO StatsReportListener: other time pct: (count: 1, mean: 85.185185, stdev: 0.000000, max: 85.185185, min: 85.185185)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	85 %	85 %	85 %	85 %	85 %	85 %	85 %	85 %	85 %
15/08/09 15:27:00 INFO ParseDriver: Parsing command: DROP TABLE q11_part_tmp_par_spark
15/08/09 15:27:00 INFO ParseDriver: Parse Completed
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO ParseDriver: Parsing command: DROP TABLE q11_part_tmp_par_spark
15/08/09 15:27:00 INFO ParseDriver: Parse Completed
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=parse start=1439134020307 end=1439134020307 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO Driver: Semantic Analysis Completed
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1439134020307 end=1439134020346 duration=39 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=compile start=1439134020306 end=1439134020347 duration=41 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO Driver: Starting command: DROP TABLE q11_part_tmp_par_spark
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1439134020306 end=1439134020347 duration=41 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=runTasks start=1439134020347 end=1439134020447 duration=100 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=Driver.execute start=1439134020347 end=1439134020448 duration=101 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/09 15:27:00 INFO Driver: OK
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=releaseLocks start=1439134020448 end=1439134020448 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=Driver.run start=1439134020306 end=1439134020448 duration=142 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/09 15:27:00 INFO DAGScheduler: Got job 1 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/09 15:27:00 INFO DAGScheduler: Final stage: Stage 1(collect at SparkPlan.scala:84)
15/08/09 15:27:00 INFO DAGScheduler: Parents of final stage: List()
15/08/09 15:27:00 INFO DAGScheduler: Missing parents: List()
15/08/09 15:27:00 INFO DAGScheduler: Submitting Stage 1 (MappedRDD[5] at map at SparkPlan.scala:84), which has no missing parents
15/08/09 15:27:00 INFO MemoryStore: ensureFreeSpace(1896) called with curMem=3104, maxMem=3333968363
15/08/09 15:27:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 1896.0 B, free 3.1 GB)
15/08/09 15:27:00 INFO MemoryStore: ensureFreeSpace(1207) called with curMem=5000, maxMem=3333968363
15/08/09 15:27:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1207.0 B, free 3.1 GB)
15/08/09 15:27:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:44535 (size: 1207.0 B, free: 3.1 GB)
15/08/09 15:27:00 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/08/09 15:27:00 INFO DefaultExecutionContext: Created broadcast 1 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1 (MappedRDD[5] at map at SparkPlan.scala:84)
15/08/09 15:27:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/08/09 15:27:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/09 15:27:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/08/09 15:27:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 618 bytes result sent to driver
15/08/09 15:27:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 31 ms on localhost (1/1)
15/08/09 15:27:00 INFO DAGScheduler: Stage 1 (collect at SparkPlan.scala:84) finished in 0.036 s
15/08/09 15:27:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/08/09 15:27:00 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@56d58984
15/08/09 15:27:00 INFO DAGScheduler: Job 1 finished: collect at SparkPlan.scala:84, took 0.070439 s
Time taken: 0.369 seconds
15/08/09 15:27:00 INFO CliDriver: Time taken: 0.369 seconds
15/08/09 15:27:00 INFO StatsReportListener: task runtime:(count: 1, mean: 31.000000, stdev: 0.000000, max: 31.000000, min: 31.000000)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	31.0 ms	31.0 ms	31.0 ms	31.0 ms	31.0 ms	31.0 ms	31.0 ms	31.0 ms	31.0 ms
15/08/09 15:27:00 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/09 15:27:00 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 6.451613, stdev: 0.000000, max: 6.451613, min: 6.451613)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %	 6 %
15/08/09 15:27:00 INFO StatsReportListener: other time pct: (count: 1, mean: 93.548387, stdev: 0.000000, max: 93.548387, min: 93.548387)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	94 %	94 %	94 %	94 %	94 %	94 %	94 %	94 %	94 %
15/08/09 15:27:00 INFO ParseDriver: Parsing command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/09 15:27:00 INFO ParseDriver: Parse Completed
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO ParseDriver: Parsing command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/09 15:27:00 INFO ParseDriver: Parse Completed
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=parse start=1439134020748 end=1439134020750 duration=2 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO SemanticAnalyzer: Starting Semantic Analysis
15/08/09 15:27:00 INFO SemanticAnalyzer: Creating table q11_important_stock_par_spark position=13
15/08/09 15:27:00 INFO Driver: Semantic Analysis Completed
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1439134020750 end=1439134020808 duration=58 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=compile start=1439134020747 end=1439134020812 duration=65 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO Driver: Starting command: create table q11_important_stock_par_spark(ps_partkey INT, value DOUBLE) STORED AS parquet
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1439134020745 end=1439134020814 duration=69 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=runTasks start=1439134020814 end=1439134020882 duration=68 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=Driver.execute start=1439134020812 end=1439134020882 duration=70 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/09 15:27:00 INFO Driver: OK
15/08/09 15:27:00 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=releaseLocks start=1439134020882 end=1439134020883 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO PerfLogger: </PERFLOG method=Driver.run start=1439134020745 end=1439134020883 duration=138 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:00 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/09 15:27:00 INFO DAGScheduler: Got job 2 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/09 15:27:00 INFO DAGScheduler: Final stage: Stage 2(collect at SparkPlan.scala:84)
15/08/09 15:27:00 INFO DAGScheduler: Parents of final stage: List()
15/08/09 15:27:00 INFO DAGScheduler: Missing parents: List()
15/08/09 15:27:00 INFO DAGScheduler: Submitting Stage 2 (MappedRDD[8] at map at SparkPlan.scala:84), which has no missing parents
15/08/09 15:27:00 INFO MemoryStore: ensureFreeSpace(2560) called with curMem=6207, maxMem=3333968363
15/08/09 15:27:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 3.1 GB)
15/08/09 15:27:00 INFO MemoryStore: ensureFreeSpace(1562) called with curMem=8767, maxMem=3333968363
15/08/09 15:27:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1562.0 B, free 3.1 GB)
15/08/09 15:27:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:44535 (size: 1562.0 B, free: 3.1 GB)
15/08/09 15:27:00 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/08/09 15:27:00 INFO DefaultExecutionContext: Created broadcast 2 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 2 (MappedRDD[8] at map at SparkPlan.scala:84)
15/08/09 15:27:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/08/09 15:27:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/09 15:27:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
15/08/09 15:27:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 618 bytes result sent to driver
15/08/09 15:27:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 32 ms on localhost (1/1)
15/08/09 15:27:00 INFO DAGScheduler: Stage 2 (collect at SparkPlan.scala:84) finished in 0.040 s
15/08/09 15:27:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/08/09 15:27:00 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1e3d102a
15/08/09 15:27:00 INFO DAGScheduler: Job 2 finished: collect at SparkPlan.scala:84, took 0.073111 s
15/08/09 15:27:00 INFO StatsReportListener: task runtime:(count: 1, mean: 32.000000, stdev: 0.000000, max: 32.000000, min: 32.000000)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms	32.0 ms
15/08/09 15:27:00 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/09 15:27:00 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 25.000000, stdev: 0.000000, max: 25.000000, min: 25.000000)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	25 %	25 %	25 %	25 %	25 %	25 %	25 %	25 %	25 %
15/08/09 15:27:00 INFO StatsReportListener: other time pct: (count: 1, mean: 75.000000, stdev: 0.000000, max: 75.000000, min: 75.000000)
15/08/09 15:27:00 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:00 INFO StatsReportListener: 	75 %	75 %	75 %	75 %	75 %	75 %	75 %	75 %	75 %
Time taken: 0.457 seconds
15/08/09 15:27:00 INFO CliDriver: Time taken: 0.457 seconds
15/08/09 15:27:01 INFO ParseDriver: Parsing command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/09 15:27:01 INFO ParseDriver: Parse Completed
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO Driver: Concurrency mode is disabled, not creating a lock manager
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO ParseDriver: Parsing command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/09 15:27:01 INFO ParseDriver: Parse Completed
15/08/09 15:27:01 INFO PerfLogger: </PERFLOG method=parse start=1439134021131 end=1439134021131 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO SemanticAnalyzer: Starting Semantic Analysis
15/08/09 15:27:01 INFO SemanticAnalyzer: Creating table q11_part_tmp_par_spark position=13
15/08/09 15:27:01 INFO Driver: Semantic Analysis Completed
15/08/09 15:27:01 INFO PerfLogger: </PERFLOG method=semanticAnalyze start=1439134021132 end=1439134021147 duration=15 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/08/09 15:27:01 INFO PerfLogger: </PERFLOG method=compile start=1439134021130 end=1439134021148 duration=18 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO Driver: Starting command: create table q11_part_tmp_par_spark(ps_partkey int, part_value double) STORED AS parquet
15/08/09 15:27:01 INFO PerfLogger: </PERFLOG method=TimeToSubmit start=1439134021130 end=1439134021148 duration=18 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: </PERFLOG method=runTasks start=1439134021149 end=1439134021192 duration=43 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: </PERFLOG method=Driver.execute start=1439134021148 end=1439134021193 duration=45 from=org.apache.hadoop.hive.ql.Driver>
OK
15/08/09 15:27:01 INFO Driver: OK
15/08/09 15:27:01 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: </PERFLOG method=releaseLocks start=1439134021193 end=1439134021193 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO PerfLogger: </PERFLOG method=Driver.run start=1439134021129 end=1439134021193 duration=64 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:01 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/09 15:27:01 INFO DAGScheduler: Got job 3 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/09 15:27:01 INFO DAGScheduler: Final stage: Stage 3(collect at SparkPlan.scala:84)
15/08/09 15:27:01 INFO DAGScheduler: Parents of final stage: List()
15/08/09 15:27:01 INFO DAGScheduler: Missing parents: List()
15/08/09 15:27:01 INFO DAGScheduler: Submitting Stage 3 (MappedRDD[11] at map at SparkPlan.scala:84), which has no missing parents
15/08/09 15:27:01 INFO MemoryStore: ensureFreeSpace(2560) called with curMem=10329, maxMem=3333968363
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 3.1 GB)
15/08/09 15:27:01 INFO MemoryStore: ensureFreeSpace(1562) called with curMem=12889, maxMem=3333968363
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1562.0 B, free 3.1 GB)
15/08/09 15:27:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:44535 (size: 1562.0 B, free: 3.1 GB)
15/08/09 15:27:01 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/08/09 15:27:01 INFO DefaultExecutionContext: Created broadcast 3 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 3 (MappedRDD[11] at map at SparkPlan.scala:84)
15/08/09 15:27:01 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
15/08/09 15:27:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/09 15:27:01 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
15/08/09 15:27:01 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 618 bytes result sent to driver
15/08/09 15:27:01 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 28 ms on localhost (1/1)
15/08/09 15:27:01 INFO DAGScheduler: Stage 3 (collect at SparkPlan.scala:84) finished in 0.037 s
15/08/09 15:27:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/08/09 15:27:01 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@64379207
15/08/09 15:27:01 INFO DAGScheduler: Job 3 finished: collect at SparkPlan.scala:84, took 0.068285 s
Time taken: 0.279 seconds
15/08/09 15:27:01 INFO CliDriver: Time taken: 0.279 seconds
15/08/09 15:27:01 INFO StatsReportListener: task runtime:(count: 1, mean: 28.000000, stdev: 0.000000, max: 28.000000, min: 28.000000)
15/08/09 15:27:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:01 INFO StatsReportListener: 	28.0 ms	28.0 ms	28.0 ms	28.0 ms	28.0 ms	28.0 ms	28.0 ms	28.0 ms	28.0 ms
15/08/09 15:27:01 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/09 15:27:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:01 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/09 15:27:01 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 10.714286, stdev: 0.000000, max: 10.714286, min: 10.714286)
15/08/09 15:27:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:01 INFO StatsReportListener: 	11 %	11 %	11 %	11 %	11 %	11 %	11 %	11 %	11 %
15/08/09 15:27:01 INFO StatsReportListener: other time pct: (count: 1, mean: 89.285714, stdev: 0.000000, max: 89.285714, min: 89.285714)
15/08/09 15:27:01 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:01 INFO StatsReportListener: 	89 %	89 %	89 %	89 %	89 %	89 %	89 %	89 %	89 %
15/08/09 15:27:01 INFO ParseDriver: Parsing command: insert into table q11_part_tmp_par_spark
select ps_partkey, sum(ps_supplycost * ps_availqty) as part_value
from nation_par n
        join supplier_par s on s.s_nationkey = n.n_nationkey and n.n_name = 'RUSSIA'
        join partsupp_par ps on ps.ps_suppkey = s.s_suppkey
group by ps_partkey
15/08/09 15:27:01 INFO ParseDriver: Parse Completed
15/08/09 15:27:01 INFO BlockManager: Removing broadcast 0
15/08/09 15:27:01 INFO BlockManager: Removing block broadcast_0
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_0 of size 1896 dropped from memory (free 3333955808)
15/08/09 15:27:01 INFO BlockManager: Removing block broadcast_0_piece0
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_0_piece0 of size 1208 dropped from memory (free 3333957016)
15/08/09 15:27:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:44535 in memory (size: 1208.0 B, free: 3.1 GB)
15/08/09 15:27:01 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/08/09 15:27:01 INFO ContextCleaner: Cleaned broadcast 0
15/08/09 15:27:01 INFO BlockManager: Removing broadcast 3
15/08/09 15:27:01 INFO BlockManager: Removing block broadcast_3_piece0
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_3_piece0 of size 1562 dropped from memory (free 3333958578)
15/08/09 15:27:01 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:44535 in memory (size: 1562.0 B, free: 3.1 GB)
15/08/09 15:27:01 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/08/09 15:27:01 INFO BlockManager: Removing block broadcast_3
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_3 of size 2560 dropped from memory (free 3333961138)
15/08/09 15:27:01 INFO ContextCleaner: Cleaned broadcast 3
15/08/09 15:27:01 INFO BlockManager: Removing broadcast 2
15/08/09 15:27:01 INFO BlockManager: Removing block broadcast_2
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_2 of size 2560 dropped from memory (free 3333963698)
15/08/09 15:27:01 INFO BlockManager: Removing block broadcast_2_piece0
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_2_piece0 of size 1562 dropped from memory (free 3333965260)
15/08/09 15:27:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:44535 in memory (size: 1562.0 B, free: 3.1 GB)
15/08/09 15:27:01 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/08/09 15:27:01 INFO ContextCleaner: Cleaned broadcast 2
15/08/09 15:27:01 INFO BlockManager: Removing broadcast 1
15/08/09 15:27:01 INFO BlockManager: Removing block broadcast_1_piece0
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_1_piece0 of size 1207 dropped from memory (free 3333966467)
15/08/09 15:27:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:44535 in memory (size: 1207.0 B, free: 3.1 GB)
15/08/09 15:27:01 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/08/09 15:27:01 INFO BlockManager: Removing block broadcast_1
15/08/09 15:27:01 INFO MemoryStore: Block broadcast_1 of size 1896 dropped from memory (free 3333968363)
15/08/09 15:27:01 INFO ContextCleaner: Cleaned broadcast 1
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
15/08/09 15:27:02 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(n_nationkey#31, n_name#32, n_regionkey#33, n_comment#34)
15/08/09 15:27:02 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(s_suppkey#35, s_name#36, s_address#37, s_nationkey#38, s_phone#39, s_acctbal#40, s_comment#41)
15/08/09 15:27:03 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#42, ps_suppkey#43, ps_availqty#44, ps_supplycost#45, ps_comment#46)
15/08/09 15:27:03 INFO MemoryStore: ensureFreeSpace(281594) called with curMem=0, maxMem=3333968363
15/08/09 15:27:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 275.0 KB, free 3.1 GB)
15/08/09 15:27:03 INFO MemoryStore: ensureFreeSpace(281634) called with curMem=281594, maxMem=3333968363
15/08/09 15:27:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 275.0 KB, free 3.1 GB)
15/08/09 15:27:03 INFO MemoryStore: ensureFreeSpace(31895) called with curMem=563228, maxMem=3333968363
15/08/09 15:27:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/09 15:27:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:44535 (size: 31.1 KB, free: 3.1 GB)
15/08/09 15:27:03 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/08/09 15:27:03 INFO DefaultExecutionContext: Created broadcast 4 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/09 15:27:03 INFO MemoryStore: ensureFreeSpace(31949) called with curMem=595123, maxMem=3333968363
15/08/09 15:27:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 31.2 KB, free 3.1 GB)
15/08/09 15:27:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:44535 (size: 31.2 KB, free: 3.1 GB)
15/08/09 15:27:03 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0
15/08/09 15:27:03 INFO DefaultExecutionContext: Created broadcast 5 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/09 15:27:03 INFO FileInputFormat: Total input paths to process : 1
15/08/09 15:27:03 INFO ParquetInputFormat: Total input paths to process : 1
15/08/09 15:27:03 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/09 15:27:03 INFO ParquetFileReader: reading another 1 footers
15/08/09 15:27:03 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/09 15:27:03 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0; isDirectory=false; length=1493206; replication=1; blocksize=134217728; modification_time=1439133407981; access_time=1439133407618; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 35 ms
15/08/09 15:27:03 INFO deprecation: mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
15/08/09 15:27:03 INFO deprecation: mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
15/08/09 15:27:03 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/09 15:27:03 INFO DefaultExecutionContext: Starting job: collect at BroadcastHashJoin.scala:53
15/08/09 15:27:03 INFO DAGScheduler: Got job 4 (collect at BroadcastHashJoin.scala:53) with 1 output partitions (allowLocal=false)
15/08/09 15:27:03 INFO DAGScheduler: Final stage: Stage 4(collect at BroadcastHashJoin.scala:53)
15/08/09 15:27:03 INFO DAGScheduler: Parents of final stage: List()
15/08/09 15:27:03 INFO DAGScheduler: Missing parents: List()
15/08/09 15:27:03 INFO DAGScheduler: Submitting Stage 4 (MappedRDD[28] at map at BroadcastHashJoin.scala:53), which has no missing parents
15/08/09 15:27:03 INFO MemoryStore: ensureFreeSpace(2488) called with curMem=627072, maxMem=3333968363
15/08/09 15:27:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.4 KB, free 3.1 GB)
15/08/09 15:27:03 INFO MemoryStore: ensureFreeSpace(1470) called with curMem=629560, maxMem=3333968363
15/08/09 15:27:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1470.0 B, free 3.1 GB)
15/08/09 15:27:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:44535 (size: 1470.0 B, free: 3.1 GB)
15/08/09 15:27:03 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/08/09 15:27:03 INFO DefaultExecutionContext: Created broadcast 6 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:03 INFO DAGScheduler: Submitting 1 missing tasks from Stage 4 (MappedRDD[28] at map at BroadcastHashJoin.scala:53)
15/08/09 15:27:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15/08/09 15:27:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, ANY, 1586 bytes)
15/08/09 15:27:03 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
15/08/09 15:27:03 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/supplier_par/000000_0 start: 0 end: 1493206 length: 1493206 hosts: [] requestedSchema: message root {
  optional int32 s_suppkey;
  optional int32 s_nationkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_name","type":"string","nullable":true,"metadata":{}},{"name":"s_address","type":"string","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_phone","type":"string","nullable":true,"metadata":{}},{"name":"s_acctbal","type":"double","nullable":true,"metadata":{}},{"name":"s_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"s_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"s_nationkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:03 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:03 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 10000 records.
15/08/09 15:27:03 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:03 INFO InternalParquetRecordReader: block read in memory in 43 ms. row count = 10000
15/08/09 15:27:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 144335 bytes result sent to driver
15/08/09 15:27:04 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 594 ms on localhost (1/1)
15/08/09 15:27:04 INFO DAGScheduler: Stage 4 (collect at BroadcastHashJoin.scala:53) finished in 0.790 s
15/08/09 15:27:04 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/08/09 15:27:04 INFO DAGScheduler: Job 4 finished: collect at BroadcastHashJoin.scala:53, took 0.831882 s
15/08/09 15:27:04 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@286f238a
15/08/09 15:27:04 INFO StatsReportListener: task runtime:(count: 1, mean: 594.000000, stdev: 0.000000, max: 594.000000, min: 594.000000)
15/08/09 15:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:04 INFO StatsReportListener: 	594.0 ms	594.0 ms	594.0 ms	594.0 ms	594.0 ms	594.0 ms	594.0 ms	594.0 ms	594.0 ms
15/08/09 15:27:04 INFO StatsReportListener: task result size:(count: 1, mean: 144335.000000, stdev: 0.000000, max: 144335.000000, min: 144335.000000)
15/08/09 15:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:04 INFO StatsReportListener: 	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB	141.0 KB
15/08/09 15:27:04 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 63.636364, stdev: 0.000000, max: 63.636364, min: 63.636364)
15/08/09 15:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:04 INFO StatsReportListener: 	64 %	64 %	64 %	64 %	64 %	64 %	64 %	64 %	64 %
15/08/09 15:27:04 INFO StatsReportListener: other time pct: (count: 1, mean: 36.363636, stdev: 0.000000, max: 36.363636, min: 36.363636)
15/08/09 15:27:04 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:04 INFO StatsReportListener: 	36 %	36 %	36 %	36 %	36 %	36 %	36 %	36 %	36 %
15/08/09 15:27:04 INFO MemoryStore: ensureFreeSpace(65648) called with curMem=631030, maxMem=3333968363
15/08/09 15:27:04 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 64.1 KB, free 3.1 GB)
15/08/09 15:27:04 INFO MemoryStore: ensureFreeSpace(55237) called with curMem=696678, maxMem=3333968363
15/08/09 15:27:04 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 53.9 KB, free 3.1 GB)
15/08/09 15:27:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:44535 (size: 53.9 KB, free: 3.1 GB)
15/08/09 15:27:04 INFO BlockManagerMaster: Updated info of block broadcast_7_piece0
15/08/09 15:27:04 INFO DefaultExecutionContext: Created broadcast 7 from broadcast at BroadcastHashJoin.scala:55
15/08/09 15:27:04 INFO MemoryStore: ensureFreeSpace(281194) called with curMem=751915, maxMem=3333968363
15/08/09 15:27:04 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 274.6 KB, free 3.1 GB)
15/08/09 15:27:04 INFO MemoryStore: ensureFreeSpace(31858) called with curMem=1033109, maxMem=3333968363
15/08/09 15:27:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/09 15:27:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:44535 (size: 31.1 KB, free: 3.1 GB)
15/08/09 15:27:04 INFO BlockManagerMaster: Updated info of block broadcast_8_piece0
15/08/09 15:27:04 INFO DefaultExecutionContext: Created broadcast 8 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/09 15:27:04 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/08/09 15:27:04 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/08/09 15:27:04 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/08/09 15:27:04 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/08/09 15:27:04 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/08/09 15:27:04 INFO DefaultExecutionContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/08/09 15:27:04 INFO FileInputFormat: Total input paths to process : 1
15/08/09 15:27:04 INFO ParquetInputFormat: Total input paths to process : 1
15/08/09 15:27:04 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/09 15:27:04 INFO ParquetFileReader: reading another 1 footers
15/08/09 15:27:04 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/09 15:27:04 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0; isDirectory=false; length=3216; replication=1; blocksize=134217728; modification_time=1439133403129; access_time=1439133403010; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 17 ms
15/08/09 15:27:04 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/09 15:27:04 INFO FileInputFormat: Total input paths to process : 10
15/08/09 15:27:04 INFO ParquetInputFormat: Total input paths to process : 10
15/08/09 15:27:04 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/09 15:27:04 INFO ParquetFileReader: reading another 10 footers
15/08/09 15:27:04 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/09 15:27:04 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0; isDirectory=false; length=11478369; replication=1; blocksize=134217728; modification_time=1439133411746; access_time=1439133409409; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0; isDirectory=false; length=11437027; replication=1; blocksize=134217728; modification_time=1439133410518; access_time=1439133409379; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0; isDirectory=false; length=11437121; replication=1; blocksize=134217728; modification_time=1439133410755; access_time=1439133409301; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0; isDirectory=false; length=11437895; replication=1; blocksize=134217728; modification_time=1439133410723; access_time=1439133409433; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0; isDirectory=false; length=11436112; replication=1; blocksize=134217728; modification_time=1439133411066; access_time=1439133409263; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0; isDirectory=false; length=11361987; replication=1; blocksize=134217728; modification_time=1439133410702; access_time=1439133409149; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0; isDirectory=false; length=11361789; replication=1; blocksize=134217728; modification_time=1439133413216; access_time=1439133410916; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0; isDirectory=false; length=11362156; replication=1; blocksize=134217728; modification_time=1439133412699; access_time=1439133411387; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0; isDirectory=false; length=11362379; replication=1; blocksize=134217728; modification_time=1439133413154; access_time=1439133411549; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0; isDirectory=false; length=11361401; replication=1; blocksize=134217728; modification_time=1439133412845; access_time=1439133411784; owner=leonidas; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 46 ms
15/08/09 15:27:04 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/09 15:27:04 INFO DAGScheduler: Registering RDD 30 (mapPartitions at Exchange.scala:64)
15/08/09 15:27:04 INFO DAGScheduler: Registering RDD 39 (mapPartitions at Exchange.scala:64)
15/08/09 15:27:04 INFO DAGScheduler: Registering RDD 45 (mapPartitions at Exchange.scala:64)
15/08/09 15:27:04 INFO DAGScheduler: Got job 5 (runJob at InsertIntoHiveTable.scala:93) with 200 output partitions (allowLocal=false)
15/08/09 15:27:04 INFO DAGScheduler: Final stage: Stage 8(runJob at InsertIntoHiveTable.scala:93)
15/08/09 15:27:04 INFO DAGScheduler: Parents of final stage: List(Stage 7)
15/08/09 15:27:04 INFO DAGScheduler: Missing parents: List(Stage 7)
15/08/09 15:27:05 INFO DAGScheduler: Submitting Stage 5 (MapPartitionsRDD[30] at mapPartitions at Exchange.scala:64), which has no missing parents
15/08/09 15:27:05 INFO MemoryStore: ensureFreeSpace(7264) called with curMem=1064967, maxMem=3333968363
15/08/09 15:27:05 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.1 KB, free 3.1 GB)
15/08/09 15:27:05 INFO MemoryStore: ensureFreeSpace(4177) called with curMem=1072231, maxMem=3333968363
15/08/09 15:27:05 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 3.1 GB)
15/08/09 15:27:05 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:44535 (size: 4.1 KB, free: 3.1 GB)
15/08/09 15:27:05 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/08/09 15:27:05 INFO DefaultExecutionContext: Created broadcast 9 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:05 INFO DAGScheduler: Submitting 10 missing tasks from Stage 5 (MapPartitionsRDD[30] at mapPartitions at Exchange.scala:64)
15/08/09 15:27:05 INFO TaskSchedulerImpl: Adding task set 5.0 with 10 tasks
15/08/09 15:27:05 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, ANY, 1581 bytes)
15/08/09 15:27:05 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, ANY, 1585 bytes)
15/08/09 15:27:05 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 7, localhost, ANY, 1584 bytes)
15/08/09 15:27:05 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 8, localhost, ANY, 1584 bytes)
15/08/09 15:27:05 INFO DAGScheduler: Submitting Stage 6 (MapPartitionsRDD[39] at mapPartitions at Exchange.scala:64), which has no missing parents
15/08/09 15:27:05 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 9, localhost, ANY, 1584 bytes)
15/08/09 15:27:05 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 10, localhost, ANY, 1584 bytes)
15/08/09 15:27:05 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 11, localhost, ANY, 1583 bytes)
15/08/09 15:27:05 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 12, localhost, ANY, 1582 bytes)
15/08/09 15:27:05 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 13, localhost, ANY, 1583 bytes)
15/08/09 15:27:05 INFO MemoryStore: ensureFreeSpace(10096) called with curMem=1076408, maxMem=3333968363
15/08/09 15:27:05 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 14, localhost, ANY, 1582 bytes)
15/08/09 15:27:05 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.9 KB, free 3.1 GB)
15/08/09 15:27:05 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
15/08/09 15:27:05 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
15/08/09 15:27:05 INFO Executor: Running task 2.0 in stage 5.0 (TID 7)
15/08/09 15:27:05 INFO Executor: Running task 3.0 in stage 5.0 (TID 8)
15/08/09 15:27:05 INFO Executor: Running task 4.0 in stage 5.0 (TID 9)
15/08/09 15:27:05 INFO Executor: Running task 5.0 in stage 5.0 (TID 10)
15/08/09 15:27:05 INFO Executor: Running task 6.0 in stage 5.0 (TID 11)
15/08/09 15:27:05 INFO MemoryStore: ensureFreeSpace(5665) called with curMem=1086504, maxMem=3333968363
15/08/09 15:27:05 INFO Executor: Running task 7.0 in stage 5.0 (TID 12)
15/08/09 15:27:05 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.5 KB, free 3.1 GB)
15/08/09 15:27:05 INFO Executor: Running task 8.0 in stage 5.0 (TID 13)
15/08/09 15:27:05 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:44535 (size: 5.5 KB, free: 3.1 GB)
15/08/09 15:27:05 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/08/09 15:27:05 INFO Executor: Running task 9.0 in stage 5.0 (TID 14)
15/08/09 15:27:05 INFO DefaultExecutionContext: Created broadcast 10 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:05 INFO DAGScheduler: Submitting 1 missing tasks from Stage 6 (MapPartitionsRDD[39] at mapPartitions at Exchange.scala:64)
15/08/09 15:27:05 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/08/09 15:27:05 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 15, localhost, ANY, 1552 bytes)
15/08/09 15:27:05 INFO Executor: Running task 0.0 in stage 6.0 (TID 15)
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000000_0 start: 0 end: 11478369 length: 11478369 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000008_0 start: 0 end: 11362379 length: 11362379 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000005_0 start: 0 end: 11361987 length: 11361987 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000003_0 start: 0 end: 11437895 length: 11437895 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000006_0 start: 0 end: 11361789 length: 11361789 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000002_0 start: 0 end: 11437121 length: 11437121 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000009_0 start: 0 end: 11361401 length: 11361401 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000004_0 start: 0 end: 11436112 length: 11436112 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000001_0 start: 0 end: 11437027 length: 11437027 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/partsupp_par/000007_0 start: 0 end: 11362156 length: 11362156 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double ps_supplycost;
  optional int32 ps_availqty;
  optional int32 ps_suppkey;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"ps_supplycost","type":"double","nullable":true,"metadata":{}},{"name":"ps_availqty","type":"integer","nullable":true,"metadata":{}},{"name":"ps_suppkey","type":"integer","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79589 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79706 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80615 records.
15/08/09 15:27:05 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/nation_par/000000_0 start: 0 end: 3216 length: 3216 hosts: [] requestedSchema: message root {
  optional int32 n_nationkey;
  optional binary n_name (UTF8);
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}},{"name":"n_regionkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_comment","type":"string","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"n_nationkey","type":"integer","nullable":true,"metadata":{}},{"name":"n_name","type":"string","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:05 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80265 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79826 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79667 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80195 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80206 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 80260 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 79671 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 25 records.
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 25
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 22 ms. row count = 80615
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 80260
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 79667
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 27 ms. row count = 79671
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 29 ms. row count = 80206
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 30 ms. row count = 80265
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 25 ms. row count = 79589
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 80195
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 26 ms. row count = 79826
15/08/09 15:27:05 INFO InternalParquetRecordReader: block read in memory in 32 ms. row count = 79706
15/08/09 15:27:05 INFO Executor: Finished task 0.0 in stage 6.0 (TID 15). 2019 bytes result sent to driver
15/08/09 15:27:05 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 15) in 823 ms on localhost (1/1)
15/08/09 15:27:05 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/08/09 15:27:05 INFO DAGScheduler: Stage 6 (mapPartitions at Exchange.scala:64) finished in 0.828 s
15/08/09 15:27:05 INFO DAGScheduler: looking for newly runnable stages
15/08/09 15:27:05 INFO DAGScheduler: running: Set(Stage 5)
15/08/09 15:27:05 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@75b39f6
15/08/09 15:27:05 INFO DAGScheduler: waiting: Set(Stage 7, Stage 8)
15/08/09 15:27:05 INFO DAGScheduler: failed: Set()
15/08/09 15:27:05 INFO StatsReportListener: task runtime:(count: 1, mean: 823.000000, stdev: 0.000000, max: 823.000000, min: 823.000000)
15/08/09 15:27:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:05 INFO StatsReportListener: 	823.0 ms	823.0 ms	823.0 ms	823.0 ms	823.0 ms	823.0 ms	823.0 ms	823.0 ms	823.0 ms
15/08/09 15:27:05 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 13231.000000, stdev: 0.000000, max: 13231.000000, min: 13231.000000)
15/08/09 15:27:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:05 INFO StatsReportListener: 	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB	12.9 KB
15/08/09 15:27:05 INFO StatsReportListener: task result size:(count: 1, mean: 2019.000000, stdev: 0.000000, max: 2019.000000, min: 2019.000000)
15/08/09 15:27:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:05 INFO StatsReportListener: 	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B	2019.0 B
15/08/09 15:27:05 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 97.205346, stdev: 0.000000, max: 97.205346, min: 97.205346)
15/08/09 15:27:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:05 INFO StatsReportListener: 	97 %	97 %	97 %	97 %	97 %	97 %	97 %	97 %	97 %
15/08/09 15:27:05 INFO StatsReportListener: other time pct: (count: 1, mean: 2.794654, stdev: 0.000000, max: 2.794654, min: 2.794654)
15/08/09 15:27:05 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:05 INFO StatsReportListener: 	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %	 3 %
15/08/09 15:27:06 INFO DAGScheduler: Missing parents for Stage 7: List(Stage 5)
15/08/09 15:27:06 INFO DAGScheduler: Missing parents for Stage 8: List(Stage 7)
15/08/09 15:27:06 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 2182 bytes result sent to driver
15/08/09 15:27:06 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 1669 ms on localhost (1/10)
15/08/09 15:27:06 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2182 bytes result sent to driver
15/08/09 15:27:06 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1799 ms on localhost (2/10)
15/08/09 15:27:06 INFO Executor: Finished task 6.0 in stage 5.0 (TID 11). 2182 bytes result sent to driver
15/08/09 15:27:06 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 11) in 1849 ms on localhost (3/10)
15/08/09 15:27:06 INFO Executor: Finished task 3.0 in stage 5.0 (TID 8). 2182 bytes result sent to driver
15/08/09 15:27:06 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 8) in 1875 ms on localhost (4/10)
15/08/09 15:27:07 INFO Executor: Finished task 5.0 in stage 5.0 (TID 10). 2182 bytes result sent to driver
15/08/09 15:27:07 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 10) in 1887 ms on localhost (5/10)
15/08/09 15:27:07 INFO Executor: Finished task 8.0 in stage 5.0 (TID 13). 2182 bytes result sent to driver
15/08/09 15:27:07 INFO Executor: Finished task 4.0 in stage 5.0 (TID 9). 2182 bytes result sent to driver
15/08/09 15:27:07 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 13) in 1894 ms on localhost (6/10)
15/08/09 15:27:07 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 9) in 1902 ms on localhost (7/10)
15/08/09 15:27:07 INFO Executor: Finished task 7.0 in stage 5.0 (TID 12). 2182 bytes result sent to driver
15/08/09 15:27:07 INFO Executor: Finished task 9.0 in stage 5.0 (TID 14). 2182 bytes result sent to driver
15/08/09 15:27:07 INFO Executor: Finished task 2.0 in stage 5.0 (TID 7). 2182 bytes result sent to driver
15/08/09 15:27:07 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 12) in 1912 ms on localhost (8/10)
15/08/09 15:27:07 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 14) in 1912 ms on localhost (9/10)
15/08/09 15:27:07 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 7) in 1924 ms on localhost (10/10)
15/08/09 15:27:07 INFO DAGScheduler: Stage 5 (mapPartitions at Exchange.scala:64) finished in 1.926 s
15/08/09 15:27:07 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/08/09 15:27:07 INFO DAGScheduler: looking for newly runnable stages
15/08/09 15:27:07 INFO DAGScheduler: running: Set()
15/08/09 15:27:07 INFO DAGScheduler: waiting: Set(Stage 7, Stage 8)
15/08/09 15:27:07 INFO DAGScheduler: failed: Set()
15/08/09 15:27:07 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@644151dc
15/08/09 15:27:07 INFO StatsReportListener: task runtime:(count: 10, mean: 1862.300000, stdev: 73.244863, max: 1924.000000, min: 1669.000000)
15/08/09 15:27:07 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:07 INFO StatsReportListener: 	1.7 s	1.7 s	1.8 s	1.8 s	1.9 s	1.9 s	1.9 s	1.9 s	1.9 s
15/08/09 15:27:07 INFO StatsReportListener: shuffle bytes written:(count: 10, mean: 7582991.200000, stdev: 26756.903116, max: 7616204.000000, min: 7549451.000000)
15/08/09 15:27:07 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:07 INFO StatsReportListener: 	7.2 MB	7.2 MB	7.2 MB	7.2 MB	7.3 MB	7.3 MB	7.3 MB	7.3 MB	7.3 MB
15/08/09 15:27:07 INFO StatsReportListener: fetch wait time:(count: 10, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/09 15:27:07 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:07 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms
15/08/09 15:27:07 INFO StatsReportListener: remote bytes read:(count: 10, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/09 15:27:07 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:07 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/09 15:27:07 INFO StatsReportListener: task result size:(count: 10, mean: 2182.000000, stdev: 0.000000, max: 2182.000000, min: 2182.000000)
15/08/09 15:27:07 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:07 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/09 15:27:07 INFO StatsReportListener: executor (non-fetch) time pct: (count: 10, mean: 98.416996, stdev: 0.213417, max: 98.633018, min: 97.973333)
15/08/09 15:27:07 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:07 INFO StatsReportListener: 	98 %	98 %	98 %	98 %	99 %	99 %	99 %	99 %	99 %
15/08/09 15:27:07 INFO StatsReportListener: fetch wait time pct: (count: 10, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/09 15:27:07 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:07 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/08/09 15:27:07 INFO StatsReportListener: other time pct: (count: 10, mean: 1.583004, stdev: 0.213417, max: 2.026667, min: 1.366982)
15/08/09 15:27:07 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:07 INFO StatsReportListener: 	 1 %	 1 %	 1 %	 1 %	 2 %	 2 %	 2 %	 2 %	 2 %
15/08/09 15:27:07 INFO DAGScheduler: Missing parents for Stage 7: List()
15/08/09 15:27:07 INFO DAGScheduler: Missing parents for Stage 8: List(Stage 7)
15/08/09 15:27:07 INFO DAGScheduler: Submitting Stage 7 (MapPartitionsRDD[45] at mapPartitions at Exchange.scala:64), which is now runnable
15/08/09 15:27:07 INFO MemoryStore: ensureFreeSpace(13544) called with curMem=1092169, maxMem=3333968363
15/08/09 15:27:07 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.2 KB, free 3.1 GB)
15/08/09 15:27:07 INFO MemoryStore: ensureFreeSpace(7369) called with curMem=1105713, maxMem=3333968363
15/08/09 15:27:07 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.2 KB, free 3.1 GB)
15/08/09 15:27:07 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:44535 (size: 7.2 KB, free: 3.1 GB)
15/08/09 15:27:07 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/08/09 15:27:07 INFO DefaultExecutionContext: Created broadcast 11 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:07 INFO DAGScheduler: Submitting 200 missing tasks from Stage 7 (MapPartitionsRDD[45] at mapPartitions at Exchange.scala:64)
15/08/09 15:27:07 INFO TaskSchedulerImpl: Adding task set 7.0 with 200 tasks
15/08/09 15:27:07 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 16, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 17, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 18, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 19, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 20, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 21, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 22, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 23, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 24, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 25, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 26, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 27, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 28, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 29, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 30, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 31, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:07 INFO Executor: Running task 0.0 in stage 7.0 (TID 16)
15/08/09 15:27:07 INFO Executor: Running task 1.0 in stage 7.0 (TID 17)
15/08/09 15:27:07 INFO Executor: Running task 6.0 in stage 7.0 (TID 22)
15/08/09 15:27:07 INFO Executor: Running task 4.0 in stage 7.0 (TID 20)
15/08/09 15:27:07 INFO Executor: Running task 2.0 in stage 7.0 (TID 18)
15/08/09 15:27:07 INFO Executor: Running task 3.0 in stage 7.0 (TID 19)
15/08/09 15:27:07 INFO Executor: Running task 5.0 in stage 7.0 (TID 21)
15/08/09 15:27:07 INFO Executor: Running task 7.0 in stage 7.0 (TID 23)
15/08/09 15:27:07 INFO Executor: Running task 9.0 in stage 7.0 (TID 25)
15/08/09 15:27:07 INFO Executor: Running task 8.0 in stage 7.0 (TID 24)
15/08/09 15:27:07 INFO Executor: Running task 10.0 in stage 7.0 (TID 26)
15/08/09 15:27:07 INFO Executor: Running task 11.0 in stage 7.0 (TID 27)
15/08/09 15:27:07 INFO Executor: Running task 12.0 in stage 7.0 (TID 28)
15/08/09 15:27:07 INFO Executor: Running task 14.0 in stage 7.0 (TID 30)
15/08/09 15:27:07 INFO Executor: Running task 15.0 in stage 7.0 (TID 31)
15/08/09 15:27:07 INFO Executor: Running task 13.0 in stage 7.0 (TID 29)
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/09 15:27:08 INFO Executor: Finished task 12.0 in stage 7.0 (TID 28). 1124 bytes result sent to driver
15/08/09 15:27:08 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 32, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:08 INFO Executor: Finished task 15.0 in stage 7.0 (TID 31). 1124 bytes result sent to driver
15/08/09 15:27:08 INFO Executor: Finished task 7.0 in stage 7.0 (TID 23). 1124 bytes result sent to driver
15/08/09 15:27:08 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 28) in 957 ms on localhost (1/200)
15/08/09 15:27:08 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 33, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:08 INFO Executor: Running task 17.0 in stage 7.0 (TID 33)
15/08/09 15:27:08 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 31) in 964 ms on localhost (2/200)
15/08/09 15:27:08 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 34, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:08 INFO Executor: Running task 18.0 in stage 7.0 (TID 34)
15/08/09 15:27:08 INFO Executor: Running task 16.0 in stage 7.0 (TID 32)
15/08/09 15:27:08 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 23) in 974 ms on localhost (3/200)
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:08 INFO BlockManager: Removing broadcast 10
15/08/09 15:27:08 INFO BlockManager: Removing block broadcast_10
15/08/09 15:27:08 INFO MemoryStore: Block broadcast_10 of size 10096 dropped from memory (free 3332865377)
15/08/09 15:27:08 INFO BlockManager: Removing block broadcast_10_piece0
15/08/09 15:27:08 INFO MemoryStore: Block broadcast_10_piece0 of size 5665 dropped from memory (free 3332871042)
15/08/09 15:27:08 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:44535 in memory (size: 5.5 KB, free: 3.1 GB)
15/08/09 15:27:08 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/08/09 15:27:08 INFO ContextCleaner: Cleaned broadcast 10
15/08/09 15:27:08 INFO BlockManager: Removing broadcast 9
15/08/09 15:27:08 INFO BlockManager: Removing block broadcast_9
15/08/09 15:27:08 INFO MemoryStore: Block broadcast_9 of size 7264 dropped from memory (free 3332878306)
15/08/09 15:27:08 INFO BlockManager: Removing block broadcast_9_piece0
15/08/09 15:27:08 INFO MemoryStore: Block broadcast_9_piece0 of size 4177 dropped from memory (free 3332882483)
15/08/09 15:27:08 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:44535 in memory (size: 4.1 KB, free: 3.1 GB)
15/08/09 15:27:08 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/08/09 15:27:08 INFO ContextCleaner: Cleaned broadcast 9
15/08/09 15:27:08 INFO BlockManager: Removing broadcast 6
15/08/09 15:27:08 INFO BlockManager: Removing block broadcast_6_piece0
15/08/09 15:27:08 INFO MemoryStore: Block broadcast_6_piece0 of size 1470 dropped from memory (free 3332883953)
15/08/09 15:27:08 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:44535 in memory (size: 1470.0 B, free: 3.1 GB)
15/08/09 15:27:08 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/08/09 15:27:08 INFO BlockManager: Removing block broadcast_6
15/08/09 15:27:08 INFO MemoryStore: Block broadcast_6 of size 2488 dropped from memory (free 3332886441)
15/08/09 15:27:08 INFO ContextCleaner: Cleaned broadcast 6
15/08/09 15:27:08 INFO BlockManager: Removing broadcast 4
15/08/09 15:27:08 INFO BlockManager: Removing block broadcast_4
15/08/09 15:27:08 INFO MemoryStore: Block broadcast_4 of size 281594 dropped from memory (free 3333168035)
15/08/09 15:27:08 INFO BlockManager: Removing block broadcast_4_piece0
15/08/09 15:27:08 INFO MemoryStore: Block broadcast_4_piece0 of size 31895 dropped from memory (free 3333199930)
15/08/09 15:27:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:44535 in memory (size: 31.1 KB, free: 3.1 GB)
15/08/09 15:27:08 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/08/09 15:27:08 INFO ContextCleaner: Cleaned broadcast 4
15/08/09 15:27:08 INFO Executor: Finished task 0.0 in stage 7.0 (TID 16). 1124 bytes result sent to driver
15/08/09 15:27:08 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 35, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:08 INFO Executor: Running task 19.0 in stage 7.0 (TID 35)
15/08/09 15:27:08 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 16) in 1746 ms on localhost (4/200)
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 9.0 in stage 7.0 (TID 25). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 36, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 20.0 in stage 7.0 (TID 36)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 25) in 1952 ms on localhost (5/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 8.0 in stage 7.0 (TID 24). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 37, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 21.0 in stage 7.0 (TID 37)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 24) in 2029 ms on localhost (6/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 21.0 in stage 7.0 (TID 37). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 38, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 22.0 in stage 7.0 (TID 38)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 37) in 53 ms on localhost (7/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 13.0 in stage 7.0 (TID 29). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 39, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 23.0 in stage 7.0 (TID 39)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 29) in 2106 ms on localhost (8/200)
15/08/09 15:27:09 INFO Executor: Finished task 14.0 in stage 7.0 (TID 30). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 40, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 24.0 in stage 7.0 (TID 40)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 30) in 2112 ms on localhost (9/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 5.0 in stage 7.0 (TID 21). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 41, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 25.0 in stage 7.0 (TID 41)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 21) in 2153 ms on localhost (10/200)
15/08/09 15:27:09 INFO Executor: Finished task 11.0 in stage 7.0 (TID 27). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 42, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 26.0 in stage 7.0 (TID 42)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 27) in 2152 ms on localhost (11/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 10.0 in stage 7.0 (TID 26). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO Executor: Finished task 4.0 in stage 7.0 (TID 20). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 43, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 27.0 in stage 7.0 (TID 43)
15/08/09 15:27:09 INFO Executor: Finished task 18.0 in stage 7.0 (TID 34). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 26) in 2180 ms on localhost (12/200)
15/08/09 15:27:09 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 44, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 28.0 in stage 7.0 (TID 44)
15/08/09 15:27:09 INFO Executor: Finished task 2.0 in stage 7.0 (TID 18). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 20) in 2189 ms on localhost (13/200)
15/08/09 15:27:09 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 45, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 29.0 in stage 7.0 (TID 45)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 34) in 1217 ms on localhost (14/200)
15/08/09 15:27:09 INFO Executor: Finished task 3.0 in stage 7.0 (TID 19). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 46, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 30.0 in stage 7.0 (TID 46)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 18) in 2195 ms on localhost (15/200)
15/08/09 15:27:09 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 47, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 31.0 in stage 7.0 (TID 47)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 19) in 2196 ms on localhost (16/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 16.0 in stage 7.0 (TID 32). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 48, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 32) in 1239 ms on localhost (17/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Running task 32.0 in stage 7.0 (TID 48)
15/08/09 15:27:09 INFO Executor: Finished task 19.0 in stage 7.0 (TID 35). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 49, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Finished task 17.0 in stage 7.0 (TID 33). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO Executor: Running task 33.0 in stage 7.0 (TID 49)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 35) in 478 ms on localhost (18/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 50, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 34.0 in stage 7.0 (TID 50)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 33) in 1253 ms on localhost (19/200)
15/08/09 15:27:09 INFO Executor: Finished task 1.0 in stage 7.0 (TID 17). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 51, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 17) in 2224 ms on localhost (20/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO Executor: Finished task 6.0 in stage 7.0 (TID 22). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO Executor: Running task 35.0 in stage 7.0 (TID 51)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/09 15:27:09 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 52, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 36.0 in stage 7.0 (TID 52)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 22) in 2232 ms on localhost (21/200)
15/08/09 15:27:09 INFO Executor: Finished task 20.0 in stage 7.0 (TID 36). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 53, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 37.0 in stage 7.0 (TID 53)
15/08/09 15:27:09 INFO Executor: Finished task 31.0 in stage 7.0 (TID 47). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 36) in 283 ms on localhost (22/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 54, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 38.0 in stage 7.0 (TID 54)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 47) in 45 ms on localhost (23/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 32.0 in stage 7.0 (TID 48). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 55, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 39.0 in stage 7.0 (TID 55)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 48) in 92 ms on localhost (24/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO Executor: Finished task 22.0 in stage 7.0 (TID 38). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 56, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 40.0 in stage 7.0 (TID 56)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 38) in 233 ms on localhost (25/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 24.0 in stage 7.0 (TID 40). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 57, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 41.0 in stage 7.0 (TID 57)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 40) in 446 ms on localhost (26/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 26.0 in stage 7.0 (TID 42). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 58, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 42) in 468 ms on localhost (27/200)
15/08/09 15:27:09 INFO Executor: Running task 42.0 in stage 7.0 (TID 58)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 23.0 in stage 7.0 (TID 39). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 59, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 43.0 in stage 7.0 (TID 59)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 39) in 578 ms on localhost (28/200)
15/08/09 15:27:09 INFO Executor: Finished task 27.0 in stage 7.0 (TID 43). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 60, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 44.0 in stage 7.0 (TID 60)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 43) in 511 ms on localhost (29/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 25.0 in stage 7.0 (TID 41). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 61, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO Executor: Running task 45.0 in stage 7.0 (TID 61)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 41) in 547 ms on localhost (30/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 43.0 in stage 7.0 (TID 59). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 62, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 46.0 in stage 7.0 (TID 62)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 59) in 51 ms on localhost (31/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 46.0 in stage 7.0 (TID 62). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 63, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 62) in 52 ms on localhost (32/200)
15/08/09 15:27:09 INFO Executor: Running task 47.0 in stage 7.0 (TID 63)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO Executor: Finished task 30.0 in stage 7.0 (TID 46). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 64, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 48.0 in stage 7.0 (TID 64)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 46) in 639 ms on localhost (33/200)
15/08/09 15:27:09 INFO Executor: Finished task 33.0 in stage 7.0 (TID 49). 1124 bytes result sent to driver
15/08/09 15:27:09 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 65, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:09 INFO Executor: Running task 49.0 in stage 7.0 (TID 65)
15/08/09 15:27:09 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 49) in 623 ms on localhost (34/200)
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO Executor: Finished task 28.0 in stage 7.0 (TID 44). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 50.0 in stage 7.0 (TID 66, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 50.0 in stage 7.0 (TID 66)
15/08/09 15:27:10 INFO Executor: Finished task 29.0 in stage 7.0 (TID 45). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 44) in 716 ms on localhost (35/200)
15/08/09 15:27:10 INFO TaskSetManager: Starting task 51.0 in stage 7.0 (TID 67, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 51.0 in stage 7.0 (TID 67)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 45) in 716 ms on localhost (36/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 34.0 in stage 7.0 (TID 50). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 52.0 in stage 7.0 (TID 68, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 52.0 in stage 7.0 (TID 68)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 50) in 702 ms on localhost (37/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 38.0 in stage 7.0 (TID 54). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 53.0 in stage 7.0 (TID 69, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 53.0 in stage 7.0 (TID 69)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 54) in 708 ms on localhost (38/200)
15/08/09 15:27:10 INFO Executor: Finished task 37.0 in stage 7.0 (TID 53). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 54.0 in stage 7.0 (TID 70, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 54.0 in stage 7.0 (TID 70)
15/08/09 15:27:10 INFO Executor: Finished task 50.0 in stage 7.0 (TID 66). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 53) in 714 ms on localhost (39/200)
15/08/09 15:27:10 INFO TaskSetManager: Starting task 55.0 in stage 7.0 (TID 71, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 55.0 in stage 7.0 (TID 71)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO TaskSetManager: Finished task 50.0 in stage 7.0 (TID 66) in 49 ms on localhost (40/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 36.0 in stage 7.0 (TID 52). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO Executor: Finished task 52.0 in stage 7.0 (TID 68). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 56.0 in stage 7.0 (TID 72, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 56.0 in stage 7.0 (TID 72)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 52) in 746 ms on localhost (41/200)
15/08/09 15:27:10 INFO TaskSetManager: Starting task 57.0 in stage 7.0 (TID 73, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 57.0 in stage 7.0 (TID 73)
15/08/09 15:27:10 INFO Executor: Finished task 35.0 in stage 7.0 (TID 51). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Finished task 52.0 in stage 7.0 (TID 68) in 64 ms on localhost (42/200)
15/08/09 15:27:10 INFO TaskSetManager: Starting task 58.0 in stage 7.0 (TID 74, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 58.0 in stage 7.0 (TID 74)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 51) in 763 ms on localhost (43/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 39.0 in stage 7.0 (TID 55). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 59.0 in stage 7.0 (TID 75, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 59.0 in stage 7.0 (TID 75)
15/08/09 15:27:10 INFO Executor: Finished task 40.0 in stage 7.0 (TID 56). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 55) in 710 ms on localhost (44/200)
15/08/09 15:27:10 INFO TaskSetManager: Starting task 60.0 in stage 7.0 (TID 76, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 60.0 in stage 7.0 (TID 76)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 56) in 689 ms on localhost (45/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO Executor: Finished task 56.0 in stage 7.0 (TID 72). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 61.0 in stage 7.0 (TID 77, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 61.0 in stage 7.0 (TID 77)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 56.0 in stage 7.0 (TID 72) in 51 ms on localhost (46/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 59.0 in stage 7.0 (TID 75). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 62.0 in stage 7.0 (TID 78, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 62.0 in stage 7.0 (TID 78)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 59.0 in stage 7.0 (TID 75) in 42 ms on localhost (47/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO Executor: Finished task 41.0 in stage 7.0 (TID 57). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 63.0 in stage 7.0 (TID 79, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 63.0 in stage 7.0 (TID 79)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 57) in 530 ms on localhost (48/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO Executor: Finished task 42.0 in stage 7.0 (TID 58). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 64.0 in stage 7.0 (TID 80, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 64.0 in stage 7.0 (TID 80)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 58) in 496 ms on localhost (49/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 45.0 in stage 7.0 (TID 61). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 65.0 in stage 7.0 (TID 81, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 65.0 in stage 7.0 (TID 81)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 61) in 430 ms on localhost (50/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 44.0 in stage 7.0 (TID 60). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 66.0 in stage 7.0 (TID 82, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 66.0 in stage 7.0 (TID 82)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 60) in 454 ms on localhost (51/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 47.0 in stage 7.0 (TID 63). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 67.0 in stage 7.0 (TID 83, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 67.0 in stage 7.0 (TID 83)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 63) in 498 ms on localhost (52/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 49.0 in stage 7.0 (TID 65). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO TaskSetManager: Starting task 68.0 in stage 7.0 (TID 84, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 68.0 in stage 7.0 (TID 84)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 65) in 459 ms on localhost (53/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO Executor: Finished task 48.0 in stage 7.0 (TID 64). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 69.0 in stage 7.0 (TID 85, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 69.0 in stage 7.0 (TID 85)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 64) in 850 ms on localhost (54/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 51.0 in stage 7.0 (TID 67). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 70.0 in stage 7.0 (TID 86, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 51.0 in stage 7.0 (TID 67) in 790 ms on localhost (55/200)
15/08/09 15:27:10 INFO Executor: Running task 70.0 in stage 7.0 (TID 86)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 69.0 in stage 7.0 (TID 85). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 71.0 in stage 7.0 (TID 87, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 71.0 in stage 7.0 (TID 87)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 69.0 in stage 7.0 (TID 85) in 45 ms on localhost (56/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 70.0 in stage 7.0 (TID 86). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 72.0 in stage 7.0 (TID 88, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 72.0 in stage 7.0 (TID 88)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 70.0 in stage 7.0 (TID 86) in 43 ms on localhost (57/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 54.0 in stage 7.0 (TID 70). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 73.0 in stage 7.0 (TID 89, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 73.0 in stage 7.0 (TID 89)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 54.0 in stage 7.0 (TID 70) in 860 ms on localhost (58/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:10 INFO Executor: Finished task 53.0 in stage 7.0 (TID 69). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 74.0 in stage 7.0 (TID 90, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 74.0 in stage 7.0 (TID 90)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 53.0 in stage 7.0 (TID 69) in 873 ms on localhost (59/200)
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:10 INFO Executor: Finished task 58.0 in stage 7.0 (TID 74). 1124 bytes result sent to driver
15/08/09 15:27:10 INFO TaskSetManager: Starting task 75.0 in stage 7.0 (TID 91, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:10 INFO Executor: Running task 75.0 in stage 7.0 (TID 91)
15/08/09 15:27:10 INFO TaskSetManager: Finished task 58.0 in stage 7.0 (TID 74) in 860 ms on localhost (60/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 55.0 in stage 7.0 (TID 71). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 76.0 in stage 7.0 (TID 92, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 76.0 in stage 7.0 (TID 92)
15/08/09 15:27:11 INFO Executor: Finished task 60.0 in stage 7.0 (TID 76). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Finished task 55.0 in stage 7.0 (TID 71) in 917 ms on localhost (61/200)
15/08/09 15:27:11 INFO TaskSetManager: Starting task 77.0 in stage 7.0 (TID 93, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 77.0 in stage 7.0 (TID 93)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 60.0 in stage 7.0 (TID 76) in 868 ms on localhost (62/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 62.0 in stage 7.0 (TID 78). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 78.0 in stage 7.0 (TID 94, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 78.0 in stage 7.0 (TID 94)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 62.0 in stage 7.0 (TID 78) in 859 ms on localhost (63/200)
15/08/09 15:27:11 INFO Executor: Finished task 57.0 in stage 7.0 (TID 73). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 79.0 in stage 7.0 (TID 95, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 79.0 in stage 7.0 (TID 95)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 57.0 in stage 7.0 (TID 73) in 924 ms on localhost (64/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO Executor: Finished task 61.0 in stage 7.0 (TID 77). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO TaskSetManager: Starting task 80.0 in stage 7.0 (TID 96, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO Executor: Running task 80.0 in stage 7.0 (TID 96)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO TaskSetManager: Finished task 61.0 in stage 7.0 (TID 77) in 882 ms on localhost (65/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 63.0 in stage 7.0 (TID 79). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 81.0 in stage 7.0 (TID 97, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 81.0 in stage 7.0 (TID 97)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 63.0 in stage 7.0 (TID 79) in 836 ms on localhost (66/200)
15/08/09 15:27:11 INFO Executor: Finished task 64.0 in stage 7.0 (TID 80). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 82.0 in stage 7.0 (TID 98, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO TaskSetManager: Finished task 64.0 in stage 7.0 (TID 80) in 816 ms on localhost (67/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO Executor: Running task 82.0 in stage 7.0 (TID 98)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 65.0 in stage 7.0 (TID 81). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO Executor: Finished task 66.0 in stage 7.0 (TID 82). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 83.0 in stage 7.0 (TID 99, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 65.0 in stage 7.0 (TID 81) in 818 ms on localhost (68/200)
15/08/09 15:27:11 INFO Executor: Running task 83.0 in stage 7.0 (TID 99)
15/08/09 15:27:11 INFO TaskSetManager: Starting task 84.0 in stage 7.0 (TID 100, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 84.0 in stage 7.0 (TID 100)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 66.0 in stage 7.0 (TID 82) in 803 ms on localhost (69/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 68.0 in stage 7.0 (TID 84). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 85.0 in stage 7.0 (TID 101, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 85.0 in stage 7.0 (TID 101)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 68.0 in stage 7.0 (TID 84) in 699 ms on localhost (70/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 67.0 in stage 7.0 (TID 83). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 86.0 in stage 7.0 (TID 102, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 86.0 in stage 7.0 (TID 102)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 67.0 in stage 7.0 (TID 83) in 720 ms on localhost (71/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 72.0 in stage 7.0 (TID 88). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 87.0 in stage 7.0 (TID 103, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 87.0 in stage 7.0 (TID 103)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 72.0 in stage 7.0 (TID 88) in 344 ms on localhost (72/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 71.0 in stage 7.0 (TID 87). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 88.0 in stage 7.0 (TID 104, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 88.0 in stage 7.0 (TID 104)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 71.0 in stage 7.0 (TID 87) in 407 ms on localhost (73/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 74.0 in stage 7.0 (TID 90). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 89.0 in stage 7.0 (TID 105, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 89.0 in stage 7.0 (TID 105)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 74.0 in stage 7.0 (TID 90) in 448 ms on localhost (74/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 73.0 in stage 7.0 (TID 89). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 90.0 in stage 7.0 (TID 106, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 90.0 in stage 7.0 (TID 106)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 73.0 in stage 7.0 (TID 89) in 524 ms on localhost (75/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 75.0 in stage 7.0 (TID 91). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 91.0 in stage 7.0 (TID 107, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 91.0 in stage 7.0 (TID 107)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 75.0 in stage 7.0 (TID 91) in 512 ms on localhost (76/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO Executor: Finished task 76.0 in stage 7.0 (TID 92). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 92.0 in stage 7.0 (TID 108, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 92.0 in stage 7.0 (TID 108)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 76.0 in stage 7.0 (TID 92) in 706 ms on localhost (77/200)
15/08/09 15:27:11 INFO Executor: Finished task 77.0 in stage 7.0 (TID 93). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 93.0 in stage 7.0 (TID 109, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 93.0 in stage 7.0 (TID 109)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 77.0 in stage 7.0 (TID 93) in 707 ms on localhost (78/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 79.0 in stage 7.0 (TID 95). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 94.0 in stage 7.0 (TID 110, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 79.0 in stage 7.0 (TID 95) in 691 ms on localhost (79/200)
15/08/09 15:27:11 INFO Executor: Running task 94.0 in stage 7.0 (TID 110)
15/08/09 15:27:11 INFO Executor: Finished task 80.0 in stage 7.0 (TID 96). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 95.0 in stage 7.0 (TID 111, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 95.0 in stage 7.0 (TID 111)
15/08/09 15:27:11 INFO Executor: Finished task 78.0 in stage 7.0 (TID 94). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO TaskSetManager: Finished task 80.0 in stage 7.0 (TID 96) in 691 ms on localhost (80/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO TaskSetManager: Starting task 96.0 in stage 7.0 (TID 112, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 96.0 in stage 7.0 (TID 112)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 78.0 in stage 7.0 (TID 94) in 704 ms on localhost (81/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 81.0 in stage 7.0 (TID 97). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 97.0 in stage 7.0 (TID 113, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 97.0 in stage 7.0 (TID 113)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 81.0 in stage 7.0 (TID 97) in 714 ms on localhost (82/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 94.0 in stage 7.0 (TID 110). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 98.0 in stage 7.0 (TID 114, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 94.0 in stage 7.0 (TID 110) in 84 ms on localhost (83/200)
15/08/09 15:27:11 INFO Executor: Running task 98.0 in stage 7.0 (TID 114)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 84.0 in stage 7.0 (TID 100). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 99.0 in stage 7.0 (TID 115, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 84.0 in stage 7.0 (TID 100) in 757 ms on localhost (84/200)
15/08/09 15:27:11 INFO Executor: Running task 99.0 in stage 7.0 (TID 115)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO Executor: Finished task 97.0 in stage 7.0 (TID 113). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO Executor: Finished task 85.0 in stage 7.0 (TID 101). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 100.0 in stage 7.0 (TID 116, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 100.0 in stage 7.0 (TID 116)
15/08/09 15:27:11 INFO Executor: Finished task 82.0 in stage 7.0 (TID 98). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Finished task 97.0 in stage 7.0 (TID 113) in 89 ms on localhost (85/200)
15/08/09 15:27:11 INFO TaskSetManager: Starting task 101.0 in stage 7.0 (TID 117, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 101.0 in stage 7.0 (TID 117)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 85.0 in stage 7.0 (TID 101) in 739 ms on localhost (86/200)
15/08/09 15:27:11 INFO Executor: Finished task 83.0 in stage 7.0 (TID 99). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 102.0 in stage 7.0 (TID 118, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 102.0 in stage 7.0 (TID 118)
15/08/09 15:27:11 INFO TaskSetManager: Starting task 103.0 in stage 7.0 (TID 119, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 103.0 in stage 7.0 (TID 119)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 83.0 in stage 7.0 (TID 99) in 791 ms on localhost (87/200)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 82.0 in stage 7.0 (TID 98) in 806 ms on localhost (88/200)
15/08/09 15:27:11 INFO Executor: Finished task 86.0 in stage 7.0 (TID 102). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO TaskSetManager: Starting task 104.0 in stage 7.0 (TID 120, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO Executor: Running task 104.0 in stage 7.0 (TID 120)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO TaskSetManager: Finished task 86.0 in stage 7.0 (TID 102) in 740 ms on localhost (89/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 88.0 in stage 7.0 (TID 104). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 105.0 in stage 7.0 (TID 121, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 105.0 in stage 7.0 (TID 121)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 88.0 in stage 7.0 (TID 104) in 653 ms on localhost (90/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO Executor: Finished task 87.0 in stage 7.0 (TID 103). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO TaskSetManager: Starting task 106.0 in stage 7.0 (TID 122, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 106.0 in stage 7.0 (TID 122)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 87.0 in stage 7.0 (TID 103) in 711 ms on localhost (91/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 89.0 in stage 7.0 (TID 105). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 107.0 in stage 7.0 (TID 123, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 107.0 in stage 7.0 (TID 123)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 89.0 in stage 7.0 (TID 105) in 535 ms on localhost (92/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Finished task 90.0 in stage 7.0 (TID 106). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO Executor: Finished task 91.0 in stage 7.0 (TID 107). 1124 bytes result sent to driver
15/08/09 15:27:11 INFO TaskSetManager: Starting task 108.0 in stage 7.0 (TID 124, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO Executor: Running task 108.0 in stage 7.0 (TID 124)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 90.0 in stage 7.0 (TID 106) in 484 ms on localhost (93/200)
15/08/09 15:27:11 INFO TaskSetManager: Starting task 109.0 in stage 7.0 (TID 125, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:11 INFO TaskSetManager: Finished task 91.0 in stage 7.0 (TID 107) in 464 ms on localhost (94/200)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO Executor: Running task 109.0 in stage 7.0 (TID 125)
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 92.0 in stage 7.0 (TID 108). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 110.0 in stage 7.0 (TID 126, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 110.0 in stage 7.0 (TID 126)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 92.0 in stage 7.0 (TID 108) in 363 ms on localhost (95/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 93.0 in stage 7.0 (TID 109). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 111.0 in stage 7.0 (TID 127, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 93.0 in stage 7.0 (TID 109) in 416 ms on localhost (96/200)
15/08/09 15:27:12 INFO Executor: Running task 111.0 in stage 7.0 (TID 127)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 95.0 in stage 7.0 (TID 111). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 112.0 in stage 7.0 (TID 128, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 112.0 in stage 7.0 (TID 128)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 95.0 in stage 7.0 (TID 111) in 629 ms on localhost (97/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 96.0 in stage 7.0 (TID 112). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 113.0 in stage 7.0 (TID 129, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 113.0 in stage 7.0 (TID 129)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 96.0 in stage 7.0 (TID 112) in 787 ms on localhost (98/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 98.0 in stage 7.0 (TID 114). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 114.0 in stage 7.0 (TID 130, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 114.0 in stage 7.0 (TID 130)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 98.0 in stage 7.0 (TID 114) in 758 ms on localhost (99/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 103.0 in stage 7.0 (TID 119). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 115.0 in stage 7.0 (TID 131, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 115.0 in stage 7.0 (TID 131)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 103.0 in stage 7.0 (TID 119) in 779 ms on localhost (100/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 99.0 in stage 7.0 (TID 115). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 116.0 in stage 7.0 (TID 132, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 99.0 in stage 7.0 (TID 115) in 843 ms on localhost (101/200)
15/08/09 15:27:12 INFO Executor: Running task 116.0 in stage 7.0 (TID 132)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 100.0 in stage 7.0 (TID 116). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 117.0 in stage 7.0 (TID 133, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 117.0 in stage 7.0 (TID 133)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 100.0 in stage 7.0 (TID 116) in 856 ms on localhost (102/200)
15/08/09 15:27:12 INFO Executor: Finished task 101.0 in stage 7.0 (TID 117). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 118.0 in stage 7.0 (TID 134, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 118.0 in stage 7.0 (TID 134)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:12 INFO TaskSetManager: Finished task 101.0 in stage 7.0 (TID 117) in 855 ms on localhost (103/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 102.0 in stage 7.0 (TID 118). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 119.0 in stage 7.0 (TID 135, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 119.0 in stage 7.0 (TID 135)
15/08/09 15:27:12 INFO Executor: Finished task 104.0 in stage 7.0 (TID 120). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Finished task 102.0 in stage 7.0 (TID 118) in 859 ms on localhost (104/200)
15/08/09 15:27:12 INFO TaskSetManager: Starting task 120.0 in stage 7.0 (TID 136, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Finished task 106.0 in stage 7.0 (TID 122). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Finished task 104.0 in stage 7.0 (TID 120) in 850 ms on localhost (105/200)
15/08/09 15:27:12 INFO TaskSetManager: Starting task 121.0 in stage 7.0 (TID 137, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 121.0 in stage 7.0 (TID 137)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 106.0 in stage 7.0 (TID 122) in 809 ms on localhost (106/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Running task 120.0 in stage 7.0 (TID 136)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 109.0 in stage 7.0 (TID 125). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 122.0 in stage 7.0 (TID 138, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 122.0 in stage 7.0 (TID 138)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 109.0 in stage 7.0 (TID 125) in 791 ms on localhost (107/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 105.0 in stage 7.0 (TID 121). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO TaskSetManager: Starting task 123.0 in stage 7.0 (TID 139, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 123.0 in stage 7.0 (TID 139)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO TaskSetManager: Finished task 105.0 in stage 7.0 (TID 121) in 832 ms on localhost (108/200)
15/08/09 15:27:12 INFO Executor: Finished task 107.0 in stage 7.0 (TID 123). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 124.0 in stage 7.0 (TID 140, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Running task 124.0 in stage 7.0 (TID 140)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO TaskSetManager: Finished task 107.0 in stage 7.0 (TID 123) in 818 ms on localhost (109/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:12 INFO Executor: Finished task 108.0 in stage 7.0 (TID 124). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 125.0 in stage 7.0 (TID 141, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 125.0 in stage 7.0 (TID 141)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 108.0 in stage 7.0 (TID 124) in 821 ms on localhost (110/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 110.0 in stage 7.0 (TID 126). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 126.0 in stage 7.0 (TID 142, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 110.0 in stage 7.0 (TID 126) in 709 ms on localhost (111/200)
15/08/09 15:27:12 INFO Executor: Running task 126.0 in stage 7.0 (TID 142)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 111.0 in stage 7.0 (TID 127). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 127.0 in stage 7.0 (TID 143, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 127.0 in stage 7.0 (TID 143)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 111.0 in stage 7.0 (TID 127) in 776 ms on localhost (112/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 112.0 in stage 7.0 (TID 128). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 128.0 in stage 7.0 (TID 144, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 128.0 in stage 7.0 (TID 144)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 112.0 in stage 7.0 (TID 128) in 585 ms on localhost (113/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:12 INFO Executor: Finished task 113.0 in stage 7.0 (TID 129). 1124 bytes result sent to driver
15/08/09 15:27:12 INFO TaskSetManager: Starting task 129.0 in stage 7.0 (TID 145, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:12 INFO Executor: Running task 129.0 in stage 7.0 (TID 145)
15/08/09 15:27:12 INFO TaskSetManager: Finished task 113.0 in stage 7.0 (TID 129) in 432 ms on localhost (114/200)
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 114.0 in stage 7.0 (TID 130). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 130.0 in stage 7.0 (TID 146, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 130.0 in stage 7.0 (TID 146)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 114.0 in stage 7.0 (TID 130) in 429 ms on localhost (115/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 115.0 in stage 7.0 (TID 131). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 131.0 in stage 7.0 (TID 147, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 131.0 in stage 7.0 (TID 147)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 115.0 in stage 7.0 (TID 131) in 483 ms on localhost (116/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 116.0 in stage 7.0 (TID 132). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 132.0 in stage 7.0 (TID 148, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 132.0 in stage 7.0 (TID 148)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 116.0 in stage 7.0 (TID 132) in 675 ms on localhost (117/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 117.0 in stage 7.0 (TID 133). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO Executor: Finished task 119.0 in stage 7.0 (TID 135). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 133.0 in stage 7.0 (TID 149, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 133.0 in stage 7.0 (TID 149)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 117.0 in stage 7.0 (TID 133) in 811 ms on localhost (118/200)
15/08/09 15:27:13 INFO TaskSetManager: Starting task 134.0 in stage 7.0 (TID 150, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 134.0 in stage 7.0 (TID 150)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 119.0 in stage 7.0 (TID 135) in 799 ms on localhost (119/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO Executor: Finished task 123.0 in stage 7.0 (TID 139). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 135.0 in stage 7.0 (TID 151, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 135.0 in stage 7.0 (TID 151)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 123.0 in stage 7.0 (TID 139) in 820 ms on localhost (120/200)
15/08/09 15:27:13 INFO Executor: Finished task 122.0 in stage 7.0 (TID 138). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 136.0 in stage 7.0 (TID 152, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Finished task 133.0 in stage 7.0 (TID 149). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO Executor: Running task 136.0 in stage 7.0 (TID 152)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO TaskSetManager: Finished task 122.0 in stage 7.0 (TID 138) in 831 ms on localhost (121/200)
15/08/09 15:27:13 INFO TaskSetManager: Starting task 137.0 in stage 7.0 (TID 153, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 137.0 in stage 7.0 (TID 153)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:13 INFO TaskSetManager: Finished task 133.0 in stage 7.0 (TID 149) in 57 ms on localhost (122/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 121.0 in stage 7.0 (TID 137). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 138.0 in stage 7.0 (TID 154, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 138.0 in stage 7.0 (TID 154)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 121.0 in stage 7.0 (TID 137) in 863 ms on localhost (123/200)
15/08/09 15:27:13 INFO Executor: Finished task 118.0 in stage 7.0 (TID 134). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 139.0 in stage 7.0 (TID 155, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 139.0 in stage 7.0 (TID 155)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 118.0 in stage 7.0 (TID 134) in 879 ms on localhost (124/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 120.0 in stage 7.0 (TID 136). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 140.0 in stage 7.0 (TID 156, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 140.0 in stage 7.0 (TID 156)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 120.0 in stage 7.0 (TID 136) in 913 ms on localhost (125/200)
15/08/09 15:27:13 INFO Executor: Finished task 126.0 in stage 7.0 (TID 142). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 141.0 in stage 7.0 (TID 157, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 141.0 in stage 7.0 (TID 157)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 126.0 in stage 7.0 (TID 142) in 876 ms on localhost (126/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 124.0 in stage 7.0 (TID 140). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 142.0 in stage 7.0 (TID 158, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 142.0 in stage 7.0 (TID 158)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 124.0 in stage 7.0 (TID 140) in 916 ms on localhost (127/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 128.0 in stage 7.0 (TID 144). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO TaskSetManager: Starting task 143.0 in stage 7.0 (TID 159, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 128.0 in stage 7.0 (TID 144) in 729 ms on localhost (128/200)
15/08/09 15:27:13 INFO Executor: Running task 143.0 in stage 7.0 (TID 159)
15/08/09 15:27:13 INFO Executor: Finished task 125.0 in stage 7.0 (TID 141). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 144.0 in stage 7.0 (TID 160, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Finished task 127.0 in stage 7.0 (TID 143). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO Executor: Running task 144.0 in stage 7.0 (TID 160)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 125.0 in stage 7.0 (TID 141) in 907 ms on localhost (129/200)
15/08/09 15:27:13 INFO TaskSetManager: Starting task 145.0 in stage 7.0 (TID 161, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 145.0 in stage 7.0 (TID 161)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 127.0 in stage 7.0 (TID 143) in 776 ms on localhost (130/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO Executor: Finished task 129.0 in stage 7.0 (TID 145). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 131.0 in stage 7.0 (TID 147). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 146.0 in stage 7.0 (TID 162, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Finished task 130.0 in stage 7.0 (TID 146). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO Executor: Running task 146.0 in stage 7.0 (TID 162)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 129.0 in stage 7.0 (TID 145) in 736 ms on localhost (131/200)
15/08/09 15:27:13 INFO TaskSetManager: Starting task 147.0 in stage 7.0 (TID 163, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 131.0 in stage 7.0 (TID 147) in 564 ms on localhost (132/200)
15/08/09 15:27:13 INFO Executor: Running task 147.0 in stage 7.0 (TID 163)
15/08/09 15:27:13 INFO TaskSetManager: Starting task 148.0 in stage 7.0 (TID 164, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 148.0 in stage 7.0 (TID 164)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 130.0 in stage 7.0 (TID 146) in 700 ms on localhost (133/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO Executor: Finished task 132.0 in stage 7.0 (TID 148). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO TaskSetManager: Starting task 149.0 in stage 7.0 (TID 165, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 149.0 in stage 7.0 (TID 165)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO TaskSetManager: Finished task 132.0 in stage 7.0 (TID 148) in 346 ms on localhost (134/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO Executor: Finished task 134.0 in stage 7.0 (TID 150). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 150.0 in stage 7.0 (TID 166, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 150.0 in stage 7.0 (TID 166)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 134.0 in stage 7.0 (TID 150) in 198 ms on localhost (135/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 144.0 in stage 7.0 (TID 160). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 151.0 in stage 7.0 (TID 167, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 151.0 in stage 7.0 (TID 167)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 144.0 in stage 7.0 (TID 160) in 73 ms on localhost (136/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO Executor: Finished task 150.0 in stage 7.0 (TID 166). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 152.0 in stage 7.0 (TID 168, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 152.0 in stage 7.0 (TID 168)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 150.0 in stage 7.0 (TID 166) in 69 ms on localhost (137/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 136.0 in stage 7.0 (TID 152). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 153.0 in stage 7.0 (TID 169, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 153.0 in stage 7.0 (TID 169)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 136.0 in stage 7.0 (TID 152) in 341 ms on localhost (138/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 135.0 in stage 7.0 (TID 151). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 154.0 in stage 7.0 (TID 170, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 154.0 in stage 7.0 (TID 170)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 135.0 in stage 7.0 (TID 151) in 363 ms on localhost (139/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO Executor: Finished task 153.0 in stage 7.0 (TID 169). 1124 bytes result sent to driver
15/08/09 15:27:13 INFO TaskSetManager: Starting task 155.0 in stage 7.0 (TID 171, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:13 INFO Executor: Running task 155.0 in stage 7.0 (TID 171)
15/08/09 15:27:13 INFO TaskSetManager: Finished task 153.0 in stage 7.0 (TID 169) in 42 ms on localhost (140/200)
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 139.0 in stage 7.0 (TID 155). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 156.0 in stage 7.0 (TID 172, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 156.0 in stage 7.0 (TID 172)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 139.0 in stage 7.0 (TID 155) in 446 ms on localhost (141/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 138.0 in stage 7.0 (TID 154). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO TaskSetManager: Starting task 157.0 in stage 7.0 (TID 173, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 157.0 in stage 7.0 (TID 173)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO TaskSetManager: Finished task 138.0 in stage 7.0 (TID 154) in 465 ms on localhost (142/200)
15/08/09 15:27:14 INFO Executor: Finished task 137.0 in stage 7.0 (TID 153). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 158.0 in stage 7.0 (TID 174, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 158.0 in stage 7.0 (TID 174)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 137.0 in stage 7.0 (TID 153) in 527 ms on localhost (143/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 140.0 in stage 7.0 (TID 156). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 159.0 in stage 7.0 (TID 175, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 159.0 in stage 7.0 (TID 175)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 140.0 in stage 7.0 (TID 156) in 548 ms on localhost (144/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO Executor: Finished task 141.0 in stage 7.0 (TID 157). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 160.0 in stage 7.0 (TID 176, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 141.0 in stage 7.0 (TID 157) in 567 ms on localhost (145/200)
15/08/09 15:27:14 INFO Executor: Running task 160.0 in stage 7.0 (TID 176)
15/08/09 15:27:14 INFO Executor: Finished task 148.0 in stage 7.0 (TID 164). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 161.0 in stage 7.0 (TID 177, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 161.0 in stage 7.0 (TID 177)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 148.0 in stage 7.0 (TID 164) in 530 ms on localhost (146/200)
15/08/09 15:27:14 INFO Executor: Finished task 143.0 in stage 7.0 (TID 159). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 162.0 in stage 7.0 (TID 178, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 162.0 in stage 7.0 (TID 178)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 143.0 in stage 7.0 (TID 159) in 555 ms on localhost (147/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 149.0 in stage 7.0 (TID 165). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO Executor: Finished task 145.0 in stage 7.0 (TID 161). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 163.0 in stage 7.0 (TID 179, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 149.0 in stage 7.0 (TID 165) in 542 ms on localhost (148/200)
15/08/09 15:27:14 INFO Executor: Running task 163.0 in stage 7.0 (TID 179)
15/08/09 15:27:14 INFO TaskSetManager: Starting task 164.0 in stage 7.0 (TID 180, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 164.0 in stage 7.0 (TID 180)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 145.0 in stage 7.0 (TID 161) in 567 ms on localhost (149/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 142.0 in stage 7.0 (TID 158). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 165.0 in stage 7.0 (TID 181, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 165.0 in stage 7.0 (TID 181)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 142.0 in stage 7.0 (TID 158) in 603 ms on localhost (150/200)
15/08/09 15:27:14 INFO Executor: Finished task 146.0 in stage 7.0 (TID 162). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO TaskSetManager: Starting task 166.0 in stage 7.0 (TID 182, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Running task 166.0 in stage 7.0 (TID 182)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 146.0 in stage 7.0 (TID 162) in 589 ms on localhost (151/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO Executor: Finished task 147.0 in stage 7.0 (TID 163). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 167.0 in stage 7.0 (TID 183, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Finished task 162.0 in stage 7.0 (TID 178). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO Executor: Running task 167.0 in stage 7.0 (TID 183)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 147.0 in stage 7.0 (TID 163) in 593 ms on localhost (152/200)
15/08/09 15:27:14 INFO TaskSetManager: Starting task 168.0 in stage 7.0 (TID 184, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 168.0 in stage 7.0 (TID 184)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 162.0 in stage 7.0 (TID 178) in 61 ms on localhost (153/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 164.0 in stage 7.0 (TID 180). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO Executor: Finished task 152.0 in stage 7.0 (TID 168). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 169.0 in stage 7.0 (TID 185, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 164.0 in stage 7.0 (TID 180) in 62 ms on localhost (154/200)
15/08/09 15:27:14 INFO TaskSetManager: Starting task 170.0 in stage 7.0 (TID 186, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 152.0 in stage 7.0 (TID 168) in 520 ms on localhost (155/200)
15/08/09 15:27:14 INFO Executor: Running task 169.0 in stage 7.0 (TID 185)
15/08/09 15:27:14 INFO Executor: Running task 170.0 in stage 7.0 (TID 186)
15/08/09 15:27:14 INFO Executor: Finished task 151.0 in stage 7.0 (TID 167). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 171.0 in stage 7.0 (TID 187, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO TaskSetManager: Finished task 151.0 in stage 7.0 (TID 167) in 572 ms on localhost (156/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 167.0 in stage 7.0 (TID 183). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 172.0 in stage 7.0 (TID 188, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 172.0 in stage 7.0 (TID 188)
15/08/09 15:27:14 INFO Executor: Running task 171.0 in stage 7.0 (TID 187)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 167.0 in stage 7.0 (TID 183) in 41 ms on localhost (157/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 154.0 in stage 7.0 (TID 170). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 173.0 in stage 7.0 (TID 189, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 173.0 in stage 7.0 (TID 189)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 154.0 in stage 7.0 (TID 170) in 406 ms on localhost (158/200)
15/08/09 15:27:14 INFO Executor: Finished task 155.0 in stage 7.0 (TID 171). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO TaskSetManager: Starting task 174.0 in stage 7.0 (TID 190, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 174.0 in stage 7.0 (TID 190)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO TaskSetManager: Finished task 155.0 in stage 7.0 (TID 171) in 384 ms on localhost (159/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 169.0 in stage 7.0 (TID 185). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 175.0 in stage 7.0 (TID 191, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 175.0 in stage 7.0 (TID 191)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 169.0 in stage 7.0 (TID 185) in 66 ms on localhost (160/200)
15/08/09 15:27:14 INFO Executor: Finished task 156.0 in stage 7.0 (TID 172). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 176.0 in stage 7.0 (TID 192, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 176.0 in stage 7.0 (TID 192)
15/08/09 15:27:14 INFO Executor: Finished task 158.0 in stage 7.0 (TID 174). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Finished task 156.0 in stage 7.0 (TID 172) in 335 ms on localhost (161/200)
15/08/09 15:27:14 INFO TaskSetManager: Starting task 177.0 in stage 7.0 (TID 193, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 177.0 in stage 7.0 (TID 193)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO TaskSetManager: Finished task 158.0 in stage 7.0 (TID 174) in 288 ms on localhost (162/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO Executor: Finished task 157.0 in stage 7.0 (TID 173). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 178.0 in stage 7.0 (TID 194, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 178.0 in stage 7.0 (TID 194)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 157.0 in stage 7.0 (TID 173) in 349 ms on localhost (163/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO Executor: Finished task 173.0 in stage 7.0 (TID 189). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 179.0 in stage 7.0 (TID 195, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 179.0 in stage 7.0 (TID 195)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO TaskSetManager: Finished task 173.0 in stage 7.0 (TID 189) in 75 ms on localhost (164/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 159.0 in stage 7.0 (TID 175). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 180.0 in stage 7.0 (TID 196, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 180.0 in stage 7.0 (TID 196)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 159.0 in stage 7.0 (TID 175) in 431 ms on localhost (165/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 161.0 in stage 7.0 (TID 177). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO Executor: Finished task 180.0 in stage 7.0 (TID 196). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 181.0 in stage 7.0 (TID 197, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 181.0 in stage 7.0 (TID 197)
15/08/09 15:27:14 INFO TaskSetManager: Starting task 182.0 in stage 7.0 (TID 198, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 182.0 in stage 7.0 (TID 198)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 161.0 in stage 7.0 (TID 177) in 439 ms on localhost (166/200)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 180.0 in stage 7.0 (TID 196) in 46 ms on localhost (167/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 163.0 in stage 7.0 (TID 179). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 183.0 in stage 7.0 (TID 199, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 183.0 in stage 7.0 (TID 199)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 163.0 in stage 7.0 (TID 179) in 618 ms on localhost (168/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 160.0 in stage 7.0 (TID 176). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 184.0 in stage 7.0 (TID 200, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 184.0 in stage 7.0 (TID 200)
15/08/09 15:27:14 INFO Executor: Finished task 165.0 in stage 7.0 (TID 181). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Finished task 160.0 in stage 7.0 (TID 176) in 730 ms on localhost (169/200)
15/08/09 15:27:14 INFO TaskSetManager: Starting task 185.0 in stage 7.0 (TID 201, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 185.0 in stage 7.0 (TID 201)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 165.0 in stage 7.0 (TID 181) in 684 ms on localhost (170/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO Executor: Finished task 168.0 in stage 7.0 (TID 184). 1124 bytes result sent to driver
15/08/09 15:27:14 INFO TaskSetManager: Starting task 186.0 in stage 7.0 (TID 202, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:14 INFO Executor: Running task 186.0 in stage 7.0 (TID 202)
15/08/09 15:27:14 INFO TaskSetManager: Finished task 168.0 in stage 7.0 (TID 184) in 679 ms on localhost (171/200)
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO Executor: Finished task 184.0 in stage 7.0 (TID 200). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 187.0 in stage 7.0 (TID 203, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Running task 187.0 in stage 7.0 (TID 203)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 184.0 in stage 7.0 (TID 200) in 41 ms on localhost (172/200)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Finished task 166.0 in stage 7.0 (TID 182). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 188.0 in stage 7.0 (TID 204, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 166.0 in stage 7.0 (TID 182) in 729 ms on localhost (173/200)
15/08/09 15:27:15 INFO Executor: Running task 188.0 in stage 7.0 (TID 204)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Finished task 186.0 in stage 7.0 (TID 202). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO TaskSetManager: Starting task 189.0 in stage 7.0 (TID 205, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Running task 189.0 in stage 7.0 (TID 205)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 186.0 in stage 7.0 (TID 202) in 44 ms on localhost (174/200)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Finished task 170.0 in stage 7.0 (TID 186). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 190.0 in stage 7.0 (TID 206, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Running task 190.0 in stage 7.0 (TID 206)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 170.0 in stage 7.0 (TID 186) in 724 ms on localhost (175/200)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Finished task 174.0 in stage 7.0 (TID 190). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 191.0 in stage 7.0 (TID 207, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Finished task 172.0 in stage 7.0 (TID 188). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 174.0 in stage 7.0 (TID 190) in 740 ms on localhost (176/200)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 192.0 in stage 7.0 (TID 208, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Running task 192.0 in stage 7.0 (TID 208)
15/08/09 15:27:15 INFO Executor: Running task 191.0 in stage 7.0 (TID 207)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 172.0 in stage 7.0 (TID 188) in 753 ms on localhost (177/200)
15/08/09 15:27:15 INFO Executor: Finished task 171.0 in stage 7.0 (TID 187). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 193.0 in stage 7.0 (TID 209, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Running task 193.0 in stage 7.0 (TID 209)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 171.0 in stage 7.0 (TID 187) in 763 ms on localhost (178/200)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Finished task 177.0 in stage 7.0 (TID 193). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 194.0 in stage 7.0 (TID 210, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Running task 194.0 in stage 7.0 (TID 210)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 177.0 in stage 7.0 (TID 193) in 723 ms on localhost (179/200)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Finished task 175.0 in stage 7.0 (TID 191). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 195.0 in stage 7.0 (TID 211, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 175.0 in stage 7.0 (TID 191) in 751 ms on localhost (180/200)
15/08/09 15:27:15 INFO Executor: Finished task 176.0 in stage 7.0 (TID 192). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 196.0 in stage 7.0 (TID 212, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Finished task 179.0 in stage 7.0 (TID 195). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 176.0 in stage 7.0 (TID 192) in 753 ms on localhost (181/200)
15/08/09 15:27:15 INFO Executor: Running task 195.0 in stage 7.0 (TID 211)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 197.0 in stage 7.0 (TID 213, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Running task 196.0 in stage 7.0 (TID 212)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 179.0 in stage 7.0 (TID 195) in 722 ms on localhost (182/200)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Running task 197.0 in stage 7.0 (TID 213)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Finished task 190.0 in stage 7.0 (TID 206). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 198.0 in stage 7.0 (TID 214, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO Executor: Running task 198.0 in stage 7.0 (TID 214)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 190.0 in stage 7.0 (TID 206) in 117 ms on localhost (183/200)
15/08/09 15:27:15 INFO Executor: Finished task 178.0 in stage 7.0 (TID 194). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Starting task 199.0 in stage 7.0 (TID 215, localhost, PROCESS_LOCAL, 1462 bytes)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO TaskSetManager: Finished task 178.0 in stage 7.0 (TID 194) in 755 ms on localhost (184/200)
15/08/09 15:27:15 INFO Executor: Running task 199.0 in stage 7.0 (TID 215)
15/08/09 15:27:15 INFO Executor: Finished task 182.0 in stage 7.0 (TID 198). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 182.0 in stage 7.0 (TID 198) in 497 ms on localhost (185/200)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 10 non-empty blocks out of 10 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO Executor: Finished task 181.0 in stage 7.0 (TID 197). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 181.0 in stage 7.0 (TID 197) in 504 ms on localhost (186/200)
15/08/09 15:27:15 INFO Executor: Finished task 183.0 in stage 7.0 (TID 199). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 183.0 in stage 7.0 (TID 199) in 314 ms on localhost (187/200)
15/08/09 15:27:15 INFO Executor: Finished task 195.0 in stage 7.0 (TID 211). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 195.0 in stage 7.0 (TID 211) in 66 ms on localhost (188/200)
15/08/09 15:27:15 INFO Executor: Finished task 185.0 in stage 7.0 (TID 201). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 185.0 in stage 7.0 (TID 201) in 247 ms on localhost (189/200)
15/08/09 15:27:15 INFO Executor: Finished task 197.0 in stage 7.0 (TID 213). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 197.0 in stage 7.0 (TID 213) in 79 ms on localhost (190/200)
15/08/09 15:27:15 INFO Executor: Finished task 187.0 in stage 7.0 (TID 203). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 187.0 in stage 7.0 (TID 203) in 228 ms on localhost (191/200)
15/08/09 15:27:15 INFO Executor: Finished task 188.0 in stage 7.0 (TID 204). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 188.0 in stage 7.0 (TID 204) in 255 ms on localhost (192/200)
15/08/09 15:27:15 INFO Executor: Finished task 189.0 in stage 7.0 (TID 205). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 189.0 in stage 7.0 (TID 205) in 265 ms on localhost (193/200)
15/08/09 15:27:15 INFO Executor: Finished task 193.0 in stage 7.0 (TID 209). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 193.0 in stage 7.0 (TID 209) in 232 ms on localhost (194/200)
15/08/09 15:27:15 INFO Executor: Finished task 191.0 in stage 7.0 (TID 207). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 191.0 in stage 7.0 (TID 207) in 275 ms on localhost (195/200)
15/08/09 15:27:15 INFO Executor: Finished task 192.0 in stage 7.0 (TID 208). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 192.0 in stage 7.0 (TID 208) in 292 ms on localhost (196/200)
15/08/09 15:27:15 INFO Executor: Finished task 196.0 in stage 7.0 (TID 212). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 196.0 in stage 7.0 (TID 212) in 272 ms on localhost (197/200)
15/08/09 15:27:15 INFO Executor: Finished task 194.0 in stage 7.0 (TID 210). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO Executor: Finished task 198.0 in stage 7.0 (TID 214). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 194.0 in stage 7.0 (TID 210) in 292 ms on localhost (198/200)
15/08/09 15:27:15 INFO TaskSetManager: Finished task 198.0 in stage 7.0 (TID 214) in 258 ms on localhost (199/200)
15/08/09 15:27:15 INFO Executor: Finished task 199.0 in stage 7.0 (TID 215). 1124 bytes result sent to driver
15/08/09 15:27:15 INFO TaskSetManager: Finished task 199.0 in stage 7.0 (TID 215) in 257 ms on localhost (200/200)
15/08/09 15:27:15 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/08/09 15:27:15 INFO DAGScheduler: Stage 7 (mapPartitions at Exchange.scala:64) finished in 8.276 s
15/08/09 15:27:15 INFO DAGScheduler: looking for newly runnable stages
15/08/09 15:27:15 INFO DAGScheduler: running: Set()
15/08/09 15:27:15 INFO DAGScheduler: waiting: Set(Stage 8)
15/08/09 15:27:15 INFO DAGScheduler: failed: Set()
15/08/09 15:27:15 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@1d12b190
15/08/09 15:27:15 INFO StatsReportListener: task runtime:(count: 200, mean: 652.500000, stdev: 474.784277, max: 2232.000000, min: 41.000000)
15/08/09 15:27:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:15 INFO StatsReportListener: 	41.0 ms	51.0 ms	69.0 ms	363.0 ms	653.0 ms	806.0 ms	924.0 ms	2.1 s	2.2 s
15/08/09 15:27:15 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 7580.820000, stdev: 3876.470167, max: 17153.000000, min: 0.000000)
15/08/09 15:27:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:15 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	6.3 KB	8.1 KB	9.9 KB	11.7 KB	13.4 KB	16.8 KB
15/08/09 15:27:15 INFO DAGScheduler: Missing parents for Stage 8: List()
15/08/09 15:27:15 INFO DAGScheduler: Submitting Stage 8 (MapPartitionsRDD[48] at mapPartitions at Aggregate.scala:151), which is now runnable
15/08/09 15:27:15 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.120000, stdev: 0.368239, max: 2.000000, min: 0.000000)
15/08/09 15:27:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:15 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/09 15:27:15 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/09 15:27:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:15 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/09 15:27:15 INFO StatsReportListener: task result size:(count: 200, mean: 1124.000000, stdev: 0.000000, max: 1124.000000, min: 1124.000000)
15/08/09 15:27:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:15 INFO StatsReportListener: 	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B	1124.0 B
15/08/09 15:27:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 96.936276, stdev: 5.643815, max: 99.669239, min: 50.580046)
15/08/09 15:27:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:15 INFO StatsReportListener: 	51 %	87 %	90 %	98 %	99 %	99 %	99 %	100 %	100 %
15/08/09 15:27:15 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.062747, stdev: 0.355829, max: 3.921569, min: 0.000000)
15/08/09 15:27:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:15 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 4 %
15/08/09 15:27:15 INFO StatsReportListener: other time pct: (count: 200, mean: 3.000977, stdev: 5.550143, max: 49.187935, min: 0.330761)
15/08/09 15:27:15 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:15 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 1 %	 2 %	10 %	13 %	49 %
15/08/09 15:27:15 INFO MemoryStore: ensureFreeSpace(149584) called with curMem=768433, maxMem=3333968363
15/08/09 15:27:15 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 146.1 KB, free 3.1 GB)
15/08/09 15:27:15 INFO MemoryStore: ensureFreeSpace(65024) called with curMem=918017, maxMem=3333968363
15/08/09 15:27:15 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 63.5 KB, free 3.1 GB)
15/08/09 15:27:15 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:44535 (size: 63.5 KB, free: 3.1 GB)
15/08/09 15:27:15 INFO BlockManagerMaster: Updated info of block broadcast_12_piece0
15/08/09 15:27:15 INFO DefaultExecutionContext: Created broadcast 12 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:15 INFO DAGScheduler: Submitting 200 missing tasks from Stage 8 (MapPartitionsRDD[48] at mapPartitions at Aggregate.scala:151)
15/08/09 15:27:15 INFO TaskSchedulerImpl: Adding task set 8.0 with 200 tasks
15/08/09 15:27:15 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 216, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 217, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 218, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 219, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 220, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 221, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 222, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 223, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 224, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 225, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 226, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 227, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 228, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 229, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 230, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 231, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:15 INFO Executor: Running task 1.0 in stage 8.0 (TID 217)
15/08/09 15:27:15 INFO Executor: Running task 6.0 in stage 8.0 (TID 222)
15/08/09 15:27:15 INFO Executor: Running task 5.0 in stage 8.0 (TID 221)
15/08/09 15:27:15 INFO Executor: Running task 4.0 in stage 8.0 (TID 220)
15/08/09 15:27:15 INFO Executor: Running task 3.0 in stage 8.0 (TID 219)
15/08/09 15:27:15 INFO Executor: Running task 2.0 in stage 8.0 (TID 218)
15/08/09 15:27:15 INFO Executor: Running task 0.0 in stage 8.0 (TID 216)
15/08/09 15:27:15 INFO Executor: Running task 7.0 in stage 8.0 (TID 223)
15/08/09 15:27:15 INFO Executor: Running task 13.0 in stage 8.0 (TID 229)
15/08/09 15:27:15 INFO Executor: Running task 11.0 in stage 8.0 (TID 227)
15/08/09 15:27:15 INFO Executor: Running task 12.0 in stage 8.0 (TID 228)
15/08/09 15:27:15 INFO Executor: Running task 9.0 in stage 8.0 (TID 225)
15/08/09 15:27:15 INFO Executor: Running task 8.0 in stage 8.0 (TID 224)
15/08/09 15:27:15 INFO Executor: Running task 10.0 in stage 8.0 (TID 226)
15/08/09 15:27:15 INFO Executor: Running task 14.0 in stage 8.0 (TID 230)
15/08/09 15:27:15 INFO Executor: Running task 15.0 in stage 8.0 (TID 231)
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2644028b
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b03074d
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2fa276a3
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@958a5d5
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000012_228/part-00012
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e7263f4
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000001_217/part-00001
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000005_221/part-00005
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000002_218/part-00002
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000014_230/part-00014
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6032acbb
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000013_229/part-00013
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@c7e48c4
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000008_224/part-00008
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3314c97a
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000006_222/part-00006
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19b782c9
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000009_225/part-00009
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@30cba438
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000015_231/part-00015
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@37da93ab
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000010_226/part-00010
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@284d495b
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000003_219/part-00003
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@135e6c3d
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000011_227/part-00011
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@604b2b52
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000000_216/part-00000
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8aca2ff
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000007_223/part-00007
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7526a158
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000004_220/part-00004
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO BlockManager: Removing broadcast 11
15/08/09 15:27:16 INFO BlockManager: Removing block broadcast_11
15/08/09 15:27:16 INFO MemoryStore: Block broadcast_11 of size 13544 dropped from memory (free 3332998866)
15/08/09 15:27:16 INFO BlockManager: Removing block broadcast_11_piece0
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@25492ebe
15/08/09 15:27:16 INFO MemoryStore: Block broadcast_11_piece0 of size 7369 dropped from memory (free 3333006235)
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@c6eab17
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d6a1fd4
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d529bc
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3ba6f9a8
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4f50a330
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3709ccfe
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3031bf62
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@120bd3a6
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4674408b
15/08/09 15:27:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:44535 in memory (size: 7.2 KB, free: 3.1 GB)
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4c816393
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@424fd79f
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42b06f22
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7a3989cf
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5786b3b9
15/08/09 15:27:16 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/08/09 15:27:16 INFO ContextCleaner: Cleaned broadcast 11
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d4fcb36
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,836
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,076
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,396
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,136
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,816
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,136
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,616
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,356
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/09 15:27:16 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,156
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 603B for [ps_partkey] INT32: 140 values, 567B raw, 567B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 619B for [ps_partkey] INT32: 144 values, 583B raw, 583B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,171B for [part_value] DOUBLE: 140 values, 1,127B raw, 1,127B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 667B for [ps_partkey] INT32: 156 values, 631B raw, 631B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,203B for [part_value] DOUBLE: 144 values, 1,159B raw, 1,159B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 615B for [ps_partkey] INT32: 143 values, 579B raw, 579B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 711B for [ps_partkey] INT32: 167 values, 675B raw, 675B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,387B for [part_value] DOUBLE: 167 values, 1,343B raw, 1,343B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,299B for [part_value] DOUBLE: 156 values, 1,255B raw, 1,255B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 751B for [ps_partkey] INT32: 177 values, 715B raw, 715B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 659B for [ps_partkey] INT32: 154 values, 623B raw, 623B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,467B for [part_value] DOUBLE: 177 values, 1,423B raw, 1,423B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 815B for [ps_partkey] INT32: 193 values, 779B raw, 779B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,195B for [part_value] DOUBLE: 143 values, 1,151B raw, 1,151B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 555B for [ps_partkey] INT32: 128 values, 519B raw, 519B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,595B for [part_value] DOUBLE: 193 values, 1,551B raw, 1,551B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,283B for [part_value] DOUBLE: 154 values, 1,239B raw, 1,239B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,075B for [part_value] DOUBLE: 128 values, 1,031B raw, 1,031B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000003_219' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000003
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000003_219: Committed
15/08/09 15:27:16 INFO Executor: Finished task 3.0 in stage 8.0 (TID 219). 781 bytes result sent to driver
15/08/09 15:27:16 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 232, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 16.0 in stage 8.0 (TID 232)
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000000_216' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000000
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000006_222' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000006
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000000_216: Committed
15/08/09 15:27:16 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 219) in 1221 ms on localhost (1/200)
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000006_222: Committed
15/08/09 15:27:16 INFO Executor: Finished task 0.0 in stage 8.0 (TID 216). 781 bytes result sent to driver
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000004_220' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000004
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000004_220: Committed
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000013_229' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000013
15/08/09 15:27:16 INFO Executor: Finished task 6.0 in stage 8.0 (TID 222). 781 bytes result sent to driver
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000013_229: Committed
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000008_224' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000008
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000008_224: Committed
15/08/09 15:27:16 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 233, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000012_228' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000012
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000012_228: Committed
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000015_231' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000015
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000015_231: Committed
15/08/09 15:27:16 INFO Executor: Running task 17.0 in stage 8.0 (TID 233)
15/08/09 15:27:16 INFO Executor: Finished task 13.0 in stage 8.0 (TID 229). 781 bytes result sent to driver
15/08/09 15:27:16 INFO Executor: Finished task 8.0 in stage 8.0 (TID 224). 781 bytes result sent to driver
15/08/09 15:27:16 INFO Executor: Finished task 12.0 in stage 8.0 (TID 228). 781 bytes result sent to driver
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000011_227' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000011
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000010_226' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000010
15/08/09 15:27:16 INFO Executor: Finished task 4.0 in stage 8.0 (TID 220). 781 bytes result sent to driver
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000010_226: Committed
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000011_227: Committed
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000009_225' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000009
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000007_223' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000007
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000009_225: Committed
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000005_221' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000005
15/08/09 15:27:16 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 216) in 1227 ms on localhost (2/200)
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000005_221: Committed
15/08/09 15:27:16 INFO Executor: Finished task 15.0 in stage 8.0 (TID 231). 781 bytes result sent to driver
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000007_223: Committed
15/08/09 15:27:16 INFO Executor: Finished task 10.0 in stage 8.0 (TID 226). 781 bytes result sent to driver
15/08/09 15:27:16 INFO Executor: Finished task 11.0 in stage 8.0 (TID 227). 781 bytes result sent to driver
15/08/09 15:27:16 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 234, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 18.0 in stage 8.0 (TID 234)
15/08/09 15:27:16 INFO Executor: Finished task 9.0 in stage 8.0 (TID 225). 781 bytes result sent to driver
15/08/09 15:27:16 INFO Executor: Finished task 5.0 in stage 8.0 (TID 221). 781 bytes result sent to driver
15/08/09 15:27:16 INFO Executor: Finished task 7.0 in stage 8.0 (TID 223). 781 bytes result sent to driver
15/08/09 15:27:16 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 235, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 19.0 in stage 8.0 (TID 235)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 236, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 20.0 in stage 8.0 (TID 236)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 237, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 21.0 in stage 8.0 (TID 237)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 238, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 22.0 in stage 8.0 (TID 238)
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000001_217' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000001
15/08/09 15:27:16 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 239, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000001_217: Committed
15/08/09 15:27:16 INFO Executor: Running task 23.0 in stage 8.0 (TID 239)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 240, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 24.0 in stage 8.0 (TID 240)
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000002_218' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000002
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000002_218: Committed
15/08/09 15:27:16 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 222) in 1233 ms on localhost (3/200)
15/08/09 15:27:16 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000014_230' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000014
15/08/09 15:27:16 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000014_230: Committed
15/08/09 15:27:16 INFO Executor: Finished task 2.0 in stage 8.0 (TID 218). 781 bytes result sent to driver
15/08/09 15:27:16 INFO Executor: Finished task 1.0 in stage 8.0 (TID 217). 781 bytes result sent to driver
15/08/09 15:27:16 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 228) in 1232 ms on localhost (4/200)
15/08/09 15:27:16 INFO Executor: Finished task 14.0 in stage 8.0 (TID 230). 781 bytes result sent to driver
15/08/09 15:27:16 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 224) in 1235 ms on localhost (5/200)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 229) in 1234 ms on localhost (6/200)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 226) in 1236 ms on localhost (7/200)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 231) in 1235 ms on localhost (8/200)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 220) in 1240 ms on localhost (9/200)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 241, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 25.0 in stage 8.0 (TID 241)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 227) in 1240 ms on localhost (10/200)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 242, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 26.0 in stage 8.0 (TID 242)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 225) in 1241 ms on localhost (11/200)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 243, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 27.0 in stage 8.0 (TID 243)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 221) in 1244 ms on localhost (12/200)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 223) in 1248 ms on localhost (13/200)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 244, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 28.0 in stage 8.0 (TID 244)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 218) in 1252 ms on localhost (14/200)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 245, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 29.0 in stage 8.0 (TID 245)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 217) in 1255 ms on localhost (15/200)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 246, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 30.0 in stage 8.0 (TID 246)
15/08/09 15:27:16 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 247, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:16 INFO Executor: Running task 31.0 in stage 8.0 (TID 247)
15/08/09 15:27:16 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 230) in 1253 ms on localhost (16/200)
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@38f3450d
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000016_232/part-00016
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d75b8d5
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000017_233/part-00017
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:16 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:16 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:16 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:16 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@277e5943
15/08/09 15:27:16 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000024_240/part-00024
15/08/09 15:27:16 INFO CodecConfig: Compression set to false
15/08/09 15:27:16 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6acf1cb5
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000029_245/part-00029
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66f08b4b
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@163e7a94
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5fe195f4
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000022_238/part-00022
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4787e015
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@533bf759
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4aa569e2
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2988a763
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000021_237/part-00021
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000020_236/part-00020
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31ca9ab7
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000026_242/part-00026
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,856
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 759B for [ps_partkey] INT32: 179 values, 723B raw, 723B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,483B for [part_value] DOUBLE: 179 values, 1,439B raw, 1,439B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@124fc546
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000028_244/part-00028
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@79d1810a
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000031_247/part-00031
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f7b0848
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000023_239/part-00023
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@65cdff1
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000027_243/part-00027
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3b724a7a
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4c77ae13
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4545889d
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c4cd67f
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000030_246/part-00030
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,276
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@68964291
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 643B for [ps_partkey] INT32: 150 values, 607B raw, 607B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,251B for [part_value] DOUBLE: 150 values, 1,207B raw, 1,207B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3be1ff89
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000019_235/part-00019
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,356
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c8c2e54
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6735c478
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,036
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 659B for [ps_partkey] INT32: 154 values, 623B raw, 623B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,283B for [part_value] DOUBLE: 154 values, 1,239B raw, 1,239B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 395B for [ps_partkey] INT32: 88 values, 359B raw, 359B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 755B for [part_value] DOUBLE: 88 values, 711B raw, 711B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,296
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c36348d
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000025_241/part-00025
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7628a997
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@685fea49
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@710e9427
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4c3a6e4
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000018_234/part-00018
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 647B for [ps_partkey] INT32: 151 values, 611B raw, 611B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,259B for [part_value] DOUBLE: 151 values, 1,215B raw, 1,215B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67f8c292
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@58eeb3b9
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000017_233' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000017
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000017_233: Committed
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000024_240' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000024
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000024_240: Committed
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO Executor: Finished task 24.0 in stage 8.0 (TID 240). 781 bytes result sent to driver
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO Executor: Finished task 17.0 in stage 8.0 (TID 233). 781 bytes result sent to driver
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 248, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 32.0 in stage 8.0 (TID 248)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 240) in 518 ms on localhost (17/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 249, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 33.0 in stage 8.0 (TID 249)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000029_245' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000029
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000029_245: Committed
15/08/09 15:27:17 INFO Executor: Finished task 29.0 in stage 8.0 (TID 245). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 233) in 528 ms on localhost (18/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 250, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 34.0 in stage 8.0 (TID 250)
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@768ebd52
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000016_232' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000016
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000016_232: Committed
15/08/09 15:27:17 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 245) in 506 ms on localhost (19/200)
15/08/09 15:27:17 INFO Executor: Finished task 16.0 in stage 8.0 (TID 232). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 251, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 35.0 in stage 8.0 (TID 251)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000020_236' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000020
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000020_236: Committed
15/08/09 15:27:17 INFO Executor: Finished task 20.0 in stage 8.0 (TID 236). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 232) in 545 ms on localhost (20/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 252, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 36.0 in stage 8.0 (TID 252)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 236) in 538 ms on localhost (21/200)
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000031_247' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000031
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000031_247: Committed
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000022_238' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000022
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000022_238: Committed
15/08/09 15:27:17 INFO Executor: Finished task 22.0 in stage 8.0 (TID 238). 781 bytes result sent to driver
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000026_242' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000026
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000026_242: Committed
15/08/09 15:27:17 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 253, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Finished task 31.0 in stage 8.0 (TID 247). 781 bytes result sent to driver
15/08/09 15:27:17 INFO Executor: Running task 37.0 in stage 8.0 (TID 253)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 254, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Finished task 26.0 in stage 8.0 (TID 242). 781 bytes result sent to driver
15/08/09 15:27:17 INFO Executor: Running task 38.0 in stage 8.0 (TID 254)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 238) in 548 ms on localhost (22/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 255, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 247) in 525 ms on localhost (23/200)
15/08/09 15:27:17 INFO Executor: Running task 39.0 in stage 8.0 (TID 255)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 242) in 538 ms on localhost (24/200)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000030_246' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000030
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000030_246: Committed
15/08/09 15:27:17 INFO Executor: Finished task 30.0 in stage 8.0 (TID 246). 781 bytes result sent to driver
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000028_244' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000028
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000028_244: Committed
15/08/09 15:27:17 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 256, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 40.0 in stage 8.0 (TID 256)
15/08/09 15:27:17 INFO Executor: Finished task 28.0 in stage 8.0 (TID 244). 781 bytes result sent to driver
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000019_235' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000019
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000019_235: Committed
15/08/09 15:27:17 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 246) in 536 ms on localhost (25/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 257, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Finished task 19.0 in stage 8.0 (TID 235). 781 bytes result sent to driver
15/08/09 15:27:17 INFO Executor: Running task 41.0 in stage 8.0 (TID 257)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 258, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 42.0 in stage 8.0 (TID 258)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 244) in 542 ms on localhost (26/200)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 235) in 569 ms on localhost (27/200)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000025_241' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000025
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000025_241: Committed
15/08/09 15:27:17 INFO Executor: Finished task 25.0 in stage 8.0 (TID 241). 781 bytes result sent to driver
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000023_239' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000023
15/08/09 15:27:17 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 259, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000023_239: Committed
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000027_243' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000027
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000027_243: Committed
15/08/09 15:27:17 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 241) in 560 ms on localhost (28/200)
15/08/09 15:27:17 INFO Executor: Running task 43.0 in stage 8.0 (TID 259)
15/08/09 15:27:17 INFO Executor: Finished task 23.0 in stage 8.0 (TID 239). 781 bytes result sent to driver
15/08/09 15:27:17 INFO Executor: Finished task 27.0 in stage 8.0 (TID 243). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 260, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 44.0 in stage 8.0 (TID 260)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 239) in 571 ms on localhost (29/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 261, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 45.0 in stage 8.0 (TID 261)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 243) in 560 ms on localhost (30/200)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000018_234' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000018
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000018_234: Committed
15/08/09 15:27:17 INFO Executor: Finished task 18.0 in stage 8.0 (TID 234). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 262, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO Executor: Running task 46.0 in stage 8.0 (TID 262)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 234) in 584 ms on localhost (31/200)
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6976def6
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000034_250/part-00034
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@351ac41c
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,116
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@70412580
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000038_254/part-00038
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 811B for [ps_partkey] INT32: 192 values, 775B raw, 775B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,587B for [part_value] DOUBLE: 192 values, 1,543B raw, 1,543B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@579752cb
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000035_251/part-00035
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@371aaf9b
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000042_258/part-00042
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@38ff0560
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6eb9fedb
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000036_252/part-00036
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000039_255/part-00039
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2a791bf9
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@751b8e37
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6415f8f1
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000032_248/part-00032
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@39c9098f
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000040_256/part-00040
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@543d0e6b
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000043_259/part-00043
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@47abf723
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4afcd2a9
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000046_262/part-00046
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1c6a7bb5
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,676
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 523B for [ps_partkey] INT32: 120 values, 487B raw, 487B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,011B for [part_value] DOUBLE: 120 values, 967B raw, 967B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@18ff9807
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7a337e6c
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@337971d6
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1d5055f8
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5d49d022
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000033_249/part-00033
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000045_261/part-00045
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000021_237' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000021
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000021_237: Committed
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,576
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/09 15:27:17 INFO Executor: Finished task 21.0 in stage 8.0 (TID 237). 781 bytes result sent to driver
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 703B for [ps_partkey] INT32: 165 values, 667B raw, 667B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,371B for [part_value] DOUBLE: 165 values, 1,327B raw, 1,327B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 263, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO Executor: Running task 47.0 in stage 8.0 (TID 263)
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 237) in 748 ms on localhost (32/200)
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d742f8c
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f1654b4
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000034_250' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000034
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000034_250: Committed
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO Executor: Finished task 34.0 in stage 8.0 (TID 250). 781 bytes result sent to driver
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c818ab5
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000044_260/part-00044
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7e27c884
15/08/09 15:27:17 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 264, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 48.0 in stage 8.0 (TID 264)
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 250) in 237 ms on localhost (33/200)
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000037_253/part-00037
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b4dbf1d
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000041_257/part-00041
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@507a7cc0
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b5b0ab
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@705bc2a0
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000038_254' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000038
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000038_254: Committed
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO Executor: Finished task 38.0 in stage 8.0 (TID 254). 781 bytes result sent to driver
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 265, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 49.0 in stage 8.0 (TID 265)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 254) in 253 ms on localhost (34/200)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000032_248' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000032
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@24611a8
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@46312dc2
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000032_248: Committed
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000039_255' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000039
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000036_252' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000036
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000039_255: Committed
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000036_252: Committed
15/08/09 15:27:17 INFO Executor: Finished task 32.0 in stage 8.0 (TID 248). 781 bytes result sent to driver
15/08/09 15:27:17 INFO Executor: Finished task 39.0 in stage 8.0 (TID 255). 781 bytes result sent to driver
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000046_262' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000046
15/08/09 15:27:17 INFO Executor: Finished task 36.0 in stage 8.0 (TID 252). 781 bytes result sent to driver
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000046_262: Committed
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,636
15/08/09 15:27:17 INFO TaskSetManager: Starting task 50.0 in stage 8.0 (TID 266, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,816
15/08/09 15:27:17 INFO Executor: Running task 50.0 in stage 8.0 (TID 266)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 51.0 in stage 8.0 (TID 267, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Finished task 46.0 in stage 8.0 (TID 262). 781 bytes result sent to driver
15/08/09 15:27:17 INFO Executor: Running task 51.0 in stage 8.0 (TID 267)
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 515B for [ps_partkey] INT32: 118 values, 479B raw, 479B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 995B for [part_value] DOUBLE: 118 values, 951B raw, 951B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 255) in 260 ms on localhost (35/200)
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 551B for [ps_partkey] INT32: 127 values, 515B raw, 515B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,067B for [part_value] DOUBLE: 127 values, 1,023B raw, 1,023B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 248) in 291 ms on localhost (36/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 52.0 in stage 8.0 (TID 268, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 52.0 in stage 8.0 (TID 268)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 252) in 277 ms on localhost (37/200)
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO TaskSetManager: Starting task 53.0 in stage 8.0 (TID 269, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 53.0 in stage 8.0 (TID 269)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 262) in 233 ms on localhost (38/200)
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000045_261' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000045
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000045_261: Committed
15/08/09 15:27:17 INFO Executor: Finished task 45.0 in stage 8.0 (TID 261). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 54.0 in stage 8.0 (TID 270, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 54.0 in stage 8.0 (TID 270)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 261) in 247 ms on localhost (39/200)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000037_253' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000037
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000037_253: Committed
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000033_249' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000033
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000033_249: Committed
15/08/09 15:27:17 INFO Executor: Finished task 37.0 in stage 8.0 (TID 253). 781 bytes result sent to driver
15/08/09 15:27:17 INFO Executor: Finished task 33.0 in stage 8.0 (TID 249). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 55.0 in stage 8.0 (TID 271, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 55.0 in stage 8.0 (TID 271)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 253) in 285 ms on localhost (40/200)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000041_257' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000041
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000041_257: Committed
15/08/09 15:27:17 INFO TaskSetManager: Starting task 56.0 in stage 8.0 (TID 272, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 56.0 in stage 8.0 (TID 272)
15/08/09 15:27:17 INFO Executor: Finished task 41.0 in stage 8.0 (TID 257). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 249) in 306 ms on localhost (41/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 57.0 in stage 8.0 (TID 273, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 57.0 in stage 8.0 (TID 273)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 257) in 269 ms on localhost (42/200)
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b5d6833
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000047_263/part-00047
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@41642356
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000048_264/part-00048
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e1b1f0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000049_265/part-00049
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8690487
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000052_268/part-00052
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6fd90d1a
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@12eb5f54
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000053_269/part-00053
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000050_266/part-00050
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@16de87f7
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6bb074db
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@48d3eb60
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@510210fa
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,176
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000051_267/part-00051
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ea8b6a2
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 423B for [ps_partkey] INT32: 95 values, 387B raw, 387B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@166e763b
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 811B for [part_value] DOUBLE: 95 values, 767B raw, 767B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d0c0554
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,636
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 515B for [ps_partkey] INT32: 118 values, 479B raw, 479B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 995B for [part_value] DOUBLE: 118 values, 951B raw, 951B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@696408a6
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000054_270/part-00054
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6485989f
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,696
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 527B for [ps_partkey] INT32: 121 values, 491B raw, 491B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,019B for [part_value] DOUBLE: 121 values, 975B raw, 975B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a0e6b89
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000056_272/part-00056
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000050_266' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000050
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000050_266: Committed
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000049_265' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000049
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000049_265: Committed
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@507b018d
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000057_273/part-00057
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000048_264' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000048
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000048_264: Committed
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO Executor: Finished task 50.0 in stage 8.0 (TID 266). 781 bytes result sent to driver
15/08/09 15:27:17 INFO Executor: Finished task 49.0 in stage 8.0 (TID 265). 781 bytes result sent to driver
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO Executor: Finished task 48.0 in stage 8.0 (TID 264). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 58.0 in stage 8.0 (TID 274, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d313a8b
15/08/09 15:27:17 INFO Executor: Running task 58.0 in stage 8.0 (TID 274)
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000052_268' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000052
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000053_269' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000053
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000052_268: Committed
15/08/09 15:27:17 INFO TaskSetManager: Starting task 59.0 in stage 8.0 (TID 275, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO Executor: Running task 59.0 in stage 8.0 (TID 275)
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000053_269: Committed
15/08/09 15:27:17 INFO Executor: Finished task 52.0 in stage 8.0 (TID 268). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Finished task 50.0 in stage 8.0 (TID 266) in 303 ms on localhost (43/200)
15/08/09 15:27:17 INFO Executor: Finished task 53.0 in stage 8.0 (TID 269). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 60.0 in stage 8.0 (TID 276, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 60.0 in stage 8.0 (TID 276)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 265) in 316 ms on localhost (44/200)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 264) in 355 ms on localhost (45/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 61.0 in stage 8.0 (TID 277, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 61.0 in stage 8.0 (TID 277)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 52.0 in stage 8.0 (TID 268) in 305 ms on localhost (46/200)
15/08/09 15:27:17 INFO TaskSetManager: Starting task 62.0 in stage 8.0 (TID 278, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 62.0 in stage 8.0 (TID 278)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 53.0 in stage 8.0 (TID 269) in 305 ms on localhost (47/200)
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@37a88071
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17889743
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4c68ff2e
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000055_271/part-00055
15/08/09 15:27:17 INFO CodecConfig: Compression set to false
15/08/09 15:27:17 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:17 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000051_267' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000051
15/08/09 15:27:17 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000051_267: Committed
15/08/09 15:27:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:17 INFO Executor: Finished task 51.0 in stage 8.0 (TID 267). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 63.0 in stage 8.0 (TID 279, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 63.0 in stage 8.0 (TID 279)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 51.0 in stage 8.0 (TID 267) in 324 ms on localhost (48/200)
15/08/09 15:27:17 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000054_270' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000054
15/08/09 15:27:17 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000054_270: Committed
15/08/09 15:27:17 INFO Executor: Finished task 54.0 in stage 8.0 (TID 270). 781 bytes result sent to driver
15/08/09 15:27:17 INFO TaskSetManager: Starting task 64.0 in stage 8.0 (TID 280, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:17 INFO Executor: Running task 64.0 in stage 8.0 (TID 280)
15/08/09 15:27:17 INFO TaskSetManager: Finished task 54.0 in stage 8.0 (TID 270) in 319 ms on localhost (49/200)
15/08/09 15:27:17 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7fad8c13
15/08/09 15:27:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,336
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 455B for [ps_partkey] INT32: 103 values, 419B raw, 419B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ColumnChunkPageWriteStore: written 875B for [part_value] DOUBLE: 103 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000056_272' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000056
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000056_272: Committed
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO Executor: Finished task 56.0 in stage 8.0 (TID 272). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/09 15:27:18 INFO TaskSetManager: Starting task 65.0 in stage 8.0 (TID 281, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 65.0 in stage 8.0 (TID 281)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 56.0 in stage 8.0 (TID 272) in 350 ms on localhost (50/200)
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000055_271' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000055
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000042_258' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000042
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000055_271: Committed
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000042_258: Committed
15/08/09 15:27:18 INFO Executor: Finished task 42.0 in stage 8.0 (TID 258). 781 bytes result sent to driver
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000035_251' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000035
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000035_251: Committed
15/08/09 15:27:18 INFO Executor: Finished task 55.0 in stage 8.0 (TID 271). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 66.0 in stage 8.0 (TID 282, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Finished task 35.0 in stage 8.0 (TID 251). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO Executor: Running task 66.0 in stage 8.0 (TID 282)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 258) in 632 ms on localhost (51/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 67.0 in stage 8.0 (TID 283, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 67.0 in stage 8.0 (TID 283)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 55.0 in stage 8.0 (TID 271) in 369 ms on localhost (52/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 68.0 in stage 8.0 (TID 284, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000040_256' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000040
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000040_256: Committed
15/08/09 15:27:18 INFO Executor: Running task 68.0 in stage 8.0 (TID 284)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 251) in 667 ms on localhost (53/200)
15/08/09 15:27:18 INFO Executor: Finished task 40.0 in stage 8.0 (TID 256). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 69.0 in stage 8.0 (TID 285, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 256) in 639 ms on localhost (54/200)
15/08/09 15:27:18 INFO Executor: Running task 69.0 in stage 8.0 (TID 285)
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000043_259' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000043
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000043_259: Committed
15/08/09 15:27:18 INFO Executor: Finished task 43.0 in stage 8.0 (TID 259). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 70.0 in stage 8.0 (TID 286, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 70.0 in stage 8.0 (TID 286)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 259) in 635 ms on localhost (55/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@584245c
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000062_278/part-00062
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@266f16d8
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000063_279/part-00063
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2355ab68
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000061_277/part-00061
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7b111b27
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000058_274/part-00058
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@200ddc90
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000060_276/part-00060
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000044_260' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000044
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000044_260: Committed
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@41956007
15/08/09 15:27:18 INFO Executor: Finished task 44.0 in stage 8.0 (TID 260). 781 bytes result sent to driver
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,056
15/08/09 15:27:18 INFO TaskSetManager: Starting task 71.0 in stage 8.0 (TID 287, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 71.0 in stage 8.0 (TID 287)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ff2b93
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 599B for [ps_partkey] INT32: 139 values, 563B raw, 563B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,163B for [part_value] DOUBLE: 139 values, 1,119B raw, 1,119B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e85c16e
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c8f3375
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000059_275/part-00059
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 260) in 788 ms on localhost (56/200)
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@42958ba1
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6802192d
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,516
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 491B for [ps_partkey] INT32: 112 values, 455B raw, 455B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,336
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 947B for [part_value] DOUBLE: 112 values, 903B raw, 903B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@580b4da4
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4e167724
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000064_280/part-00064
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 855B for [ps_partkey] INT32: 203 values, 819B raw, 819B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,675B for [part_value] DOUBLE: 203 values, 1,631B raw, 1,631B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6589dd14
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000065_281/part-00065
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000063_279' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000063
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000063_279: Committed
15/08/09 15:27:18 INFO Executor: Finished task 63.0 in stage 8.0 (TID 279). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 72.0 in stage 8.0 (TID 288, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 72.0 in stage 8.0 (TID 288)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 63.0 in stage 8.0 (TID 279) in 261 ms on localhost (57/200)
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000062_278' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000062
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000062_278: Committed
15/08/09 15:27:18 INFO Executor: Finished task 62.0 in stage 8.0 (TID 278). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 73.0 in stage 8.0 (TID 289, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 73.0 in stage 8.0 (TID 289)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 62.0 in stage 8.0 (TID 278) in 284 ms on localhost (58/200)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@384ae1b6
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c289099
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000067_283/part-00067
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e406297
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3622f828
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000069_285/part-00069
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000060_276' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000060
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000060_276: Committed
15/08/09 15:27:18 INFO Executor: Finished task 60.0 in stage 8.0 (TID 276). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 74.0 in stage 8.0 (TID 290, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 74.0 in stage 8.0 (TID 290)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 60.0 in stage 8.0 (TID 276) in 299 ms on localhost (59/200)
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000059_275' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000059
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000059_275: Committed
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7c2e291e
15/08/09 15:27:18 INFO Executor: Finished task 59.0 in stage 8.0 (TID 275). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000066_282/part-00066
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO TaskSetManager: Starting task 75.0 in stage 8.0 (TID 291, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO Executor: Running task 75.0 in stage 8.0 (TID 291)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 59.0 in stage 8.0 (TID 275) in 305 ms on localhost (60/200)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@275a743b
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000068_284/part-00068
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@500d7bfe
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6e27ba07
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000070_286/part-00070
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@154d25a8
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72f27d7c
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,816
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f0edd06
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 751B for [ps_partkey] INT32: 177 values, 715B raw, 715B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,467B for [part_value] DOUBLE: 177 values, 1,423B raw, 1,423B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,716
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 531B for [ps_partkey] INT32: 122 values, 495B raw, 495B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,027B for [part_value] DOUBLE: 122 values, 983B raw, 983B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b39405f
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000064_280' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000064
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000064_280: Committed
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/09 15:27:18 INFO Executor: Finished task 64.0 in stage 8.0 (TID 280). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO TaskSetManager: Starting task 76.0 in stage 8.0 (TID 292, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO Executor: Running task 76.0 in stage 8.0 (TID 292)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000065_281' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000065
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000065_281: Committed
15/08/09 15:27:18 INFO TaskSetManager: Finished task 64.0 in stage 8.0 (TID 280) in 304 ms on localhost (61/200)
15/08/09 15:27:18 INFO Executor: Finished task 65.0 in stage 8.0 (TID 281). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO TaskSetManager: Starting task 77.0 in stage 8.0 (TID 293, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 77.0 in stage 8.0 (TID 293)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 65.0 in stage 8.0 (TID 281) in 267 ms on localhost (62/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000067_283' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000067
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000067_283: Committed
15/08/09 15:27:18 INFO Executor: Finished task 67.0 in stage 8.0 (TID 283). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 78.0 in stage 8.0 (TID 294, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 78.0 in stage 8.0 (TID 294)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 67.0 in stage 8.0 (TID 283) in 258 ms on localhost (63/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fa57753
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000071_287/part-00071
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000068_284' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000068
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000068_284: Committed
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000070_286' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000070
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000070_286: Committed
15/08/09 15:27:18 INFO Executor: Finished task 68.0 in stage 8.0 (TID 284). 781 bytes result sent to driver
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000066_282' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000066
15/08/09 15:27:18 INFO Executor: Finished task 70.0 in stage 8.0 (TID 286). 781 bytes result sent to driver
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000066_282: Committed
15/08/09 15:27:18 INFO TaskSetManager: Starting task 79.0 in stage 8.0 (TID 295, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 79.0 in stage 8.0 (TID 295)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO TaskSetManager: Finished task 68.0 in stage 8.0 (TID 284) in 281 ms on localhost (64/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 80.0 in stage 8.0 (TID 296, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Finished task 66.0 in stage 8.0 (TID 282). 781 bytes result sent to driver
15/08/09 15:27:18 INFO Executor: Running task 80.0 in stage 8.0 (TID 296)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 70.0 in stage 8.0 (TID 286) in 274 ms on localhost (65/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 81.0 in stage 8.0 (TID 297, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 81.0 in stage 8.0 (TID 297)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 66.0 in stage 8.0 (TID 282) in 287 ms on localhost (66/200)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@73d784ef
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61d223b2
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000072_288/part-00072
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000047_263' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000047
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000047_263: Committed
15/08/09 15:27:18 INFO Executor: Finished task 47.0 in stage 8.0 (TID 263). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 82.0 in stage 8.0 (TID 298, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 82.0 in stage 8.0 (TID 298)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 263) in 754 ms on localhost (67/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000071_287' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000071
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000071_287: Committed
15/08/09 15:27:18 INFO Executor: Finished task 71.0 in stage 8.0 (TID 287). 781 bytes result sent to driver
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@12f7d10d
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000074_290/part-00074
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO TaskSetManager: Starting task 83.0 in stage 8.0 (TID 299, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO Executor: Running task 83.0 in stage 8.0 (TID 299)
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO TaskSetManager: Finished task 71.0 in stage 8.0 (TID 287) in 158 ms on localhost (68/200)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@777a9064
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000073_289/part-00073
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4267e01e
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6550f88b
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,236
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7aa0129
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 635B for [ps_partkey] INT32: 148 values, 599B raw, 599B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000075_291/part-00075
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,235B for [part_value] DOUBLE: 148 values, 1,191B raw, 1,191B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1150e003
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,016
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 591B for [ps_partkey] INT32: 137 values, 555B raw, 555B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,147B for [part_value] DOUBLE: 137 values, 1,103B raw, 1,103B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@534a678f
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000076_292/part-00076
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@39d50801
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000077_293/part-00077
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@193ca9a0
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5da45451
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000078_294/part-00078
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000072_288' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000072
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000072_288: Committed
15/08/09 15:27:18 INFO Executor: Finished task 72.0 in stage 8.0 (TID 288). 781 bytes result sent to driver
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000057_273' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000057
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000057_273: Committed
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d08fb16
15/08/09 15:27:18 INFO TaskSetManager: Starting task 84.0 in stage 8.0 (TID 300, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/09 15:27:18 INFO Executor: Running task 84.0 in stage 8.0 (TID 300)
15/08/09 15:27:18 INFO Executor: Finished task 57.0 in stage 8.0 (TID 273). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Finished task 72.0 in stage 8.0 (TID 288) in 241 ms on localhost (69/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 85.0 in stage 8.0 (TID 301, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 85.0 in stage 8.0 (TID 301)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO TaskSetManager: Finished task 57.0 in stage 8.0 (TID 273) in 808 ms on localhost (70/200)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@73b65b7c
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000074_290' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000074
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000074_290: Committed
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d0e1f91
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000073_289' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000073
15/08/09 15:27:18 INFO Executor: Finished task 74.0 in stage 8.0 (TID 290). 781 bytes result sent to driver
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,096
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000073_289: Committed
15/08/09 15:27:18 INFO Executor: Finished task 73.0 in stage 8.0 (TID 289). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 86.0 in stage 8.0 (TID 302, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 86.0 in stage 8.0 (TID 302)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 74.0 in stage 8.0 (TID 290) in 239 ms on localhost (71/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 87.0 in stage 8.0 (TID 303, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 807B for [ps_partkey] INT32: 191 values, 771B raw, 771B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO Executor: Running task 87.0 in stage 8.0 (TID 303)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,579B for [part_value] DOUBLE: 191 values, 1,535B raw, 1,535B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO TaskSetManager: Finished task 73.0 in stage 8.0 (TID 289) in 252 ms on localhost (72/200)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2d7db15e
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000080_296/part-00080
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000076_292' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000076
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000076_292: Committed
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a59876f
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000079_295/part-00079
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO Executor: Finished task 76.0 in stage 8.0 (TID 292). 781 bytes result sent to driver
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000077_293' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000077
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000077_293: Committed
15/08/09 15:27:18 INFO TaskSetManager: Starting task 88.0 in stage 8.0 (TID 304, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 88.0 in stage 8.0 (TID 304)
15/08/09 15:27:18 INFO Executor: Finished task 77.0 in stage 8.0 (TID 293). 781 bytes result sent to driver
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000075_291' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000075
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000075_291: Committed
15/08/09 15:27:18 INFO TaskSetManager: Finished task 76.0 in stage 8.0 (TID 292) in 225 ms on localhost (73/200)
15/08/09 15:27:18 INFO Executor: Finished task 75.0 in stage 8.0 (TID 291). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 89.0 in stage 8.0 (TID 305, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 89.0 in stage 8.0 (TID 305)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d29810f
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000081_297/part-00081
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO TaskSetManager: Finished task 77.0 in stage 8.0 (TID 293) in 224 ms on localhost (74/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 90.0 in stage 8.0 (TID 306, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 90.0 in stage 8.0 (TID 306)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO TaskSetManager: Finished task 75.0 in stage 8.0 (TID 291) in 258 ms on localhost (75/200)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@63c3bce2
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7c55f31a
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000078_294' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000078
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@21d6105e
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000078_294: Committed
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000082_298/part-00082
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO Executor: Finished task 78.0 in stage 8.0 (TID 294). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 91.0 in stage 8.0 (TID 307, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 91.0 in stage 8.0 (TID 307)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 78.0 in stage 8.0 (TID 294) in 233 ms on localhost (76/200)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12d59e45
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5828ee6e
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000083_299/part-00083
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000079_295' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000079
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000079_295: Committed
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000080_296' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000080
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000080_296: Committed
15/08/09 15:27:18 INFO Executor: Finished task 79.0 in stage 8.0 (TID 295). 781 bytes result sent to driver
15/08/09 15:27:18 INFO Executor: Finished task 80.0 in stage 8.0 (TID 296). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5d9f09b9
15/08/09 15:27:18 INFO TaskSetManager: Starting task 92.0 in stage 8.0 (TID 308, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 92.0 in stage 8.0 (TID 308)
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/09 15:27:18 INFO TaskSetManager: Finished task 79.0 in stage 8.0 (TID 295) in 229 ms on localhost (77/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 93.0 in stage 8.0 (TID 309, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 93.0 in stage 8.0 (TID 309)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 80.0 in stage 8.0 (TID 296) in 230 ms on localhost (78/200)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51562210
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000081_297' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000081
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000081_297: Committed
15/08/09 15:27:18 INFO Executor: Finished task 81.0 in stage 8.0 (TID 297). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 94.0 in stage 8.0 (TID 310, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 94.0 in stage 8.0 (TID 310)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 81.0 in stage 8.0 (TID 297) in 248 ms on localhost (79/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000082_298' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000082
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000082_298: Committed
15/08/09 15:27:18 INFO Executor: Finished task 82.0 in stage 8.0 (TID 298). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 95.0 in stage 8.0 (TID 311, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 95.0 in stage 8.0 (TID 311)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 82.0 in stage 8.0 (TID 298) in 243 ms on localhost (80/200)
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000083_299' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000083
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000083_299: Committed
15/08/09 15:27:18 INFO Executor: Finished task 83.0 in stage 8.0 (TID 299). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 96.0 in stage 8.0 (TID 312, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 96.0 in stage 8.0 (TID 312)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 83.0 in stage 8.0 (TID 299) in 233 ms on localhost (81/200)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4daa3604
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000084_300/part-00084
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e9b093e
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000087_303/part-00087
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2fc0caf
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000085_301/part-00085
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@68783a68
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1f5cc100
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2fca8dab
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000086_302/part-00086
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@33525927
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,316
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 651B for [ps_partkey] INT32: 152 values, 615B raw, 615B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,267B for [part_value] DOUBLE: 152 values, 1,223B raw, 1,223B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@61a073a5
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000061_277' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000061
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000061_277: Committed
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000085_301' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000085
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000085_301: Committed
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000084_300' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000084
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000084_300: Committed
15/08/09 15:27:18 INFO Executor: Finished task 61.0 in stage 8.0 (TID 277). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 97.0 in stage 8.0 (TID 313, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 97.0 in stage 8.0 (TID 313)
15/08/09 15:27:18 INFO Executor: Finished task 84.0 in stage 8.0 (TID 300). 781 bytes result sent to driver
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000087_303' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000087
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000087_303: Committed
15/08/09 15:27:18 INFO TaskSetManager: Finished task 61.0 in stage 8.0 (TID 277) in 675 ms on localhost (82/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 98.0 in stage 8.0 (TID 314, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Finished task 87.0 in stage 8.0 (TID 303). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Finished task 84.0 in stage 8.0 (TID 300) in 160 ms on localhost (83/200)
15/08/09 15:27:18 INFO Executor: Running task 98.0 in stage 8.0 (TID 314)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 99.0 in stage 8.0 (TID 315, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 99.0 in stage 8.0 (TID 315)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 87.0 in stage 8.0 (TID 303) in 143 ms on localhost (84/200)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@47ee051a
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000089_305/part-00089
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO Executor: Finished task 85.0 in stage 8.0 (TID 301). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 100.0 in stage 8.0 (TID 316, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 100.0 in stage 8.0 (TID 316)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 85.0 in stage 8.0 (TID 301) in 166 ms on localhost (85/200)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7eb42a6f
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000088_304/part-00088
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000058_274' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000058
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000058_274: Committed
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2b6459ef
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000091_307/part-00091
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO Executor: Finished task 58.0 in stage 8.0 (TID 274). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 101.0 in stage 8.0 (TID 317, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 101.0 in stage 8.0 (TID 317)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 58.0 in stage 8.0 (TID 274) in 858 ms on localhost (86/200)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1e9cda26
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a1e55bb
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000090_306/part-00090
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1c203f2b
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,276
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a50503d
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000093_309/part-00093
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 843B for [ps_partkey] INT32: 200 values, 807B raw, 807B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000069_285' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000069
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000069_285: Committed
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,651B for [part_value] DOUBLE: 200 values, 1,607B raw, 1,607B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO Executor: Finished task 69.0 in stage 8.0 (TID 285). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 102.0 in stage 8.0 (TID 318, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 102.0 in stage 8.0 (TID 318)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 69.0 in stage 8.0 (TID 285) in 783 ms on localhost (87/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@900f1db
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@11534466
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000092_308/part-00092
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,656
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 519B for [ps_partkey] INT32: 119 values, 483B raw, 483B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,003B for [part_value] DOUBLE: 119 values, 959B raw, 959B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@555ab203
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,736
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 735B for [ps_partkey] INT32: 173 values, 699B raw, 699B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,435B for [part_value] DOUBLE: 173 values, 1,391B raw, 1,391B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3e21bb52
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7942c94b
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c46970
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000096_312/part-00096
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@dde5c4f
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000095_311/part-00095
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3303bb25
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@39f3944d
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000094_310/part-00094
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000089_305' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000089
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000089_305: Committed
15/08/09 15:27:18 INFO Executor: Finished task 89.0 in stage 8.0 (TID 305). 781 bytes result sent to driver
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,556
15/08/09 15:27:18 INFO TaskSetManager: Starting task 103.0 in stage 8.0 (TID 319, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 103.0 in stage 8.0 (TID 319)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 89.0 in stage 8.0 (TID 305) in 383 ms on localhost (88/200)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 499B for [ps_partkey] INT32: 114 values, 463B raw, 463B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 963B for [part_value] DOUBLE: 114 values, 919B raw, 919B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000091_307' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000091
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000091_307: Committed
15/08/09 15:27:18 INFO Executor: Finished task 91.0 in stage 8.0 (TID 307). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1efed8af
15/08/09 15:27:18 INFO TaskSetManager: Starting task 104.0 in stage 8.0 (TID 320, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 104.0 in stage 8.0 (TID 320)
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/09 15:27:18 INFO TaskSetManager: Finished task 91.0 in stage 8.0 (TID 307) in 375 ms on localhost (89/200)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000090_306' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000090
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000090_306: Committed
15/08/09 15:27:18 INFO Executor: Finished task 90.0 in stage 8.0 (TID 306). 781 bytes result sent to driver
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000093_309' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000093
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000093_309: Committed
15/08/09 15:27:18 INFO TaskSetManager: Starting task 105.0 in stage 8.0 (TID 321, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@78a34226
15/08/09 15:27:18 INFO Executor: Finished task 93.0 in stage 8.0 (TID 309). 781 bytes result sent to driver
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/09 15:27:18 INFO Executor: Running task 105.0 in stage 8.0 (TID 321)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2dea9a99
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000101_317/part-00101
15/08/09 15:27:18 INFO TaskSetManager: Finished task 90.0 in stage 8.0 (TID 306) in 399 ms on localhost (90/200)
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000092_308' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000092
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000092_308: Committed
15/08/09 15:27:18 INFO Executor: Finished task 92.0 in stage 8.0 (TID 308). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 106.0 in stage 8.0 (TID 322, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 106.0 in stage 8.0 (TID 322)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@52043896
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000100_316/part-00100
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO TaskSetManager: Finished task 93.0 in stage 8.0 (TID 309) in 370 ms on localhost (91/200)
15/08/09 15:27:18 INFO TaskSetManager: Starting task 107.0 in stage 8.0 (TID 323, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 107.0 in stage 8.0 (TID 323)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 92.0 in stage 8.0 (TID 308) in 372 ms on localhost (92/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6133293f
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000097_313/part-00097
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5083a184
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7c301bfc
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fc9d6b9
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000098_314/part-00098
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@18871cbe
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,756
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 739B for [ps_partkey] INT32: 174 values, 703B raw, 703B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,443B for [part_value] DOUBLE: 174 values, 1,399B raw, 1,399B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000099_315/part-00099
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000096_312' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000096
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000096_312: Committed
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO Executor: Finished task 96.0 in stage 8.0 (TID 312). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 108.0 in stage 8.0 (TID 324, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 108.0 in stage 8.0 (TID 324)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 96.0 in stage 8.0 (TID 312) in 363 ms on localhost (93/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@854ca44
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000094_310' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000094
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000094_310: Committed
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO Executor: Finished task 94.0 in stage 8.0 (TID 310). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000102_318/part-00102
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000095_311' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000095
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000095_311: Committed
15/08/09 15:27:18 INFO TaskSetManager: Starting task 109.0 in stage 8.0 (TID 325, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 109.0 in stage 8.0 (TID 325)
15/08/09 15:27:18 INFO Executor: Finished task 95.0 in stage 8.0 (TID 311). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Finished task 94.0 in stage 8.0 (TID 310) in 385 ms on localhost (94/200)
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:18 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:18 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:18 INFO TaskSetManager: Starting task 110.0 in stage 8.0 (TID 326, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 110.0 in stage 8.0 (TID 326)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 95.0 in stage 8.0 (TID 311) in 373 ms on localhost (95/200)
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4874f2b9
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,396
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 667B for [ps_partkey] INT32: 156 values, 631B raw, 631B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,299B for [part_value] DOUBLE: 156 values, 1,255B raw, 1,255B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@429c7123
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3ab9c8d3
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,516
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 691B for [ps_partkey] INT32: 162 values, 655B raw, 655B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,347B for [part_value] DOUBLE: 162 values, 1,303B raw, 1,303B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@68ee722b
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000100_316' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000100
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000100_316: Committed
15/08/09 15:27:18 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/09 15:27:18 INFO Executor: Finished task 100.0 in stage 8.0 (TID 316). 781 bytes result sent to driver
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:18 INFO TaskSetManager: Starting task 111.0 in stage 8.0 (TID 327, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:18 INFO Executor: Running task 111.0 in stage 8.0 (TID 327)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 100.0 in stage 8.0 (TID 316) in 341 ms on localhost (96/200)
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000098_314' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000098
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000098_314: Committed
15/08/09 15:27:18 INFO Executor: Finished task 98.0 in stage 8.0 (TID 314). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 112.0 in stage 8.0 (TID 328, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 112.0 in stage 8.0 (TID 328)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 98.0 in stage 8.0 (TID 314) in 364 ms on localhost (97/200)
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000099_315' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000099
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000099_315: Committed
15/08/09 15:27:18 INFO Executor: Finished task 99.0 in stage 8.0 (TID 315). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 113.0 in stage 8.0 (TID 329, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000097_313' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000097
15/08/09 15:27:18 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000097_313: Committed
15/08/09 15:27:18 INFO Executor: Running task 113.0 in stage 8.0 (TID 329)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 99.0 in stage 8.0 (TID 315) in 366 ms on localhost (98/200)
15/08/09 15:27:18 INFO Executor: Finished task 97.0 in stage 8.0 (TID 313). 781 bytes result sent to driver
15/08/09 15:27:18 INFO TaskSetManager: Starting task 114.0 in stage 8.0 (TID 330, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:18 INFO Executor: Running task 114.0 in stage 8.0 (TID 330)
15/08/09 15:27:18 INFO TaskSetManager: Finished task 97.0 in stage 8.0 (TID 313) in 371 ms on localhost (99/200)
15/08/09 15:27:18 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@128e337c
15/08/09 15:27:18 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000103_319/part-00103
15/08/09 15:27:18 INFO CodecConfig: Compression set to false
15/08/09 15:27:18 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:18 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000102_318' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000102
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000102_318: Committed
15/08/09 15:27:19 INFO Executor: Finished task 102.0 in stage 8.0 (TID 318). 781 bytes result sent to driver
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO TaskSetManager: Starting task 115.0 in stage 8.0 (TID 331, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO TaskSetManager: Finished task 102.0 in stage 8.0 (TID 318) in 296 ms on localhost (100/200)
15/08/09 15:27:19 INFO Executor: Running task 115.0 in stage 8.0 (TID 331)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@434ead9
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000106_322/part-00106
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17561093
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@33c79236
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000104_320/part-00104
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000086_302' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000086
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000086_302: Committed
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO Executor: Finished task 86.0 in stage 8.0 (TID 302). 781 bytes result sent to driver
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@264db59d
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000105_321/part-00105
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO TaskSetManager: Starting task 116.0 in stage 8.0 (TID 332, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO Executor: Running task 116.0 in stage 8.0 (TID 332)
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO TaskSetManager: Finished task 86.0 in stage 8.0 (TID 302) in 638 ms on localhost (101/200)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@744f581b
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5219045
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000107_323/part-00107
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4608fd6a
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,796
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 747B for [ps_partkey] INT32: 176 values, 711B raw, 711B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,459B for [part_value] DOUBLE: 176 values, 1,415B raw, 1,415B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2216764d
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,756
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 539B for [ps_partkey] INT32: 124 values, 503B raw, 503B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,043B for [part_value] DOUBLE: 124 values, 999B raw, 999B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@8449381
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,616
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 711B for [ps_partkey] INT32: 167 values, 675B raw, 675B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,387B for [part_value] DOUBLE: 167 values, 1,343B raw, 1,343B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000103_319' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000103
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000103_319: Committed
15/08/09 15:27:19 INFO Executor: Finished task 103.0 in stage 8.0 (TID 319). 781 bytes result sent to driver
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000106_322' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000106
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000106_322: Committed
15/08/09 15:27:19 INFO TaskSetManager: Starting task 117.0 in stage 8.0 (TID 333, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 117.0 in stage 8.0 (TID 333)
15/08/09 15:27:19 INFO Executor: Finished task 106.0 in stage 8.0 (TID 322). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Finished task 103.0 in stage 8.0 (TID 319) in 267 ms on localhost (102/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 118.0 in stage 8.0 (TID 334, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 118.0 in stage 8.0 (TID 334)
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO TaskSetManager: Finished task 106.0 in stage 8.0 (TID 322) in 244 ms on localhost (103/200)
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000104_320' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000104
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000104_320: Committed
15/08/09 15:27:19 INFO Executor: Finished task 104.0 in stage 8.0 (TID 320). 781 bytes result sent to driver
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@96c8057
15/08/09 15:27:19 INFO TaskSetManager: Starting task 119.0 in stage 8.0 (TID 335, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000105_321' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000105
15/08/09 15:27:19 INFO Executor: Running task 119.0 in stage 8.0 (TID 335)
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000105_321: Committed
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000109_325/part-00109
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO TaskSetManager: Finished task 104.0 in stage 8.0 (TID 320) in 267 ms on localhost (104/200)
15/08/09 15:27:19 INFO Executor: Finished task 105.0 in stage 8.0 (TID 321). 781 bytes result sent to driver
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000107_323' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000107
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000107_323: Committed
15/08/09 15:27:19 INFO TaskSetManager: Starting task 120.0 in stage 8.0 (TID 336, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 120.0 in stage 8.0 (TID 336)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2812f2e
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000108_324/part-00108
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO Executor: Finished task 107.0 in stage 8.0 (TID 323). 781 bytes result sent to driver
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@452f7cdd
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000110_326/part-00110
15/08/09 15:27:19 INFO TaskSetManager: Finished task 105.0 in stage 8.0 (TID 321) in 258 ms on localhost (105/200)
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO TaskSetManager: Starting task 121.0 in stage 8.0 (TID 337, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 121.0 in stage 8.0 (TID 337)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 107.0 in stage 8.0 (TID 323) in 253 ms on localhost (106/200)
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@27ae061e
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@268e84c1
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@648d0223
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1bcbbc70
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000111_327/part-00111
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5cc06b51
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000113_329/part-00113
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000108_324' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000108
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000108_324: Committed
15/08/09 15:27:19 INFO Executor: Finished task 108.0 in stage 8.0 (TID 324). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 122.0 in stage 8.0 (TID 338, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 122.0 in stage 8.0 (TID 338)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@28797b5a
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000112_328/part-00112
15/08/09 15:27:19 INFO TaskSetManager: Finished task 108.0 in stage 8.0 (TID 324) in 270 ms on localhost (107/200)
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000109_325' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000109
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000109_325: Committed
15/08/09 15:27:19 INFO Executor: Finished task 109.0 in stage 8.0 (TID 325). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 123.0 in stage 8.0 (TID 339, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 123.0 in stage 8.0 (TID 339)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 109.0 in stage 8.0 (TID 325) in 267 ms on localhost (108/200)
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1dd7b905
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3db40691
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,476
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000110_326' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000110
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000110_326: Committed
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 683B for [ps_partkey] INT32: 160 values, 647B raw, 647B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,331B for [part_value] DOUBLE: 160 values, 1,287B raw, 1,287B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO Executor: Finished task 110.0 in stage 8.0 (TID 326). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 124.0 in stage 8.0 (TID 340, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 124.0 in stage 8.0 (TID 340)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 110.0 in stage 8.0 (TID 326) in 271 ms on localhost (109/200)
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@b61b626
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6801c7e3
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4cea3d6d
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000114_330/part-00114
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000116_332/part-00116
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000111_327' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000111
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000111_327: Committed
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO Executor: Finished task 111.0 in stage 8.0 (TID 327). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 125.0 in stage 8.0 (TID 341, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 125.0 in stage 8.0 (TID 341)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 111.0 in stage 8.0 (TID 327) in 265 ms on localhost (110/200)
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@179fb4d
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000115_331/part-00115
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15e93322
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a665b22
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000117_333/part-00117
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000112_328' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000112
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000112_328: Committed
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1aaf678c
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000118_334/part-00118
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO Executor: Finished task 112.0 in stage 8.0 (TID 328). 781 bytes result sent to driver
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO TaskSetManager: Starting task 126.0 in stage 8.0 (TID 342, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO Executor: Running task 126.0 in stage 8.0 (TID 342)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 112.0 in stage 8.0 (TID 328) in 265 ms on localhost (111/200)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@58995e29
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000121_337/part-00121
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3cf50330
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000120_336/part-00120
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@393e0393
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,396
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 867B for [ps_partkey] INT32: 206 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,699B for [part_value] DOUBLE: 206 values, 1,655B raw, 1,655B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2700ceae
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000119_335/part-00119
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000088_304' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000088
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000088_304: Committed
15/08/09 15:27:19 INFO Executor: Finished task 88.0 in stage 8.0 (TID 304). 781 bytes result sent to driver
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@168c8560
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,836
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 555B for [ps_partkey] INT32: 128 values, 519B raw, 519B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,075B for [part_value] DOUBLE: 128 values, 1,031B raw, 1,031B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@20ed26ac
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7cd67450
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@636e7037
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67895de0
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,116
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,336
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/09 15:27:19 INFO TaskSetManager: Starting task 127.0 in stage 8.0 (TID 343, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 127.0 in stage 8.0 (TID 343)
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 655B for [ps_partkey] INT32: 153 values, 619B raw, 619B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,275B for [part_value] DOUBLE: 153 values, 1,231B raw, 1,231B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 811B for [ps_partkey] INT32: 192 values, 775B raw, 775B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,587B for [part_value] DOUBLE: 192 values, 1,543B raw, 1,543B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO TaskSetManager: Finished task 88.0 in stage 8.0 (TID 304) in 937 ms on localhost (112/200)
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@74e5fca4
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,056
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,696
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 799B for [ps_partkey] INT32: 189 values, 763B raw, 763B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 727B for [ps_partkey] INT32: 171 values, 691B raw, 691B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,563B for [part_value] DOUBLE: 189 values, 1,519B raw, 1,519B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,419B for [part_value] DOUBLE: 171 values, 1,375B raw, 1,375B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000116_332' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000116
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000116_332: Committed
15/08/09 15:27:19 INFO Executor: Finished task 116.0 in stage 8.0 (TID 332). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 128.0 in stage 8.0 (TID 344, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 116.0 in stage 8.0 (TID 332) in 332 ms on localhost (113/200)
15/08/09 15:27:19 INFO Executor: Running task 128.0 in stage 8.0 (TID 344)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000101_317' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000101
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000101_317: Committed
15/08/09 15:27:19 INFO Executor: Finished task 101.0 in stage 8.0 (TID 317). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 129.0 in stage 8.0 (TID 345, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000114_330' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000114
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000114_330: Committed
15/08/09 15:27:19 INFO TaskSetManager: Finished task 101.0 in stage 8.0 (TID 317) in 659 ms on localhost (114/200)
15/08/09 15:27:19 INFO Executor: Running task 129.0 in stage 8.0 (TID 345)
15/08/09 15:27:19 INFO Executor: Finished task 114.0 in stage 8.0 (TID 330). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 130.0 in stage 8.0 (TID 346, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 130.0 in stage 8.0 (TID 346)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 114.0 in stage 8.0 (TID 330) in 463 ms on localhost (115/200)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000115_331' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000115
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000115_331: Committed
15/08/09 15:27:19 INFO Executor: Finished task 115.0 in stage 8.0 (TID 331). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 131.0 in stage 8.0 (TID 347, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 131.0 in stage 8.0 (TID 347)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 115.0 in stage 8.0 (TID 331) in 356 ms on localhost (116/200)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000121_337' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000121
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000118_334' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000118
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000120_336' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000120
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000118_334: Committed
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000120_336: Committed
15/08/09 15:27:19 INFO Executor: Finished task 120.0 in stage 8.0 (TID 336). 781 bytes result sent to driver
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000121_337: Committed
15/08/09 15:27:19 INFO Executor: Finished task 118.0 in stage 8.0 (TID 334). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 132.0 in stage 8.0 (TID 348, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 132.0 in stage 8.0 (TID 348)
15/08/09 15:27:19 INFO Executor: Finished task 121.0 in stage 8.0 (TID 337). 781 bytes result sent to driver
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO TaskSetManager: Finished task 120.0 in stage 8.0 (TID 336) in 306 ms on localhost (117/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 133.0 in stage 8.0 (TID 349, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 133.0 in stage 8.0 (TID 349)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000117_333' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000117
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000117_333: Committed
15/08/09 15:27:19 INFO TaskSetManager: Finished task 118.0 in stage 8.0 (TID 334) in 316 ms on localhost (118/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 134.0 in stage 8.0 (TID 350, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 121.0 in stage 8.0 (TID 337) in 307 ms on localhost (119/200)
15/08/09 15:27:19 INFO Executor: Running task 134.0 in stage 8.0 (TID 350)
15/08/09 15:27:19 INFO Executor: Finished task 117.0 in stage 8.0 (TID 333). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 135.0 in stage 8.0 (TID 351, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 135.0 in stage 8.0 (TID 351)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 117.0 in stage 8.0 (TID 333) in 322 ms on localhost (120/200)
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d693553
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000122_338/part-00122
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1146c693
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000124_340/part-00124
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d96e5c1
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000123_339/part-00123
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6844b248
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,776
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 743B for [ps_partkey] INT32: 175 values, 707B raw, 707B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,451B for [part_value] DOUBLE: 175 values, 1,407B raw, 1,407B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@605b3ba5
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000125_341/part-00125
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a8ff0f
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,876
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 763B for [ps_partkey] INT32: 180 values, 727B raw, 727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,491B for [part_value] DOUBLE: 180 values, 1,447B raw, 1,447B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17e2218b
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,236
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 635B for [ps_partkey] INT32: 148 values, 599B raw, 599B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,235B for [part_value] DOUBLE: 148 values, 1,191B raw, 1,191B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000122_338' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000122
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72396c01
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000122_338: Committed
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,736
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000123_339' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000123
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000123_339: Committed
15/08/09 15:27:19 INFO Executor: Finished task 122.0 in stage 8.0 (TID 338). 781 bytes result sent to driver
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 535B for [ps_partkey] INT32: 123 values, 499B raw, 499B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,035B for [part_value] DOUBLE: 123 values, 991B raw, 991B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO TaskSetManager: Starting task 136.0 in stage 8.0 (TID 352, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Finished task 123.0 in stage 8.0 (TID 339). 781 bytes result sent to driver
15/08/09 15:27:19 INFO Executor: Running task 136.0 in stage 8.0 (TID 352)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 122.0 in stage 8.0 (TID 338) in 320 ms on localhost (121/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 137.0 in stage 8.0 (TID 353, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 123.0 in stage 8.0 (TID 339) in 318 ms on localhost (122/200)
15/08/09 15:27:19 INFO Executor: Running task 137.0 in stage 8.0 (TID 353)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@66d36fff
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000126_342/part-00126
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62338dca
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000128_344/part-00128
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000125_341' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000125
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000125_341: Committed
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000124_340' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000124
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000124_340: Committed
15/08/09 15:27:19 INFO Executor: Finished task 125.0 in stage 8.0 (TID 341). 781 bytes result sent to driver
15/08/09 15:27:19 INFO Executor: Finished task 124.0 in stage 8.0 (TID 340). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 138.0 in stage 8.0 (TID 354, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 138.0 in stage 8.0 (TID 354)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4348d378
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000127_343/part-00127
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO TaskSetManager: Finished task 125.0 in stage 8.0 (TID 341) in 325 ms on localhost (123/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 139.0 in stage 8.0 (TID 355, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 139.0 in stage 8.0 (TID 355)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 124.0 in stage 8.0 (TID 340) in 356 ms on localhost (124/200)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d0dcfad
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1fce6465
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000129_345/part-00129
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,656
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@700833ca
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 719B for [ps_partkey] INT32: 169 values, 683B raw, 683B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,403B for [part_value] DOUBLE: 169 values, 1,359B raw, 1,359B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@247fff9a
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000134_350/part-00134
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1668383
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@71440e2c
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2afea4cc
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000131_347/part-00131
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1429c13c
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000133_349/part-00133
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@414d547b
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,056
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 599B for [ps_partkey] INT32: 139 values, 563B raw, 563B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,163B for [part_value] DOUBLE: 139 values, 1,119B raw, 1,119B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16b05283
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000130_346/part-00130
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7b2d5edf
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000132_348/part-00132
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@bd2aa5e
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000135_351/part-00135
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@280dcd70
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000128_344' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000128
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000128_344: Committed
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000126_342' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000126
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000126_342: Committed
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2f742cbf
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/09 15:27:19 INFO Executor: Finished task 126.0 in stage 8.0 (TID 342). 781 bytes result sent to driver
15/08/09 15:27:19 INFO Executor: Finished task 128.0 in stage 8.0 (TID 344). 781 bytes result sent to driver
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@365b417e
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2128d313
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/09 15:27:19 INFO TaskSetManager: Starting task 140.0 in stage 8.0 (TID 356, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 140.0 in stage 8.0 (TID 356)
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO TaskSetManager: Finished task 126.0 in stage 8.0 (TID 342) in 552 ms on localhost (125/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 141.0 in stage 8.0 (TID 357, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 141.0 in stage 8.0 (TID 357)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 128.0 in stage 8.0 (TID 344) in 357 ms on localhost (126/200)
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@642819e0
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000127_343' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000127
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000127_343: Committed
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO Executor: Finished task 127.0 in stage 8.0 (TID 343). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 142.0 in stage 8.0 (TID 358, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 142.0 in stage 8.0 (TID 358)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000113_329' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000113
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000113_329: Committed
15/08/09 15:27:19 INFO TaskSetManager: Finished task 127.0 in stage 8.0 (TID 343) in 380 ms on localhost (127/200)
15/08/09 15:27:19 INFO Executor: Finished task 113.0 in stage 8.0 (TID 329). 781 bytes result sent to driver
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000134_350' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000134
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000134_350: Committed
15/08/09 15:27:19 INFO TaskSetManager: Starting task 143.0 in stage 8.0 (TID 359, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 143.0 in stage 8.0 (TID 359)
15/08/09 15:27:19 INFO Executor: Finished task 134.0 in stage 8.0 (TID 350). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Finished task 113.0 in stage 8.0 (TID 329) in 824 ms on localhost (128/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 144.0 in stage 8.0 (TID 360, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 144.0 in stage 8.0 (TID 360)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 134.0 in stage 8.0 (TID 350) in 350 ms on localhost (129/200)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000131_347' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000131
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000131_347: Committed
15/08/09 15:27:19 INFO Executor: Finished task 131.0 in stage 8.0 (TID 347). 781 bytes result sent to driver
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ccaa0b7
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000136_352/part-00136
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO TaskSetManager: Finished task 131.0 in stage 8.0 (TID 347) in 359 ms on localhost (130/200)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@d1edb7d
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000137_353/part-00137
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO TaskSetManager: Starting task 145.0 in stage 8.0 (TID 361, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO Executor: Running task 145.0 in stage 8.0 (TID 361)
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000133_349' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000133
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000133_349: Committed
15/08/09 15:27:19 INFO Executor: Finished task 133.0 in stage 8.0 (TID 349). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 146.0 in stage 8.0 (TID 362, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 146.0 in stage 8.0 (TID 362)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 133.0 in stage 8.0 (TID 349) in 361 ms on localhost (131/200)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000132_348' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000132
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000132_348: Committed
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7184bc95
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000138_354/part-00138
15/08/09 15:27:19 INFO Executor: Finished task 132.0 in stage 8.0 (TID 348). 781 bytes result sent to driver
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO TaskSetManager: Starting task 147.0 in stage 8.0 (TID 363, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 147.0 in stage 8.0 (TID 363)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 132.0 in stage 8.0 (TID 348) in 372 ms on localhost (132/200)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000135_351' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000135
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000135_351: Committed
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@659b788c
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,736
15/08/09 15:27:19 INFO Executor: Finished task 135.0 in stage 8.0 (TID 351). 781 bytes result sent to driver
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 535B for [ps_partkey] INT32: 123 values, 499B raw, 499B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000130_346' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000130
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000130_346: Committed
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,035B for [part_value] DOUBLE: 123 values, 991B raw, 991B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO TaskSetManager: Starting task 148.0 in stage 8.0 (TID 364, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 148.0 in stage 8.0 (TID 364)
15/08/09 15:27:19 INFO Executor: Finished task 130.0 in stage 8.0 (TID 346). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Finished task 135.0 in stage 8.0 (TID 351) in 373 ms on localhost (133/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 149.0 in stage 8.0 (TID 365, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 149.0 in stage 8.0 (TID 365)
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@393a05e8
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/09 15:27:19 INFO TaskSetManager: Finished task 130.0 in stage 8.0 (TID 346) in 388 ms on localhost (134/200)
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3633d70a
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000139_355/part-00139
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2bb8c1ed
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,536
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 495B for [ps_partkey] INT32: 113 values, 459B raw, 459B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 955B for [part_value] DOUBLE: 113 values, 911B raw, 911B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000137_353' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000137
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000137_353: Committed
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000119_335' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000119
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000119_335: Committed
15/08/09 15:27:19 INFO Executor: Finished task 137.0 in stage 8.0 (TID 353). 781 bytes result sent to driver
15/08/09 15:27:19 INFO Executor: Finished task 119.0 in stage 8.0 (TID 335). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 150.0 in stage 8.0 (TID 366, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 150.0 in stage 8.0 (TID 366)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 137.0 in stage 8.0 (TID 353) in 350 ms on localhost (135/200)
15/08/09 15:27:19 INFO TaskSetManager: Starting task 151.0 in stage 8.0 (TID 367, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 151.0 in stage 8.0 (TID 367)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 119.0 in stage 8.0 (TID 335) in 721 ms on localhost (136/200)
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000136_352' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000136
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000136_352: Committed
15/08/09 15:27:19 INFO Executor: Finished task 136.0 in stage 8.0 (TID 352). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 152.0 in stage 8.0 (TID 368, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 136.0 in stage 8.0 (TID 352) in 358 ms on localhost (137/200)
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO Executor: Running task 152.0 in stage 8.0 (TID 368)
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e9f3910
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,096
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 607B for [ps_partkey] INT32: 141 values, 571B raw, 571B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,179B for [part_value] DOUBLE: 141 values, 1,135B raw, 1,135B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000138_354' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000138
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000138_354: Committed
15/08/09 15:27:19 INFO Executor: Finished task 138.0 in stage 8.0 (TID 354). 781 bytes result sent to driver
15/08/09 15:27:19 INFO TaskSetManager: Starting task 153.0 in stage 8.0 (TID 369, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 153.0 in stage 8.0 (TID 369)
15/08/09 15:27:19 INFO TaskSetManager: Finished task 138.0 in stage 8.0 (TID 354) in 348 ms on localhost (138/200)
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17514268
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000143_359/part-00143
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@628c42cc
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000140_356/part-00140
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a43e049
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000144_360/part-00144
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000139_355' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000139
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000139_355: Committed
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO Executor: Finished task 139.0 in stage 8.0 (TID 355). 781 bytes result sent to driver
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a255d65
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000141_357/part-00141
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d35d6b
15/08/09 15:27:19 INFO TaskSetManager: Starting task 154.0 in stage 8.0 (TID 370, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000142_358/part-00142
15/08/09 15:27:19 INFO Executor: Running task 154.0 in stage 8.0 (TID 370)
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d9d6ba6
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO TaskSetManager: Finished task 139.0 in stage 8.0 (TID 355) in 373 ms on localhost (139/200)
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3a0676af
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,336
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,896
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 455B for [ps_partkey] INT32: 103 values, 419B raw, 419B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 767B for [ps_partkey] INT32: 181 values, 731B raw, 731B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@796f0839
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 875B for [part_value] DOUBLE: 103 values, 831B raw, 831B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,499B for [part_value] DOUBLE: 181 values, 1,455B raw, 1,455B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@32e9c040
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000145_361/part-00145
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@48e83b6b
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000146_362/part-00146
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3afa1dcc
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000147_363/part-00147
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f52951b
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000148_364/part-00148
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17aa240f
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,656
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@580d34f6
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 719B for [ps_partkey] INT32: 169 values, 683B raw, 683B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,496
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,403B for [part_value] DOUBLE: 169 values, 1,359B raw, 1,359B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b102fa3
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000149_365/part-00149
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@66f6bf66
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 487B for [ps_partkey] INT32: 111 values, 451B raw, 451B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 939B for [part_value] DOUBLE: 111 values, 895B raw, 895B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6f7794d2
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@13cba02a
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64321b93
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:19 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@509c18b9
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000150_366/part-00150
15/08/09 15:27:19 INFO CodecConfig: Compression set to false
15/08/09 15:27:19 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:19 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:19 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:19 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:19 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000140_356' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000140
15/08/09 15:27:19 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000140_356: Committed
15/08/09 15:27:19 INFO Executor: Finished task 140.0 in stage 8.0 (TID 356). 781 bytes result sent to driver
15/08/09 15:27:19 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a7a5a31
15/08/09 15:27:19 INFO TaskSetManager: Starting task 155.0 in stage 8.0 (TID 371, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:19 INFO Executor: Running task 155.0 in stage 8.0 (TID 371)
15/08/09 15:27:19 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,436
15/08/09 15:27:19 INFO TaskSetManager: Finished task 140.0 in stage 8.0 (TID 356) in 193 ms on localhost (140/200)
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 475B for [ps_partkey] INT32: 108 values, 439B raw, 439B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 915B for [part_value] DOUBLE: 108 values, 871B raw, 871B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000143_359' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000143
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000143_359: Committed
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000144_360' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000144
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000144_360: Committed
15/08/09 15:27:20 INFO Executor: Finished task 144.0 in stage 8.0 (TID 360). 781 bytes result sent to driver
15/08/09 15:27:20 INFO Executor: Finished task 143.0 in stage 8.0 (TID 359). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 156.0 in stage 8.0 (TID 372, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 156.0 in stage 8.0 (TID 372)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 144.0 in stage 8.0 (TID 360) in 189 ms on localhost (141/200)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 157.0 in stage 8.0 (TID 373, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 157.0 in stage 8.0 (TID 373)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 143.0 in stage 8.0 (TID 359) in 191 ms on localhost (142/200)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000141_357' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000141
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000141_357: Committed
15/08/09 15:27:20 INFO Executor: Finished task 141.0 in stage 8.0 (TID 357). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1e14528d
15/08/09 15:27:20 INFO TaskSetManager: Starting task 158.0 in stage 8.0 (TID 374, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,536
15/08/09 15:27:20 INFO Executor: Running task 158.0 in stage 8.0 (TID 374)
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 695B for [ps_partkey] INT32: 163 values, 659B raw, 659B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,355B for [part_value] DOUBLE: 163 values, 1,311B raw, 1,311B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO TaskSetManager: Finished task 141.0 in stage 8.0 (TID 357) in 374 ms on localhost (143/200)
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1252641c
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000142_358' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000142
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000142_358: Committed
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000146_362' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000146
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000146_362: Committed
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000145_361' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000145
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000145_361: Committed
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000152_368/part-00152
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO Executor: Finished task 142.0 in stage 8.0 (TID 358). 781 bytes result sent to driver
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO Executor: Finished task 146.0 in stage 8.0 (TID 362). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000148_364' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000148
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000148_364: Committed
15/08/09 15:27:20 INFO TaskSetManager: Starting task 159.0 in stage 8.0 (TID 375, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Finished task 148.0 in stage 8.0 (TID 364). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO Executor: Running task 159.0 in stage 8.0 (TID 375)
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7bd6ae73
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000153_369/part-00153
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO Executor: Finished task 145.0 in stage 8.0 (TID 361). 781 bytes result sent to driver
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6ee095d4
15/08/09 15:27:20 INFO TaskSetManager: Finished task 142.0 in stage 8.0 (TID 358) in 375 ms on localhost (144/200)
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000151_367/part-00151
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO TaskSetManager: Starting task 160.0 in stage 8.0 (TID 376, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 160.0 in stage 8.0 (TID 376)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 161.0 in stage 8.0 (TID 377, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000147_363' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000147
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000147_363: Committed
15/08/09 15:27:20 INFO Executor: Running task 161.0 in stage 8.0 (TID 377)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 162.0 in stage 8.0 (TID 378, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 162.0 in stage 8.0 (TID 378)
15/08/09 15:27:20 INFO Executor: Finished task 147.0 in stage 8.0 (TID 363). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Finished task 146.0 in stage 8.0 (TID 362) in 368 ms on localhost (145/200)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 145.0 in stage 8.0 (TID 361) in 373 ms on localhost (146/200)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 148.0 in stage 8.0 (TID 364) in 354 ms on localhost (147/200)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO TaskSetManager: Starting task 163.0 in stage 8.0 (TID 379, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 163.0 in stage 8.0 (TID 379)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 147.0 in stage 8.0 (TID 363) in 362 ms on localhost (148/200)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7142c516
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,076
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 803B for [ps_partkey] INT32: 190 values, 767B raw, 767B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,571B for [part_value] DOUBLE: 190 values, 1,527B raw, 1,527B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@29d124f0
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,536
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000149_365' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000149
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000149_365: Committed
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 495B for [ps_partkey] INT32: 113 values, 459B raw, 459B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO Executor: Finished task 149.0 in stage 8.0 (TID 365). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@189481d1
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 955B for [part_value] DOUBLE: 113 values, 911B raw, 911B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,816
15/08/09 15:27:20 INFO TaskSetManager: Starting task 164.0 in stage 8.0 (TID 380, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 164.0 in stage 8.0 (TID 380)
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 551B for [ps_partkey] INT32: 127 values, 515B raw, 515B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO TaskSetManager: Finished task 149.0 in stage 8.0 (TID 365) in 381 ms on localhost (149/200)
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,067B for [part_value] DOUBLE: 127 values, 1,023B raw, 1,023B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000129_345' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000129
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000129_345: Committed
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO Executor: Finished task 129.0 in stage 8.0 (TID 345). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 165.0 in stage 8.0 (TID 381, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 165.0 in stage 8.0 (TID 381)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 129.0 in stage 8.0 (TID 345) in 776 ms on localhost (150/200)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000150_366' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000150
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000150_366: Committed
15/08/09 15:27:20 INFO Executor: Finished task 150.0 in stage 8.0 (TID 366). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 166.0 in stage 8.0 (TID 382, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 166.0 in stage 8.0 (TID 382)
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@436b8777
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000154_370/part-00154
15/08/09 15:27:20 INFO TaskSetManager: Finished task 150.0 in stage 8.0 (TID 366) in 360 ms on localhost (151/200)
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000151_367' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000151
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000151_367: Committed
15/08/09 15:27:20 INFO Executor: Finished task 151.0 in stage 8.0 (TID 367). 781 bytes result sent to driver
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000152_368' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000152
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000152_368: Committed
15/08/09 15:27:20 INFO TaskSetManager: Starting task 167.0 in stage 8.0 (TID 383, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 167.0 in stage 8.0 (TID 383)
15/08/09 15:27:20 INFO Executor: Finished task 152.0 in stage 8.0 (TID 368). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Finished task 151.0 in stage 8.0 (TID 367) in 368 ms on localhost (152/200)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 168.0 in stage 8.0 (TID 384, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 168.0 in stage 8.0 (TID 384)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@752afb22
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,056
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 799B for [ps_partkey] INT32: 189 values, 763B raw, 763B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,563B for [part_value] DOUBLE: 189 values, 1,519B raw, 1,519B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO TaskSetManager: Finished task 152.0 in stage 8.0 (TID 368) in 366 ms on localhost (153/200)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000153_369' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000153
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000153_369: Committed
15/08/09 15:27:20 INFO Executor: Finished task 153.0 in stage 8.0 (TID 369). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO TaskSetManager: Starting task 169.0 in stage 8.0 (TID 385, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 169.0 in stage 8.0 (TID 385)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 153.0 in stage 8.0 (TID 369) in 346 ms on localhost (154/200)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000154_370' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000154
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000154_370: Committed
15/08/09 15:27:20 INFO Executor: Finished task 154.0 in stage 8.0 (TID 370). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 170.0 in stage 8.0 (TID 386, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 170.0 in stage 8.0 (TID 386)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 154.0 in stage 8.0 (TID 370) in 345 ms on localhost (155/200)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@67effa1c
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000158_374/part-00158
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61a1ec
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000157_373/part-00157
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@15edbed1
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,636
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 715B for [ps_partkey] INT32: 168 values, 679B raw, 679B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,395B for [part_value] DOUBLE: 168 values, 1,351B raw, 1,351B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@bc6d906
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000155_371/part-00155
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2bd21ef4
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000156_372/part-00156
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@18e070c
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000159_375/part-00159
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1370a5a0
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,456
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 679B for [ps_partkey] INT32: 159 values, 643B raw, 643B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,323B for [part_value] DOUBLE: 159 values, 1,279B raw, 1,279B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@233f70e0
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000161_377/part-00161
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4b5f56d4
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,716
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 731B for [ps_partkey] INT32: 172 values, 695B raw, 695B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,427B for [part_value] DOUBLE: 172 values, 1,383B raw, 1,383B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b4ba52d
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000158_374' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000158
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000158_374: Committed
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO Executor: Finished task 158.0 in stage 8.0 (TID 374). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@469c0f20
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000162_378/part-00162
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2fa52217
15/08/09 15:27:20 INFO TaskSetManager: Starting task 171.0 in stage 8.0 (TID 387, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 171.0 in stage 8.0 (TID 387)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 158.0 in stage 8.0 (TID 374) in 162 ms on localhost (156/200)
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,256
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3302e49a
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 639B for [ps_partkey] INT32: 149 values, 603B raw, 603B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,243B for [part_value] DOUBLE: 149 values, 1,199B raw, 1,199B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000164_380/part-00164
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@134b738c
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000163_379/part-00163
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@343f4842
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000165_381/part-00165
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@45148d7b
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51a6ffea
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000160_376/part-00160
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000157_373' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000157
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000157_373: Committed
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO Executor: Finished task 157.0 in stage 8.0 (TID 373). 781 bytes result sent to driver
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,596
15/08/09 15:27:20 INFO TaskSetManager: Starting task 172.0 in stage 8.0 (TID 388, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 172.0 in stage 8.0 (TID 388)
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 507B for [ps_partkey] INT32: 116 values, 471B raw, 471B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 979B for [part_value] DOUBLE: 116 values, 935B raw, 935B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO TaskSetManager: Finished task 157.0 in stage 8.0 (TID 373) in 352 ms on localhost (157/200)
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@13e5c3bf
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,996
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@17c3ba77
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69e3790b
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000166_382/part-00166
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 587B for [ps_partkey] INT32: 136 values, 551B raw, 551B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,139B for [part_value] DOUBLE: 136 values, 1,095B raw, 1,095B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@551ff77c
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,116
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 611B for [ps_partkey] INT32: 142 values, 575B raw, 575B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,187B for [part_value] DOUBLE: 142 values, 1,143B raw, 1,143B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3caa3d4f
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,476
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 483B for [ps_partkey] INT32: 110 values, 447B raw, 447B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 931B for [part_value] DOUBLE: 110 values, 887B raw, 887B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14892ff5
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000168_384/part-00168
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@682103a7
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5db41792
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000169_385/part-00169
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a77a24d
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,556
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 499B for [ps_partkey] INT32: 114 values, 463B raw, 463B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 963B for [part_value] DOUBLE: 114 values, 919B raw, 919B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4f2b67f1
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000170_386/part-00170
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5fc068c
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,796
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c1e7bf6
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000167_383/part-00167
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 547B for [ps_partkey] INT32: 126 values, 511B raw, 511B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,059B for [part_value] DOUBLE: 126 values, 1,015B raw, 1,015B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000165_381' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000165
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000165_381: Committed
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000159_375' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000159
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000156_372' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000156
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000159_375: Committed
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000156_372: Committed
15/08/09 15:27:20 INFO Executor: Finished task 165.0 in stage 8.0 (TID 381). 781 bytes result sent to driver
15/08/09 15:27:20 INFO Executor: Finished task 156.0 in stage 8.0 (TID 372). 781 bytes result sent to driver
15/08/09 15:27:20 INFO Executor: Finished task 159.0 in stage 8.0 (TID 375). 781 bytes result sent to driver
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000161_377' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000161
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000161_377: Committed
15/08/09 15:27:20 INFO TaskSetManager: Starting task 173.0 in stage 8.0 (TID 389, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 173.0 in stage 8.0 (TID 389)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 165.0 in stage 8.0 (TID 381) in 302 ms on localhost (158/200)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 174.0 in stage 8.0 (TID 390, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 174.0 in stage 8.0 (TID 390)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 156.0 in stage 8.0 (TID 372) in 529 ms on localhost (159/200)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 175.0 in stage 8.0 (TID 391, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 175.0 in stage 8.0 (TID 391)
15/08/09 15:27:20 INFO Executor: Finished task 161.0 in stage 8.0 (TID 377). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Finished task 159.0 in stage 8.0 (TID 375) in 347 ms on localhost (160/200)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 176.0 in stage 8.0 (TID 392, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 176.0 in stage 8.0 (TID 392)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 161.0 in stage 8.0 (TID 377) in 343 ms on localhost (161/200)
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@28882410
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,376
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a7542e6
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 663B for [ps_partkey] INT32: 155 values, 627B raw, 627B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,291B for [part_value] DOUBLE: 155 values, 1,247B raw, 1,247B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,996
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 787B for [ps_partkey] INT32: 186 values, 751B raw, 751B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,539B for [part_value] DOUBLE: 186 values, 1,495B raw, 1,495B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000160_376' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000160
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000160_376: Committed
15/08/09 15:27:20 INFO Executor: Finished task 160.0 in stage 8.0 (TID 376). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@53ad12ed
15/08/09 15:27:20 INFO TaskSetManager: Starting task 177.0 in stage 8.0 (TID 393, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 177.0 in stage 8.0 (TID 393)
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,576
15/08/09 15:27:20 INFO TaskSetManager: Finished task 160.0 in stage 8.0 (TID 376) in 355 ms on localhost (162/200)
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 903B for [ps_partkey] INT32: 215 values, 867B raw, 867B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,771B for [part_value] DOUBLE: 215 values, 1,727B raw, 1,727B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000166_382' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000166
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000166_382: Committed
15/08/09 15:27:20 INFO Executor: Finished task 166.0 in stage 8.0 (TID 382). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 178.0 in stage 8.0 (TID 394, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 178.0 in stage 8.0 (TID 394)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 166.0 in stage 8.0 (TID 382) in 316 ms on localhost (163/200)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000168_384' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000168
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000168_384: Committed
15/08/09 15:27:20 INFO Executor: Finished task 168.0 in stage 8.0 (TID 384). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 179.0 in stage 8.0 (TID 395, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 179.0 in stage 8.0 (TID 395)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000169_385' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000169
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000169_385: Committed
15/08/09 15:27:20 INFO TaskSetManager: Finished task 168.0 in stage 8.0 (TID 384) in 317 ms on localhost (164/200)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000170_386' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000170
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO Executor: Finished task 169.0 in stage 8.0 (TID 385). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000170_386: Committed
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO TaskSetManager: Starting task 180.0 in stage 8.0 (TID 396, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Finished task 170.0 in stage 8.0 (TID 386). 781 bytes result sent to driver
15/08/09 15:27:20 INFO Executor: Running task 180.0 in stage 8.0 (TID 396)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 169.0 in stage 8.0 (TID 385) in 313 ms on localhost (165/200)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 181.0 in stage 8.0 (TID 397, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 181.0 in stage 8.0 (TID 397)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000167_383' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000167
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000167_383: Committed
15/08/09 15:27:20 INFO TaskSetManager: Finished task 170.0 in stage 8.0 (TID 386) in 296 ms on localhost (166/200)
15/08/09 15:27:20 INFO Executor: Finished task 167.0 in stage 8.0 (TID 383). 781 bytes result sent to driver
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10265e9c
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000171_387/part-00171
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO TaskSetManager: Starting task 182.0 in stage 8.0 (TID 398, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 182.0 in stage 8.0 (TID 398)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO TaskSetManager: Finished task 167.0 in stage 8.0 (TID 383) in 333 ms on localhost (167/200)
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3f8680c6
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000172_388/part-00172
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@50b210a3
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,916
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 771B for [ps_partkey] INT32: 182 values, 735B raw, 735B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,507B for [part_value] DOUBLE: 182 values, 1,463B raw, 1,463B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@45e3dd3c
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,856
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 559B for [ps_partkey] INT32: 129 values, 523B raw, 523B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,083B for [part_value] DOUBLE: 129 values, 1,039B raw, 1,039B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000172_388' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000172
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000172_388: Committed
15/08/09 15:27:20 INFO Executor: Finished task 172.0 in stage 8.0 (TID 388). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 183.0 in stage 8.0 (TID 399, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 183.0 in stage 8.0 (TID 399)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 172.0 in stage 8.0 (TID 388) in 267 ms on localhost (168/200)
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53a71e4d
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000175_391/part-00175
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6d3747a9
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000176_392/part-00176
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@182d72b1
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,196
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4fc4cedd
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000173_389/part-00173
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 827B for [ps_partkey] INT32: 196 values, 791B raw, 791B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,619B for [part_value] DOUBLE: 196 values, 1,575B raw, 1,575B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7660ebad
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,216
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 831B for [ps_partkey] INT32: 197 values, 795B raw, 795B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,627B for [part_value] DOUBLE: 197 values, 1,583B raw, 1,583B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@34ded0df
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@52737ee8
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1062fb97
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000174_390/part-00174
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000177_393/part-00177
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000178_394/part-00178
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6a437754
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000179_395/part-00179
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24e14fd5
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000180_396/part-00180
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6138573a
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@39e6d2b3
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000181_397/part-00181
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,596
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 707B for [ps_partkey] INT32: 166 values, 671B raw, 671B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,379B for [part_value] DOUBLE: 166 values, 1,335B raw, 1,335B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ca0a1b8
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,996
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 787B for [ps_partkey] INT32: 186 values, 751B raw, 751B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,539B for [part_value] DOUBLE: 186 values, 1,495B raw, 1,495B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c798cc0
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000175_391' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000175
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000175_391: Committed
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,756
15/08/09 15:27:20 INFO Executor: Finished task 175.0 in stage 8.0 (TID 391). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2191008c
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,796
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 939B for [ps_partkey] INT32: 224 values, 903B raw, 903B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 747B for [ps_partkey] INT32: 176 values, 711B raw, 711B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,843B for [part_value] DOUBLE: 224 values, 1,799B raw, 1,799B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2d1389bf
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5db871b7
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,459B for [part_value] DOUBLE: 176 values, 1,415B raw, 1,415B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,036
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 595B for [ps_partkey] INT32: 138 values, 559B raw, 559B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,155B for [part_value] DOUBLE: 138 values, 1,111B raw, 1,111B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO TaskSetManager: Starting task 184.0 in stage 8.0 (TID 400, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 184.0 in stage 8.0 (TID 400)
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e8f350
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,462,196
15/08/09 15:27:20 INFO TaskSetManager: Finished task 175.0 in stage 8.0 (TID 391) in 157 ms on localhost (169/200)
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69af2b87
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,027B for [ps_partkey] INT32: 246 values, 991B raw, 991B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000182_398/part-00182
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 2,019B for [part_value] DOUBLE: 246 values, 1,975B raw, 1,975B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000176_392' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000176
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000176_392: Committed
15/08/09 15:27:20 INFO Executor: Finished task 176.0 in stage 8.0 (TID 392). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO TaskSetManager: Starting task 185.0 in stage 8.0 (TID 401, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 185.0 in stage 8.0 (TID 401)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 176.0 in stage 8.0 (TID 392) in 162 ms on localhost (170/200)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000173_389' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000173
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000173_389: Committed
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000177_393' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000177
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000177_393: Committed
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@28efd58
15/08/09 15:27:20 INFO Executor: Finished task 173.0 in stage 8.0 (TID 389). 781 bytes result sent to driver
15/08/09 15:27:20 INFO Executor: Finished task 177.0 in stage 8.0 (TID 393). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 186.0 in stage 8.0 (TID 402, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 173.0 in stage 8.0 (TID 389) in 182 ms on localhost (171/200)
15/08/09 15:27:20 INFO Executor: Running task 186.0 in stage 8.0 (TID 402)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 187.0 in stage 8.0 (TID 403, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,216
15/08/09 15:27:20 INFO Executor: Running task 187.0 in stage 8.0 (TID 403)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 177.0 in stage 8.0 (TID 393) in 169 ms on localhost (172/200)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000180_396' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000180
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000180_396: Committed
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 631B for [ps_partkey] INT32: 147 values, 595B raw, 595B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,227B for [part_value] DOUBLE: 147 values, 1,183B raw, 1,183B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO Executor: Finished task 180.0 in stage 8.0 (TID 396). 781 bytes result sent to driver
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000181_397' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000181
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000179_395' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000179
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000181_397: Committed
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000179_395: Committed
15/08/09 15:27:20 INFO TaskSetManager: Starting task 188.0 in stage 8.0 (TID 404, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 188.0 in stage 8.0 (TID 404)
15/08/09 15:27:20 INFO Executor: Finished task 181.0 in stage 8.0 (TID 397). 781 bytes result sent to driver
15/08/09 15:27:20 INFO Executor: Finished task 179.0 in stage 8.0 (TID 395). 781 bytes result sent to driver
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000174_390' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000174
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000174_390: Committed
15/08/09 15:27:20 INFO TaskSetManager: Finished task 180.0 in stage 8.0 (TID 396) in 158 ms on localhost (173/200)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO TaskSetManager: Starting task 189.0 in stage 8.0 (TID 405, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 189.0 in stage 8.0 (TID 405)
15/08/09 15:27:20 INFO Executor: Finished task 174.0 in stage 8.0 (TID 390). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Finished task 181.0 in stage 8.0 (TID 397) in 158 ms on localhost (174/200)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 190.0 in stage 8.0 (TID 406, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 190.0 in stage 8.0 (TID 406)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 191.0 in stage 8.0 (TID 407, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 191.0 in stage 8.0 (TID 407)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 179.0 in stage 8.0 (TID 395) in 164 ms on localhost (175/200)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 174.0 in stage 8.0 (TID 390) in 197 ms on localhost (176/200)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000178_394' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000178
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000178_394: Committed
15/08/09 15:27:20 INFO Executor: Finished task 178.0 in stage 8.0 (TID 394). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 192.0 in stage 8.0 (TID 408, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 192.0 in stage 8.0 (TID 408)
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@39ce2b28
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000183_399/part-00183
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO TaskSetManager: Finished task 178.0 in stage 8.0 (TID 394) in 192 ms on localhost (177/200)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000182_398' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000182
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000182_398: Committed
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO Executor: Finished task 182.0 in stage 8.0 (TID 398). 781 bytes result sent to driver
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77cfeae5
15/08/09 15:27:20 INFO TaskSetManager: Starting task 193.0 in stage 8.0 (TID 409, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 193.0 in stage 8.0 (TID 409)
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,976
15/08/09 15:27:20 INFO TaskSetManager: Finished task 182.0 in stage 8.0 (TID 398) in 181 ms on localhost (178/200)
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 583B for [ps_partkey] INT32: 135 values, 547B raw, 547B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,131B for [part_value] DOUBLE: 135 values, 1,087B raw, 1,087B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000164_380' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000164
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000164_380: Committed
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000155_371' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000155
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000155_371: Committed
15/08/09 15:27:20 INFO Executor: Finished task 164.0 in stage 8.0 (TID 380). 781 bytes result sent to driver
15/08/09 15:27:20 INFO Executor: Finished task 155.0 in stage 8.0 (TID 371). 781 bytes result sent to driver
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000163_379' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000163
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000163_379: Committed
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000162_378' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000162
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000162_378: Committed
15/08/09 15:27:20 INFO TaskSetManager: Starting task 194.0 in stage 8.0 (TID 410, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Finished task 163.0 in stage 8.0 (TID 379). 781 bytes result sent to driver
15/08/09 15:27:20 INFO Executor: Running task 194.0 in stage 8.0 (TID 410)
15/08/09 15:27:20 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000183_399' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000183
15/08/09 15:27:20 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000183_399: Committed
15/08/09 15:27:20 INFO Executor: Finished task 162.0 in stage 8.0 (TID 378). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 195.0 in stage 8.0 (TID 411, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 195.0 in stage 8.0 (TID 411)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 196.0 in stage 8.0 (TID 412, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 196.0 in stage 8.0 (TID 412)
15/08/09 15:27:20 INFO Executor: Finished task 183.0 in stage 8.0 (TID 399). 781 bytes result sent to driver
15/08/09 15:27:20 INFO TaskSetManager: Starting task 197.0 in stage 8.0 (TID 413, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 197.0 in stage 8.0 (TID 413)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 163.0 in stage 8.0 (TID 379) in 760 ms on localhost (179/200)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 164.0 in stage 8.0 (TID 380) in 734 ms on localhost (180/200)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 162.0 in stage 8.0 (TID 378) in 766 ms on localhost (181/200)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 155.0 in stage 8.0 (TID 371) in 963 ms on localhost (182/200)
15/08/09 15:27:20 INFO TaskSetManager: Starting task 198.0 in stage 8.0 (TID 414, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:20 INFO Executor: Running task 198.0 in stage 8.0 (TID 414)
15/08/09 15:27:20 INFO TaskSetManager: Finished task 183.0 in stage 8.0 (TID 399) in 338 ms on localhost (183/200)
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10458938
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000184_400/part-00184
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22d35561
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000185_401/part-00185
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@528046d
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000188_404/part-00188
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@30974b7e
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000187_403/part-00187
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32c0d858
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b46c9fc
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000186_402/part-00186
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:20 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c1e21a4
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@16d29890
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000189_405/part-00189
15/08/09 15:27:20 INFO CodecConfig: Compression set to false
15/08/09 15:27:20 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,876
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:20 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:20 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:20 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 563B for [ps_partkey] INT32: 130 values, 527B raw, 527B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ColumnChunkPageWriteStore: written 1,091B for [part_value] DOUBLE: 130 values, 1,047B raw, 1,047B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:20 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@749ec3ed
15/08/09 15:27:20 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,176
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@694f2bbd
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000191_407/part-00191
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 623B for [ps_partkey] INT32: 145 values, 587B raw, 587B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,211B for [part_value] DOUBLE: 145 values, 1,167B raw, 1,167B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6bf7dcc9
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5dc0477f
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000192_408/part-00192
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000190_406/part-00190
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2aa5abc6
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,461,156
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@258a585d
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,836
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 819B for [ps_partkey] INT32: 194 values, 783B raw, 783B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,603B for [part_value] DOUBLE: 194 values, 1,559B raw, 1,559B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 755B for [ps_partkey] INT32: 178 values, 719B raw, 719B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,475B for [part_value] DOUBLE: 178 values, 1,431B raw, 1,431B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3b949f87
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,956
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 579B for [ps_partkey] INT32: 134 values, 543B raw, 543B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5409ae
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,123B for [part_value] DOUBLE: 134 values, 1,079B raw, 1,079B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000171_387' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000171
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000171_387: Committed
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@426a49cc
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,136
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 615B for [ps_partkey] INT32: 143 values, 579B raw, 579B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO Executor: Finished task 171.0 in stage 8.0 (TID 387). 781 bytes result sent to driver
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,195B for [part_value] DOUBLE: 143 values, 1,151B raw, 1,151B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000184_400' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000184
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000184_400: Committed
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6fe5950e
15/08/09 15:27:21 INFO TaskSetManager: Starting task 199.0 in stage 8.0 (TID 415, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:21 INFO Executor: Finished task 184.0 in stage 8.0 (TID 400). 781 bytes result sent to driver
15/08/09 15:27:21 INFO Executor: Running task 199.0 in stage 8.0 (TID 415)
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,936
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 575B for [ps_partkey] INT32: 133 values, 539B raw, 539B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,115B for [part_value] DOUBLE: 133 values, 1,071B raw, 1,071B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO TaskSetManager: Finished task 171.0 in stage 8.0 (TID 387) in 678 ms on localhost (184/200)
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000185_401' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000185
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000185_401: Committed
15/08/09 15:27:21 INFO Executor: Finished task 185.0 in stage 8.0 (TID 401). 781 bytes result sent to driver
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000188_404' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000188
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000188_404: Committed
15/08/09 15:27:21 INFO Executor: Finished task 188.0 in stage 8.0 (TID 404). 781 bytes result sent to driver
15/08/09 15:27:21 INFO TaskSetManager: Finished task 184.0 in stage 8.0 (TID 400) in 335 ms on localhost (185/200)
15/08/09 15:27:21 INFO TaskSetManager: Finished task 185.0 in stage 8.0 (TID 401) in 329 ms on localhost (186/200)
15/08/09 15:27:21 INFO TaskSetManager: Finished task 188.0 in stage 8.0 (TID 404) in 303 ms on localhost (187/200)
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000187_403' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000187
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000187_403: Committed
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@51facf2d
15/08/09 15:27:21 INFO Executor: Finished task 187.0 in stage 8.0 (TID 403). 781 bytes result sent to driver
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000193_409/part-00193
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO TaskSetManager: Finished task 187.0 in stage 8.0 (TID 403) in 329 ms on localhost (188/200)
15/08/09 15:27:21 INFO ShuffleBlockFetcherIterator: Getting 169 non-empty blocks out of 200 blocks
15/08/09 15:27:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000186_402' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000186
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000186_402: Committed
15/08/09 15:27:21 INFO Executor: Finished task 186.0 in stage 8.0 (TID 402). 781 bytes result sent to driver
15/08/09 15:27:21 INFO TaskSetManager: Finished task 186.0 in stage 8.0 (TID 402) in 347 ms on localhost (189/200)
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000190_406' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000190
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000190_406: Committed
15/08/09 15:27:21 INFO Executor: Finished task 190.0 in stage 8.0 (TID 406). 781 bytes result sent to driver
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000191_407' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000191
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000191_407: Committed
15/08/09 15:27:21 INFO Executor: Finished task 191.0 in stage 8.0 (TID 407). 781 bytes result sent to driver
15/08/09 15:27:21 INFO TaskSetManager: Finished task 190.0 in stage 8.0 (TID 406) in 339 ms on localhost (190/200)
15/08/09 15:27:21 INFO TaskSetManager: Finished task 191.0 in stage 8.0 (TID 407) in 340 ms on localhost (191/200)
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000192_408' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000192
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000192_408: Committed
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@325e4043
15/08/09 15:27:21 INFO Executor: Finished task 192.0 in stage 8.0 (TID 408). 781 bytes result sent to driver
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,956
15/08/09 15:27:21 INFO TaskSetManager: Finished task 192.0 in stage 8.0 (TID 408) in 329 ms on localhost (192/200)
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 779B for [ps_partkey] INT32: 184 values, 743B raw, 743B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,523B for [part_value] DOUBLE: 184 values, 1,479B raw, 1,479B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7cf492e8
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1cd8fd6f
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000194_410/part-00194
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000196_412/part-00196
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69e69134
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000195_411/part-00195
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@50f8866d
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000198_414/part-00198
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@8ea8a9c
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000197_413/part-00197
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64f77082
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,276
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@566248a0
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 643B for [ps_partkey] INT32: 150 values, 607B raw, 607B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4e8e2cae
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,251B for [part_value] DOUBLE: 150 values, 1,207B raw, 1,207B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,776
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,896
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6dc0bb42
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,459,776
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 567B for [ps_partkey] INT32: 131 values, 531B raw, 531B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 743B for [ps_partkey] INT32: 175 values, 707B raw, 707B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,099B for [part_value] DOUBLE: 131 values, 1,055B raw, 1,055B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,451B for [part_value] DOUBLE: 175 values, 1,407B raw, 1,407B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 543B for [ps_partkey] INT32: 125 values, 507B raw, 507B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,051B for [part_value] DOUBLE: 125 values, 1,007B raw, 1,007B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@754b1d24
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,436
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000193_409' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000193
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000193_409: Committed
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 675B for [ps_partkey] INT32: 158 values, 639B raw, 639B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,315B for [part_value] DOUBLE: 158 values, 1,271B raw, 1,271B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO Executor: Finished task 193.0 in stage 8.0 (TID 409). 781 bytes result sent to driver
15/08/09 15:27:21 INFO TaskSetManager: Finished task 193.0 in stage 8.0 (TID 409) in 352 ms on localhost (193/200)
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000195_411' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000195
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000195_411: Committed
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000194_410' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000194
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000194_410: Committed
15/08/09 15:27:21 INFO Executor: Finished task 195.0 in stage 8.0 (TID 411). 781 bytes result sent to driver
15/08/09 15:27:21 INFO Executor: Finished task 194.0 in stage 8.0 (TID 410). 781 bytes result sent to driver
15/08/09 15:27:21 INFO TaskSetManager: Finished task 195.0 in stage 8.0 (TID 411) in 172 ms on localhost (194/200)
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000197_413' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000197
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000197_413: Committed
15/08/09 15:27:21 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c31ce93
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0008_m_000199_415/part-00199
15/08/09 15:27:21 INFO CodecConfig: Compression set to false
15/08/09 15:27:21 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:21 INFO TaskSetManager: Finished task 194.0 in stage 8.0 (TID 410) in 175 ms on localhost (195/200)
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:21 INFO Executor: Finished task 197.0 in stage 8.0 (TID 413). 781 bytes result sent to driver
15/08/09 15:27:21 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:21 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:21 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:21 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:21 INFO TaskSetManager: Finished task 197.0 in stage 8.0 (TID 413) in 175 ms on localhost (196/200)
15/08/09 15:27:21 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@678f16fa
15/08/09 15:27:21 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,460,496
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 687B for [ps_partkey] INT32: 161 values, 651B raw, 651B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO ColumnChunkPageWriteStore: written 1,339B for [part_value] DOUBLE: 161 values, 1,295B raw, 1,295B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000199_415' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000199
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000199_415: Committed
15/08/09 15:27:21 INFO Executor: Finished task 199.0 in stage 8.0 (TID 415). 781 bytes result sent to driver
15/08/09 15:27:21 INFO TaskSetManager: Finished task 199.0 in stage 8.0 (TID 415) in 157 ms on localhost (197/200)
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000189_405' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000189
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000189_405: Committed
15/08/09 15:27:21 INFO Executor: Finished task 189.0 in stage 8.0 (TID 405). 781 bytes result sent to driver
15/08/09 15:27:21 INFO TaskSetManager: Finished task 189.0 in stage 8.0 (TID 405) in 751 ms on localhost (198/200)
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000198_414' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000198
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000198_414: Committed
15/08/09 15:27:21 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0008_m_000196_412' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_temporary/0/task_201508091527_0008_m_000196
15/08/09 15:27:21 INFO SparkHiveWriterContainer: attempt_201508091527_0008_m_000196_412: Committed
15/08/09 15:27:21 INFO Executor: Finished task 196.0 in stage 8.0 (TID 412). 781 bytes result sent to driver
15/08/09 15:27:21 INFO Executor: Finished task 198.0 in stage 8.0 (TID 414). 781 bytes result sent to driver
15/08/09 15:27:21 INFO TaskSetManager: Finished task 196.0 in stage 8.0 (TID 412) in 573 ms on localhost (199/200)
15/08/09 15:27:21 INFO TaskSetManager: Finished task 198.0 in stage 8.0 (TID 414) in 570 ms on localhost (200/200)
15/08/09 15:27:21 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/08/09 15:27:21 INFO DAGScheduler: Stage 8 (runJob at InsertIntoHiveTable.scala:93) finished in 5.931 s
15/08/09 15:27:21 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@19be8b74
15/08/09 15:27:21 INFO DAGScheduler: Job 5 finished: runJob at InsertIntoHiveTable.scala:93, took 16.705156 s
15/08/09 15:27:21 INFO StatsReportListener: task runtime:(count: 200, mean: 445.905000, stdev: 287.297661, max: 1255.000000, min: 143.000000)
15/08/09 15:27:21 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:21 INFO StatsReportListener: 	143.0 ms	166.0 ms	193.0 ms	267.0 ms	348.0 ms	542.0 ms	824.0 ms	1.2 s	1.3 s
15/08/09 15:27:21 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.395000, stdev: 0.921398, max: 7.000000, min: 0.000000)
15/08/09 15:27:21 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:21 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms	7.0 ms
15/08/09 15:27:21 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/09 15:27:21 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:21 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/09 15:27:21 INFO StatsReportListener: task result size:(count: 200, mean: 781.000000, stdev: 0.000000, max: 781.000000, min: 781.000000)
15/08/09 15:27:21 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:21 INFO StatsReportListener: 	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B
15/08/09 15:27:21 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 98.381003, stdev: 1.502602, max: 99.757282, min: 81.963714)
15/08/09 15:27:21 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:21 INFO StatsReportListener: 	82 %	97 %	97 %	98 %	99 %	99 %	99 %	100 %	100 %
15/08/09 15:27:21 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.079995, stdev: 0.178481, max: 1.153846, min: 0.000000)
15/08/09 15:27:21 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:21 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/09 15:27:21 INFO StatsReportListener: other time pct: (count: 200, mean: 1.539002, stdev: 1.486735, max: 18.036286, min: 0.242718)
15/08/09 15:27:21 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:21 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 1 %	 2 %	 3 %	 3 %	18 %
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/_SUCCESS;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/_SUCCESS;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00000;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00001;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00002;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00003;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00004;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00005;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00006;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00007;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00008;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00009;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00010;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00011;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00012;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00013;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00014;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00015;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00016;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00017;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00018;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00019;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00020;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00021;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00022;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00023;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00024;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00025;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00026;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00027;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00028;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00029;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00030;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00031;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00032;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00033;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00034;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00035;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00036;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00037;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00038;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00039;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00040;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00041;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00042;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00043;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00044;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00045;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00046;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00047;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00048;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00049;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00050;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00051;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00052;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00053;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00054;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00055;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00056;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00057;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00058;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00059;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00060;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00061;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00062;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00063;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00064;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00065;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00066;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00067;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00068;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00069;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00070;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00071;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00072;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00073;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00074;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00075;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00076;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00077;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00078;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00079;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00080;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00081;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00082;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00083;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00084;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00085;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00086;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00087;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00088;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00089;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00090;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00091;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00092;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00093;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00094;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00095;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00096;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00097;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00098;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00099;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00100;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00101;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00102;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00103;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00104;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00105;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00106;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00107;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00108;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00109;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00110;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00111;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00112;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00113;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00114;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00115;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00116;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00117;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00118;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00119;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00120;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00121;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00122;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00123;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00124;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00125;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00126;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00127;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00128;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00129;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00130;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00131;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00132;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00133;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00134;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00135;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00136;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00137;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00138;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00139;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00140;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00141;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00142;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00143;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00144;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00145;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00146;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00147;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00148;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00149;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00150;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00151;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00152;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00153;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00154;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00155;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00156;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00157;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00158;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00159;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00160;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00161;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00162;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00163;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00164;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00165;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00166;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00167;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00168;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00169;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00170;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00171;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00172;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00173;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00174;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00175;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00176;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00177;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00178;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00179;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00180;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00181;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00182;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00183;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00184;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00185;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00186;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00187;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00188;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00189;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00190;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00191;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00192;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00193;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00194;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00195;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00196;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00197;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00198;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198;Status:true
15/08/09 15:27:23 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-03_064_7088477411125250078-1/-ext-10000/part-00199;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199;Status:true
15/08/09 15:27:23 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/09 15:27:23 INFO DAGScheduler: Got job 6 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/09 15:27:23 INFO DAGScheduler: Final stage: Stage 9(collect at SparkPlan.scala:84)
15/08/09 15:27:23 INFO DAGScheduler: Parents of final stage: List()
15/08/09 15:27:23 INFO DAGScheduler: Missing parents: List()
15/08/09 15:27:23 INFO DAGScheduler: Submitting Stage 9 (MappedRDD[50] at map at SparkPlan.scala:84), which has no missing parents
15/08/09 15:27:23 INFO MemoryStore: ensureFreeSpace(3240) called with curMem=962128, maxMem=3333968363
15/08/09 15:27:23 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 3.2 KB, free 3.1 GB)
15/08/09 15:27:23 INFO MemoryStore: ensureFreeSpace(1941) called with curMem=965368, maxMem=3333968363
15/08/09 15:27:23 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1941.0 B, free 3.1 GB)
15/08/09 15:27:23 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:44535 (size: 1941.0 B, free: 3.1 GB)
15/08/09 15:27:23 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/08/09 15:27:23 INFO DefaultExecutionContext: Created broadcast 13 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:23 INFO DAGScheduler: Submitting 1 missing tasks from Stage 9 (MappedRDD[50] at map at SparkPlan.scala:84)
15/08/09 15:27:23 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/08/09 15:27:23 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 416, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/09 15:27:23 INFO Executor: Running task 0.0 in stage 9.0 (TID 416)
15/08/09 15:27:23 INFO Executor: Finished task 0.0 in stage 9.0 (TID 416). 618 bytes result sent to driver
15/08/09 15:27:23 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 416) in 8 ms on localhost (1/1)
15/08/09 15:27:23 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/08/09 15:27:23 INFO DAGScheduler: Stage 9 (collect at SparkPlan.scala:84) finished in 0.010 s
15/08/09 15:27:23 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@c3abd91
15/08/09 15:27:23 INFO DAGScheduler: Job 6 finished: collect at SparkPlan.scala:84, took 0.026915 s
Time taken: 22.392 seconds
15/08/09 15:27:23 INFO CliDriver: Time taken: 22.392 seconds
15/08/09 15:27:23 INFO StatsReportListener: task runtime:(count: 1, mean: 8.000000, stdev: 0.000000, max: 8.000000, min: 8.000000)
15/08/09 15:27:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:23 INFO StatsReportListener: 	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms	8.0 ms
15/08/09 15:27:23 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/09 15:27:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:23 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/09 15:27:23 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 37.500000, stdev: 0.000000, max: 37.500000, min: 37.500000)
15/08/09 15:27:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:23 INFO StatsReportListener: 	38 %	38 %	38 %	38 %	38 %	38 %	38 %	38 %	38 %
15/08/09 15:27:23 INFO StatsReportListener: other time pct: (count: 1, mean: 62.500000, stdev: 0.000000, max: 62.500000, min: 62.500000)
15/08/09 15:27:23 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:23 INFO StatsReportListener: 	63 %	63 %	63 %	63 %	63 %	63 %	63 %	63 %	63 %
15/08/09 15:27:23 INFO ParseDriver: Parsing command: insert into table q11_important_stock_par_spark
select ps_partkey, part_value as value
from (select sum(part_value) as total_value from q11_part_tmp_par_spark) sum_tmp
        join q11_part_tmp_par_spark
where part_value > total_value * 0.0001
order by value desc
15/08/09 15:27:23 INFO ParseDriver: Parse Completed
15/08/09 15:27:23 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#114, part_value#115)
15/08/09 15:27:24 INFO ParquetTypesConverter: Falling back to schema conversion from Parquet types; result: ArrayBuffer(ps_partkey#118, part_value#119)
15/08/09 15:27:24 INFO MemoryStore: ensureFreeSpace(280818) called with curMem=967309, maxMem=3333968363
15/08/09 15:27:24 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 274.2 KB, free 3.1 GB)
15/08/09 15:27:24 INFO MemoryStore: ensureFreeSpace(31760) called with curMem=1248127, maxMem=3333968363
15/08/09 15:27:24 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 31.0 KB, free 3.1 GB)
15/08/09 15:27:24 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:44535 (size: 31.0 KB, free: 3.1 GB)
15/08/09 15:27:24 INFO BlockManagerMaster: Updated info of block broadcast_14_piece0
15/08/09 15:27:24 INFO DefaultExecutionContext: Created broadcast 14 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/09 15:27:24 INFO MemoryStore: ensureFreeSpace(280962) called with curMem=1279887, maxMem=3333968363
15/08/09 15:27:24 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 274.4 KB, free 3.1 GB)
15/08/09 15:27:24 INFO MemoryStore: ensureFreeSpace(31842) called with curMem=1560849, maxMem=3333968363
15/08/09 15:27:24 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.1 KB, free 3.1 GB)
15/08/09 15:27:24 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:44535 (size: 31.1 KB, free: 3.1 GB)
15/08/09 15:27:24 INFO BlockManagerMaster: Updated info of block broadcast_15_piece0
15/08/09 15:27:24 INFO DefaultExecutionContext: Created broadcast 15 from NewHadoopRDD at ParquetTableOperations.scala:119
15/08/09 15:27:24 INFO FileInputFormat: Total input paths to process : 200
15/08/09 15:27:24 INFO ParquetInputFormat: Total input paths to process : 200
15/08/09 15:27:24 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/09 15:27:24 INFO ParquetFileReader: reading another 200 footers
15/08/09 15:27:24 INFO ParquetFileReader: Initiating action with parallelism: 5
15/08/09 15:27:24 INFO FilteringParquetRowInputFormat: Fetched [LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000; isDirectory=false; length=2638; replication=1; blocksize=134217728; modification_time=1439134036795; access_time=1439134036147; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001; isDirectory=false; length=2326; replication=1; blocksize=134217728; modification_time=1439134036798; access_time=1439134036112; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1439134036796; access_time=1439134036114; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1439134036799; access_time=1439134036135; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004; isDirectory=false; length=2194; replication=1; blocksize=134217728; modification_time=1439134036798; access_time=1439134036196; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005; isDirectory=false; length=2446; replication=1; blocksize=134217728; modification_time=1439134036796; access_time=1439134036113; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006; isDirectory=false; length=1858; replication=1; blocksize=134217728; modification_time=1439134036799; access_time=1439134036126; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1439134036798; access_time=1439134036196; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1439134036796; access_time=1439134036117; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009; isDirectory=false; length=2050; replication=1; blocksize=134217728; modification_time=1439134036797; access_time=1439134036134; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010; isDirectory=false; length=2002; replication=1; blocksize=134217728; modification_time=1439134036797; access_time=1439134036136; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1439134036795; access_time=1439134036139; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012; isDirectory=false; length=2038; replication=1; blocksize=134217728; modification_time=1439134036795; access_time=1439134036114; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013; isDirectory=false; length=2170; replication=1; blocksize=134217728; modification_time=1439134036796; access_time=1439134036113; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1439134036801; access_time=1439134036118; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1439134036798; access_time=1439134036134; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1439134037137; access_time=1439134036978; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1439134037125; access_time=1439134036999; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1439134037400; access_time=1439134037340; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1439134037367; access_time=1439134037095; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1439134037345; access_time=1439134037062; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021; isDirectory=false; length=2122; replication=1; blocksize=134217728; modification_time=1439134037542; access_time=1439134037061; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022; isDirectory=false; length=2170; replication=1; blocksize=134217728; modification_time=1439134037345; access_time=1439134037063; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1439134037382; access_time=1439134037083; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024; isDirectory=false; length=2470; replication=1; blocksize=134217728; modification_time=1439134037135; access_time=1439134037004; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1439134037384; access_time=1439134037137; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1439134037351; access_time=1439134037060; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1439134037385; access_time=1439134037075; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028; isDirectory=false; length=2134; replication=1; blocksize=134217728; modification_time=1439134037363; access_time=1439134037084; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1439134037141; access_time=1439134037012; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1439134037363; access_time=1439134037096; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031; isDirectory=false; length=1378; replication=1; blocksize=134217728; modification_time=1439134037339; access_time=1439134037084; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032; isDirectory=false; length=2302; replication=1; blocksize=134217728; modification_time=1439134037618; access_time=1439134037538; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1439134037641; access_time=1439134037602; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034; isDirectory=false; length=2626; replication=1; blocksize=134217728; modification_time=1439134037549; access_time=1439134037493; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1439134038009; access_time=1439134037523; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1439134037619; access_time=1439134037539; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1439134037640; access_time=1439134037604; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1439134037590; access_time=1439134037522; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1439134037619; access_time=1439134037540; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1439134038019; access_time=1439134037548; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041; isDirectory=false; length=1738; replication=1; blocksize=134217728; modification_time=1439134037650; access_time=1439134037608; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042; isDirectory=false; length=1762; replication=1; blocksize=134217728; modification_time=1439134038005; access_time=1439134037531; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1439134038023; access_time=1439134037574; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044; isDirectory=false; length=1846; replication=1; blocksize=134217728; modification_time=1439134038067; access_time=1439134037605; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1439134037642; access_time=1439134037577; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1439134037627; access_time=1439134037573; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1439134038323; access_time=1439134037866; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1439134037922; access_time=1439134037873; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049; isDirectory=false; length=1462; replication=1; blocksize=134217728; modification_time=1439134037926; access_time=1439134037885; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1439134037925; access_time=1439134037897; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051; isDirectory=false; length=1774; replication=1; blocksize=134217728; modification_time=1439134037953; access_time=1439134037913; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1439134037928; access_time=1439134037886; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053; isDirectory=false; length=1738; replication=1; blocksize=134217728; modification_time=1439134037928; access_time=1439134037897; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1439134037958; access_time=1439134037926; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055; isDirectory=false; length=1558; replication=1; blocksize=134217728; modification_time=1439134038005; access_time=1439134037964; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1439134037983; access_time=1439134037936; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1439134038443; access_time=1439134037944; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058; isDirectory=false; length=1666; replication=1; blocksize=134217728; modification_time=1439134038635; access_time=1439134038191; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1439134038235; access_time=1439134038199; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060; isDirectory=false; length=2758; replication=1; blocksize=134217728; modification_time=1439134038233; access_time=1439134038191; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1439134038611; access_time=1439134038073; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1439134038208; access_time=1439134038065; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063; isDirectory=false; length=1990; replication=1; blocksize=134217728; modification_time=1439134038208; access_time=1439134038067; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1439134038253; access_time=1439134038220; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1439134038251; access_time=1439134038221; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066; isDirectory=false; length=2446; replication=1; blocksize=134217728; modification_time=1439134038284; access_time=1439134038252; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1439134038276; access_time=1439134038238; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068; isDirectory=false; length=1786; replication=1; blocksize=134217728; modification_time=1439134038284; access_time=1439134038254; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1439134038793; access_time=1439134038241; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1439134038287; access_time=1439134038258; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1439134038333; access_time=1439134038309; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1439134038445; access_time=1439134038331; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073; isDirectory=false; length=1966; replication=1; blocksize=134217728; modification_time=1439134038463; access_time=1439134038354; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074; isDirectory=false; length=2098; replication=1; blocksize=134217728; modification_time=1439134038467; access_time=1439134038345; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1439134038486; access_time=1439134038445; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1439134038480; access_time=1439134038450; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1439134038485; access_time=1439134038453; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078; isDirectory=false; length=2614; replication=1; blocksize=134217728; modification_time=1439134038499; access_time=1439134038466; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1439134038523; access_time=1439134038498; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1439134038522; access_time=1439134038494; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1439134038536; access_time=1439134038508; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1439134038564; access_time=1439134038518; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1439134038567; access_time=1439134038536; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1439134038612; access_time=1439134038581; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1439134038613; access_time=1439134038589; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1439134039104; access_time=1439134038602; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087; isDirectory=false; length=2146; replication=1; blocksize=134217728; modification_time=1439134038613; access_time=1439134038587; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088; isDirectory=false; length=2722; replication=1; blocksize=134217728; modification_time=1439134039259; access_time=1439134038798; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1439134038852; access_time=1439134038632; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090; isDirectory=false; length=2398; replication=1; blocksize=134217728; modification_time=1439134038876; access_time=1439134038806; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091; isDirectory=false; length=1750; replication=1; blocksize=134217728; modification_time=1439134038872; access_time=1439134038797; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1439134038881; access_time=1439134038828; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1439134038878; access_time=1439134038813; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1439134038927; access_time=1439134038893; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1439134038927; access_time=1439134038876; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096; isDirectory=false; length=1690; replication=1; blocksize=134217728; modification_time=1439134038922; access_time=1439134038862; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097; isDirectory=false; length=2194; replication=1; blocksize=134217728; modification_time=1439134038972; access_time=1439134038932; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1439134038970; access_time=1439134038935; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099; isDirectory=false; length=2266; replication=1; blocksize=134217728; modification_time=1439134038971; access_time=1439134038937; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1439134038959; access_time=1439134038911; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101; isDirectory=false; length=2410; replication=1; blocksize=134217728; modification_time=1439134039428; access_time=1439134038909; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1439134038995; access_time=1439134038953; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1439134039137; access_time=1439134039003; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104; isDirectory=false; length=2434; replication=1; blocksize=134217728; modification_time=1439134039147; access_time=1439134039119; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105; isDirectory=false; length=1810; replication=1; blocksize=134217728; modification_time=1439134039149; access_time=1439134039129; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1439134039138; access_time=1439134039115; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107; isDirectory=false; length=2326; replication=1; blocksize=134217728; modification_time=1439134039153; access_time=1439134039129; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1439134039193; access_time=1439134039164; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1439134039200; access_time=1439134039162; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1439134039205; access_time=1439134039165; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1439134039226; access_time=1439134039201; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1439134039241; access_time=1439134039211; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113; isDirectory=false; length=2242; replication=1; blocksize=134217728; modification_time=1439134039642; access_time=1439134039201; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114; isDirectory=false; length=2794; replication=1; blocksize=134217728; modification_time=1439134039441; access_time=1439134039238; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115; isDirectory=false; length=1858; replication=1; blocksize=134217728; modification_time=1439134039448; access_time=1439134039260; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1439134039436; access_time=1439134039236; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117; isDirectory=false; length=2590; replication=1; blocksize=134217728; modification_time=1439134039457; access_time=1439134039261; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1439134039454; access_time=1439134039261; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119; isDirectory=false; length=2374; replication=1; blocksize=134217728; modification_time=1439134039858; access_time=1439134039266; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120; isDirectory=false; length=2626; replication=1; blocksize=134217728; modification_time=1439134039453; access_time=1439134039262; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121; isDirectory=false; length=2158; replication=1; blocksize=134217728; modification_time=1439134039448; access_time=1439134039262; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122; isDirectory=false; length=2422; replication=1; blocksize=134217728; modification_time=1439134039512; access_time=1439134039484; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123; isDirectory=false; length=2482; replication=1; blocksize=134217728; modification_time=1439134039514; access_time=1439134039488; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124; isDirectory=false; length=2098; replication=1; blocksize=134217728; modification_time=1439134039549; access_time=1439134039489; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125; isDirectory=false; length=1798; replication=1; blocksize=134217728; modification_time=1439134039551; access_time=1439134039506; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126; isDirectory=false; length=2350; replication=1; blocksize=134217728; modification_time=1439134039614; access_time=1439134039555; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1439134039642; access_time=1439134039572; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1439134039613; access_time=1439134039572; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1439134040217; access_time=1439134039580; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1439134039823; access_time=1439134039624; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1439134039808; access_time=1439134039600; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1439134039815; access_time=1439134039624; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1439134039817; access_time=1439134039608; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134; isDirectory=false; length=1990; replication=1; blocksize=134217728; modification_time=1439134039806; access_time=1439134039589; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1439134039825; access_time=1439134039625; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1439134039861; access_time=1439134039826; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137; isDirectory=false; length=1798; replication=1; blocksize=134217728; modification_time=1439134039854; access_time=1439134039830; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138; isDirectory=false; length=1678; replication=1; blocksize=134217728; modification_time=1439134039900; access_time=1439134039839; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139; isDirectory=false; length=2014; replication=1; blocksize=134217728; modification_time=1439134039916; access_time=1439134039873; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140; isDirectory=false; length=2494; replication=1; blocksize=134217728; modification_time=1439134039967; access_time=1439134039926; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141; isDirectory=false; length=1654; replication=1; blocksize=134217728; modification_time=1439134039996; access_time=1439134039945; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142; isDirectory=false; length=2350; replication=1; blocksize=134217728; modification_time=1439134039998; access_time=1439134039943; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143; isDirectory=false; length=1558; replication=1; blocksize=134217728; modification_time=1439134039985; access_time=1439134039926; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1439134039986; access_time=1439134039929; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1439134039998; access_time=1439134039951; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1439134039998; access_time=1439134039957; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1439134040003; access_time=1439134039957; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1439134040004; access_time=1439134039958; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149; isDirectory=false; length=1618; replication=1; blocksize=134217728; modification_time=1439134040187; access_time=1439134039987; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150; isDirectory=false; length=2278; replication=1; blocksize=134217728; modification_time=1439134040222; access_time=1439134040005; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151; isDirectory=false; length=2602; replication=1; blocksize=134217728; modification_time=1439134040237; access_time=1439134040197; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152; isDirectory=false; length=1678; replication=1; blocksize=134217728; modification_time=1439134040237; access_time=1439134040196; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153; isDirectory=false; length=1846; replication=1; blocksize=134217728; modification_time=1439134040242; access_time=1439134040197; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154; isDirectory=false; length=2590; replication=1; blocksize=134217728; modification_time=1439134040262; access_time=1439134040242; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155; isDirectory=false; length=2110; replication=1; blocksize=134217728; modification_time=1439134040936; access_time=1439134040308; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1439134040388; access_time=1439134040307; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157; isDirectory=false; length=2230; replication=1; blocksize=134217728; modification_time=1439134040333; access_time=1439134040297; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158; isDirectory=false; length=2338; replication=1; blocksize=134217728; modification_time=1439134040328; access_time=1439134040286; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159; isDirectory=false; length=2386; replication=1; blocksize=134217728; modification_time=1439134040388; access_time=1439134040312; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1439134040531; access_time=1439134040360; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161; isDirectory=false; length=1714; replication=1; blocksize=134217728; modification_time=1439134040394; access_time=1439134040334; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162; isDirectory=false; length=1954; replication=1; blocksize=134217728; modification_time=1439134040936; access_time=1439134040351; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163; isDirectory=false; length=1642; replication=1; blocksize=134217728; modification_time=1439134040941; access_time=1439134040358; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1439134040936; access_time=1439134040354; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165; isDirectory=false; length=2026; replication=1; blocksize=134217728; modification_time=1439134040388; access_time=1439134040357; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166; isDirectory=false; length=1690; replication=1; blocksize=134217728; modification_time=1439134040537; access_time=1439134040391; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167; isDirectory=false; length=2902; replication=1; blocksize=134217728; modification_time=1439134040563; access_time=1439134040532; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168; isDirectory=false; length=1834; replication=1; blocksize=134217728; modification_time=1439134040548; access_time=1439134040395; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169; isDirectory=false; length=2182; replication=1; blocksize=134217728; modification_time=1439134040556; access_time=1439134040525; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170; isDirectory=false; length=2554; replication=1; blocksize=134217728; modification_time=1439134040560; access_time=1439134040527; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171; isDirectory=false; length=2506; replication=1; blocksize=134217728; modification_time=1439134041008; access_time=1439134040580; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172; isDirectory=false; length=1870; replication=1; blocksize=134217728; modification_time=1439134040609; access_time=1439134040586; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173; isDirectory=false; length=2314; replication=1; blocksize=134217728; modification_time=1439134040695; access_time=1439134040656; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174; isDirectory=false; length=3010; replication=1; blocksize=134217728; modification_time=1439134040705; access_time=1439134040664; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175; isDirectory=false; length=2674; replication=1; blocksize=134217728; modification_time=1439134040666; access_time=1439134040636; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176; isDirectory=false; length=2686; replication=1; blocksize=134217728; modification_time=1439134040673; access_time=1439134040638; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177; isDirectory=false; length=2554; replication=1; blocksize=134217728; modification_time=1439134040701; access_time=1439134040665; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178; isDirectory=false; length=3274; replication=1; blocksize=134217728; modification_time=1439134040709; access_time=1439134040666; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1439134040707; access_time=1439134040667; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180; isDirectory=false; length=1978; replication=1; blocksize=134217728; modification_time=1439134040705; access_time=1439134040674; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181; isDirectory=false; length=2434; replication=1; blocksize=134217728; modification_time=1439134040707; access_time=1439134040675; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182; isDirectory=false; length=2086; replication=1; blocksize=134217728; modification_time=1439134040749; access_time=1439134040700; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183; isDirectory=false; length=1942; replication=1; blocksize=134217728; modification_time=1439134040945; access_time=1439134040748; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1439134041009; access_time=1439134040978; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185; isDirectory=false; length=1882; replication=1; blocksize=134217728; modification_time=1439134041012; access_time=1439134040986; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186; isDirectory=false; length=2458; replication=1; blocksize=134217728; modification_time=1439134041027; access_time=1439134040995; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187; isDirectory=false; length=2650; replication=1; blocksize=134217728; modification_time=1439134041025; access_time=1439134040994; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188; isDirectory=false; length=2062; replication=1; blocksize=134217728; modification_time=1439134041013; access_time=1439134040988; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189; isDirectory=false; length=1930; replication=1; blocksize=134217728; modification_time=1439134041458; access_time=1439134040998; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190; isDirectory=false; length=1918; replication=1; blocksize=134217728; modification_time=1439134041058; access_time=1439134041007; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1439134041060; access_time=1439134041003; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192; isDirectory=false; length=2038; replication=1; blocksize=134217728; modification_time=1439134041060; access_time=1439134041006; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193; isDirectory=false; length=2530; replication=1; blocksize=134217728; modification_time=1439134041092; access_time=1439134041047; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194; isDirectory=false; length=1894; replication=1; blocksize=134217728; modification_time=1439134041117; access_time=1439134041084; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195; isDirectory=false; length=2122; replication=1; blocksize=134217728; modification_time=1439134041116; access_time=1439134041083; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196; isDirectory=false; length=1822; replication=1; blocksize=134217728; modification_time=1439134041518; access_time=1439134041084; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197; isDirectory=false; length=2218; replication=1; blocksize=134217728; modification_time=1439134041120; access_time=1439134041092; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198; isDirectory=false; length=2422; replication=1; blocksize=134217728; modification_time=1439134041517; access_time=1439134041088; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}, LocatedFileStatus{path=hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199; isDirectory=false; length=2254; replication=1; blocksize=134217728; modification_time=1439134041165; access_time=1439134041135; owner=hive; group=hdfs; permission=rw-r--r--; isSymlink=false}] footers in 222 ms
15/08/09 15:27:24 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/09 15:27:24 INFO DefaultExecutionContext: Starting job: RangePartitioner at Exchange.scala:88
15/08/09 15:27:24 INFO FileInputFormat: Total input paths to process : 200
15/08/09 15:27:24 INFO ParquetInputFormat: Total input paths to process : 200
15/08/09 15:27:24 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/08/09 15:27:25 INFO DAGScheduler: Registering RDD 63 (mapPartitions at Exchange.scala:100)
15/08/09 15:27:25 INFO DAGScheduler: Got job 7 (RangePartitioner at Exchange.scala:88) with 200 output partitions (allowLocal=false)
15/08/09 15:27:25 INFO DAGScheduler: Final stage: Stage 11(RangePartitioner at Exchange.scala:88)
15/08/09 15:27:25 INFO DAGScheduler: Parents of final stage: List(Stage 10)
15/08/09 15:27:25 INFO DAGScheduler: Missing parents: List(Stage 10)
15/08/09 15:27:25 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[63] at mapPartitions at Exchange.scala:100), which has no missing parents
15/08/09 15:27:25 INFO MemoryStore: ensureFreeSpace(8200) called with curMem=1592691, maxMem=3333968363
15/08/09 15:27:25 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 8.0 KB, free 3.1 GB)
15/08/09 15:27:25 INFO MemoryStore: ensureFreeSpace(4450) called with curMem=1600891, maxMem=3333968363
15/08/09 15:27:25 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.3 KB, free 3.1 GB)
15/08/09 15:27:25 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:44535 (size: 4.3 KB, free: 3.1 GB)
15/08/09 15:27:25 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/08/09 15:27:25 INFO DefaultExecutionContext: Created broadcast 16 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:25 INFO DAGScheduler: Submitting 200 missing tasks from Stage 10 (MapPartitionsRDD[63] at mapPartitions at Exchange.scala:100)
15/08/09 15:27:25 INFO TaskSchedulerImpl: Adding task set 10.0 with 200 tasks
15/08/09 15:27:25 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 417, localhost, ANY, 1525 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 418, localhost, ANY, 1526 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 419, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 420, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 421, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 422, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 423, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 424, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 425, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 426, localhost, ANY, 1524 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 10.0 in stage 10.0 (TID 427, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 11.0 in stage 10.0 (TID 428, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 12.0 in stage 10.0 (TID 429, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 13.0 in stage 10.0 (TID 430, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 14.0 in stage 10.0 (TID 431, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 15.0 in stage 10.0 (TID 432, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 0.0 in stage 10.0 (TID 417)
15/08/09 15:27:25 INFO Executor: Running task 3.0 in stage 10.0 (TID 420)
15/08/09 15:27:25 INFO Executor: Running task 2.0 in stage 10.0 (TID 419)
15/08/09 15:27:25 INFO Executor: Running task 1.0 in stage 10.0 (TID 418)
15/08/09 15:27:25 INFO Executor: Running task 5.0 in stage 10.0 (TID 422)
15/08/09 15:27:25 INFO Executor: Running task 6.0 in stage 10.0 (TID 423)
15/08/09 15:27:25 INFO Executor: Running task 9.0 in stage 10.0 (TID 426)
15/08/09 15:27:25 INFO Executor: Running task 4.0 in stage 10.0 (TID 421)
15/08/09 15:27:25 INFO Executor: Running task 13.0 in stage 10.0 (TID 430)
15/08/09 15:27:25 INFO Executor: Running task 10.0 in stage 10.0 (TID 427)
15/08/09 15:27:25 INFO Executor: Running task 12.0 in stage 10.0 (TID 429)
15/08/09 15:27:25 INFO Executor: Running task 15.0 in stage 10.0 (TID 432)
15/08/09 15:27:25 INFO Executor: Running task 11.0 in stage 10.0 (TID 428)
15/08/09 15:27:25 INFO Executor: Running task 8.0 in stage 10.0 (TID 425)
15/08/09 15:27:25 INFO Executor: Running task 7.0 in stage 10.0 (TID 424)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 14.0 in stage 10.0 (TID 431)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 144
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 193
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 126
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 129
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 156
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 138
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 140
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 154
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 8 ms. row count = 142
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/09 15:27:25 INFO Executor: Finished task 2.0 in stage 10.0 (TID 419). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 15.0 in stage 10.0 (TID 432). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 14.0 in stage 10.0 (TID 431). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 167
15/08/09 15:27:25 INFO Executor: Finished task 3.0 in stage 10.0 (TID 420). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 0.0 in stage 10.0 (TID 417). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 4.0 in stage 10.0 (TID 421). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 7.0 in stage 10.0 (TID 424). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 16.0 in stage 10.0 (TID 433, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 11.0 in stage 10.0 (TID 428). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 6.0 in stage 10.0 (TID 423). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 16.0 in stage 10.0 (TID 433)
15/08/09 15:27:25 INFO Executor: Finished task 5.0 in stage 10.0 (TID 422). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 10.0 in stage 10.0 (TID 427). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 12.0 in stage 10.0 (TID 429). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 13.0 in stage 10.0 (TID 430). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 419) in 65 ms on localhost (1/200)
15/08/09 15:27:25 INFO Executor: Finished task 9.0 in stage 10.0 (TID 426). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 17.0 in stage 10.0 (TID 434, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO Executor: Running task 17.0 in stage 10.0 (TID 434)
15/08/09 15:27:25 INFO Executor: Finished task 1.0 in stage 10.0 (TID 418). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 8.0 in stage 10.0 (TID 425). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 15.0 in stage 10.0 (TID 432) in 63 ms on localhost (2/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 18.0 in stage 10.0 (TID 435, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 18.0 in stage 10.0 (TID 435)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 14.0 in stage 10.0 (TID 431) in 65 ms on localhost (3/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 19.0 in stage 10.0 (TID 436, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 19.0 in stage 10.0 (TID 436)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 420) in 73 ms on localhost (4/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 20.0 in stage 10.0 (TID 437, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 20.0 in stage 10.0 (TID 437)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 21.0 in stage 10.0 (TID 438, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 21.0 in stage 10.0 (TID 438)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 417) in 78 ms on localhost (5/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 22.0 in stage 10.0 (TID 439, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 22.0 in stage 10.0 (TID 439)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 421) in 80 ms on localhost (6/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/09 15:27:25 INFO TaskSetManager: Starting task 23.0 in stage 10.0 (TID 440, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 137
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 23.0 in stage 10.0 (TID 440)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 424) in 86 ms on localhost (7/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 11.0 in stage 10.0 (TID 428) in 88 ms on localhost (8/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 171
15/08/09 15:27:25 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 423) in 93 ms on localhost (9/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 121
15/08/09 15:27:25 INFO Executor: Finished task 17.0 in stage 10.0 (TID 434). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 18.0 in stage 10.0 (TID 435). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 24.0 in stage 10.0 (TID 441, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 24.0 in stage 10.0 (TID 441)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 25.0 in stage 10.0 (TID 442, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Running task 25.0 in stage 10.0 (TID 442)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/09 15:27:25 INFO Executor: Finished task 16.0 in stage 10.0 (TID 433). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 19.0 in stage 10.0 (TID 436). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 422) in 99 ms on localhost (10/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 26.0 in stage 10.0 (TID 443, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 26.0 in stage 10.0 (TID 443)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 150
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 20.0 in stage 10.0 (TID 437). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 10.0 in stage 10.0 (TID 427) in 99 ms on localhost (11/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 27.0 in stage 10.0 (TID 444, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 22.0 in stage 10.0 (TID 439). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 27.0 in stage 10.0 (TID 444)
15/08/09 15:27:25 INFO Executor: Finished task 23.0 in stage 10.0 (TID 440). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 12.0 in stage 10.0 (TID 429) in 103 ms on localhost (12/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 28.0 in stage 10.0 (TID 445, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 21.0 in stage 10.0 (TID 438). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 28.0 in stage 10.0 (TID 445)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 13.0 in stage 10.0 (TID 430) in 106 ms on localhost (13/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 29.0 in stage 10.0 (TID 446, localhost, ANY, 1526 bytes)
15/08/09 15:27:25 INFO Executor: Running task 29.0 in stage 10.0 (TID 446)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 426) in 180 ms on localhost (14/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 179
15/08/09 15:27:25 INFO TaskSetManager: Starting task 30.0 in stage 10.0 (TID 447, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 30.0 in stage 10.0 (TID 447)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 31.0 in stage 10.0 (TID 448, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 31.0 in stage 10.0 (TID 448)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 32.0 in stage 10.0 (TID 449, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 32.0 in stage 10.0 (TID 449)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 121
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 33.0 in stage 10.0 (TID 450, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 33.0 in stage 10.0 (TID 450)
15/08/09 15:27:25 INFO BlockManager: Removing broadcast 13
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO BlockManager: Removing block broadcast_13_piece0
15/08/09 15:27:25 INFO MemoryStore: Block broadcast_13_piece0 of size 1941 dropped from memory (free 3332364963)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/09 15:27:25 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:44535 in memory (size: 1941.0 B, free: 3.1 GB)
15/08/09 15:27:25 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/08/09 15:27:25 INFO BlockManager: Removing block broadcast_13
15/08/09 15:27:25 INFO MemoryStore: Block broadcast_13 of size 3240 dropped from memory (free 3332368203)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO ContextCleaner: Cleaned broadcast 13
15/08/09 15:27:25 INFO Executor: Finished task 24.0 in stage 10.0 (TID 441). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 418) in 190 ms on localhost (15/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 26.0 in stage 10.0 (TID 443). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 25.0 in stage 10.0 (TID 442). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 18.0 in stage 10.0 (TID 435) in 122 ms on localhost (16/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/09 15:27:25 INFO TaskSetManager: Finished task 17.0 in stage 10.0 (TID 434) in 127 ms on localhost (17/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 425) in 193 ms on localhost (18/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 34.0 in stage 10.0 (TID 451, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 88
15/08/09 15:27:25 INFO Executor: Running task 34.0 in stage 10.0 (TID 451)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 196
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 124
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 151
15/08/09 15:27:25 INFO TaskSetManager: Finished task 16.0 in stage 10.0 (TID 433) in 138 ms on localhost (19/200)
15/08/09 15:27:25 INFO Executor: Finished task 27.0 in stage 10.0 (TID 444). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 31.0 in stage 10.0 (TID 448). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/09 15:27:25 INFO TaskSetManager: Starting task 35.0 in stage 10.0 (TID 452, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 35.0 in stage 10.0 (TID 452)
15/08/09 15:27:25 INFO Executor: Finished task 29.0 in stage 10.0 (TID 446). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 28.0 in stage 10.0 (TID 445). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 19.0 in stage 10.0 (TID 436) in 131 ms on localhost (20/200)
15/08/09 15:27:25 INFO Executor: Finished task 30.0 in stage 10.0 (TID 447). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 36.0 in stage 10.0 (TID 453, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 149
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 165
15/08/09 15:27:25 INFO Executor: Running task 36.0 in stage 10.0 (TID 453)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 20.0 in stage 10.0 (TID 437) in 131 ms on localhost (21/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 37.0 in stage 10.0 (TID 454, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 37.0 in stage 10.0 (TID 454)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 33.0 in stage 10.0 (TID 450). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 22.0 in stage 10.0 (TID 439) in 127 ms on localhost (22/200)
15/08/09 15:27:25 INFO Executor: Finished task 32.0 in stage 10.0 (TID 449). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 38.0 in stage 10.0 (TID 455, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 38.0 in stage 10.0 (TID 455)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 39.0 in stage 10.0 (TID 456, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 39.0 in stage 10.0 (TID 456)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 23.0 in stage 10.0 (TID 440) in 124 ms on localhost (23/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 21.0 in stage 10.0 (TID 438) in 142 ms on localhost (24/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 40.0 in stage 10.0 (TID 457, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 40.0 in stage 10.0 (TID 457)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 192
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 24.0 in stage 10.0 (TID 441) in 122 ms on localhost (25/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 171
15/08/09 15:27:25 INFO TaskSetManager: Finished task 26.0 in stage 10.0 (TID 443) in 121 ms on localhost (26/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 41.0 in stage 10.0 (TID 458, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 34.0 in stage 10.0 (TID 451). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 41.0 in stage 10.0 (TID 458)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 42.0 in stage 10.0 (TID 459, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Running task 42.0 in stage 10.0 (TID 459)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 25.0 in stage 10.0 (TID 442) in 126 ms on localhost (27/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 35.0 in stage 10.0 (TID 452). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 129
15/08/09 15:27:25 INFO TaskSetManager: Finished task 27.0 in stage 10.0 (TID 444) in 122 ms on localhost (28/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 181
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 43.0 in stage 10.0 (TID 460, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 43.0 in stage 10.0 (TID 460)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 44.0 in stage 10.0 (TID 461, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 134
15/08/09 15:27:25 INFO Executor: Running task 44.0 in stage 10.0 (TID 461)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:25 INFO TaskSetManager: Finished task 31.0 in stage 10.0 (TID 448) in 45 ms on localhost (29/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 37.0 in stage 10.0 (TID 454). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 36.0 in stage 10.0 (TID 453). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 45.0 in stage 10.0 (TID 462, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 45.0 in stage 10.0 (TID 462)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 38.0 in stage 10.0 (TID 455). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/09 15:27:25 INFO Executor: Finished task 39.0 in stage 10.0 (TID 456). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 29.0 in stage 10.0 (TID 446) in 122 ms on localhost (30/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 120
15/08/09 15:27:25 INFO TaskSetManager: Starting task 46.0 in stage 10.0 (TID 463, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 118
15/08/09 15:27:25 INFO Executor: Finished task 40.0 in stage 10.0 (TID 457). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 46.0 in stage 10.0 (TID 463)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 28.0 in stage 10.0 (TID 445) in 128 ms on localhost (31/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 47.0 in stage 10.0 (TID 464, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 42.0 in stage 10.0 (TID 459). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 41.0 in stage 10.0 (TID 458). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 30.0 in stage 10.0 (TID 447) in 55 ms on localhost (32/200)
15/08/09 15:27:25 INFO Executor: Running task 47.0 in stage 10.0 (TID 464)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 48.0 in stage 10.0 (TID 465, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 48.0 in stage 10.0 (TID 465)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 33.0 in stage 10.0 (TID 450) in 54 ms on localhost (33/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/09 15:27:25 INFO TaskSetManager: Starting task 49.0 in stage 10.0 (TID 466, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 49.0 in stage 10.0 (TID 466)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 32.0 in stage 10.0 (TID 449) in 57 ms on localhost (34/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 45.0 in stage 10.0 (TID 462). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 43.0 in stage 10.0 (TID 460). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 44.0 in stage 10.0 (TID 461). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 34.0 in stage 10.0 (TID 451) in 51 ms on localhost (35/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 50.0 in stage 10.0 (TID 467, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 50.0 in stage 10.0 (TID 467)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 51.0 in stage 10.0 (TID 468, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 51.0 in stage 10.0 (TID 468)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/09 15:27:25 INFO TaskSetManager: Finished task 35.0 in stage 10.0 (TID 452) in 49 ms on localhost (36/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 37.0 in stage 10.0 (TID 454) in 46 ms on localhost (37/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 52.0 in stage 10.0 (TID 469, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 46.0 in stage 10.0 (TID 463). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 47.0 in stage 10.0 (TID 464). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 52.0 in stage 10.0 (TID 469)
15/08/09 15:27:25 INFO Executor: Finished task 48.0 in stage 10.0 (TID 465). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 53.0 in stage 10.0 (TID 470, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 95
15/08/09 15:27:25 INFO Executor: Running task 53.0 in stage 10.0 (TID 470)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 36.0 in stage 10.0 (TID 453) in 54 ms on localhost (38/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 54.0 in stage 10.0 (TID 471, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 49.0 in stage 10.0 (TID 466). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 38.0 in stage 10.0 (TID 455) in 53 ms on localhost (39/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/09 15:27:25 INFO Executor: Running task 54.0 in stage 10.0 (TID 471)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 39.0 in stage 10.0 (TID 456) in 54 ms on localhost (40/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:25 INFO TaskSetManager: Starting task 55.0 in stage 10.0 (TID 472, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 55.0 in stage 10.0 (TID 472)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 56.0 in stage 10.0 (TID 473, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 40.0 in stage 10.0 (TID 457) in 48 ms on localhost (41/200)
15/08/09 15:27:25 INFO Executor: Finished task 51.0 in stage 10.0 (TID 468). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 50.0 in stage 10.0 (TID 467). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 56.0 in stage 10.0 (TID 473)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 57.0 in stage 10.0 (TID 474, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 57.0 in stage 10.0 (TID 474)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 42.0 in stage 10.0 (TID 459) in 43 ms on localhost (42/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 124
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 58.0 in stage 10.0 (TID 475, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 41.0 in stage 10.0 (TID 458) in 50 ms on localhost (43/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/09 15:27:25 INFO Executor: Running task 58.0 in stage 10.0 (TID 475)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 118
15/08/09 15:27:25 INFO TaskSetManager: Finished task 45.0 in stage 10.0 (TID 462) in 44 ms on localhost (44/200)
15/08/09 15:27:25 INFO Executor: Finished task 52.0 in stage 10.0 (TID 469). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 59.0 in stage 10.0 (TID 476, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 54.0 in stage 10.0 (TID 471). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 53.0 in stage 10.0 (TID 470). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 59.0 in stage 10.0 (TID 476)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 133
15/08/09 15:27:25 INFO TaskSetManager: Finished task 43.0 in stage 10.0 (TID 460) in 50 ms on localhost (45/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 103
15/08/09 15:27:25 INFO TaskSetManager: Starting task 60.0 in stage 10.0 (TID 477, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 60.0 in stage 10.0 (TID 477)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 61.0 in stage 10.0 (TID 478, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 61.0 in stage 10.0 (TID 478)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/09 15:27:25 INFO TaskSetManager: Finished task 44.0 in stage 10.0 (TID 461) in 56 ms on localhost (46/200)
15/08/09 15:27:25 INFO Executor: Finished task 55.0 in stage 10.0 (TID 472). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 62.0 in stage 10.0 (TID 479, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 62.0 in stage 10.0 (TID 479)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 57.0 in stage 10.0 (TID 474). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 46.0 in stage 10.0 (TID 463) in 54 ms on localhost (47/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 63.0 in stage 10.0 (TID 480, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 63.0 in stage 10.0 (TID 480)
15/08/09 15:27:25 INFO Executor: Finished task 56.0 in stage 10.0 (TID 473). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 47.0 in stage 10.0 (TID 464) in 52 ms on localhost (48/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 64.0 in stage 10.0 (TID 481, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 112
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 64.0 in stage 10.0 (TID 481)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 48.0 in stage 10.0 (TID 465) in 51 ms on localhost (49/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 58.0 in stage 10.0 (TID 475). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 145
15/08/09 15:27:25 INFO Executor: Finished task 59.0 in stage 10.0 (TID 476). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 65.0 in stage 10.0 (TID 482, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 65.0 in stage 10.0 (TID 482)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 203
15/08/09 15:27:25 INFO TaskSetManager: Finished task 49.0 in stage 10.0 (TID 466) in 54 ms on localhost (50/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 66.0 in stage 10.0 (TID 483, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/09 15:27:25 INFO Executor: Finished task 61.0 in stage 10.0 (TID 478). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 66.0 in stage 10.0 (TID 483)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 51.0 in stage 10.0 (TID 468) in 51 ms on localhost (51/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/09 15:27:25 INFO Executor: Finished task 60.0 in stage 10.0 (TID 477). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 67.0 in stage 10.0 (TID 484, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Running task 67.0 in stage 10.0 (TID 484)
15/08/09 15:27:25 INFO Executor: Finished task 62.0 in stage 10.0 (TID 479). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/09 15:27:25 INFO TaskSetManager: Finished task 50.0 in stage 10.0 (TID 467) in 55 ms on localhost (52/200)
15/08/09 15:27:25 INFO Executor: Finished task 63.0 in stage 10.0 (TID 480). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 68.0 in stage 10.0 (TID 485, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 68.0 in stage 10.0 (TID 485)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 69.0 in stage 10.0 (TID 486, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Running task 69.0 in stage 10.0 (TID 486)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 52.0 in stage 10.0 (TID 469) in 52 ms on localhost (53/200)
15/08/09 15:27:25 INFO Executor: Finished task 64.0 in stage 10.0 (TID 481). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 70.0 in stage 10.0 (TID 487, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 70.0 in stage 10.0 (TID 487)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 54.0 in stage 10.0 (TID 471) in 48 ms on localhost (54/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 53.0 in stage 10.0 (TID 470) in 52 ms on localhost (55/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 159
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 55.0 in stage 10.0 (TID 472) in 46 ms on localhost (56/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 71.0 in stage 10.0 (TID 488, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 71.0 in stage 10.0 (TID 488)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 72.0 in stage 10.0 (TID 489, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 72.0 in stage 10.0 (TID 489)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 57.0 in stage 10.0 (TID 474) in 46 ms on localhost (57/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 65.0 in stage 10.0 (TID 482). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 56.0 in stage 10.0 (TID 473) in 49 ms on localhost (58/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 73.0 in stage 10.0 (TID 490, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 73.0 in stage 10.0 (TID 490)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 177
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 74.0 in stage 10.0 (TID 491, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 122
15/08/09 15:27:25 INFO Executor: Running task 74.0 in stage 10.0 (TID 491)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 58.0 in stage 10.0 (TID 475) in 47 ms on localhost (59/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 59.0 in stage 10.0 (TID 476) in 41 ms on localhost (60/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 66.0 in stage 10.0 (TID 483). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 138
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 134
15/08/09 15:27:25 INFO TaskSetManager: Starting task 75.0 in stage 10.0 (TID 492, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 160
15/08/09 15:27:25 INFO Executor: Finished task 68.0 in stage 10.0 (TID 485). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 75.0 in stage 10.0 (TID 492)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 76.0 in stage 10.0 (TID 493, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Running task 76.0 in stage 10.0 (TID 493)
15/08/09 15:27:25 INFO Executor: Finished task 67.0 in stage 10.0 (TID 484). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 61.0 in stage 10.0 (TID 478) in 38 ms on localhost (61/200)
15/08/09 15:27:25 INFO Executor: Finished task 70.0 in stage 10.0 (TID 487). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:25 INFO Executor: Finished task 69.0 in stage 10.0 (TID 486). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 77.0 in stage 10.0 (TID 494, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 77.0 in stage 10.0 (TID 494)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 60.0 in stage 10.0 (TID 477) in 42 ms on localhost (62/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 160
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 148
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 158
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 78.0 in stage 10.0 (TID 495, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 73.0 in stage 10.0 (TID 490). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 78.0 in stage 10.0 (TID 495)
15/08/09 15:27:25 INFO Executor: Finished task 71.0 in stage 10.0 (TID 488). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 62.0 in stage 10.0 (TID 479) in 41 ms on localhost (63/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 79.0 in stage 10.0 (TID 496, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 72.0 in stage 10.0 (TID 489). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 79.0 in stage 10.0 (TID 496)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 74.0 in stage 10.0 (TID 491). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 63.0 in stage 10.0 (TID 480) in 41 ms on localhost (64/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/09 15:27:25 INFO TaskSetManager: Finished task 64.0 in stage 10.0 (TID 481) in 41 ms on localhost (65/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/09 15:27:25 INFO TaskSetManager: Starting task 80.0 in stage 10.0 (TID 497, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 80.0 in stage 10.0 (TID 497)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 81.0 in stage 10.0 (TID 498, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 81.0 in stage 10.0 (TID 498)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 65.0 in stage 10.0 (TID 482) in 45 ms on localhost (66/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 191
15/08/09 15:27:25 INFO TaskSetManager: Starting task 82.0 in stage 10.0 (TID 499, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 82.0 in stage 10.0 (TID 499)
15/08/09 15:27:25 INFO Executor: Finished task 77.0 in stage 10.0 (TID 494). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 76.0 in stage 10.0 (TID 493). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 66.0 in stage 10.0 (TID 483) in 42 ms on localhost (67/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 83.0 in stage 10.0 (TID 500, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 75.0 in stage 10.0 (TID 492). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 83.0 in stage 10.0 (TID 500)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/09 15:27:25 INFO TaskSetManager: Finished task 68.0 in stage 10.0 (TID 485) in 40 ms on localhost (68/200)
15/08/09 15:27:25 INFO Executor: Finished task 78.0 in stage 10.0 (TID 495). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 84.0 in stage 10.0 (TID 501, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 84.0 in stage 10.0 (TID 501)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 85.0 in stage 10.0 (TID 502, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 80.0 in stage 10.0 (TID 497). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 85.0 in stage 10.0 (TID 502)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:25 INFO Executor: Finished task 79.0 in stage 10.0 (TID 496). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 67.0 in stage 10.0 (TID 484) in 46 ms on localhost (69/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 70.0 in stage 10.0 (TID 487) in 42 ms on localhost (70/200)
15/08/09 15:27:25 INFO Executor: Finished task 81.0 in stage 10.0 (TID 498). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 86.0 in stage 10.0 (TID 503, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/09 15:27:25 INFO Executor: Running task 86.0 in stage 10.0 (TID 503)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 69.0 in stage 10.0 (TID 486) in 46 ms on localhost (71/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 87.0 in stage 10.0 (TID 504, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:25 INFO Executor: Running task 87.0 in stage 10.0 (TID 504)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 82.0 in stage 10.0 (TID 499). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 73.0 in stage 10.0 (TID 490) in 39 ms on localhost (72/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 163
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/09 15:27:25 INFO TaskSetManager: Starting task 88.0 in stage 10.0 (TID 505, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 88.0 in stage 10.0 (TID 505)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 83.0 in stage 10.0 (TID 500). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 71.0 in stage 10.0 (TID 488) in 46 ms on localhost (73/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 89.0 in stage 10.0 (TID 506, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 85.0 in stage 10.0 (TID 502). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 89.0 in stage 10.0 (TID 506)
15/08/09 15:27:25 INFO Executor: Finished task 84.0 in stage 10.0 (TID 501). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 72.0 in stage 10.0 (TID 489) in 48 ms on localhost (74/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 141
15/08/09 15:27:25 INFO TaskSetManager: Starting task 90.0 in stage 10.0 (TID 507, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO Executor: Running task 90.0 in stage 10.0 (TID 507)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 74.0 in stage 10.0 (TID 491) in 49 ms on localhost (75/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 91.0 in stage 10.0 (TID 508, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 91.0 in stage 10.0 (TID 508)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/09 15:27:25 INFO TaskSetManager: Finished task 77.0 in stage 10.0 (TID 494) in 43 ms on localhost (76/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 92.0 in stage 10.0 (TID 509, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 92.0 in stage 10.0 (TID 509)
15/08/09 15:27:25 INFO Executor: Finished task 86.0 in stage 10.0 (TID 503). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 200
15/08/09 15:27:25 INFO TaskSetManager: Finished task 76.0 in stage 10.0 (TID 493) in 48 ms on localhost (77/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:25 INFO TaskSetManager: Starting task 93.0 in stage 10.0 (TID 510, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 87.0 in stage 10.0 (TID 504). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 93.0 in stage 10.0 (TID 510)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 75.0 in stage 10.0 (TID 492) in 53 ms on localhost (78/200)
15/08/09 15:27:25 INFO Executor: Finished task 88.0 in stage 10.0 (TID 505). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 94.0 in stage 10.0 (TID 511, localhost, ANY, 1526 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 94.0 in stage 10.0 (TID 511)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 78.0 in stage 10.0 (TID 495) in 48 ms on localhost (79/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 95.0 in stage 10.0 (TID 512, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 173
15/08/09 15:27:25 INFO Executor: Running task 95.0 in stage 10.0 (TID 512)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 119
15/08/09 15:27:25 INFO Executor: Finished task 89.0 in stage 10.0 (TID 506). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 80.0 in stage 10.0 (TID 497) in 44 ms on localhost (80/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 96.0 in stage 10.0 (TID 513, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:25 INFO Executor: Running task 96.0 in stage 10.0 (TID 513)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 97.0 in stage 10.0 (TID 514, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 90.0 in stage 10.0 (TID 507). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 91.0 in stage 10.0 (TID 508). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 97.0 in stage 10.0 (TID 514)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 92.0 in stage 10.0 (TID 509). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 79.0 in stage 10.0 (TID 496) in 50 ms on localhost (81/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/09 15:27:25 INFO Executor: Finished task 93.0 in stage 10.0 (TID 510). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 81.0 in stage 10.0 (TID 498) in 49 ms on localhost (82/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 98.0 in stage 10.0 (TID 515, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 98.0 in stage 10.0 (TID 515)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 82.0 in stage 10.0 (TID 499) in 46 ms on localhost (83/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 99.0 in stage 10.0 (TID 516, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 94.0 in stage 10.0 (TID 511). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/09 15:27:25 INFO TaskSetManager: Finished task 83.0 in stage 10.0 (TID 500) in 47 ms on localhost (84/200)
15/08/09 15:27:25 INFO Executor: Running task 99.0 in stage 10.0 (TID 516)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 100.0 in stage 10.0 (TID 517, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 100.0 in stage 10.0 (TID 517)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/09 15:27:25 INFO TaskSetManager: Finished task 85.0 in stage 10.0 (TID 502) in 46 ms on localhost (85/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 96.0 in stage 10.0 (TID 513). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 84.0 in stage 10.0 (TID 501) in 48 ms on localhost (86/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 101.0 in stage 10.0 (TID 518, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:25 INFO Executor: Running task 101.0 in stage 10.0 (TID 518)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 86.0 in stage 10.0 (TID 503) in 46 ms on localhost (87/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 102.0 in stage 10.0 (TID 519, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 206
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 103.0 in stage 10.0 (TID 520, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 102.0 in stage 10.0 (TID 519)
15/08/09 15:27:25 INFO Executor: Running task 103.0 in stage 10.0 (TID 520)
15/08/09 15:27:25 INFO Executor: Finished task 97.0 in stage 10.0 (TID 514). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 95.0 in stage 10.0 (TID 512). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 87.0 in stage 10.0 (TID 504) in 46 ms on localhost (88/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 104.0 in stage 10.0 (TID 521, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 104.0 in stage 10.0 (TID 521)
15/08/09 15:27:25 INFO Executor: Finished task 98.0 in stage 10.0 (TID 515). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 88.0 in stage 10.0 (TID 505) in 46 ms on localhost (89/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 89.0 in stage 10.0 (TID 506) in 43 ms on localhost (90/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 105.0 in stage 10.0 (TID 522, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 105.0 in stage 10.0 (TID 522)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 162
15/08/09 15:27:25 INFO TaskSetManager: Finished task 90.0 in stage 10.0 (TID 507) in 43 ms on localhost (91/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 106.0 in stage 10.0 (TID 523, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 106.0 in stage 10.0 (TID 523)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/09 15:27:25 INFO Executor: Finished task 99.0 in stage 10.0 (TID 516). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:25 INFO TaskSetManager: Starting task 107.0 in stage 10.0 (TID 524, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 107.0 in stage 10.0 (TID 524)
15/08/09 15:27:25 INFO Executor: Finished task 100.0 in stage 10.0 (TID 517). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 174
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 91.0 in stage 10.0 (TID 508) in 42 ms on localhost (92/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 108.0 in stage 10.0 (TID 525, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:25 INFO Executor: Finished task 103.0 in stage 10.0 (TID 520). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 176
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 108.0 in stage 10.0 (TID 525)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 92.0 in stage 10.0 (TID 509) in 42 ms on localhost (93/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/09 15:27:25 INFO Executor: Finished task 101.0 in stage 10.0 (TID 518). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 109.0 in stage 10.0 (TID 526, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 102.0 in stage 10.0 (TID 519). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 93.0 in stage 10.0 (TID 510) in 42 ms on localhost (94/200)
15/08/09 15:27:25 INFO Executor: Running task 109.0 in stage 10.0 (TID 526)
15/08/09 15:27:25 INFO Executor: Finished task 104.0 in stage 10.0 (TID 521). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 110.0 in stage 10.0 (TID 527, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 105.0 in stage 10.0 (TID 522). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 110.0 in stage 10.0 (TID 527)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 94.0 in stage 10.0 (TID 511) in 42 ms on localhost (95/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 111.0 in stage 10.0 (TID 528, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 111.0 in stage 10.0 (TID 528)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 172
15/08/09 15:27:25 INFO TaskSetManager: Finished task 96.0 in stage 10.0 (TID 513) in 39 ms on localhost (96/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 167
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:25 INFO TaskSetManager: Starting task 112.0 in stage 10.0 (TID 529, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 97.0 in stage 10.0 (TID 514) in 39 ms on localhost (97/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/09 15:27:25 INFO Executor: Running task 112.0 in stage 10.0 (TID 529)
15/08/09 15:27:25 INFO Executor: Finished task 107.0 in stage 10.0 (TID 524). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 95.0 in stage 10.0 (TID 512) in 44 ms on localhost (98/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 106.0 in stage 10.0 (TID 523). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 113.0 in stage 10.0 (TID 530, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 113.0 in stage 10.0 (TID 530)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 114.0 in stage 10.0 (TID 531, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 114.0 in stage 10.0 (TID 531)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 108.0 in stage 10.0 (TID 525). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 98.0 in stage 10.0 (TID 515) in 41 ms on localhost (99/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:25 INFO TaskSetManager: Starting task 115.0 in stage 10.0 (TID 532, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 115.0 in stage 10.0 (TID 532)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 99.0 in stage 10.0 (TID 516) in 40 ms on localhost (100/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:25 INFO TaskSetManager: Starting task 116.0 in stage 10.0 (TID 533, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:25 INFO Executor: Running task 116.0 in stage 10.0 (TID 533)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 100.0 in stage 10.0 (TID 517) in 40 ms on localhost (101/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 110.0 in stage 10.0 (TID 527). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 109.0 in stage 10.0 (TID 526). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 117.0 in stage 10.0 (TID 534, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO Executor: Running task 117.0 in stage 10.0 (TID 534)
15/08/09 15:27:25 INFO Executor: Finished task 112.0 in stage 10.0 (TID 529). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 103.0 in stage 10.0 (TID 520) in 38 ms on localhost (102/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 118.0 in stage 10.0 (TID 535, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 118.0 in stage 10.0 (TID 535)
15/08/09 15:27:25 INFO Executor: Finished task 111.0 in stage 10.0 (TID 528). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 101.0 in stage 10.0 (TID 518) in 44 ms on localhost (103/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 119.0 in stage 10.0 (TID 536, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 119.0 in stage 10.0 (TID 536)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 102.0 in stage 10.0 (TID 519) in 42 ms on localhost (104/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 120.0 in stage 10.0 (TID 537, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 160
15/08/09 15:27:25 INFO Executor: Finished task 114.0 in stage 10.0 (TID 531). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/09 15:27:25 INFO TaskSetManager: Finished task 104.0 in stage 10.0 (TID 521) in 42 ms on localhost (105/200)
15/08/09 15:27:25 INFO Executor: Running task 120.0 in stage 10.0 (TID 537)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 121.0 in stage 10.0 (TID 538, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO Executor: Running task 121.0 in stage 10.0 (TID 538)
15/08/09 15:27:25 INFO Executor: Finished task 115.0 in stage 10.0 (TID 532). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 105.0 in stage 10.0 (TID 522) in 45 ms on localhost (106/200)
15/08/09 15:27:25 INFO Executor: Finished task 113.0 in stage 10.0 (TID 530). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 122.0 in stage 10.0 (TID 539, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 122.0 in stage 10.0 (TID 539)
15/08/09 15:27:25 INFO Executor: Finished task 116.0 in stage 10.0 (TID 533). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 107.0 in stage 10.0 (TID 524) in 45 ms on localhost (107/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 123.0 in stage 10.0 (TID 540, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 123.0 in stage 10.0 (TID 540)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 138
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 189
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/09 15:27:25 INFO TaskSetManager: Finished task 106.0 in stage 10.0 (TID 523) in 50 ms on localhost (108/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 124.0 in stage 10.0 (TID 541, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 124.0 in stage 10.0 (TID 541)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 119.0 in stage 10.0 (TID 536). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 108.0 in stage 10.0 (TID 525) in 49 ms on localhost (109/200)
15/08/09 15:27:25 INFO Executor: Finished task 117.0 in stage 10.0 (TID 534). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 125.0 in stage 10.0 (TID 542, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 118.0 in stage 10.0 (TID 535). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 125.0 in stage 10.0 (TID 542)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 110.0 in stage 10.0 (TID 527) in 53 ms on localhost (110/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 109.0 in stage 10.0 (TID 526) in 57 ms on localhost (111/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 126.0 in stage 10.0 (TID 543, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 126.0 in stage 10.0 (TID 543)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 127.0 in stage 10.0 (TID 544, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 127.0 in stage 10.0 (TID 544)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 128.0 in stage 10.0 (TID 545, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 128.0 in stage 10.0 (TID 545)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 111.0 in stage 10.0 (TID 528) in 56 ms on localhost (112/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 112.0 in stage 10.0 (TID 529) in 56 ms on localhost (113/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 129.0 in stage 10.0 (TID 546, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 129.0 in stage 10.0 (TID 546)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 114.0 in stage 10.0 (TID 531) in 53 ms on localhost (114/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 115.0 in stage 10.0 (TID 532) in 50 ms on localhost (115/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 130.0 in stage 10.0 (TID 547, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 130.0 in stage 10.0 (TID 547)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 131.0 in stage 10.0 (TID 548, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 131.0 in stage 10.0 (TID 548)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 113.0 in stage 10.0 (TID 530) in 61 ms on localhost (116/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 132.0 in stage 10.0 (TID 549, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 132.0 in stage 10.0 (TID 549)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 133.0 in stage 10.0 (TID 550, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 133.0 in stage 10.0 (TID 550)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 134.0 in stage 10.0 (TID 551, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 135.0 in stage 10.0 (TID 552, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Running task 134.0 in stage 10.0 (TID 551)
15/08/09 15:27:25 INFO Executor: Running task 135.0 in stage 10.0 (TID 552)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 117.0 in stage 10.0 (TID 534) in 54 ms on localhost (117/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 153
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 20 ms. row count = 175
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 19 ms. row count = 192
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 119.0 in stage 10.0 (TID 536) in 49 ms on localhost (118/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 120.0 in stage 10.0 (TID 537). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 122.0 in stage 10.0 (TID 539). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 118.0 in stage 10.0 (TID 535) in 53 ms on localhost (119/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 116.0 in stage 10.0 (TID 533) in 63 ms on localhost (120/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 181
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 121.0 in stage 10.0 (TID 538). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 163
15/08/09 15:27:25 INFO TaskSetManager: Starting task 136.0 in stage 10.0 (TID 553, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 123
15/08/09 15:27:25 INFO Executor: Finished task 128.0 in stage 10.0 (TID 545). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 120.0 in stage 10.0 (TID 537) in 55 ms on localhost (121/200)
15/08/09 15:27:25 INFO Executor: Running task 136.0 in stage 10.0 (TID 553)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 131.0 in stage 10.0 (TID 548). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 180
15/08/09 15:27:25 INFO Executor: Finished task 124.0 in stage 10.0 (TID 541). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 129.0 in stage 10.0 (TID 546). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 137.0 in stage 10.0 (TID 554, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/09 15:27:25 INFO Executor: Finished task 125.0 in stage 10.0 (TID 542). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 137.0 in stage 10.0 (TID 554)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/09 15:27:25 INFO TaskSetManager: Finished task 122.0 in stage 10.0 (TID 539) in 50 ms on localhost (122/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 138.0 in stage 10.0 (TID 555, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 130.0 in stage 10.0 (TID 547). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 139.0 in stage 10.0 (TID 556, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 138.0 in stage 10.0 (TID 555)
15/08/09 15:27:25 INFO Executor: Finished task 123.0 in stage 10.0 (TID 540). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 139.0 in stage 10.0 (TID 556)
15/08/09 15:27:25 INFO Executor: Finished task 126.0 in stage 10.0 (TID 543). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 135.0 in stage 10.0 (TID 552). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/09 15:27:25 INFO Executor: Finished task 127.0 in stage 10.0 (TID 544). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:25 INFO TaskSetManager: Finished task 128.0 in stage 10.0 (TID 545) in 32 ms on localhost (123/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 134.0 in stage 10.0 (TID 551). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 132.0 in stage 10.0 (TID 549). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 121.0 in stage 10.0 (TID 538) in 60 ms on localhost (124/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 133.0 in stage 10.0 (TID 550). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 140.0 in stage 10.0 (TID 557, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 140.0 in stage 10.0 (TID 557)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 141.0 in stage 10.0 (TID 558, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 141.0 in stage 10.0 (TID 558)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 131.0 in stage 10.0 (TID 548) in 30 ms on localhost (125/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 130
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 142.0 in stage 10.0 (TID 559, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 142.0 in stage 10.0 (TID 559)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 143.0 in stage 10.0 (TID 560, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 143.0 in stage 10.0 (TID 560)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 144.0 in stage 10.0 (TID 561, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 144.0 in stage 10.0 (TID 561)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 123
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 113
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 124.0 in stage 10.0 (TID 541) in 61 ms on localhost (126/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 141
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 130.0 in stage 10.0 (TID 547) in 42 ms on localhost (127/200)
15/08/09 15:27:25 INFO Executor: Finished task 136.0 in stage 10.0 (TID 553). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:25 INFO Executor: Finished task 137.0 in stage 10.0 (TID 554). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 138.0 in stage 10.0 (TID 555). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 125.0 in stage 10.0 (TID 542) in 61 ms on localhost (128/200)
15/08/09 15:27:25 INFO Executor: Finished task 139.0 in stage 10.0 (TID 556). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/09 15:27:25 INFO TaskSetManager: Finished task 129.0 in stage 10.0 (TID 546) in 51 ms on localhost (129/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 123.0 in stage 10.0 (TID 540) in 77 ms on localhost (130/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 145.0 in stage 10.0 (TID 562, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 145.0 in stage 10.0 (TID 562)
15/08/09 15:27:25 INFO Executor: Finished task 140.0 in stage 10.0 (TID 557). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 146.0 in stage 10.0 (TID 563, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/09 15:27:25 INFO Executor: Running task 146.0 in stage 10.0 (TID 563)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 126.0 in stage 10.0 (TID 543) in 64 ms on localhost (131/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 111
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 169
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 155
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 147.0 in stage 10.0 (TID 564, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 147.0 in stage 10.0 (TID 564)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 148.0 in stage 10.0 (TID 565, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 148.0 in stage 10.0 (TID 565)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 135.0 in stage 10.0 (TID 552) in 54 ms on localhost (132/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 127.0 in stage 10.0 (TID 544) in 73 ms on localhost (133/200)
15/08/09 15:27:25 INFO Executor: Finished task 143.0 in stage 10.0 (TID 560). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 144.0 in stage 10.0 (TID 561). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 134.0 in stage 10.0 (TID 551) in 60 ms on localhost (134/200)
15/08/09 15:27:25 INFO Executor: Finished task 141.0 in stage 10.0 (TID 558). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 142.0 in stage 10.0 (TID 559). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 149.0 in stage 10.0 (TID 566, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 149.0 in stage 10.0 (TID 566)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 132.0 in stage 10.0 (TID 549) in 67 ms on localhost (135/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:25 INFO TaskSetManager: Starting task 150.0 in stage 10.0 (TID 567, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:25 INFO Executor: Running task 150.0 in stage 10.0 (TID 567)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 151.0 in stage 10.0 (TID 568, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 151.0 in stage 10.0 (TID 568)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:25 INFO TaskSetManager: Finished task 133.0 in stage 10.0 (TID 550) in 69 ms on localhost (136/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 152.0 in stage 10.0 (TID 569, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO Executor: Running task 152.0 in stage 10.0 (TID 569)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 136.0 in stage 10.0 (TID 553) in 62 ms on localhost (137/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 153.0 in stage 10.0 (TID 570, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 145.0 in stage 10.0 (TID 562). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 137.0 in stage 10.0 (TID 554) in 62 ms on localhost (138/200)
15/08/09 15:27:25 INFO Executor: Finished task 147.0 in stage 10.0 (TID 564). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 146.0 in stage 10.0 (TID 563). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 148.0 in stage 10.0 (TID 565). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 153.0 in stage 10.0 (TID 570)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 154.0 in stage 10.0 (TID 571, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 154.0 in stage 10.0 (TID 571)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 138.0 in stage 10.0 (TID 555) in 61 ms on localhost (139/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 155.0 in stage 10.0 (TID 572, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 155.0 in stage 10.0 (TID 572)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 139.0 in stage 10.0 (TID 556) in 62 ms on localhost (140/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 156.0 in stage 10.0 (TID 573, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 156.0 in stage 10.0 (TID 573)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 190
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 113
15/08/09 15:27:25 INFO TaskSetManager: Finished task 140.0 in stage 10.0 (TID 557) in 59 ms on localhost (141/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/09 15:27:25 INFO Executor: Finished task 149.0 in stage 10.0 (TID 566). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 157.0 in stage 10.0 (TID 574, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/09 15:27:25 INFO Executor: Finished task 150.0 in stage 10.0 (TID 567). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 143.0 in stage 10.0 (TID 560) in 53 ms on localhost (142/200)
15/08/09 15:27:25 INFO Executor: Running task 157.0 in stage 10.0 (TID 574)
15/08/09 15:27:25 INFO Executor: Finished task 151.0 in stage 10.0 (TID 568). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 158.0 in stage 10.0 (TID 575, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 153.0 in stage 10.0 (TID 570). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Running task 158.0 in stage 10.0 (TID 575)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 152.0 in stage 10.0 (TID 569). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 144.0 in stage 10.0 (TID 561) in 54 ms on localhost (143/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 159.0 in stage 10.0 (TID 576, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/09 15:27:25 INFO TaskSetManager: Starting task 160.0 in stage 10.0 (TID 577, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 160.0 in stage 10.0 (TID 577)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 141.0 in stage 10.0 (TID 558) in 64 ms on localhost (144/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 142.0 in stage 10.0 (TID 559) in 60 ms on localhost (145/200)
15/08/09 15:27:25 INFO Executor: Running task 159.0 in stage 10.0 (TID 576)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 161.0 in stage 10.0 (TID 578, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 154.0 in stage 10.0 (TID 571). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 149
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 162.0 in stage 10.0 (TID 579, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 163.0 in stage 10.0 (TID 580, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 163.0 in stage 10.0 (TID 580)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 164.0 in stage 10.0 (TID 581, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 164.0 in stage 10.0 (TID 581)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 148.0 in stage 10.0 (TID 565) in 42 ms on localhost (146/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 145.0 in stage 10.0 (TID 562) in 49 ms on localhost (147/200)
15/08/09 15:27:25 INFO Executor: Running task 162.0 in stage 10.0 (TID 579)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 165.0 in stage 10.0 (TID 582, localhost, ANY, 1531 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 155.0 in stage 10.0 (TID 572). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 165.0 in stage 10.0 (TID 582)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 166.0 in stage 10.0 (TID 583, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 166.0 in stage 10.0 (TID 583)
15/08/09 15:27:25 INFO Executor: Finished task 156.0 in stage 10.0 (TID 573). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:25 INFO TaskSetManager: Starting task 167.0 in stage 10.0 (TID 584, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 161.0 in stage 10.0 (TID 578)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 168.0 in stage 10.0 (TID 585, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Running task 168.0 in stage 10.0 (TID 585)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 169.0 in stage 10.0 (TID 586, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 169.0 in stage 10.0 (TID 586)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 167.0 in stage 10.0 (TID 584)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 170.0 in stage 10.0 (TID 587, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 159
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 146.0 in stage 10.0 (TID 563) in 59 ms on localhost (148/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:25 INFO TaskSetManager: Finished task 147.0 in stage 10.0 (TID 564) in 60 ms on localhost (149/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 171.0 in stage 10.0 (TID 588, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 171.0 in stage 10.0 (TID 588)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 172.0 in stage 10.0 (TID 589, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 170.0 in stage 10.0 (TID 587)
15/08/09 15:27:25 INFO Executor: Running task 172.0 in stage 10.0 (TID 589)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 151.0 in stage 10.0 (TID 568) in 45 ms on localhost (150/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 150.0 in stage 10.0 (TID 567) in 48 ms on localhost (151/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/09 15:27:25 INFO Executor: Finished task 164.0 in stage 10.0 (TID 581). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 149.0 in stage 10.0 (TID 566) in 56 ms on localhost (152/200)
15/08/09 15:27:25 INFO Executor: Finished task 157.0 in stage 10.0 (TID 574). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 158.0 in stage 10.0 (TID 575). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 154.0 in stage 10.0 (TID 571) in 41 ms on localhost (153/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 172
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 160.0 in stage 10.0 (TID 577). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 152.0 in stage 10.0 (TID 569) in 49 ms on localhost (154/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 110
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/09 15:27:25 INFO TaskSetManager: Finished task 153.0 in stage 10.0 (TID 570) in 46 ms on localhost (155/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 126
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 136
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 155
15/08/09 15:27:25 INFO TaskSetManager: Starting task 173.0 in stage 10.0 (TID 590, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO Executor: Running task 173.0 in stage 10.0 (TID 590)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 116
15/08/09 15:27:25 INFO TaskSetManager: Finished task 156.0 in stage 10.0 (TID 573) in 43 ms on localhost (156/200)
15/08/09 15:27:25 INFO Executor: Finished task 166.0 in stage 10.0 (TID 583). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 159.0 in stage 10.0 (TID 576). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 155.0 in stage 10.0 (TID 572) in 47 ms on localhost (157/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 168.0 in stage 10.0 (TID 585). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 169.0 in stage 10.0 (TID 586). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 162.0 in stage 10.0 (TID 579). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:25 INFO TaskSetManager: Finished task 164.0 in stage 10.0 (TID 581) in 33 ms on localhost (158/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 186
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 157.0 in stage 10.0 (TID 574) in 43 ms on localhost (159/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 161.0 in stage 10.0 (TID 578). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/09 15:27:25 INFO Executor: Finished task 165.0 in stage 10.0 (TID 582). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 172.0 in stage 10.0 (TID 589). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 163.0 in stage 10.0 (TID 580). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 170.0 in stage 10.0 (TID 587). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 174.0 in stage 10.0 (TID 591, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 174.0 in stage 10.0 (TID 591)
15/08/09 15:27:25 INFO Executor: Finished task 171.0 in stage 10.0 (TID 588). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 175.0 in stage 10.0 (TID 592, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 215
15/08/09 15:27:25 INFO Executor: Running task 175.0 in stage 10.0 (TID 592)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 158.0 in stage 10.0 (TID 575) in 49 ms on localhost (160/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 176.0 in stage 10.0 (TID 593, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Finished task 167.0 in stage 10.0 (TID 584). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Running task 176.0 in stage 10.0 (TID 593)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 160.0 in stage 10.0 (TID 577) in 51 ms on localhost (161/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 166
15/08/09 15:27:25 INFO TaskSetManager: Starting task 177.0 in stage 10.0 (TID 594, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 166.0 in stage 10.0 (TID 583) in 43 ms on localhost (162/200)
15/08/09 15:27:25 INFO Executor: Running task 177.0 in stage 10.0 (TID 594)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 159.0 in stage 10.0 (TID 576) in 55 ms on localhost (163/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 178.0 in stage 10.0 (TID 595, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 178.0 in stage 10.0 (TID 595)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 173.0 in stage 10.0 (TID 590). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 179.0 in stage 10.0 (TID 596, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 168.0 in stage 10.0 (TID 585) in 43 ms on localhost (164/200)
15/08/09 15:27:25 INFO Executor: Running task 179.0 in stage 10.0 (TID 596)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/09 15:27:25 INFO TaskSetManager: Finished task 169.0 in stage 10.0 (TID 586) in 43 ms on localhost (165/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 180.0 in stage 10.0 (TID 597, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 180.0 in stage 10.0 (TID 597)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 181.0 in stage 10.0 (TID 598, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO Executor: Running task 181.0 in stage 10.0 (TID 598)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 162.0 in stage 10.0 (TID 579) in 57 ms on localhost (166/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 174.0 in stage 10.0 (TID 591). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 161.0 in stage 10.0 (TID 578) in 60 ms on localhost (167/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 197
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 182.0 in stage 10.0 (TID 599, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 182.0 in stage 10.0 (TID 599)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 165.0 in stage 10.0 (TID 582) in 56 ms on localhost (168/200)
15/08/09 15:27:25 INFO Executor: Finished task 175.0 in stage 10.0 (TID 592). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/09 15:27:25 INFO Executor: Finished task 176.0 in stage 10.0 (TID 593). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 183.0 in stage 10.0 (TID 600, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 183.0 in stage 10.0 (TID 600)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 246
15/08/09 15:27:25 INFO TaskSetManager: Finished task 172.0 in stage 10.0 (TID 589) in 44 ms on localhost (169/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Starting task 184.0 in stage 10.0 (TID 601, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 185.0 in stage 10.0 (TID 602, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 185.0 in stage 10.0 (TID 602)
15/08/09 15:27:25 INFO Executor: Running task 184.0 in stage 10.0 (TID 601)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 178.0 in stage 10.0 (TID 595). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 177.0 in stage 10.0 (TID 594). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 170.0 in stage 10.0 (TID 587) in 55 ms on localhost (170/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Starting task 186.0 in stage 10.0 (TID 603, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 186.0 in stage 10.0 (TID 603)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/09 15:27:25 INFO TaskSetManager: Starting task 187.0 in stage 10.0 (TID 604, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO Executor: Running task 187.0 in stage 10.0 (TID 604)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO TaskSetManager: Finished task 163.0 in stage 10.0 (TID 580) in 71 ms on localhost (171/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 171.0 in stage 10.0 (TID 588) in 54 ms on localhost (172/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 167.0 in stage 10.0 (TID 584) in 66 ms on localhost (173/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 188.0 in stage 10.0 (TID 605, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 188.0 in stage 10.0 (TID 605)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 176
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 133
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 138
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 173.0 in stage 10.0 (TID 590) in 48 ms on localhost (174/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 194
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 179.0 in stage 10.0 (TID 596). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 189.0 in stage 10.0 (TID 606, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 189.0 in stage 10.0 (TID 606)
15/08/09 15:27:25 INFO Executor: Finished task 181.0 in stage 10.0 (TID 598). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 174.0 in stage 10.0 (TID 591) in 44 ms on localhost (175/200)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 190.0 in stage 10.0 (TID 607, localhost, ANY, 1528 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 190.0 in stage 10.0 (TID 607)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 187.0 in stage 10.0 (TID 604). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 130
15/08/09 15:27:25 INFO Executor: Finished task 180.0 in stage 10.0 (TID 597). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Starting task 191.0 in stage 10.0 (TID 608, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 147
15/08/09 15:27:25 INFO Executor: Running task 191.0 in stage 10.0 (TID 608)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 192.0 in stage 10.0 (TID 609, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Running task 192.0 in stage 10.0 (TID 609)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 175.0 in stage 10.0 (TID 592) in 46 ms on localhost (176/200)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 185.0 in stage 10.0 (TID 602). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 176.0 in stage 10.0 (TID 593) in 41 ms on localhost (177/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 135
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:25 INFO Executor: Finished task 182.0 in stage 10.0 (TID 599). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Starting task 193.0 in stage 10.0 (TID 610, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 193.0 in stage 10.0 (TID 610)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 178.0 in stage 10.0 (TID 595) in 38 ms on localhost (178/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 145
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 184
15/08/09 15:27:25 INFO TaskSetManager: Starting task 194.0 in stage 10.0 (TID 611, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 194.0 in stage 10.0 (TID 611)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 195.0 in stage 10.0 (TID 612, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO Executor: Running task 195.0 in stage 10.0 (TID 612)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 196.0 in stage 10.0 (TID 613, localhost, ANY, 1530 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 197.0 in stage 10.0 (TID 614, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO Executor: Running task 197.0 in stage 10.0 (TID 614)
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 134
15/08/09 15:27:25 INFO Executor: Running task 196.0 in stage 10.0 (TID 613)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 177.0 in stage 10.0 (TID 594) in 48 ms on localhost (179/200)
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 183.0 in stage 10.0 (TID 600). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/09 15:27:25 INFO TaskSetManager: Finished task 187.0 in stage 10.0 (TID 604) in 28 ms on localhost (180/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO Executor: Finished task 188.0 in stage 10.0 (TID 605). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 181.0 in stage 10.0 (TID 598) in 43 ms on localhost (181/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/09 15:27:25 INFO Executor: Finished task 189.0 in stage 10.0 (TID 606). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 184.0 in stage 10.0 (TID 601). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 179.0 in stage 10.0 (TID 596) in 49 ms on localhost (182/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 180.0 in stage 10.0 (TID 597) in 48 ms on localhost (183/200)
15/08/09 15:27:25 INFO Executor: Finished task 190.0 in stage 10.0 (TID 607). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 18 ms. row count = 178
15/08/09 15:27:25 INFO TaskSetManager: Starting task 198.0 in stage 10.0 (TID 615, localhost, ANY, 1527 bytes)
15/08/09 15:27:25 INFO Executor: Running task 198.0 in stage 10.0 (TID 615)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 192.0 in stage 10.0 (TID 609). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/09 15:27:25 INFO TaskSetManager: Starting task 199.0 in stage 10.0 (TID 616, localhost, ANY, 1529 bytes)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:25 INFO Executor: Running task 199.0 in stage 10.0 (TID 616)
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 131
15/08/09 15:27:25 INFO Executor: Finished task 186.0 in stage 10.0 (TID 603). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 185.0 in stage 10.0 (TID 602) in 41 ms on localhost (184/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 182.0 in stage 10.0 (TID 599) in 49 ms on localhost (185/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/09 15:27:25 INFO Executor: Finished task 194.0 in stage 10.0 (TID 611). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 183.0 in stage 10.0 (TID 600) in 48 ms on localhost (186/200)
15/08/09 15:27:25 INFO Executor: Finished task 195.0 in stage 10.0 (TID 612). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO Executor: Finished task 197.0 in stage 10.0 (TID 614). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:25 INFO Executor: Finished task 196.0 in stage 10.0 (TID 613). 1819 bytes result sent to driver
15/08/09 15:27:25 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:25 INFO TaskSetManager: Finished task 188.0 in stage 10.0 (TID 605) in 37 ms on localhost (187/200)
15/08/09 15:27:25 INFO Executor: Finished task 193.0 in stage 10.0 (TID 610). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 189.0 in stage 10.0 (TID 606) in 32 ms on localhost (188/200)
15/08/09 15:27:25 INFO Executor: Finished task 191.0 in stage 10.0 (TID 608). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO TaskSetManager: Finished task 184.0 in stage 10.0 (TID 601) in 49 ms on localhost (189/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 190.0 in stage 10.0 (TID 607) in 32 ms on localhost (190/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 192.0 in stage 10.0 (TID 609) in 32 ms on localhost (191/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 175
15/08/09 15:27:25 INFO TaskSetManager: Finished task 194.0 in stage 10.0 (TID 611) in 25 ms on localhost (192/200)
15/08/09 15:27:25 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:25 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:25 INFO Executor: Finished task 198.0 in stage 10.0 (TID 615). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/09 15:27:25 INFO TaskSetManager: Finished task 195.0 in stage 10.0 (TID 612) in 26 ms on localhost (193/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 186.0 in stage 10.0 (TID 603) in 56 ms on localhost (194/200)
15/08/09 15:27:25 INFO Executor: Finished task 199.0 in stage 10.0 (TID 616). 1819 bytes result sent to driver
15/08/09 15:27:25 INFO TaskSetManager: Finished task 197.0 in stage 10.0 (TID 614) in 32 ms on localhost (195/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 193.0 in stage 10.0 (TID 610) in 42 ms on localhost (196/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 196.0 in stage 10.0 (TID 613) in 36 ms on localhost (197/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 191.0 in stage 10.0 (TID 608) in 54 ms on localhost (198/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 198.0 in stage 10.0 (TID 615) in 35 ms on localhost (199/200)
15/08/09 15:27:25 INFO TaskSetManager: Finished task 199.0 in stage 10.0 (TID 616) in 35 ms on localhost (200/200)
15/08/09 15:27:25 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/08/09 15:27:25 INFO DAGScheduler: Stage 10 (mapPartitions at Exchange.scala:100) finished in 0.721 s
15/08/09 15:27:25 INFO DAGScheduler: looking for newly runnable stages
15/08/09 15:27:25 INFO DAGScheduler: running: Set()
15/08/09 15:27:25 INFO DAGScheduler: waiting: Set(Stage 11)
15/08/09 15:27:25 INFO DAGScheduler: failed: Set()
15/08/09 15:27:25 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@48ea44c8
15/08/09 15:27:25 INFO StatsReportListener: task runtime:(count: 200, mean: 58.295000, stdev: 28.245495, max: 193.000000, min: 25.000000)
15/08/09 15:27:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:25 INFO StatsReportListener: 	25.0 ms	35.0 ms	40.0 ms	43.0 ms	49.0 ms	60.0 ms	99.0 ms	127.0 ms	193.0 ms
15/08/09 15:27:25 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 56.000000, stdev: 0.000000, max: 56.000000, min: 56.000000)
15/08/09 15:27:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:25 INFO StatsReportListener: 	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B	56.0 B
15/08/09 15:27:25 INFO StatsReportListener: task result size:(count: 200, mean: 1819.000000, stdev: 0.000000, max: 1819.000000, min: 1819.000000)
15/08/09 15:27:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:25 INFO StatsReportListener: 	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B	1819.0 B
15/08/09 15:27:25 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 40.499853, stdev: 14.281955, max: 76.363636, min: 13.709677)
15/08/09 15:27:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:25 INFO StatsReportListener: 	14 %	21 %	26 %	30 %	37 %	49 %	61 %	72 %	76 %
15/08/09 15:27:25 INFO DAGScheduler: Missing parents for Stage 11: List()
15/08/09 15:27:25 INFO DAGScheduler: Submitting Stage 11 (MapPartitionsRDD[77] at RangePartitioner at Exchange.scala:88), which is now runnable
15/08/09 15:27:25 INFO StatsReportListener: other time pct: (count: 200, mean: 59.500147, stdev: 14.281955, max: 86.290323, min: 23.636364)
15/08/09 15:27:25 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:25 INFO StatsReportListener: 	24 %	29 %	39 %	51 %	63 %	70 %	75 %	79 %	86 %
15/08/09 15:27:25 INFO MemoryStore: ensureFreeSpace(12256) called with curMem=1600160, maxMem=3333968363
15/08/09 15:27:25 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 12.0 KB, free 3.1 GB)
15/08/09 15:27:25 INFO MemoryStore: ensureFreeSpace(6466) called with curMem=1612416, maxMem=3333968363
15/08/09 15:27:25 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.3 KB, free 3.1 GB)
15/08/09 15:27:25 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:44535 (size: 6.3 KB, free: 3.1 GB)
15/08/09 15:27:25 INFO BlockManagerMaster: Updated info of block broadcast_17_piece0
15/08/09 15:27:25 INFO DefaultExecutionContext: Created broadcast 17 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:25 INFO DAGScheduler: Submitting 200 missing tasks from Stage 11 (MapPartitionsRDD[77] at RangePartitioner at Exchange.scala:88)
15/08/09 15:27:25 INFO TaskSchedulerImpl: Adding task set 11.0 with 200 tasks
15/08/09 15:27:25 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 617, localhost, ANY, 1821 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 618, localhost, ANY, 1823 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 619, localhost, ANY, 1824 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 620, localhost, ANY, 1823 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 621, localhost, ANY, 1825 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 622, localhost, ANY, 1824 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 623, localhost, ANY, 1824 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 624, localhost, ANY, 1825 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 625, localhost, ANY, 1824 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 626, localhost, ANY, 1821 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 627, localhost, ANY, 1826 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 11.0 in stage 11.0 (TID 628, localhost, ANY, 1825 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 12.0 in stage 11.0 (TID 629, localhost, ANY, 1825 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 13.0 in stage 11.0 (TID 630, localhost, ANY, 1824 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 14.0 in stage 11.0 (TID 631, localhost, ANY, 1825 bytes)
15/08/09 15:27:25 INFO TaskSetManager: Starting task 15.0 in stage 11.0 (TID 632, localhost, ANY, 1825 bytes)
15/08/09 15:27:25 INFO Executor: Running task 0.0 in stage 11.0 (TID 617)
15/08/09 15:27:25 INFO Executor: Running task 6.0 in stage 11.0 (TID 623)
15/08/09 15:27:25 INFO Executor: Running task 4.0 in stage 11.0 (TID 621)
15/08/09 15:27:25 INFO Executor: Running task 15.0 in stage 11.0 (TID 632)
15/08/09 15:27:25 INFO Executor: Running task 1.0 in stage 11.0 (TID 618)
15/08/09 15:27:25 INFO Executor: Running task 3.0 in stage 11.0 (TID 620)
15/08/09 15:27:25 INFO Executor: Running task 5.0 in stage 11.0 (TID 622)
15/08/09 15:27:25 INFO Executor: Running task 2.0 in stage 11.0 (TID 619)
15/08/09 15:27:25 INFO Executor: Running task 14.0 in stage 11.0 (TID 631)
15/08/09 15:27:25 INFO Executor: Running task 13.0 in stage 11.0 (TID 630)
15/08/09 15:27:25 INFO Executor: Running task 12.0 in stage 11.0 (TID 629)
15/08/09 15:27:25 INFO Executor: Running task 11.0 in stage 11.0 (TID 628)
15/08/09 15:27:25 INFO Executor: Running task 10.0 in stage 11.0 (TID 627)
15/08/09 15:27:25 INFO Executor: Running task 9.0 in stage 11.0 (TID 626)
15/08/09 15:27:25 INFO Executor: Running task 8.0 in stage 11.0 (TID 625)
15/08/09 15:27:25 INFO Executor: Running task 7.0 in stage 11.0 (TID 624)
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 140
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Finished task 10.0 in stage 11.0 (TID 627). 2054 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 193
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 16.0 in stage 11.0 (TID 633, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 182
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Finished task 3.0 in stage 11.0 (TID 620). 2111 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Running task 16.0 in stage 11.0 (TID 633)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/09 15:27:26 INFO Executor: Finished task 0.0 in stage 11.0 (TID 617). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 627) in 257 ms on localhost (1/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 144
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 17.0 in stage 11.0 (TID 634, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Finished task 5.0 in stage 11.0 (TID 622). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Running task 17.0 in stage 11.0 (TID 634)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/09 15:27:26 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 620) in 265 ms on localhost (2/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO Executor: Finished task 12.0 in stage 11.0 (TID 629). 2072 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 9.0 in stage 11.0 (TID 626). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO Executor: Finished task 6.0 in stage 11.0 (TID 623). 2019 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 18.0 in stage 11.0 (TID 635, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 18.0 in stage 11.0 (TID 635)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 617) in 269 ms on localhost (3/200)
15/08/09 15:27:26 INFO Executor: Finished task 13.0 in stage 11.0 (TID 630). 2237 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 2.0 in stage 11.0 (TID 619). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 19.0 in stage 11.0 (TID 636, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Running task 19.0 in stage 11.0 (TID 636)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 20.0 in stage 11.0 (TID 637, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Running task 20.0 in stage 11.0 (TID 637)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 622) in 270 ms on localhost (4/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Starting task 21.0 in stage 11.0 (TID 638, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Running task 21.0 in stage 11.0 (TID 638)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 12.0 in stage 11.0 (TID 629) in 271 ms on localhost (5/200)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 142
15/08/09 15:27:26 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 626) in 274 ms on localhost (6/200)
15/08/09 15:27:26 INFO Executor: Finished task 7.0 in stage 11.0 (TID 624). 2018 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 1.0 in stage 11.0 (TID 618). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 623) in 278 ms on localhost (7/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Starting task 22.0 in stage 11.0 (TID 639, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Finished task 8.0 in stage 11.0 (TID 625). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Running task 22.0 in stage 11.0 (TID 639)
15/08/09 15:27:26 INFO Executor: Finished task 4.0 in stage 11.0 (TID 621). 2094 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 23.0 in stage 11.0 (TID 640, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 23.0 in stage 11.0 (TID 640)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/09 15:27:26 INFO TaskSetManager: Finished task 13.0 in stage 11.0 (TID 630) in 279 ms on localhost (8/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:26 INFO TaskSetManager: Starting task 24.0 in stage 11.0 (TID 641, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 619) in 287 ms on localhost (9/200)
15/08/09 15:27:26 INFO Executor: Finished task 14.0 in stage 11.0 (TID 631). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Running task 24.0 in stage 11.0 (TID 641)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 25.0 in stage 11.0 (TID 642, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO Executor: Running task 25.0 in stage 11.0 (TID 642)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO Executor: Finished task 11.0 in stage 11.0 (TID 628). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 624) in 287 ms on localhost (10/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 26.0 in stage 11.0 (TID 643, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Running task 26.0 in stage 11.0 (TID 643)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 618) in 291 ms on localhost (11/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 27.0 in stage 11.0 (TID 644, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Running task 27.0 in stage 11.0 (TID 644)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 625) in 290 ms on localhost (12/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 28.0 in stage 11.0 (TID 645, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Running task 28.0 in stage 11.0 (TID 645)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 621) in 294 ms on localhost (13/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 29.0 in stage 11.0 (TID 646, localhost, ANY, 1822 bytes)
15/08/09 15:27:26 INFO Executor: Running task 29.0 in stage 11.0 (TID 646)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 14.0 in stage 11.0 (TID 631) in 291 ms on localhost (14/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 30.0 in stage 11.0 (TID 647, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 30.0 in stage 11.0 (TID 647)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 11.0 in stage 11.0 (TID 628) in 294 ms on localhost (15/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/09 15:27:26 INFO Executor: Finished task 15.0 in stage 11.0 (TID 632). 2112 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 31.0 in stage 11.0 (TID 648, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Running task 31.0 in stage 11.0 (TID 648)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 15.0 in stage 11.0 (TID 632) in 299 ms on localhost (16/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Finished task 27.0 in stage 11.0 (TID 644). 2019 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO TaskSetManager: Starting task 32.0 in stage 11.0 (TID 649, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Running task 32.0 in stage 11.0 (TID 649)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 151
15/08/09 15:27:26 INFO Executor: Finished task 17.0 in stage 11.0 (TID 634). 2093 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 28.0 in stage 11.0 (TID 645). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO TaskSetManager: Finished task 27.0 in stage 11.0 (TID 644) in 161 ms on localhost (17/200)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO TaskSetManager: Starting task 33.0 in stage 11.0 (TID 650, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Running task 33.0 in stage 11.0 (TID 650)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Finished task 17.0 in stage 11.0 (TID 634) in 196 ms on localhost (18/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/09 15:27:26 INFO TaskSetManager: Starting task 34.0 in stage 11.0 (TID 651, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 28.0 in stage 11.0 (TID 645) in 168 ms on localhost (19/200)
15/08/09 15:27:26 INFO Executor: Running task 34.0 in stage 11.0 (TID 651)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Finished task 23.0 in stage 11.0 (TID 640). 2130 bytes result sent to driver
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 35.0 in stage 11.0 (TID 652, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Running task 35.0 in stage 11.0 (TID 652)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:26 INFO TaskSetManager: Finished task 23.0 in stage 11.0 (TID 640) in 183 ms on localhost (20/200)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Finished task 20.0 in stage 11.0 (TID 637). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 18.0 in stage 11.0 (TID 635). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 21.0 in stage 11.0 (TID 638). 2019 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/09 15:27:26 INFO TaskSetManager: Starting task 36.0 in stage 11.0 (TID 653, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 179
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:26 INFO Executor: Running task 36.0 in stage 11.0 (TID 653)
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:26 INFO TaskSetManager: Finished task 20.0 in stage 11.0 (TID 637) in 204 ms on localhost (21/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Starting task 37.0 in stage 11.0 (TID 654, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 37.0 in stage 11.0 (TID 654)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/09 15:27:26 INFO TaskSetManager: Starting task 38.0 in stage 11.0 (TID 655, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/09 15:27:26 INFO Executor: Running task 38.0 in stage 11.0 (TID 655)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 21.0 in stage 11.0 (TID 638) in 203 ms on localhost (22/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO Executor: Finished task 19.0 in stage 11.0 (TID 636). 2146 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 24.0 in stage 11.0 (TID 641). 2001 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 18.0 in stage 11.0 (TID 635) in 210 ms on localhost (23/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Finished task 26.0 in stage 11.0 (TID 643). 2036 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO Executor: Finished task 30.0 in stage 11.0 (TID 647). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 39.0 in stage 11.0 (TID 656, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Running task 39.0 in stage 11.0 (TID 656)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 154
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 88
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/09 15:27:26 INFO TaskSetManager: Finished task 19.0 in stage 11.0 (TID 636) in 211 ms on localhost (24/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 40.0 in stage 11.0 (TID 657, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 40.0 in stage 11.0 (TID 657)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 24.0 in stage 11.0 (TID 641) in 199 ms on localhost (25/200)
15/08/09 15:27:26 INFO Executor: Finished task 25.0 in stage 11.0 (TID 642). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 31.0 in stage 11.0 (TID 648). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 41.0 in stage 11.0 (TID 658, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 41.0 in stage 11.0 (TID 658)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO TaskSetManager: Starting task 42.0 in stage 11.0 (TID 659, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO Executor: Finished task 16.0 in stage 11.0 (TID 633). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Running task 42.0 in stage 11.0 (TID 659)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 43.0 in stage 11.0 (TID 660, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Finished task 22.0 in stage 11.0 (TID 639). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Running task 43.0 in stage 11.0 (TID 660)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 44.0 in stage 11.0 (TID 661, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Finished task 29.0 in stage 11.0 (TID 646). 2166 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Running task 44.0 in stage 11.0 (TID 661)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 45.0 in stage 11.0 (TID 662, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 45.0 in stage 11.0 (TID 662)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 46.0 in stage 11.0 (TID 663, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO Executor: Running task 46.0 in stage 11.0 (TID 663)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 47.0 in stage 11.0 (TID 664, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 26.0 in stage 11.0 (TID 643) in 200 ms on localhost (26/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO Executor: Running task 47.0 in stage 11.0 (TID 664)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 31.0 in stage 11.0 (TID 648) in 187 ms on localhost (27/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 25.0 in stage 11.0 (TID 642) in 204 ms on localhost (28/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 30.0 in stage 11.0 (TID 647) in 196 ms on localhost (29/200)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 29.0 in stage 11.0 (TID 646) in 199 ms on localhost (30/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 22.0 in stage 11.0 (TID 639) in 215 ms on localhost (31/200)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 16.0 in stage 11.0 (TID 633) in 237 ms on localhost (32/200)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 165
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/09 15:27:26 INFO Executor: Finished task 32.0 in stage 11.0 (TID 649). 2094 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Starting task 48.0 in stage 11.0 (TID 665, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 48.0 in stage 11.0 (TID 665)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 149
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Finished task 33.0 in stage 11.0 (TID 650). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 129
15/08/09 15:27:26 INFO TaskSetManager: Finished task 32.0 in stage 11.0 (TID 649) in 218 ms on localhost (33/200)
15/08/09 15:27:26 INFO Executor: Finished task 34.0 in stage 11.0 (TID 651). 2093 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 49.0 in stage 11.0 (TID 666, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO Executor: Running task 49.0 in stage 11.0 (TID 666)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 33.0 in stage 11.0 (TID 650) in 217 ms on localhost (34/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 50.0 in stage 11.0 (TID 667, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/09 15:27:26 INFO Executor: Running task 50.0 in stage 11.0 (TID 667)
15/08/09 15:27:26 INFO Executor: Finished task 37.0 in stage 11.0 (TID 654). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 34.0 in stage 11.0 (TID 651) in 214 ms on localhost (35/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 51.0 in stage 11.0 (TID 668, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO Executor: Running task 51.0 in stage 11.0 (TID 668)
15/08/09 15:27:26 INFO Executor: Finished task 35.0 in stage 11.0 (TID 652). 2001 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Finished task 44.0 in stage 11.0 (TID 661). 2148 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO TaskSetManager: Finished task 37.0 in stage 11.0 (TID 654) in 210 ms on localhost (36/200)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 52.0 in stage 11.0 (TID 669, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 52.0 in stage 11.0 (TID 669)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 53.0 in stage 11.0 (TID 670, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO Executor: Running task 53.0 in stage 11.0 (TID 670)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 35.0 in stage 11.0 (TID 652) in 230 ms on localhost (37/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO TaskSetManager: Finished task 44.0 in stage 11.0 (TID 661) in 208 ms on localhost (38/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Finished task 45.0 in stage 11.0 (TID 662). 2001 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 54.0 in stage 11.0 (TID 671, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Finished task 36.0 in stage 11.0 (TID 653). 2112 bytes result sent to driver
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Running task 54.0 in stage 11.0 (TID 671)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Starting task 55.0 in stage 11.0 (TID 672, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:26 INFO Executor: Running task 55.0 in stage 11.0 (TID 672)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:26 INFO TaskSetManager: Finished task 36.0 in stage 11.0 (TID 653) in 228 ms on localhost (39/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/09 15:27:26 INFO TaskSetManager: Finished task 45.0 in stage 11.0 (TID 662) in 216 ms on localhost (40/200)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO Executor: Finished task 38.0 in stage 11.0 (TID 655). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 56.0 in stage 11.0 (TID 673, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 56.0 in stage 11.0 (TID 673)
15/08/09 15:27:26 INFO Executor: Finished task 43.0 in stage 11.0 (TID 660). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 38.0 in stage 11.0 (TID 655) in 231 ms on localhost (41/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 57.0 in stage 11.0 (TID 674, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Finished task 43.0 in stage 11.0 (TID 660) in 222 ms on localhost (42/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Running task 57.0 in stage 11.0 (TID 674)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 135
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Finished task 41.0 in stage 11.0 (TID 658). 2094 bytes result sent to driver
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 58.0 in stage 11.0 (TID 675, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Finished task 40.0 in stage 11.0 (TID 657). 2093 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 121
15/08/09 15:27:26 INFO TaskSetManager: Finished task 41.0 in stage 11.0 (TID 658) in 230 ms on localhost (43/200)
15/08/09 15:27:26 INFO Executor: Running task 58.0 in stage 11.0 (TID 675)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 59.0 in stage 11.0 (TID 676, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 59.0 in stage 11.0 (TID 676)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 141
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO Executor: Finished task 46.0 in stage 11.0 (TID 663). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 40.0 in stage 11.0 (TID 657) in 234 ms on localhost (44/200)
15/08/09 15:27:26 INFO Executor: Finished task 47.0 in stage 11.0 (TID 664). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 60.0 in stage 11.0 (TID 677, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Finished task 46.0 in stage 11.0 (TID 663) in 233 ms on localhost (45/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/09 15:27:26 INFO Executor: Running task 60.0 in stage 11.0 (TID 677)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 61.0 in stage 11.0 (TID 678, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Finished task 47.0 in stage 11.0 (TID 664) in 235 ms on localhost (46/200)
15/08/09 15:27:26 INFO Executor: Finished task 39.0 in stage 11.0 (TID 656). 2019 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Running task 61.0 in stage 11.0 (TID 678)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 120
15/08/09 15:27:26 INFO TaskSetManager: Starting task 62.0 in stage 11.0 (TID 679, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 62.0 in stage 11.0 (TID 679)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO Executor: Finished task 42.0 in stage 11.0 (TID 659). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 39.0 in stage 11.0 (TID 656) in 245 ms on localhost (47/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 63.0 in stage 11.0 (TID 680, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 63.0 in stage 11.0 (TID 680)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 42.0 in stage 11.0 (TID 659) in 246 ms on localhost (48/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 95
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Finished task 49.0 in stage 11.0 (TID 666). 2019 bytes result sent to driver
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO TaskSetManager: Starting task 64.0 in stage 11.0 (TID 681, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Running task 64.0 in stage 11.0 (TID 681)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/09 15:27:26 INFO Executor: Finished task 54.0 in stage 11.0 (TID 671). 2093 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Finished task 48.0 in stage 11.0 (TID 665). 2147 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 49.0 in stage 11.0 (TID 666) in 151 ms on localhost (49/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:26 INFO TaskSetManager: Starting task 65.0 in stage 11.0 (TID 682, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 65.0 in stage 11.0 (TID 682)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 66.0 in stage 11.0 (TID 683, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 66.0 in stage 11.0 (TID 683)
15/08/09 15:27:26 INFO Executor: Finished task 50.0 in stage 11.0 (TID 667). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 67.0 in stage 11.0 (TID 684, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Finished task 56.0 in stage 11.0 (TID 673). 2019 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 54.0 in stage 11.0 (TID 671) in 158 ms on localhost (50/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO Executor: Running task 67.0 in stage 11.0 (TID 684)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Finished task 50.0 in stage 11.0 (TID 667) in 185 ms on localhost (51/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/09 15:27:26 INFO TaskSetManager: Starting task 68.0 in stage 11.0 (TID 685, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO Executor: Running task 68.0 in stage 11.0 (TID 685)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 48.0 in stage 11.0 (TID 665) in 193 ms on localhost (52/200)
15/08/09 15:27:26 INFO Executor: Finished task 52.0 in stage 11.0 (TID 669). 2019 bytes result sent to driver
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 56.0 in stage 11.0 (TID 673) in 157 ms on localhost (53/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 69.0 in stage 11.0 (TID 686, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 52.0 in stage 11.0 (TID 669) in 172 ms on localhost (54/200)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Running task 69.0 in stage 11.0 (TID 686)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/09 15:27:26 INFO Executor: Finished task 55.0 in stage 11.0 (TID 672). 2001 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 70.0 in stage 11.0 (TID 687, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 70.0 in stage 11.0 (TID 687)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Finished task 53.0 in stage 11.0 (TID 670). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 55.0 in stage 11.0 (TID 672) in 184 ms on localhost (55/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 71.0 in stage 11.0 (TID 688, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 71.0 in stage 11.0 (TID 688)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 53.0 in stage 11.0 (TID 670) in 195 ms on localhost (56/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 10 ms. row count = 121
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Finished task 51.0 in stage 11.0 (TID 668). 2072 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 57.0 in stage 11.0 (TID 674). 2112 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/09 15:27:26 INFO TaskSetManager: Starting task 72.0 in stage 11.0 (TID 689, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 51.0 in stage 11.0 (TID 668) in 228 ms on localhost (57/200)
15/08/09 15:27:26 INFO Executor: Finished task 61.0 in stage 11.0 (TID 678). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Running task 72.0 in stage 11.0 (TID 689)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 73.0 in stage 11.0 (TID 690, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 73.0 in stage 11.0 (TID 690)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 74.0 in stage 11.0 (TID 691, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 61.0 in stage 11.0 (TID 678) in 192 ms on localhost (58/200)
15/08/09 15:27:26 INFO Executor: Running task 74.0 in stage 11.0 (TID 691)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 57.0 in stage 11.0 (TID 674) in 207 ms on localhost (59/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 149
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 203
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 112
15/08/09 15:27:26 INFO Executor: Finished task 59.0 in stage 11.0 (TID 676). 2073 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 60.0 in stage 11.0 (TID 677). 2111 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 58.0 in stage 11.0 (TID 675). 2037 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 75.0 in stage 11.0 (TID 692, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 75.0 in stage 11.0 (TID 692)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 59.0 in stage 11.0 (TID 676) in 211 ms on localhost (60/200)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 76.0 in stage 11.0 (TID 693, localhost, ANY, 1827 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO TaskSetManager: Finished task 60.0 in stage 11.0 (TID 677) in 209 ms on localhost (61/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 77.0 in stage 11.0 (TID 694, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 76.0 in stage 11.0 (TID 693)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 58.0 in stage 11.0 (TID 675) in 217 ms on localhost (62/200)
15/08/09 15:27:26 INFO Executor: Running task 77.0 in stage 11.0 (TID 694)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/09 15:27:26 INFO Executor: Finished task 63.0 in stage 11.0 (TID 680). 2019 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 62.0 in stage 11.0 (TID 679). 2111 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 78.0 in stage 11.0 (TID 695, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO Executor: Running task 78.0 in stage 11.0 (TID 695)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 63.0 in stage 11.0 (TID 680) in 223 ms on localhost (63/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 79.0 in stage 11.0 (TID 696, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 79.0 in stage 11.0 (TID 696)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 62.0 in stage 11.0 (TID 679) in 232 ms on localhost (64/200)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 160
15/08/09 15:27:26 INFO Executor: Finished task 66.0 in stage 11.0 (TID 683). 2220 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 70.0 in stage 11.0 (TID 687). 2020 bytes result sent to driver
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 80.0 in stage 11.0 (TID 697, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 80.0 in stage 11.0 (TID 697)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 66.0 in stage 11.0 (TID 683) in 217 ms on localhost (65/200)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO TaskSetManager: Starting task 81.0 in stage 11.0 (TID 698, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Running task 81.0 in stage 11.0 (TID 698)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 70.0 in stage 11.0 (TID 687) in 190 ms on localhost (66/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Finished task 64.0 in stage 11.0 (TID 681). 2056 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 82.0 in stage 11.0 (TID 699, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Running task 82.0 in stage 11.0 (TID 699)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Finished task 64.0 in stage 11.0 (TID 681) in 265 ms on localhost (67/200)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Finished task 65.0 in stage 11.0 (TID 682). 2074 bytes result sent to driver
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO TaskSetManager: Starting task 83.0 in stage 11.0 (TID 700, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 83.0 in stage 11.0 (TID 700)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 65.0 in stage 11.0 (TID 682) in 233 ms on localhost (68/200)
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Finished task 71.0 in stage 11.0 (TID 688). 2074 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO TaskSetManager: Starting task 84.0 in stage 11.0 (TID 701, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO Executor: Running task 84.0 in stage 11.0 (TID 701)
15/08/09 15:27:26 INFO Executor: Finished task 67.0 in stage 11.0 (TID 684). 2038 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 134
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Finished task 71.0 in stage 11.0 (TID 688) in 209 ms on localhost (69/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 85.0 in stage 11.0 (TID 702, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Finished task 69.0 in stage 11.0 (TID 686). 2056 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Running task 85.0 in stage 11.0 (TID 702)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 122
15/08/09 15:27:26 INFO Executor: Finished task 74.0 in stage 11.0 (TID 691). 2074 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 86.0 in stage 11.0 (TID 703, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO Executor: Running task 86.0 in stage 11.0 (TID 703)
15/08/09 15:27:26 INFO Executor: Finished task 68.0 in stage 11.0 (TID 685). 2038 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 67.0 in stage 11.0 (TID 684) in 245 ms on localhost (70/200)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 69.0 in stage 11.0 (TID 686) in 240 ms on localhost (71/200)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO TaskSetManager: Starting task 87.0 in stage 11.0 (TID 704, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO Executor: Running task 87.0 in stage 11.0 (TID 704)
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 74.0 in stage 11.0 (TID 691) in 193 ms on localhost (72/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 88.0 in stage 11.0 (TID 705, localhost, ANY, 1824 bytes)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 68.0 in stage 11.0 (TID 685) in 247 ms on localhost (73/200)
15/08/09 15:27:26 INFO Executor: Running task 88.0 in stage 11.0 (TID 705)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/09 15:27:26 INFO Executor: Finished task 72.0 in stage 11.0 (TID 689). 2038 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Starting task 89.0 in stage 11.0 (TID 706, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO Executor: Running task 89.0 in stage 11.0 (TID 706)
15/08/09 15:27:26 INFO Executor: Finished task 77.0 in stage 11.0 (TID 694). 2020 bytes result sent to driver
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO TaskSetManager: Finished task 72.0 in stage 11.0 (TID 689) in 205 ms on localhost (74/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 90.0 in stage 11.0 (TID 707, localhost, ANY, 1823 bytes)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 171
15/08/09 15:27:26 INFO Executor: Running task 90.0 in stage 11.0 (TID 707)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 77.0 in stage 11.0 (TID 694) in 188 ms on localhost (75/200)
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 184
15/08/09 15:27:26 INFO Executor: Finished task 79.0 in stage 11.0 (TID 696). 2038 bytes result sent to driver
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:26 INFO TaskSetManager: Starting task 91.0 in stage 11.0 (TID 708, localhost, ANY, 1826 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO Executor: Running task 91.0 in stage 11.0 (TID 708)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO TaskSetManager: Finished task 79.0 in stage 11.0 (TID 696) in 166 ms on localhost (76/200)
15/08/09 15:27:26 INFO Executor: Finished task 73.0 in stage 11.0 (TID 690). 2055 bytes result sent to driver
15/08/09 15:27:26 INFO Executor: Finished task 75.0 in stage 11.0 (TID 692). 2131 bytes result sent to driver
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:26 INFO TaskSetManager: Starting task 92.0 in stage 11.0 (TID 709, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:26 INFO Executor: Running task 92.0 in stage 11.0 (TID 709)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 73.0 in stage 11.0 (TID 690) in 211 ms on localhost (77/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 93.0 in stage 11.0 (TID 710, localhost, ANY, 1825 bytes)
15/08/09 15:27:26 INFO TaskSetManager: Finished task 75.0 in stage 11.0 (TID 692) in 199 ms on localhost (78/200)
15/08/09 15:27:26 INFO Executor: Running task 93.0 in stage 11.0 (TID 710)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:26 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/09 15:27:26 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:26 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 191
15/08/09 15:27:26 INFO Executor: Finished task 78.0 in stage 11.0 (TID 695). 2056 bytes result sent to driver
15/08/09 15:27:26 INFO TaskSetManager: Finished task 78.0 in stage 11.0 (TID 695) in 181 ms on localhost (79/200)
15/08/09 15:27:26 INFO TaskSetManager: Starting task 94.0 in stage 11.0 (TID 711, localhost, ANY, 1823 bytes)
15/08/09 15:27:26 INFO Executor: Running task 94.0 in stage 11.0 (TID 711)
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/09 15:27:27 INFO Executor: Finished task 76.0 in stage 11.0 (TID 693). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 95.0 in stage 11.0 (TID 712, localhost, ANY, 1827 bytes)
15/08/09 15:27:27 INFO Executor: Running task 95.0 in stage 11.0 (TID 712)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 76.0 in stage 11.0 (TID 693) in 245 ms on localhost (80/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:27 INFO Executor: Finished task 81.0 in stage 11.0 (TID 698). 2055 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 96.0 in stage 11.0 (TID 713, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 96.0 in stage 11.0 (TID 713)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 81.0 in stage 11.0 (TID 698) in 181 ms on localhost (81/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/09 15:27:27 INFO Executor: Finished task 80.0 in stage 11.0 (TID 697). 2073 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 97.0 in stage 11.0 (TID 714, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO Executor: Running task 97.0 in stage 11.0 (TID 714)
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO TaskSetManager: Finished task 80.0 in stage 11.0 (TID 697) in 217 ms on localhost (82/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Finished task 82.0 in stage 11.0 (TID 699). 2130 bytes result sent to driver
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO TaskSetManager: Starting task 98.0 in stage 11.0 (TID 715, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 98.0 in stage 11.0 (TID 715)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Finished task 82.0 in stage 11.0 (TID 699) in 211 ms on localhost (83/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/09 15:27:27 INFO Executor: Finished task 83.0 in stage 11.0 (TID 700). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 85.0 in stage 11.0 (TID 702). 2113 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 86.0 in stage 11.0 (TID 703). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 99.0 in stage 11.0 (TID 716, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 83.0 in stage 11.0 (TID 700) in 218 ms on localhost (84/200)
15/08/09 15:27:27 INFO Executor: Running task 99.0 in stage 11.0 (TID 716)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 100.0 in stage 11.0 (TID 717, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 100.0 in stage 11.0 (TID 717)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO TaskSetManager: Finished task 85.0 in stage 11.0 (TID 702) in 207 ms on localhost (85/200)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Starting task 101.0 in stage 11.0 (TID 718, localhost, ANY, 1823 bytes)
15/08/09 15:27:27 INFO Executor: Running task 101.0 in stage 11.0 (TID 718)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO TaskSetManager: Finished task 86.0 in stage 11.0 (TID 703) in 208 ms on localhost (86/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 161
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 152
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Finished task 84.0 in stage 11.0 (TID 701). 2002 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 87.0 in stage 11.0 (TID 704). 2113 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 182
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 173
15/08/09 15:27:27 INFO Executor: Finished task 90.0 in stage 11.0 (TID 707). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 102.0 in stage 11.0 (TID 719, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 INFO Executor: Finished task 89.0 in stage 11.0 (TID 706). 2095 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 119
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/09 15:27:27 INFO Executor: Running task 102.0 in stage 11.0 (TID 719)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 84.0 in stage 11.0 (TID 701) in 235 ms on localhost (87/200)
15/08/09 15:27:27 INFO Executor: Finished task 91.0 in stage 11.0 (TID 708). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO TaskSetManager: Starting task 103.0 in stage 11.0 (TID 720, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Running task 103.0 in stage 11.0 (TID 720)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 200
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO TaskSetManager: Finished task 87.0 in stage 11.0 (TID 704) in 227 ms on localhost (88/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 104.0 in stage 11.0 (TID 721, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 104.0 in stage 11.0 (TID 721)
15/08/09 15:27:27 INFO Executor: Finished task 88.0 in stage 11.0 (TID 705). 2095 bytes result sent to driver
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 90.0 in stage 11.0 (TID 707) in 217 ms on localhost (89/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 108
15/08/09 15:27:27 INFO TaskSetManager: Starting task 105.0 in stage 11.0 (TID 722, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 105.0 in stage 11.0 (TID 722)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO Executor: Finished task 92.0 in stage 11.0 (TID 709). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Finished task 89.0 in stage 11.0 (TID 706) in 220 ms on localhost (90/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 106.0 in stage 11.0 (TID 723, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Finished task 93.0 in stage 11.0 (TID 710). 2055 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Running task 106.0 in stage 11.0 (TID 723)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 91.0 in stage 11.0 (TID 708) in 218 ms on localhost (91/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 107.0 in stage 11.0 (TID 724, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 88.0 in stage 11.0 (TID 705) in 233 ms on localhost (92/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 108.0 in stage 11.0 (TID 725, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 107.0 in stage 11.0 (TID 724)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 109.0 in stage 11.0 (TID 726, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 108.0 in stage 11.0 (TID 725)
15/08/09 15:27:27 INFO Executor: Running task 109.0 in stage 11.0 (TID 726)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 92.0 in stage 11.0 (TID 709) in 219 ms on localhost (93/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO TaskSetManager: Finished task 93.0 in stage 11.0 (TID 710) in 218 ms on localhost (94/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 196
15/08/09 15:27:27 INFO Executor: Finished task 94.0 in stage 11.0 (TID 711). 2095 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 110.0 in stage 11.0 (TID 727, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 INFO Executor: Running task 110.0 in stage 11.0 (TID 727)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 94.0 in stage 11.0 (TID 711) in 215 ms on localhost (95/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/09 15:27:27 INFO Executor: Finished task 95.0 in stage 11.0 (TID 712). 2131 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 111.0 in stage 11.0 (TID 728, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 111.0 in stage 11.0 (TID 728)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 95.0 in stage 11.0 (TID 712) in 214 ms on localhost (96/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 114
15/08/09 15:27:27 INFO Executor: Finished task 96.0 in stage 11.0 (TID 713). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 112.0 in stage 11.0 (TID 729, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 112.0 in stage 11.0 (TID 729)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 96.0 in stage 11.0 (TID 713) in 151 ms on localhost (97/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Finished task 97.0 in stage 11.0 (TID 714). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO TaskSetManager: Starting task 113.0 in stage 11.0 (TID 730, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Running task 113.0 in stage 11.0 (TID 730)
15/08/09 15:27:27 INFO Executor: Finished task 100.0 in stage 11.0 (TID 717). 2147 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 206
15/08/09 15:27:27 INFO TaskSetManager: Finished task 97.0 in stage 11.0 (TID 714) in 170 ms on localhost (98/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 114.0 in stage 11.0 (TID 731, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Finished task 98.0 in stage 11.0 (TID 715). 2095 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Running task 114.0 in stage 11.0 (TID 731)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 100.0 in stage 11.0 (TID 717) in 156 ms on localhost (99/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 115.0 in stage 11.0 (TID 732, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 162
15/08/09 15:27:27 INFO Executor: Running task 115.0 in stage 11.0 (TID 732)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO TaskSetManager: Finished task 98.0 in stage 11.0 (TID 715) in 169 ms on localhost (100/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO Executor: Finished task 99.0 in stage 11.0 (TID 716). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 116.0 in stage 11.0 (TID 733, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 116.0 in stage 11.0 (TID 733)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 99.0 in stage 11.0 (TID 716) in 163 ms on localhost (101/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 174
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Finished task 101.0 in stage 11.0 (TID 718). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 117.0 in stage 11.0 (TID 734, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 INFO Executor: Running task 117.0 in stage 11.0 (TID 734)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO TaskSetManager: Finished task 101.0 in stage 11.0 (TID 718) in 174 ms on localhost (102/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/09 15:27:27 INFO Executor: Finished task 103.0 in stage 11.0 (TID 720). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Finished task 108.0 in stage 11.0 (TID 725). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 118.0 in stage 11.0 (TID 735, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 118.0 in stage 11.0 (TID 735)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 176
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:27 INFO Executor: Finished task 104.0 in stage 11.0 (TID 721). 2073 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:27 INFO TaskSetManager: Finished task 103.0 in stage 11.0 (TID 720) in 168 ms on localhost (103/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 119.0 in stage 11.0 (TID 736, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Running task 119.0 in stage 11.0 (TID 736)
15/08/09 15:27:27 INFO Executor: Finished task 102.0 in stage 11.0 (TID 719). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Finished task 108.0 in stage 11.0 (TID 725) in 163 ms on localhost (104/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 120.0 in stage 11.0 (TID 737, localhost, ANY, 1823 bytes)
15/08/09 15:27:27 INFO Executor: Finished task 106.0 in stage 11.0 (TID 723). 2167 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Running task 120.0 in stage 11.0 (TID 737)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO TaskSetManager: Finished task 104.0 in stage 11.0 (TID 721) in 173 ms on localhost (105/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Starting task 121.0 in stage 11.0 (TID 738, localhost, ANY, 1823 bytes)
15/08/09 15:27:27 INFO Executor: Running task 121.0 in stage 11.0 (TID 738)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 102.0 in stage 11.0 (TID 719) in 181 ms on localhost (106/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 122.0 in stage 11.0 (TID 739, localhost, ANY, 1823 bytes)
15/08/09 15:27:27 INFO Executor: Running task 122.0 in stage 11.0 (TID 739)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 106.0 in stage 11.0 (TID 723) in 173 ms on localhost (107/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO Executor: Finished task 107.0 in stage 11.0 (TID 724). 2113 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 123.0 in stage 11.0 (TID 740, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 123.0 in stage 11.0 (TID 740)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 107.0 in stage 11.0 (TID 724) in 178 ms on localhost (108/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 124
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:27 INFO Executor: Finished task 105.0 in stage 11.0 (TID 722). 2094 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 124.0 in stage 11.0 (TID 741, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 INFO Executor: Finished task 109.0 in stage 11.0 (TID 726). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Running task 124.0 in stage 11.0 (TID 741)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 105.0 in stage 11.0 (TID 722) in 188 ms on localhost (109/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 125.0 in stage 11.0 (TID 742, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 125.0 in stage 11.0 (TID 742)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 109.0 in stage 11.0 (TID 726) in 184 ms on localhost (110/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 178
15/08/09 15:27:27 INFO Executor: Finished task 110.0 in stage 11.0 (TID 727). 2149 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 126.0 in stage 11.0 (TID 743, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 INFO Executor: Running task 126.0 in stage 11.0 (TID 743)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 110.0 in stage 11.0 (TID 727) in 183 ms on localhost (111/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/09 15:27:27 INFO Executor: Finished task 112.0 in stage 11.0 (TID 729). 2055 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 127.0 in stage 11.0 (TID 744, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 127.0 in stage 11.0 (TID 744)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 112.0 in stage 11.0 (TID 729) in 157 ms on localhost (112/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/09 15:27:27 INFO Executor: Finished task 111.0 in stage 11.0 (TID 728). 2002 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 128.0 in stage 11.0 (TID 745, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 111.0 in stage 11.0 (TID 728) in 198 ms on localhost (113/200)
15/08/09 15:27:27 INFO Executor: Running task 128.0 in stage 11.0 (TID 745)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Finished task 113.0 in stage 11.0 (TID 730). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO TaskSetManager: Starting task 129.0 in stage 11.0 (TID 746, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 129.0 in stage 11.0 (TID 746)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 113.0 in stage 11.0 (TID 730) in 173 ms on localhost (114/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 128
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 135
15/08/09 15:27:27 INFO Executor: Finished task 115.0 in stage 11.0 (TID 732). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 116.0 in stage 11.0 (TID 733). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 130.0 in stage 11.0 (TID 747, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 115.0 in stage 11.0 (TID 732) in 168 ms on localhost (115/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 131.0 in stage 11.0 (TID 748, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 131.0 in stage 11.0 (TID 748)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Running task 130.0 in stage 11.0 (TID 747)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Finished task 116.0 in stage 11.0 (TID 733) in 167 ms on localhost (116/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/09 15:27:27 INFO Executor: Finished task 117.0 in stage 11.0 (TID 734). 2113 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 180
15/08/09 15:27:27 INFO TaskSetManager: Starting task 132.0 in stage 11.0 (TID 749, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 132.0 in stage 11.0 (TID 749)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 117.0 in stage 11.0 (TID 734) in 168 ms on localhost (117/200)
15/08/09 15:27:27 INFO Executor: Finished task 123.0 in stage 11.0 (TID 740). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 133.0 in stage 11.0 (TID 750, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 133.0 in stage 11.0 (TID 750)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 123.0 in stage 11.0 (TID 740) in 139 ms on localhost (118/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO Executor: Finished task 125.0 in stage 11.0 (TID 742). 2002 bytes result sent to driver
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Starting task 134.0 in stage 11.0 (TID 751, localhost, ANY, 1827 bytes)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO Executor: Running task 134.0 in stage 11.0 (TID 751)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Finished task 125.0 in stage 11.0 (TID 742) in 134 ms on localhost (119/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Finished task 118.0 in stage 11.0 (TID 735). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 135.0 in stage 11.0 (TID 752, localhost, ANY, 1827 bytes)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Running task 135.0 in stage 11.0 (TID 752)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 192
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO TaskSetManager: Finished task 118.0 in stage 11.0 (TID 735) in 178 ms on localhost (120/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 175
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 206
15/08/09 15:27:27 INFO Executor: Finished task 120.0 in stage 11.0 (TID 737). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 122.0 in stage 11.0 (TID 739). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 124.0 in stage 11.0 (TID 741). 2130 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 114.0 in stage 11.0 (TID 731). 2095 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 11 ms. row count = 171
15/08/09 15:27:27 INFO TaskSetManager: Starting task 136.0 in stage 11.0 (TID 753, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 120.0 in stage 11.0 (TID 737) in 184 ms on localhost (121/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 137.0 in stage 11.0 (TID 754, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Finished task 119.0 in stage 11.0 (TID 736). 2095 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 138.0 in stage 11.0 (TID 755, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 139.0 in stage 11.0 (TID 756, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 139.0 in stage 11.0 (TID 756)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 140.0 in stage 11.0 (TID 757, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 140.0 in stage 11.0 (TID 757)
15/08/09 15:27:27 INFO Executor: Running task 137.0 in stage 11.0 (TID 754)
15/08/09 15:27:27 INFO Executor: Running task 136.0 in stage 11.0 (TID 753)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 119.0 in stage 11.0 (TID 736) in 190 ms on localhost (122/200)
15/08/09 15:27:27 INFO Executor: Running task 138.0 in stage 11.0 (TID 755)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 124.0 in stage 11.0 (TID 741) in 171 ms on localhost (123/200)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 114.0 in stage 11.0 (TID 731) in 234 ms on localhost (124/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 122.0 in stage 11.0 (TID 739) in 187 ms on localhost (125/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/09 15:27:27 INFO Executor: Finished task 121.0 in stage 11.0 (TID 738). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 141.0 in stage 11.0 (TID 758, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 INFO Executor: Running task 141.0 in stage 11.0 (TID 758)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 121.0 in stage 11.0 (TID 738) in 200 ms on localhost (126/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 142
15/08/09 15:27:27 INFO Executor: Finished task 127.0 in stage 11.0 (TID 744). 2020 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 142.0 in stage 11.0 (TID 759, localhost, ANY, 1823 bytes)
15/08/09 15:27:27 INFO Executor: Running task 142.0 in stage 11.0 (TID 759)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 127.0 in stage 11.0 (TID 744) in 193 ms on localhost (127/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Finished task 126.0 in stage 11.0 (TID 743). 2094 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 143.0 in stage 11.0 (TID 760, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 143.0 in stage 11.0 (TID 760)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 126.0 in stage 11.0 (TID 743) in 248 ms on localhost (128/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 108
15/08/09 15:27:27 INFO Executor: Finished task 132.0 in stage 11.0 (TID 749). 2002 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO TaskSetManager: Starting task 144.0 in stage 11.0 (TID 761, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 163
15/08/09 15:27:27 INFO Executor: Finished task 129.0 in stage 11.0 (TID 746). 2095 bytes result sent to driver
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Running task 144.0 in stage 11.0 (TID 761)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 132.0 in stage 11.0 (TID 749) in 149 ms on localhost (129/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 145.0 in stage 11.0 (TID 762, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 145.0 in stage 11.0 (TID 762)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 129.0 in stage 11.0 (TID 746) in 235 ms on localhost (130/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 181
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Finished task 128.0 in stage 11.0 (TID 745). 2149 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 146.0 in stage 11.0 (TID 763, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 146.0 in stage 11.0 (TID 763)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 128.0 in stage 11.0 (TID 745) in 290 ms on localhost (131/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/09 15:27:27 INFO Executor: Finished task 130.0 in stage 11.0 (TID 747). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Starting task 147.0 in stage 11.0 (TID 764, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 147.0 in stage 11.0 (TID 764)
15/08/09 15:27:27 INFO Executor: Finished task 133.0 in stage 11.0 (TID 750). 2020 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Finished task 130.0 in stage 11.0 (TID 747) in 253 ms on localhost (132/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 148.0 in stage 11.0 (TID 765, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 148.0 in stage 11.0 (TID 765)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 133.0 in stage 11.0 (TID 750) in 235 ms on localhost (133/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO Executor: Finished task 131.0 in stage 11.0 (TID 748). 2056 bytes result sent to driver
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Starting task 149.0 in stage 11.0 (TID 766, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 149.0 in stage 11.0 (TID 766)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO TaskSetManager: Finished task 131.0 in stage 11.0 (TID 748) in 260 ms on localhost (134/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/09 15:27:27 INFO Executor: Finished task 140.0 in stage 11.0 (TID 757). 2020 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 150.0 in stage 11.0 (TID 767, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 150.0 in stage 11.0 (TID 767)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 140.0 in stage 11.0 (TID 757) in 212 ms on localhost (135/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Finished task 134.0 in stage 11.0 (TID 751). 2020 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 135.0 in stage 11.0 (TID 752). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/09 15:27:27 INFO TaskSetManager: Starting task 151.0 in stage 11.0 (TID 768, localhost, ANY, 1823 bytes)
15/08/09 15:27:27 INFO Executor: Running task 151.0 in stage 11.0 (TID 768)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO TaskSetManager: Finished task 134.0 in stage 11.0 (TID 751) in 264 ms on localhost (136/200)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Starting task 152.0 in stage 11.0 (TID 769, localhost, ANY, 1827 bytes)
15/08/09 15:27:27 INFO Executor: Finished task 137.0 in stage 11.0 (TID 754). 2113 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Finished task 135.0 in stage 11.0 (TID 752) in 249 ms on localhost (137/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 153.0 in stage 11.0 (TID 770, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 153.0 in stage 11.0 (TID 770)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 137.0 in stage 11.0 (TID 754) in 235 ms on localhost (138/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO Executor: Running task 152.0 in stage 11.0 (TID 769)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 130
15/08/09 15:27:27 INFO Executor: Finished task 136.0 in stage 11.0 (TID 753). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 154.0 in stage 11.0 (TID 771, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO Executor: Running task 154.0 in stage 11.0 (TID 771)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 169
15/08/09 15:27:27 INFO TaskSetManager: Finished task 136.0 in stage 11.0 (TID 753) in 254 ms on localhost (139/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO Executor: Finished task 142.0 in stage 11.0 (TID 759). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO TaskSetManager: Starting task 155.0 in stage 11.0 (TID 772, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 142.0 in stage 11.0 (TID 759) in 196 ms on localhost (140/200)
15/08/09 15:27:27 INFO Executor: Running task 155.0 in stage 11.0 (TID 772)
15/08/09 15:27:27 INFO Executor: Finished task 141.0 in stage 11.0 (TID 758). 2020 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 156.0 in stage 11.0 (TID 773, localhost, ANY, 1827 bytes)
15/08/09 15:27:27 INFO Executor: Running task 156.0 in stage 11.0 (TID 773)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 141.0 in stage 11.0 (TID 758) in 244 ms on localhost (141/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 113
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Finished task 138.0 in stage 11.0 (TID 755). 2020 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/09 15:27:27 INFO TaskSetManager: Starting task 157.0 in stage 11.0 (TID 774, localhost, ANY, 1827 bytes)
15/08/09 15:27:27 INFO Executor: Running task 157.0 in stage 11.0 (TID 774)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 138.0 in stage 11.0 (TID 755) in 267 ms on localhost (142/200)
15/08/09 15:27:27 INFO Executor: Finished task 139.0 in stage 11.0 (TID 756). 2002 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 158.0 in stage 11.0 (TID 775, localhost, ANY, 1824 bytes)
15/08/09 15:27:27 INFO Executor: Running task 158.0 in stage 11.0 (TID 775)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 139.0 in stage 11.0 (TID 756) in 268 ms on localhost (143/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Finished task 143.0 in stage 11.0 (TID 760). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:27 INFO TaskSetManager: Starting task 159.0 in stage 11.0 (TID 776, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 159.0 in stage 11.0 (TID 776)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 143.0 in stage 11.0 (TID 760) in 222 ms on localhost (144/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO Executor: Finished task 144.0 in stage 11.0 (TID 761). 2020 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 160.0 in stage 11.0 (TID 777, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 160.0 in stage 11.0 (TID 777)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO TaskSetManager: Finished task 144.0 in stage 11.0 (TID 761) in 210 ms on localhost (145/200)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/09 15:27:27 INFO Executor: Finished task 145.0 in stage 11.0 (TID 762). 2095 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 161.0 in stage 11.0 (TID 778, localhost, ANY, 1827 bytes)
15/08/09 15:27:27 INFO Executor: Running task 161.0 in stage 11.0 (TID 778)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 145.0 in stage 11.0 (TID 762) in 200 ms on localhost (146/200)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 126
15/08/09 15:27:27 INFO Executor: Finished task 146.0 in stage 11.0 (TID 763). 2113 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 162.0 in stage 11.0 (TID 779, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 162.0 in stage 11.0 (TID 779)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Finished task 146.0 in stage 11.0 (TID 763) in 152 ms on localhost (147/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:27 INFO Executor: Finished task 148.0 in stage 11.0 (TID 765). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 163.0 in stage 11.0 (TID 780, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 148.0 in stage 11.0 (TID 765) in 149 ms on localhost (148/200)
15/08/09 15:27:27 INFO Executor: Running task 163.0 in stage 11.0 (TID 780)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 178
15/08/09 15:27:27 INFO Executor: Finished task 147.0 in stage 11.0 (TID 764). 2020 bytes result sent to driver
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Starting task 164.0 in stage 11.0 (TID 781, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 164.0 in stage 11.0 (TID 781)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 147.0 in stage 11.0 (TID 764) in 162 ms on localhost (149/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 163
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/09 15:27:27 INFO Executor: Finished task 149.0 in stage 11.0 (TID 766). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 165.0 in stage 11.0 (TID 782, localhost, ANY, 1827 bytes)
15/08/09 15:27:27 INFO Executor: Running task 165.0 in stage 11.0 (TID 782)
15/08/09 15:27:27 INFO Executor: Finished task 150.0 in stage 11.0 (TID 767). 2038 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Finished task 149.0 in stage 11.0 (TID 766) in 168 ms on localhost (150/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO TaskSetManager: Starting task 166.0 in stage 11.0 (TID 783, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO Executor: Running task 166.0 in stage 11.0 (TID 783)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 150.0 in stage 11.0 (TID 767) in 161 ms on localhost (151/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 127
15/08/09 15:27:27 INFO Executor: Finished task 153.0 in stage 11.0 (TID 770). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO TaskSetManager: Starting task 167.0 in stage 11.0 (TID 784, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 167.0 in stage 11.0 (TID 784)
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Finished task 153.0 in stage 11.0 (TID 770) in 163 ms on localhost (152/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 190
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/09 15:27:27 INFO Executor: Finished task 151.0 in stage 11.0 (TID 768). 2148 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO Executor: Finished task 154.0 in stage 11.0 (TID 771). 2130 bytes result sent to driver
15/08/09 15:27:27 INFO Executor: Finished task 152.0 in stage 11.0 (TID 769). 1913 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 168.0 in stage 11.0 (TID 785, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 168.0 in stage 11.0 (TID 785)
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/09 15:27:27 INFO TaskSetManager: Finished task 151.0 in stage 11.0 (TID 768) in 181 ms on localhost (153/200)
15/08/09 15:27:27 INFO Executor: Finished task 156.0 in stage 11.0 (TID 773). 2056 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 169.0 in stage 11.0 (TID 786, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 169.0 in stage 11.0 (TID 786)
15/08/09 15:27:27 INFO TaskSetManager: Finished task 154.0 in stage 11.0 (TID 771) in 162 ms on localhost (154/200)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:27 INFO TaskSetManager: Starting task 170.0 in stage 11.0 (TID 787, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 170.0 in stage 11.0 (TID 787)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Finished task 152.0 in stage 11.0 (TID 769) in 182 ms on localhost (155/200)
15/08/09 15:27:27 INFO TaskSetManager: Starting task 171.0 in stage 11.0 (TID 788, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 171.0 in stage 11.0 (TID 788)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO TaskSetManager: Finished task 156.0 in stage 11.0 (TID 773) in 160 ms on localhost (156/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/09 15:27:27 INFO Executor: Finished task 157.0 in stage 11.0 (TID 774). 2113 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 172.0 in stage 11.0 (TID 789, localhost, ANY, 1825 bytes)
15/08/09 15:27:27 INFO Executor: Running task 172.0 in stage 11.0 (TID 789)
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 149
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 157.0 in stage 11.0 (TID 774) in 167 ms on localhost (157/200)
15/08/09 15:27:27 INFO Executor: Finished task 155.0 in stage 11.0 (TID 772). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/09 15:27:27 INFO TaskSetManager: Starting task 173.0 in stage 11.0 (TID 790, localhost, ANY, 1823 bytes)
15/08/09 15:27:27 INFO Executor: Running task 173.0 in stage 11.0 (TID 790)
15/08/09 15:27:27 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:27 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 168
15/08/09 15:27:27 INFO TaskSetManager: Finished task 155.0 in stage 11.0 (TID 772) in 185 ms on localhost (158/200)
15/08/09 15:27:27 INFO Executor: Finished task 158.0 in stage 11.0 (TID 775). 2074 bytes result sent to driver
15/08/09 15:27:27 INFO TaskSetManager: Starting task 174.0 in stage 11.0 (TID 791, localhost, ANY, 1826 bytes)
15/08/09 15:27:27 INFO Executor: Running task 174.0 in stage 11.0 (TID 791)
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:27 INFO TaskSetManager: Finished task 158.0 in stage 11.0 (TID 775) in 178 ms on localhost (159/200)
15/08/09 15:27:27 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:27 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 172
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO Executor: Finished task 159.0 in stage 11.0 (TID 776). 2056 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 175.0 in stage 11.0 (TID 792, localhost, ANY, 1823 bytes)
15/08/09 15:27:28 INFO Executor: Running task 175.0 in stage 11.0 (TID 792)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 159.0 in stage 11.0 (TID 776) in 156 ms on localhost (160/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 116
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO Executor: Finished task 161.0 in stage 11.0 (TID 778). 2038 bytes result sent to driver
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO TaskSetManager: Starting task 176.0 in stage 11.0 (TID 793, localhost, ANY, 1824 bytes)
15/08/09 15:27:28 INFO Executor: Running task 176.0 in stage 11.0 (TID 793)
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO TaskSetManager: Finished task 161.0 in stage 11.0 (TID 778) in 160 ms on localhost (161/200)
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO Executor: Finished task 160.0 in stage 11.0 (TID 777). 2020 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 177.0 in stage 11.0 (TID 794, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO Executor: Running task 177.0 in stage 11.0 (TID 794)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 160.0 in stage 11.0 (TID 777) in 168 ms on localhost (162/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 110
15/08/09 15:27:28 INFO Executor: Finished task 163.0 in stage 11.0 (TID 780). 1913 bytes result sent to driver
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO TaskSetManager: Starting task 178.0 in stage 11.0 (TID 795, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO Executor: Running task 178.0 in stage 11.0 (TID 795)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 163.0 in stage 11.0 (TID 780) in 154 ms on localhost (163/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 136
15/08/09 15:27:28 INFO Executor: Finished task 162.0 in stage 11.0 (TID 779). 2094 bytes result sent to driver
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO TaskSetManager: Starting task 179.0 in stage 11.0 (TID 796, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO TaskSetManager: Finished task 162.0 in stage 11.0 (TID 779) in 172 ms on localhost (164/200)
15/08/09 15:27:28 INFO Executor: Running task 179.0 in stage 11.0 (TID 796)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:28 INFO Executor: Finished task 164.0 in stage 11.0 (TID 781). 2111 bytes result sent to driver
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO TaskSetManager: Starting task 180.0 in stage 11.0 (TID 797, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO Executor: Running task 180.0 in stage 11.0 (TID 797)
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/09 15:27:28 INFO TaskSetManager: Finished task 164.0 in stage 11.0 (TID 781) in 158 ms on localhost (165/200)
15/08/09 15:27:28 INFO Executor: Finished task 165.0 in stage 11.0 (TID 782). 2094 bytes result sent to driver
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO TaskSetManager: Starting task 181.0 in stage 11.0 (TID 798, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO Executor: Running task 181.0 in stage 11.0 (TID 798)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 165.0 in stage 11.0 (TID 782) in 147 ms on localhost (166/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 114
15/08/09 15:27:28 INFO Executor: Finished task 166.0 in stage 11.0 (TID 783). 2073 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 182.0 in stage 11.0 (TID 799, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO Executor: Running task 182.0 in stage 11.0 (TID 799)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 166.0 in stage 11.0 (TID 783) in 171 ms on localhost (167/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/09 15:27:28 INFO Executor: Finished task 169.0 in stage 11.0 (TID 786). 2020 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 183.0 in stage 11.0 (TID 800, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO Executor: Running task 183.0 in stage 11.0 (TID 800)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO Executor: Finished task 168.0 in stage 11.0 (TID 785). 2002 bytes result sent to driver
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO TaskSetManager: Finished task 169.0 in stage 11.0 (TID 786) in 157 ms on localhost (168/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO TaskSetManager: Starting task 184.0 in stage 11.0 (TID 801, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO Executor: Running task 184.0 in stage 11.0 (TID 801)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 168.0 in stage 11.0 (TID 785) in 164 ms on localhost (169/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO Executor: Finished task 171.0 in stage 11.0 (TID 788). 2131 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 185.0 in stage 11.0 (TID 802, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO TaskSetManager: Finished task 171.0 in stage 11.0 (TID 788) in 165 ms on localhost (170/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/09 15:27:28 INFO Executor: Running task 185.0 in stage 11.0 (TID 802)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO Executor: Finished task 170.0 in stage 11.0 (TID 787). 2131 bytes result sent to driver
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO TaskSetManager: Starting task 186.0 in stage 11.0 (TID 803, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO TaskSetManager: Finished task 170.0 in stage 11.0 (TID 787) in 174 ms on localhost (171/200)
15/08/09 15:27:28 INFO Executor: Running task 186.0 in stage 11.0 (TID 803)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO Executor: Finished task 172.0 in stage 11.0 (TID 789). 2095 bytes result sent to driver
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO TaskSetManager: Starting task 187.0 in stage 11.0 (TID 804, localhost, ANY, 1823 bytes)
15/08/09 15:27:28 INFO Executor: Running task 187.0 in stage 11.0 (TID 804)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 172.0 in stage 11.0 (TID 789) in 161 ms on localhost (172/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 224
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 215
15/08/09 15:27:28 INFO Executor: Finished task 174.0 in stage 11.0 (TID 791). 2130 bytes result sent to driver
15/08/09 15:27:28 INFO Executor: Finished task 167.0 in stage 11.0 (TID 784). 2094 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 188.0 in stage 11.0 (TID 805, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO Executor: Running task 188.0 in stage 11.0 (TID 805)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 174.0 in stage 11.0 (TID 791) in 155 ms on localhost (173/200)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 189.0 in stage 11.0 (TID 806, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO Executor: Running task 189.0 in stage 11.0 (TID 806)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO TaskSetManager: Finished task 167.0 in stage 11.0 (TID 784) in 204 ms on localhost (174/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/09 15:27:28 INFO Executor: Finished task 173.0 in stage 11.0 (TID 790). 1913 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 190.0 in stage 11.0 (TID 807, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO Executor: Running task 190.0 in stage 11.0 (TID 807)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 173.0 in stage 11.0 (TID 790) in 179 ms on localhost (175/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 196
15/08/09 15:27:28 INFO Executor: Finished task 175.0 in stage 11.0 (TID 792). 2113 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 191.0 in stage 11.0 (TID 808, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO Executor: Running task 191.0 in stage 11.0 (TID 808)
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO TaskSetManager: Finished task 175.0 in stage 11.0 (TID 792) in 182 ms on localhost (176/200)
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 176
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 197
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 246
15/08/09 15:27:28 INFO Executor: Finished task 176.0 in stage 11.0 (TID 793). 2095 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 192.0 in stage 11.0 (TID 809, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/09 15:27:28 INFO Executor: Running task 192.0 in stage 11.0 (TID 809)
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO TaskSetManager: Finished task 176.0 in stage 11.0 (TID 793) in 182 ms on localhost (177/200)
15/08/09 15:27:28 INFO Executor: Finished task 178.0 in stage 11.0 (TID 795). 2185 bytes result sent to driver
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 186
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:28 INFO TaskSetManager: Starting task 193.0 in stage 11.0 (TID 810, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO Executor: Running task 193.0 in stage 11.0 (TID 810)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/09 15:27:28 INFO TaskSetManager: Finished task 178.0 in stage 11.0 (TID 795) in 167 ms on localhost (178/200)
15/08/09 15:27:28 INFO Executor: Finished task 177.0 in stage 11.0 (TID 794). 2038 bytes result sent to driver
15/08/09 15:27:28 INFO Executor: Finished task 179.0 in stage 11.0 (TID 796). 2020 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 194.0 in stage 11.0 (TID 811, localhost, ANY, 1825 bytes)
15/08/09 15:27:28 INFO Executor: Running task 194.0 in stage 11.0 (TID 811)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 177.0 in stage 11.0 (TID 794) in 186 ms on localhost (179/200)
15/08/09 15:27:28 INFO Executor: Finished task 181.0 in stage 11.0 (TID 798). 2038 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 195.0 in stage 11.0 (TID 812, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO Executor: Running task 195.0 in stage 11.0 (TID 812)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/08/09 15:27:28 INFO TaskSetManager: Finished task 179.0 in stage 11.0 (TID 796) in 170 ms on localhost (180/200)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 196.0 in stage 11.0 (TID 813, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO Executor: Running task 196.0 in stage 11.0 (TID 813)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 181.0 in stage 11.0 (TID 798) in 172 ms on localhost (181/200)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 63 ms. row count = 135
15/08/09 15:27:28 INFO Executor: Finished task 183.0 in stage 11.0 (TID 800). 2113 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 197.0 in stage 11.0 (TID 814, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 183.0 in stage 11.0 (TID 800) in 214 ms on localhost (182/200)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO Executor: Running task 197.0 in stage 11.0 (TID 814)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 138
15/08/09 15:27:28 INFO Executor: Finished task 180.0 in stage 11.0 (TID 797). 2056 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Starting task 198.0 in stage 11.0 (TID 815, localhost, ANY, 1824 bytes)
15/08/09 15:27:28 INFO Executor: Running task 198.0 in stage 11.0 (TID 815)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO Executor: Finished task 184.0 in stage 11.0 (TID 801). 2020 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Finished task 180.0 in stage 11.0 (TID 797) in 279 ms on localhost (183/200)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 199.0 in stage 11.0 (TID 816, localhost, ANY, 1826 bytes)
15/08/09 15:27:28 INFO Executor: Running task 199.0 in stage 11.0 (TID 816)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 184.0 in stage 11.0 (TID 801) in 233 ms on localhost (184/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:28 INFO Executor: Finished task 186.0 in stage 11.0 (TID 803). 2074 bytes result sent to driver
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/09 15:27:28 INFO TaskSetManager: Finished task 186.0 in stage 11.0 (TID 803) in 224 ms on localhost (185/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO Executor: Finished task 182.0 in stage 11.0 (TID 799). 2056 bytes result sent to driver
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 194
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO TaskSetManager: Finished task 182.0 in stage 11.0 (TID 799) in 271 ms on localhost (186/200)
15/08/09 15:27:28 INFO Executor: Finished task 187.0 in stage 11.0 (TID 804). 2074 bytes result sent to driver
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO TaskSetManager: Finished task 187.0 in stage 11.0 (TID 804) in 224 ms on localhost (187/200)
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 130
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO Executor: Finished task 185.0 in stage 11.0 (TID 802). 2038 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Finished task 185.0 in stage 11.0 (TID 802) in 242 ms on localhost (188/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 145
15/08/09 15:27:28 INFO Executor: Finished task 188.0 in stage 11.0 (TID 805). 2002 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Finished task 188.0 in stage 11.0 (TID 805) in 233 ms on localhost (189/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/09 15:27:28 INFO Executor: Finished task 189.0 in stage 11.0 (TID 806). 2055 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Finished task 189.0 in stage 11.0 (TID 806) in 242 ms on localhost (190/200)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 143
15/08/09 15:27:28 INFO Executor: Finished task 194.0 in stage 11.0 (TID 811). 2054 bytes result sent to driver
15/08/09 15:27:28 INFO Executor: Finished task 190.0 in stage 11.0 (TID 807). 2038 bytes result sent to driver
15/08/09 15:27:28 INFO Executor: Finished task 192.0 in stage 11.0 (TID 809). 2020 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Finished task 194.0 in stage 11.0 (TID 811) in 200 ms on localhost (191/200)
15/08/09 15:27:28 INFO TaskSetManager: Finished task 190.0 in stage 11.0 (TID 807) in 252 ms on localhost (192/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO TaskSetManager: Finished task 192.0 in stage 11.0 (TID 809) in 211 ms on localhost (193/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO Executor: Finished task 193.0 in stage 11.0 (TID 810). 2037 bytes result sent to driver
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/09 15:27:28 INFO TaskSetManager: Finished task 193.0 in stage 11.0 (TID 810) in 212 ms on localhost (194/200)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO Executor: Finished task 191.0 in stage 11.0 (TID 808). 2112 bytes result sent to driver
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO TaskSetManager: Finished task 191.0 in stage 11.0 (TID 808) in 231 ms on localhost (195/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 150
15/08/09 15:27:28 INFO Executor: Finished task 195.0 in stage 11.0 (TID 812). 2074 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Finished task 195.0 in stage 11.0 (TID 812) in 211 ms on localhost (196/200)
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:28 INFO Executor: Finished task 196.0 in stage 11.0 (TID 813). 2113 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Finished task 196.0 in stage 11.0 (TID 813) in 201 ms on localhost (197/200)
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 158
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO Executor: Finished task 197.0 in stage 11.0 (TID 814). 2056 bytes result sent to driver
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 175
15/08/09 15:27:28 INFO TaskSetManager: Finished task 197.0 in stage 11.0 (TID 814) in 128 ms on localhost (198/200)
15/08/09 15:27:28 INFO Executor: Finished task 198.0 in stage 11.0 (TID 815). 2095 bytes result sent to driver
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO TaskSetManager: Finished task 198.0 in stage 11.0 (TID 815) in 119 ms on localhost (199/200)
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/09 15:27:28 INFO Executor: Finished task 199.0 in stage 11.0 (TID 816). 2131 bytes result sent to driver
15/08/09 15:27:28 INFO TaskSetManager: Finished task 199.0 in stage 11.0 (TID 816) in 112 ms on localhost (200/200)
15/08/09 15:27:28 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/08/09 15:27:28 INFO DAGScheduler: Stage 11 (RangePartitioner at Exchange.scala:88) finished in 2.613 s
15/08/09 15:27:28 INFO DAGScheduler: Job 7 finished: RangePartitioner at Exchange.scala:88, took 3.722575 s
15/08/09 15:27:28 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@6214b88d
15/08/09 15:27:28 INFO StatsReportListener: task runtime:(count: 200, mean: 205.555000, stdev: 39.305559, max: 299.000000, min: 112.000000)
15/08/09 15:27:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:28 INFO StatsReportListener: 	112.0 ms	152.0 ms	160.0 ms	173.0 ms	204.0 ms	233.0 ms	265.0 ms	279.0 ms	299.0 ms
15/08/09 15:27:28 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.360000, stdev: 1.323027, max: 14.000000, min: 0.000000)
15/08/09 15:27:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:28 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	14.0 ms
15/08/09 15:27:28 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/09 15:27:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:28 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/09 15:27:28 INFO StatsReportListener: task result size:(count: 200, mean: 2066.055000, stdev: 46.469151, max: 2237.000000, min: 1913.000000)
15/08/09 15:27:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:28 INFO StatsReportListener: 	1913.0 B	2002.0 B	2019.0 B	2037.0 B	2.0 KB	2.0 KB	2.1 KB	2.1 KB	2.2 KB
15/08/09 15:27:28 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 95.864887, stdev: 3.529926, max: 98.880597, min: 71.914894)
15/08/09 15:27:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:28 INFO StatsReportListener: 	72 %	92 %	93 %	95 %	97 %	98 %	98 %	99 %	99 %
15/08/09 15:27:28 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.179295, stdev: 0.661728, max: 7.650273, min: 0.000000)
15/08/09 15:27:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:28 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 1 %	 8 %
15/08/09 15:27:28 INFO StatsReportListener: other time pct: (count: 200, mean: 3.955818, stdev: 3.489276, max: 28.085106, min: 1.119403)
15/08/09 15:27:28 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:28 INFO StatsReportListener: 	 1 %	 1 %	 2 %	 2 %	 3 %	 4 %	 7 %	 8 %	28 %
15/08/09 15:27:28 INFO DefaultExecutionContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/08/09 15:27:28 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 203 bytes
15/08/09 15:27:28 INFO DAGScheduler: Registering RDD 75 (mapPartitions at Exchange.scala:77)
15/08/09 15:27:28 INFO DAGScheduler: Got job 8 (runJob at InsertIntoHiveTable.scala:93) with 200 output partitions (allowLocal=false)
15/08/09 15:27:28 INFO DAGScheduler: Final stage: Stage 14(runJob at InsertIntoHiveTable.scala:93)
15/08/09 15:27:28 INFO DAGScheduler: Parents of final stage: List(Stage 13)
15/08/09 15:27:28 INFO DAGScheduler: Missing parents: List(Stage 13)
15/08/09 15:27:28 INFO DAGScheduler: Submitting Stage 13 (MapPartitionsRDD[75] at mapPartitions at Exchange.scala:77), which has no missing parents
15/08/09 15:27:28 INFO MemoryStore: ensureFreeSpace(16352) called with curMem=1618882, maxMem=3333968363
15/08/09 15:27:28 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 16.0 KB, free 3.1 GB)
15/08/09 15:27:28 INFO MemoryStore: ensureFreeSpace(9186) called with curMem=1635234, maxMem=3333968363
15/08/09 15:27:28 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 9.0 KB, free 3.1 GB)
15/08/09 15:27:28 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:44535 (size: 9.0 KB, free: 3.1 GB)
15/08/09 15:27:28 INFO BlockManagerMaster: Updated info of block broadcast_18_piece0
15/08/09 15:27:28 INFO DefaultExecutionContext: Created broadcast 18 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:28 INFO DAGScheduler: Submitting 200 missing tasks from Stage 13 (MapPartitionsRDD[75] at mapPartitions at Exchange.scala:77)
15/08/09 15:27:28 INFO TaskSchedulerImpl: Adding task set 13.0 with 200 tasks
15/08/09 15:27:28 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 817, localhost, ANY, 1810 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 818, localhost, ANY, 1812 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 819, localhost, ANY, 1813 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 820, localhost, ANY, 1812 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 821, localhost, ANY, 1814 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 822, localhost, ANY, 1813 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 823, localhost, ANY, 1813 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 824, localhost, ANY, 1814 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 825, localhost, ANY, 1813 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 826, localhost, ANY, 1810 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 827, localhost, ANY, 1815 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 828, localhost, ANY, 1814 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 829, localhost, ANY, 1814 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 830, localhost, ANY, 1813 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 14.0 in stage 13.0 (TID 831, localhost, ANY, 1814 bytes)
15/08/09 15:27:28 INFO TaskSetManager: Starting task 15.0 in stage 13.0 (TID 832, localhost, ANY, 1814 bytes)
15/08/09 15:27:28 INFO Executor: Running task 1.0 in stage 13.0 (TID 818)
15/08/09 15:27:28 INFO Executor: Running task 2.0 in stage 13.0 (TID 819)
15/08/09 15:27:28 INFO Executor: Running task 3.0 in stage 13.0 (TID 820)
15/08/09 15:27:28 INFO Executor: Running task 0.0 in stage 13.0 (TID 817)
15/08/09 15:27:28 INFO Executor: Running task 8.0 in stage 13.0 (TID 825)
15/08/09 15:27:28 INFO Executor: Running task 4.0 in stage 13.0 (TID 821)
15/08/09 15:27:28 INFO Executor: Running task 5.0 in stage 13.0 (TID 822)
15/08/09 15:27:28 INFO Executor: Running task 7.0 in stage 13.0 (TID 824)
15/08/09 15:27:28 INFO Executor: Running task 6.0 in stage 13.0 (TID 823)
15/08/09 15:27:28 INFO Executor: Running task 10.0 in stage 13.0 (TID 827)
15/08/09 15:27:28 INFO Executor: Running task 13.0 in stage 13.0 (TID 830)
15/08/09 15:27:28 INFO Executor: Running task 9.0 in stage 13.0 (TID 826)
15/08/09 15:27:28 INFO Executor: Running task 11.0 in stage 13.0 (TID 828)
15/08/09 15:27:28 INFO Executor: Running task 15.0 in stage 13.0 (TID 832)
15/08/09 15:27:28 INFO Executor: Running task 14.0 in stage 13.0 (TID 831)
15/08/09 15:27:28 INFO Executor: Running task 12.0 in stage 13.0 (TID 829)
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00004 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00010 start: 0 end: 2002 length: 2002 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 140 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 156
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 140
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00005 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00009 start: 0 end: 2050 length: 2050 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00000 start: 0 end: 2638 length: 2638 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 144 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 144
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00013 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 177
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00006 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00014 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 193 records.
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00011 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 193
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 154
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00012 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 143
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00007 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00015 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00002 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00008 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00003 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00001 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:28 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:28 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 167
15/08/09 15:27:28 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 142
15/08/09 15:27:29 INFO Executor: Finished task 10.0 in stage 13.0 (TID 827). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 16.0 in stage 13.0 (TID 833, localhost, ANY, 1815 bytes)
15/08/09 15:27:29 INFO Executor: Running task 16.0 in stage 13.0 (TID 833)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 827) in 436 ms on localhost (1/200)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00016 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:29 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/09 15:27:29 INFO Executor: Finished task 9.0 in stage 13.0 (TID 826). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 17.0 in stage 13.0 (TID 834, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO Executor: Running task 17.0 in stage 13.0 (TID 834)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 826) in 659 ms on localhost (2/200)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO Executor: Finished task 4.0 in stage 13.0 (TID 821). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 18.0 in stage 13.0 (TID 835, localhost, ANY, 1815 bytes)
15/08/09 15:27:29 INFO Executor: Running task 18.0 in stage 13.0 (TID 835)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 821) in 694 ms on localhost (3/200)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO Executor: Finished task 12.0 in stage 13.0 (TID 829). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 19.0 in stage 13.0 (TID 836, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 829) in 806 ms on localhost (4/200)
15/08/09 15:27:29 INFO Executor: Running task 19.0 in stage 13.0 (TID 836)
15/08/09 15:27:29 INFO Executor: Finished task 11.0 in stage 13.0 (TID 828). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO Executor: Finished task 14.0 in stage 13.0 (TID 831). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO Executor: Finished task 6.0 in stage 13.0 (TID 823). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO Executor: Finished task 3.0 in stage 13.0 (TID 820). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 20.0 in stage 13.0 (TID 837, localhost, ANY, 1814 bytes)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 828) in 835 ms on localhost (5/200)
15/08/09 15:27:29 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00017 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:29 INFO Executor: Finished task 13.0 in stage 13.0 (TID 830). 2182 bytes result sent to driver
15/08/09 15:27:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:29 INFO Executor: Running task 20.0 in stage 13.0 (TID 837)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:29 INFO Executor: Finished task 5.0 in stage 13.0 (TID 822). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00018 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:29 INFO Executor: Finished task 1.0 in stage 13.0 (TID 818). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 21.0 in stage 13.0 (TID 838, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO Executor: Running task 21.0 in stage 13.0 (TID 838)
15/08/09 15:27:29 INFO TaskSetManager: Starting task 22.0 in stage 13.0 (TID 839, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO Executor: Running task 22.0 in stage 13.0 (TID 839)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO Executor: Finished task 8.0 in stage 13.0 (TID 825). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO Executor: Finished task 2.0 in stage 13.0 (TID 819). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO Executor: Finished task 15.0 in stage 13.0 (TID 832). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 23.0 in stage 13.0 (TID 840, localhost, ANY, 1814 bytes)
15/08/09 15:27:29 INFO Executor: Finished task 7.0 in stage 13.0 (TID 824). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO Executor: Running task 23.0 in stage 13.0 (TID 840)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 14.0 in stage 13.0 (TID 831) in 922 ms on localhost (6/200)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 820) in 932 ms on localhost (7/200)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 823) in 968 ms on localhost (8/200)
15/08/09 15:27:29 INFO TaskSetManager: Starting task 24.0 in stage 13.0 (TID 841, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 830) in 1031 ms on localhost (9/200)
15/08/09 15:27:29 INFO Executor: Running task 24.0 in stage 13.0 (TID 841)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:29 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 153
15/08/09 15:27:29 INFO TaskSetManager: Starting task 25.0 in stage 13.0 (TID 842, localhost, ANY, 1816 bytes)
15/08/09 15:27:29 INFO Executor: Running task 25.0 in stage 13.0 (TID 842)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 822) in 1209 ms on localhost (10/200)
15/08/09 15:27:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:29 INFO TaskSetManager: Starting task 26.0 in stage 13.0 (TID 843, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO Executor: Running task 26.0 in stage 13.0 (TID 843)
15/08/09 15:27:29 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/09 15:27:29 INFO TaskSetManager: Starting task 27.0 in stage 13.0 (TID 844, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO Executor: Running task 27.0 in stage 13.0 (TID 844)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 825) in 1211 ms on localhost (11/200)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 818) in 1219 ms on localhost (12/200)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO Executor: Finished task 0.0 in stage 13.0 (TID 817). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 28.0 in stage 13.0 (TID 845, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO Executor: Running task 28.0 in stage 13.0 (TID 845)
15/08/09 15:27:29 INFO TaskSetManager: Starting task 29.0 in stage 13.0 (TID 846, localhost, ANY, 1811 bytes)
15/08/09 15:27:29 INFO Executor: Running task 29.0 in stage 13.0 (TID 846)
15/08/09 15:27:29 INFO TaskSetManager: Starting task 30.0 in stage 13.0 (TID 847, localhost, ANY, 1814 bytes)
15/08/09 15:27:29 INFO Executor: Running task 30.0 in stage 13.0 (TID 847)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 819) in 1237 ms on localhost (13/200)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 824) in 1236 ms on localhost (14/200)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 15.0 in stage 13.0 (TID 832) in 1233 ms on localhost (15/200)
15/08/09 15:27:29 INFO TaskSetManager: Starting task 31.0 in stage 13.0 (TID 848, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO Executor: Running task 31.0 in stage 13.0 (TID 848)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 817) in 1242 ms on localhost (16/200)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO Executor: Finished task 16.0 in stage 13.0 (TID 833). 2182 bytes result sent to driver
15/08/09 15:27:29 INFO TaskSetManager: Starting task 32.0 in stage 13.0 (TID 849, localhost, ANY, 1813 bytes)
15/08/09 15:27:29 INFO Executor: Running task 32.0 in stage 13.0 (TID 849)
15/08/09 15:27:29 INFO TaskSetManager: Finished task 16.0 in stage 13.0 (TID 833) in 815 ms on localhost (17/200)
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:29 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00019 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:29 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 171
15/08/09 15:27:29 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00022 start: 0 end: 2170 length: 2170 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:29 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:29 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 154 records.
15/08/09 15:27:29 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:29 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 154
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00023 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00021 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 150
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00025 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00026 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00027 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00024 start: 0 end: 2470 length: 2470 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00020 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 153
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 179 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 179
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 153
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00030 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 121
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00028 start: 0 end: 2134 length: 2134 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00031 start: 0 end: 1378 length: 1378 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 151 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 151
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 88 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO Executor: Finished task 18.0 in stage 13.0 (TID 835). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 88
15/08/09 15:27:30 INFO TaskSetManager: Starting task 33.0 in stage 13.0 (TID 850, localhost, ANY, 1814 bytes)
15/08/09 15:27:30 INFO Executor: Running task 33.0 in stage 13.0 (TID 850)
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00029 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 INFO TaskSetManager: Finished task 18.0 in stage 13.0 (TID 835) in 743 ms on localhost (18/200)
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO Executor: Finished task 17.0 in stage 13.0 (TID 834). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 34.0 in stage 13.0 (TID 851, localhost, ANY, 1813 bytes)
15/08/09 15:27:30 INFO Executor: Running task 34.0 in stage 13.0 (TID 851)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO TaskSetManager: Finished task 17.0 in stage 13.0 (TID 834) in 788 ms on localhost (19/200)
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00032 start: 0 end: 2302 length: 2302 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 165 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 196
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 165
15/08/09 15:27:30 INFO Executor: Finished task 19.0 in stage 13.0 (TID 836). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 35.0 in stage 13.0 (TID 852, localhost, ANY, 1814 bytes)
15/08/09 15:27:30 INFO Executor: Running task 35.0 in stage 13.0 (TID 852)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 19.0 in stage 13.0 (TID 836) in 687 ms on localhost (20/200)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00033 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/09 15:27:30 INFO Executor: Finished task 22.0 in stage 13.0 (TID 839). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 36.0 in stage 13.0 (TID 853, localhost, ANY, 1815 bytes)
15/08/09 15:27:30 INFO Executor: Running task 36.0 in stage 13.0 (TID 853)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 22.0 in stage 13.0 (TID 839) in 755 ms on localhost (21/200)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00034 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00035 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 171
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00036 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/09 15:27:30 INFO Executor: Finished task 25.0 in stage 13.0 (TID 842). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 37.0 in stage 13.0 (TID 854, localhost, ANY, 1814 bytes)
15/08/09 15:27:30 INFO Executor: Running task 37.0 in stage 13.0 (TID 854)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 25.0 in stage 13.0 (TID 842) in 842 ms on localhost (22/200)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO Executor: Finished task 26.0 in stage 13.0 (TID 843). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 38.0 in stage 13.0 (TID 855, localhost, ANY, 1814 bytes)
15/08/09 15:27:30 INFO Executor: Running task 38.0 in stage 13.0 (TID 855)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 26.0 in stage 13.0 (TID 843) in 747 ms on localhost (23/200)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO Executor: Finished task 21.0 in stage 13.0 (TID 838). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO Executor: Finished task 27.0 in stage 13.0 (TID 844). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO Executor: Finished task 23.0 in stage 13.0 (TID 840). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 39.0 in stage 13.0 (TID 856, localhost, ANY, 1815 bytes)
15/08/09 15:27:30 INFO Executor: Running task 39.0 in stage 13.0 (TID 856)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 21.0 in stage 13.0 (TID 838) in 1155 ms on localhost (24/200)
15/08/09 15:27:30 INFO Executor: Finished task 30.0 in stage 13.0 (TID 847). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00037 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO TaskSetManager: Starting task 40.0 in stage 13.0 (TID 857, localhost, ANY, 1815 bytes)
15/08/09 15:27:30 INFO Executor: Running task 40.0 in stage 13.0 (TID 857)
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO TaskSetManager: Starting task 41.0 in stage 13.0 (TID 858, localhost, ANY, 1815 bytes)
15/08/09 15:27:30 INFO Executor: Running task 41.0 in stage 13.0 (TID 858)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 27.0 in stage 13.0 (TID 844) in 807 ms on localhost (25/200)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 23.0 in stage 13.0 (TID 840) in 1157 ms on localhost (26/200)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO TaskSetManager: Starting task 42.0 in stage 13.0 (TID 859, localhost, ANY, 1816 bytes)
15/08/09 15:27:30 INFO Executor: Running task 42.0 in stage 13.0 (TID 859)
15/08/09 15:27:30 INFO Executor: Finished task 20.0 in stage 13.0 (TID 837). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Finished task 30.0 in stage 13.0 (TID 847) in 796 ms on localhost (27/200)
15/08/09 15:27:30 INFO Executor: Finished task 24.0 in stage 13.0 (TID 841). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 43.0 in stage 13.0 (TID 860, localhost, ANY, 1813 bytes)
15/08/09 15:27:30 INFO Executor: Running task 43.0 in stage 13.0 (TID 860)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO TaskSetManager: Finished task 20.0 in stage 13.0 (TID 837) in 1212 ms on localhost (28/200)
15/08/09 15:27:30 INFO Executor: Finished task 28.0 in stage 13.0 (TID 845). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO TaskSetManager: Starting task 44.0 in stage 13.0 (TID 861, localhost, ANY, 1814 bytes)
15/08/09 15:27:30 INFO Executor: Running task 44.0 in stage 13.0 (TID 861)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 24.0 in stage 13.0 (TID 841) in 1076 ms on localhost (29/200)
15/08/09 15:27:30 INFO TaskSetManager: Starting task 45.0 in stage 13.0 (TID 862, localhost, ANY, 1814 bytes)
15/08/09 15:27:30 INFO Executor: Running task 45.0 in stage 13.0 (TID 862)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 28.0 in stage 13.0 (TID 845) in 828 ms on localhost (30/200)
15/08/09 15:27:30 INFO Executor: Finished task 31.0 in stage 13.0 (TID 848). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO Executor: Finished task 29.0 in stage 13.0 (TID 846). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:30 INFO TaskSetManager: Starting task 46.0 in stage 13.0 (TID 863, localhost, ANY, 1816 bytes)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 31.0 in stage 13.0 (TID 848) in 818 ms on localhost (31/200)
15/08/09 15:27:30 INFO Executor: Running task 46.0 in stage 13.0 (TID 863)
15/08/09 15:27:30 INFO TaskSetManager: Starting task 47.0 in stage 13.0 (TID 864, localhost, ANY, 1816 bytes)
15/08/09 15:27:30 INFO Executor: Running task 47.0 in stage 13.0 (TID 864)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 29.0 in stage 13.0 (TID 846) in 829 ms on localhost (32/200)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO Executor: Finished task 32.0 in stage 13.0 (TID 849). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO Executor: Finished task 33.0 in stage 13.0 (TID 850). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 48.0 in stage 13.0 (TID 865, localhost, ANY, 1814 bytes)
15/08/09 15:27:30 INFO Executor: Running task 48.0 in stage 13.0 (TID 865)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 32.0 in stage 13.0 (TID 849) in 827 ms on localhost (33/200)
15/08/09 15:27:30 INFO TaskSetManager: Starting task 49.0 in stage 13.0 (TID 866, localhost, ANY, 1816 bytes)
15/08/09 15:27:30 INFO Executor: Running task 49.0 in stage 13.0 (TID 866)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 33.0 in stage 13.0 (TID 850) in 650 ms on localhost (34/200)
15/08/09 15:27:30 INFO Executor: Finished task 34.0 in stage 13.0 (TID 851). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00038 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO TaskSetManager: Starting task 50.0 in stage 13.0 (TID 867, localhost, ANY, 1814 bytes)
15/08/09 15:27:30 INFO Executor: Running task 50.0 in stage 13.0 (TID 867)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:30 INFO Executor: Finished task 35.0 in stage 13.0 (TID 852). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Finished task 34.0 in stage 13.0 (TID 851) in 643 ms on localhost (35/200)
15/08/09 15:27:30 INFO TaskSetManager: Starting task 51.0 in stage 13.0 (TID 868, localhost, ANY, 1816 bytes)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO Executor: Running task 51.0 in stage 13.0 (TID 868)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO TaskSetManager: Finished task 35.0 in stage 13.0 (TID 852) in 596 ms on localhost (36/200)
15/08/09 15:27:30 INFO Executor: Finished task 36.0 in stage 13.0 (TID 853). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO TaskSetManager: Starting task 52.0 in stage 13.0 (TID 869, localhost, ANY, 1815 bytes)
15/08/09 15:27:30 INFO Executor: Running task 52.0 in stage 13.0 (TID 869)
15/08/09 15:27:30 INFO TaskSetManager: Finished task 36.0 in stage 13.0 (TID 853) in 482 ms on localhost (37/200)
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00039 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00040 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00043 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00048 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 135
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00041 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00051 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO Executor: Finished task 37.0 in stage 13.0 (TID 854). 2182 bytes result sent to driver
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO TaskSetManager: Starting task 53.0 in stage 13.0 (TID 870, localhost, ANY, 1816 bytes)
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO Executor: Running task 53.0 in stage 13.0 (TID 870)
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 118
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00042 start: 0 end: 1762 length: 1762 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 121
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO TaskSetManager: Finished task 37.0 in stage 13.0 (TID 854) in 427 ms on localhost (38/200)
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 172
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 120 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 120
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00046 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00052 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00045 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00047 start: 0 end: 1774 length: 1774 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00049 start: 0 end: 1462 length: 1462 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 124
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/09 15:27:30 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00050 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:30 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 141
15/08/09 15:27:30 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 121 records.
15/08/09 15:27:30 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:30 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 121
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00044 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 95 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 95
15/08/09 15:27:31 INFO Executor: Finished task 38.0 in stage 13.0 (TID 855). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO TaskSetManager: Starting task 54.0 in stage 13.0 (TID 871, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 54.0 in stage 13.0 (TID 871)
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 129
15/08/09 15:27:31 INFO TaskSetManager: Finished task 38.0 in stage 13.0 (TID 855) in 435 ms on localhost (39/200)
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00053 start: 0 end: 1738 length: 1738 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 118 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 118
15/08/09 15:27:31 INFO Executor: Finished task 39.0 in stage 13.0 (TID 856). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 55.0 in stage 13.0 (TID 872, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 39.0 in stage 13.0 (TID 856) in 471 ms on localhost (40/200)
15/08/09 15:27:31 INFO Executor: Running task 55.0 in stage 13.0 (TID 872)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO Executor: Finished task 40.0 in stage 13.0 (TID 857). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 56.0 in stage 13.0 (TID 873, localhost, ANY, 1814 bytes)
15/08/09 15:27:31 INFO Executor: Running task 56.0 in stage 13.0 (TID 873)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 40.0 in stage 13.0 (TID 857) in 501 ms on localhost (41/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00054 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00055 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00056 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 103
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/09 15:27:31 INFO Executor: Finished task 43.0 in stage 13.0 (TID 860). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 57.0 in stage 13.0 (TID 874, localhost, ANY, 1814 bytes)
15/08/09 15:27:31 INFO Executor: Running task 57.0 in stage 13.0 (TID 874)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 43.0 in stage 13.0 (TID 860) in 677 ms on localhost (42/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO Executor: Finished task 48.0 in stage 13.0 (TID 865). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO Executor: Finished task 42.0 in stage 13.0 (TID 859). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 58.0 in stage 13.0 (TID 875, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 58.0 in stage 13.0 (TID 875)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 48.0 in stage 13.0 (TID 865) in 675 ms on localhost (43/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO TaskSetManager: Starting task 59.0 in stage 13.0 (TID 876, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 59.0 in stage 13.0 (TID 876)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 42.0 in stage 13.0 (TID 859) in 739 ms on localhost (44/200)
15/08/09 15:27:31 INFO Executor: Finished task 51.0 in stage 13.0 (TID 868). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:31 INFO TaskSetManager: Starting task 60.0 in stage 13.0 (TID 877, localhost, ANY, 1814 bytes)
15/08/09 15:27:31 INFO Executor: Running task 60.0 in stage 13.0 (TID 877)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 51.0 in stage 13.0 (TID 868) in 690 ms on localhost (45/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00057 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/09 15:27:31 INFO Executor: Finished task 46.0 in stage 13.0 (TID 863). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00058 start: 0 end: 1666 length: 1666 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 INFO Executor: Finished task 44.0 in stage 13.0 (TID 861). 2182 bytes result sent to driver
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO TaskSetManager: Starting task 61.0 in stage 13.0 (TID 878, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 61.0 in stage 13.0 (TID 878)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 46.0 in stage 13.0 (TID 863) in 794 ms on localhost (46/200)
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 112 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO Executor: Finished task 49.0 in stage 13.0 (TID 866). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 112
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO TaskSetManager: Starting task 62.0 in stage 13.0 (TID 879, localhost, ANY, 1814 bytes)
15/08/09 15:27:31 INFO Executor: Running task 62.0 in stage 13.0 (TID 879)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 44.0 in stage 13.0 (TID 861) in 814 ms on localhost (47/200)
15/08/09 15:27:31 INFO TaskSetManager: Starting task 63.0 in stage 13.0 (TID 880, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 63.0 in stage 13.0 (TID 880)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO TaskSetManager: Finished task 49.0 in stage 13.0 (TID 866) in 779 ms on localhost (48/200)
15/08/09 15:27:31 INFO Executor: Finished task 50.0 in stage 13.0 (TID 867). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00059 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 INFO Executor: Finished task 41.0 in stage 13.0 (TID 858). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO Executor: Finished task 52.0 in stage 13.0 (TID 869). 2182 bytes result sent to driver
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO TaskSetManager: Starting task 64.0 in stage 13.0 (TID 881, localhost, ANY, 1816 bytes)
15/08/09 15:27:31 INFO Executor: Running task 64.0 in stage 13.0 (TID 881)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 50.0 in stage 13.0 (TID 867) in 783 ms on localhost (49/200)
15/08/09 15:27:31 INFO TaskSetManager: Starting task 65.0 in stage 13.0 (TID 882, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 41.0 in stage 13.0 (TID 858) in 859 ms on localhost (50/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO Executor: Running task 65.0 in stage 13.0 (TID 882)
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO TaskSetManager: Starting task 66.0 in stage 13.0 (TID 883, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 66.0 in stage 13.0 (TID 883)
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/09 15:27:31 INFO TaskSetManager: Finished task 52.0 in stage 13.0 (TID 869) in 784 ms on localhost (51/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:31 INFO Executor: Finished task 47.0 in stage 13.0 (TID 864). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO Executor: Finished task 53.0 in stage 13.0 (TID 870). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00060 start: 0 end: 2758 length: 2758 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO TaskSetManager: Starting task 67.0 in stage 13.0 (TID 884, localhost, ANY, 1816 bytes)
15/08/09 15:27:31 INFO Executor: Running task 67.0 in stage 13.0 (TID 884)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 47.0 in stage 13.0 (TID 864) in 835 ms on localhost (52/200)
15/08/09 15:27:31 INFO Executor: Finished task 45.0 in stage 13.0 (TID 862). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 68.0 in stage 13.0 (TID 885, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 68.0 in stage 13.0 (TID 885)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 53.0 in stage 13.0 (TID 870) in 603 ms on localhost (53/200)
15/08/09 15:27:31 INFO TaskSetManager: Starting task 69.0 in stage 13.0 (TID 886, localhost, ANY, 1816 bytes)
15/08/09 15:27:31 INFO Executor: Running task 69.0 in stage 13.0 (TID 886)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 45.0 in stage 13.0 (TID 862) in 854 ms on localhost (54/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 203 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO Executor: Finished task 54.0 in stage 13.0 (TID 871). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 6 ms. row count = 203
15/08/09 15:27:31 INFO TaskSetManager: Starting task 70.0 in stage 13.0 (TID 887, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 54.0 in stage 13.0 (TID 871) in 544 ms on localhost (55/200)
15/08/09 15:27:31 INFO Executor: Running task 70.0 in stage 13.0 (TID 887)
15/08/09 15:27:31 INFO Executor: Finished task 56.0 in stage 13.0 (TID 873). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 71.0 in stage 13.0 (TID 888, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 71.0 in stage 13.0 (TID 888)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 56.0 in stage 13.0 (TID 873) in 424 ms on localhost (56/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:31 INFO Executor: Finished task 55.0 in stage 13.0 (TID 872). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 72.0 in stage 13.0 (TID 889, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 72.0 in stage 13.0 (TID 889)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 55.0 in stage 13.0 (TID 872) in 481 ms on localhost (57/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO Executor: Finished task 57.0 in stage 13.0 (TID 874). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00063 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 INFO TaskSetManager: Starting task 73.0 in stage 13.0 (TID 890, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO TaskSetManager: Finished task 57.0 in stage 13.0 (TID 874) in 265 ms on localhost (58/200)
15/08/09 15:27:31 INFO Executor: Running task 73.0 in stage 13.0 (TID 890)
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00061 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00062 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO Executor: Finished task 58.0 in stage 13.0 (TID 875). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 74.0 in stage 13.0 (TID 891, localhost, ANY, 1813 bytes)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 58.0 in stage 13.0 (TID 875) in 274 ms on localhost (59/200)
15/08/09 15:27:31 INFO Executor: Running task 74.0 in stage 13.0 (TID 891)
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00065 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00066 start: 0 end: 2446 length: 2446 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 177 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 177
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00064 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 159
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00067 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/09 15:27:31 INFO Executor: Finished task 59.0 in stage 13.0 (TID 876). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 75.0 in stage 13.0 (TID 892, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO TaskSetManager: Finished task 59.0 in stage 13.0 (TID 876) in 383 ms on localhost (60/200)
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/09 15:27:31 INFO Executor: Running task 75.0 in stage 13.0 (TID 892)
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00070 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00069 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00068 start: 0 end: 1786 length: 1786 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 160
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00072 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00071 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO Executor: Finished task 60.0 in stage 13.0 (TID 877). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 122 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO TaskSetManager: Starting task 76.0 in stage 13.0 (TID 893, localhost, ANY, 1816 bytes)
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 122
15/08/09 15:27:31 INFO Executor: Running task 76.0 in stage 13.0 (TID 893)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 60.0 in stage 13.0 (TID 877) in 401 ms on localhost (61/200)
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 160
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00074 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00073 start: 0 end: 1966 length: 1966 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 137 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 148
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 137
15/08/09 15:27:31 INFO Executor: Finished task 63.0 in stage 13.0 (TID 880). 2182 bytes result sent to driver
15/08/09 15:27:31 INFO TaskSetManager: Starting task 77.0 in stage 13.0 (TID 894, localhost, ANY, 1815 bytes)
15/08/09 15:27:31 INFO Executor: Running task 77.0 in stage 13.0 (TID 894)
15/08/09 15:27:31 INFO TaskSetManager: Finished task 63.0 in stage 13.0 (TID 880) in 413 ms on localhost (62/200)
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00075 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/09 15:27:31 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00076 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:31 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:31 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:31 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:31 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 134
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00077 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 124
15/08/09 15:27:32 INFO Executor: Finished task 62.0 in stage 13.0 (TID 879). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 78.0 in stage 13.0 (TID 895, localhost, ANY, 1813 bytes)
15/08/09 15:27:32 INFO Executor: Running task 78.0 in stage 13.0 (TID 895)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 62.0 in stage 13.0 (TID 879) in 612 ms on localhost (63/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO Executor: Finished task 66.0 in stage 13.0 (TID 883). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 79.0 in stage 13.0 (TID 896, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 79.0 in stage 13.0 (TID 896)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 66.0 in stage 13.0 (TID 883) in 699 ms on localhost (64/200)
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00078 start: 0 end: 2614 length: 2614 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 191 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 191
15/08/09 15:27:32 INFO Executor: Finished task 61.0 in stage 13.0 (TID 878). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO Executor: Finished task 65.0 in stage 13.0 (TID 882). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 80.0 in stage 13.0 (TID 897, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 80.0 in stage 13.0 (TID 897)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 61.0 in stage 13.0 (TID 878) in 786 ms on localhost (65/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO TaskSetManager: Starting task 81.0 in stage 13.0 (TID 898, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 81.0 in stage 13.0 (TID 898)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 65.0 in stage 13.0 (TID 882) in 759 ms on localhost (66/200)
15/08/09 15:27:32 INFO Executor: Finished task 64.0 in stage 13.0 (TID 881). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO Executor: Finished task 67.0 in stage 13.0 (TID 884). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO TaskSetManager: Starting task 82.0 in stage 13.0 (TID 899, localhost, ANY, 1813 bytes)
15/08/09 15:27:32 INFO Executor: Running task 82.0 in stage 13.0 (TID 899)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 64.0 in stage 13.0 (TID 881) in 776 ms on localhost (67/200)
15/08/09 15:27:32 INFO Executor: Finished task 69.0 in stage 13.0 (TID 886). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:32 INFO TaskSetManager: Starting task 83.0 in stage 13.0 (TID 900, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 83.0 in stage 13.0 (TID 900)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 67.0 in stage 13.0 (TID 884) in 772 ms on localhost (68/200)
15/08/09 15:27:32 INFO Executor: Finished task 70.0 in stage 13.0 (TID 887). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO Executor: Finished task 72.0 in stage 13.0 (TID 889). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 84.0 in stage 13.0 (TID 901, localhost, ANY, 1815 bytes)
15/08/09 15:27:32 INFO Executor: Running task 84.0 in stage 13.0 (TID 901)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:32 INFO TaskSetManager: Starting task 85.0 in stage 13.0 (TID 902, localhost, ANY, 1815 bytes)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 69.0 in stage 13.0 (TID 886) in 763 ms on localhost (69/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO Executor: Running task 85.0 in stage 13.0 (TID 902)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 70.0 in stage 13.0 (TID 887) in 751 ms on localhost (70/200)
15/08/09 15:27:32 INFO TaskSetManager: Starting task 86.0 in stage 13.0 (TID 903, localhost, ANY, 1815 bytes)
15/08/09 15:27:32 INFO Executor: Running task 86.0 in stage 13.0 (TID 903)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 72.0 in stage 13.0 (TID 889) in 733 ms on localhost (71/200)
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00079 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO Executor: Finished task 68.0 in stage 13.0 (TID 885). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/09 15:27:32 INFO Executor: Finished task 71.0 in stage 13.0 (TID 888). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 87.0 in stage 13.0 (TID 904, localhost, ANY, 1813 bytes)
15/08/09 15:27:32 INFO Executor: Running task 87.0 in stage 13.0 (TID 904)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 68.0 in stage 13.0 (TID 885) in 787 ms on localhost (72/200)
15/08/09 15:27:32 INFO TaskSetManager: Starting task 88.0 in stage 13.0 (TID 905, localhost, ANY, 1813 bytes)
15/08/09 15:27:32 INFO Executor: Running task 88.0 in stage 13.0 (TID 905)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 71.0 in stage 13.0 (TID 888) in 778 ms on localhost (73/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO Executor: Finished task 74.0 in stage 13.0 (TID 891). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:32 INFO TaskSetManager: Starting task 89.0 in stage 13.0 (TID 906, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 89.0 in stage 13.0 (TID 906)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 74.0 in stage 13.0 (TID 891) in 697 ms on localhost (74/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO Executor: Finished task 75.0 in stage 13.0 (TID 892). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO Executor: Finished task 73.0 in stage 13.0 (TID 890). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO Executor: Finished task 76.0 in stage 13.0 (TID 893). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 90.0 in stage 13.0 (TID 907, localhost, ANY, 1812 bytes)
15/08/09 15:27:32 INFO Executor: Running task 90.0 in stage 13.0 (TID 907)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 75.0 in stage 13.0 (TID 892) in 592 ms on localhost (75/200)
15/08/09 15:27:32 INFO TaskSetManager: Starting task 91.0 in stage 13.0 (TID 908, localhost, ANY, 1815 bytes)
15/08/09 15:27:32 INFO Executor: Running task 91.0 in stage 13.0 (TID 908)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 73.0 in stage 13.0 (TID 890) in 762 ms on localhost (76/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO TaskSetManager: Starting task 92.0 in stage 13.0 (TID 909, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 92.0 in stage 13.0 (TID 909)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 76.0 in stage 13.0 (TID 893) in 559 ms on localhost (77/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00080 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO Executor: Finished task 77.0 in stage 13.0 (TID 894). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 93.0 in stage 13.0 (TID 910, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 93.0 in stage 13.0 (TID 910)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 77.0 in stage 13.0 (TID 894) in 482 ms on localhost (78/200)
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 145
15/08/09 15:27:32 INFO Executor: Finished task 78.0 in stage 13.0 (TID 895). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 94.0 in stage 13.0 (TID 911, localhost, ANY, 1812 bytes)
15/08/09 15:27:32 INFO Executor: Running task 94.0 in stage 13.0 (TID 911)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 78.0 in stage 13.0 (TID 895) in 339 ms on localhost (79/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00084 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00082 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00086 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00081 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00083 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00085 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 163
15/08/09 15:27:32 INFO Executor: Finished task 79.0 in stage 13.0 (TID 896). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 95.0 in stage 13.0 (TID 912, localhost, ANY, 1816 bytes)
15/08/09 15:27:32 INFO Executor: Running task 95.0 in stage 13.0 (TID 912)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 79.0 in stage 13.0 (TID 896) in 282 ms on localhost (80/200)
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00087 start: 0 end: 2146 length: 2146 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00088 start: 0 end: 2722 length: 2722 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 152 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 200 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 152
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 200
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00091 start: 0 end: 1750 length: 1750 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00092 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 119 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 119
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00093 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00089 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00094 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 133
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00090 start: 0 end: 2398 length: 2398 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 105 ms. row count = 182
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 196
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 0 ms. row count = 108
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 173 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 173
15/08/09 15:27:32 INFO Executor: Finished task 80.0 in stage 13.0 (TID 897). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 96.0 in stage 13.0 (TID 913, localhost, ANY, 1815 bytes)
15/08/09 15:27:32 INFO Executor: Running task 96.0 in stage 13.0 (TID 913)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 80.0 in stage 13.0 (TID 897) in 471 ms on localhost (81/200)
15/08/09 15:27:32 INFO Executor: Finished task 86.0 in stage 13.0 (TID 903). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO TaskSetManager: Starting task 97.0 in stage 13.0 (TID 914, localhost, ANY, 1815 bytes)
15/08/09 15:27:32 INFO Executor: Running task 97.0 in stage 13.0 (TID 914)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 86.0 in stage 13.0 (TID 903) in 433 ms on localhost (82/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00095 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 7 ms. row count = 142
15/08/09 15:27:32 INFO Executor: Finished task 82.0 in stage 13.0 (TID 899). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO Executor: Finished task 84.0 in stage 13.0 (TID 901). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 98.0 in stage 13.0 (TID 915, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 98.0 in stage 13.0 (TID 915)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 82.0 in stage 13.0 (TID 899) in 551 ms on localhost (83/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO TaskSetManager: Starting task 99.0 in stage 13.0 (TID 916, localhost, ANY, 1815 bytes)
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00096 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 INFO Executor: Running task 99.0 in stage 13.0 (TID 916)
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO TaskSetManager: Finished task 84.0 in stage 13.0 (TID 901) in 552 ms on localhost (84/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 114
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00097 start: 0 end: 2194 length: 2194 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 156 records.
15/08/09 15:27:32 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:32 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 156
15/08/09 15:27:32 INFO Executor: Finished task 81.0 in stage 13.0 (TID 898). 2182 bytes result sent to driver
15/08/09 15:27:32 INFO TaskSetManager: Starting task 100.0 in stage 13.0 (TID 917, localhost, ANY, 1814 bytes)
15/08/09 15:27:32 INFO Executor: Running task 100.0 in stage 13.0 (TID 917)
15/08/09 15:27:32 INFO TaskSetManager: Finished task 81.0 in stage 13.0 (TID 898) in 693 ms on localhost (85/200)
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00098 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:32 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00099 start: 0 end: 2266 length: 2266 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:32 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 162 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 162
15/08/09 15:27:33 INFO Executor: Finished task 83.0 in stage 13.0 (TID 900). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 101.0 in stage 13.0 (TID 918, localhost, ANY, 1812 bytes)
15/08/09 15:27:33 INFO Executor: Running task 101.0 in stage 13.0 (TID 918)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 83.0 in stage 13.0 (TID 900) in 773 ms on localhost (86/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO Executor: Finished task 85.0 in stage 13.0 (TID 902). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 102.0 in stage 13.0 (TID 919, localhost, ANY, 1813 bytes)
15/08/09 15:27:33 INFO Executor: Running task 102.0 in stage 13.0 (TID 919)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 85.0 in stage 13.0 (TID 902) in 780 ms on localhost (87/200)
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00100 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO Executor: Finished task 87.0 in stage 13.0 (TID 904). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:33 INFO TaskSetManager: Starting task 103.0 in stage 13.0 (TID 920, localhost, ANY, 1813 bytes)
15/08/09 15:27:33 INFO Executor: Running task 103.0 in stage 13.0 (TID 920)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 87.0 in stage 13.0 (TID 904) in 791 ms on localhost (88/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO Executor: Finished task 91.0 in stage 13.0 (TID 908). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO Executor: Finished task 88.0 in stage 13.0 (TID 905). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 104.0 in stage 13.0 (TID 921, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO Executor: Running task 104.0 in stage 13.0 (TID 921)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 91.0 in stage 13.0 (TID 908) in 775 ms on localhost (89/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:33 INFO Executor: Finished task 94.0 in stage 13.0 (TID 911). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 105.0 in stage 13.0 (TID 922, localhost, ANY, 1815 bytes)
15/08/09 15:27:33 INFO Executor: Running task 105.0 in stage 13.0 (TID 922)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 88.0 in stage 13.0 (TID 905) in 822 ms on localhost (90/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO TaskSetManager: Starting task 106.0 in stage 13.0 (TID 923, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO Executor: Running task 106.0 in stage 13.0 (TID 923)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 94.0 in stage 13.0 (TID 911) in 747 ms on localhost (91/200)
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00101 start: 0 end: 2410 length: 2410 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 174 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 174
15/08/09 15:27:33 INFO Executor: Finished task 93.0 in stage 13.0 (TID 910). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO Executor: Finished task 92.0 in stage 13.0 (TID 909). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 107.0 in stage 13.0 (TID 924, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO Executor: Running task 107.0 in stage 13.0 (TID 924)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 93.0 in stage 13.0 (TID 910) in 798 ms on localhost (92/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO Executor: Finished task 89.0 in stage 13.0 (TID 906). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 108.0 in stage 13.0 (TID 925, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00102 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO TaskSetManager: Finished task 92.0 in stage 13.0 (TID 909) in 820 ms on localhost (93/200)
15/08/09 15:27:33 INFO Executor: Running task 108.0 in stage 13.0 (TID 925)
15/08/09 15:27:33 INFO Executor: Finished task 90.0 in stage 13.0 (TID 907). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO Executor: Finished task 95.0 in stage 13.0 (TID 912). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:33 INFO TaskSetManager: Starting task 109.0 in stage 13.0 (TID 926, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO Executor: Running task 109.0 in stage 13.0 (TID 926)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 89.0 in stage 13.0 (TID 906) in 866 ms on localhost (94/200)
15/08/09 15:27:33 INFO Executor: Finished task 97.0 in stage 13.0 (TID 914). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:33 INFO TaskSetManager: Starting task 110.0 in stage 13.0 (TID 927, localhost, ANY, 1813 bytes)
15/08/09 15:27:33 INFO Executor: Running task 110.0 in stage 13.0 (TID 927)
15/08/09 15:27:33 INFO TaskSetManager: Starting task 111.0 in stage 13.0 (TID 928, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO Executor: Running task 111.0 in stage 13.0 (TID 928)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 90.0 in stage 13.0 (TID 907) in 863 ms on localhost (95/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO TaskSetManager: Starting task 112.0 in stage 13.0 (TID 929, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO Executor: Running task 112.0 in stage 13.0 (TID 929)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 95.0 in stage 13.0 (TID 912) in 740 ms on localhost (96/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO TaskSetManager: Finished task 97.0 in stage 13.0 (TID 914) in 490 ms on localhost (97/200)
15/08/09 15:27:33 INFO Executor: Finished task 96.0 in stage 13.0 (TID 913). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 113.0 in stage 13.0 (TID 930, localhost, ANY, 1815 bytes)
15/08/09 15:27:33 INFO Executor: Running task 113.0 in stage 13.0 (TID 930)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 96.0 in stage 13.0 (TID 913) in 526 ms on localhost (98/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00104 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00103 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00106 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 129
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00105 start: 0 end: 1810 length: 1810 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 176
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO Executor: Finished task 99.0 in stage 13.0 (TID 916). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 114.0 in stage 13.0 (TID 931, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO Executor: Running task 114.0 in stage 13.0 (TID 931)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 99.0 in stage 13.0 (TID 916) in 471 ms on localhost (99/200)
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 172
15/08/09 15:27:33 INFO Executor: Finished task 98.0 in stage 13.0 (TID 915). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 115.0 in stage 13.0 (TID 932, localhost, ANY, 1814 bytes)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:33 INFO Executor: Running task 115.0 in stage 13.0 (TID 932)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 98.0 in stage 13.0 (TID 915) in 485 ms on localhost (100/200)
15/08/09 15:27:33 INFO Executor: Finished task 100.0 in stage 13.0 (TID 917). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 116.0 in stage 13.0 (TID 933, localhost, ANY, 1815 bytes)
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 124 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO TaskSetManager: Finished task 100.0 in stage 13.0 (TID 917) in 363 ms on localhost (101/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO Executor: Running task 116.0 in stage 13.0 (TID 933)
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 124
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00107 start: 0 end: 2326 length: 2326 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 167 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 167
15/08/09 15:27:33 INFO Executor: Finished task 102.0 in stage 13.0 (TID 919). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 117.0 in stage 13.0 (TID 934, localhost, ANY, 1813 bytes)
15/08/09 15:27:33 INFO Executor: Running task 117.0 in stage 13.0 (TID 934)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 102.0 in stage 13.0 (TID 919) in 295 ms on localhost (102/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00111 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 INFO Executor: Finished task 101.0 in stage 13.0 (TID 918). 2182 bytes result sent to driver
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00108 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 INFO TaskSetManager: Starting task 118.0 in stage 13.0 (TID 935, localhost, ANY, 1815 bytes)
15/08/09 15:27:33 INFO Executor: Running task 118.0 in stage 13.0 (TID 935)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 101.0 in stage 13.0 (TID 918) in 335 ms on localhost (103/200)
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00109 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 147
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00112 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 155
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 149
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00110 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00113 start: 0 end: 2242 length: 2242 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 178
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00114 start: 0 end: 2794 length: 2794 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 160 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 160
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00116 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00115 start: 0 end: 1858 length: 1858 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 128 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 206 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 128
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 206
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00118 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 138
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00117 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 189
15/08/09 15:27:33 INFO Executor: Finished task 103.0 in stage 13.0 (TID 920). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 119.0 in stage 13.0 (TID 936, localhost, ANY, 1813 bytes)
15/08/09 15:27:33 INFO Executor: Running task 119.0 in stage 13.0 (TID 936)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 103.0 in stage 13.0 (TID 920) in 732 ms on localhost (104/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO Executor: Finished task 104.0 in stage 13.0 (TID 921). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 120.0 in stage 13.0 (TID 937, localhost, ANY, 1812 bytes)
15/08/09 15:27:33 INFO Executor: Running task 120.0 in stage 13.0 (TID 937)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 104.0 in stage 13.0 (TID 921) in 751 ms on localhost (105/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:33 INFO Executor: Finished task 106.0 in stage 13.0 (TID 923). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO TaskSetManager: Starting task 121.0 in stage 13.0 (TID 938, localhost, ANY, 1812 bytes)
15/08/09 15:27:33 INFO Executor: Running task 121.0 in stage 13.0 (TID 938)
15/08/09 15:27:33 INFO TaskSetManager: Finished task 106.0 in stage 13.0 (TID 923) in 748 ms on localhost (106/200)
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00119 start: 0 end: 2374 length: 2374 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 171 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 171
15/08/09 15:27:33 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00120 start: 0 end: 2626 length: 2626 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:33 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:33 INFO Executor: Finished task 105.0 in stage 13.0 (TID 922). 2182 bytes result sent to driver
15/08/09 15:27:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 192 records.
15/08/09 15:27:33 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:33 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 192
15/08/09 15:27:33 INFO TaskSetManager: Starting task 122.0 in stage 13.0 (TID 939, localhost, ANY, 1812 bytes)
15/08/09 15:27:33 INFO Executor: Running task 122.0 in stage 13.0 (TID 939)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 105.0 in stage 13.0 (TID 922) in 856 ms on localhost (107/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO Executor: Finished task 107.0 in stage 13.0 (TID 924). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 123.0 in stage 13.0 (TID 940, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 123.0 in stage 13.0 (TID 940)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 107.0 in stage 13.0 (TID 924) in 829 ms on localhost (108/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00121 start: 0 end: 2158 length: 2158 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 153 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 153
15/08/09 15:27:34 INFO Executor: Finished task 108.0 in stage 13.0 (TID 925). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 124.0 in stage 13.0 (TID 941, localhost, ANY, 1813 bytes)
15/08/09 15:27:34 INFO Executor: Running task 124.0 in stage 13.0 (TID 941)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 108.0 in stage 13.0 (TID 925) in 861 ms on localhost (109/200)
15/08/09 15:27:34 INFO Executor: Finished task 112.0 in stage 13.0 (TID 929). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO Executor: Finished task 111.0 in stage 13.0 (TID 928). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO TaskSetManager: Starting task 125.0 in stage 13.0 (TID 942, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 125.0 in stage 13.0 (TID 942)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 112.0 in stage 13.0 (TID 929) in 841 ms on localhost (110/200)
15/08/09 15:27:34 INFO Executor: Finished task 109.0 in stage 13.0 (TID 926). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 126.0 in stage 13.0 (TID 943, localhost, ANY, 1813 bytes)
15/08/09 15:27:34 INFO Executor: Running task 126.0 in stage 13.0 (TID 943)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 111.0 in stage 13.0 (TID 928) in 853 ms on localhost (111/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO Executor: Finished task 110.0 in stage 13.0 (TID 927). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO TaskSetManager: Starting task 127.0 in stage 13.0 (TID 944, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 127.0 in stage 13.0 (TID 944)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 109.0 in stage 13.0 (TID 926) in 897 ms on localhost (112/200)
15/08/09 15:27:34 INFO TaskSetManager: Starting task 128.0 in stage 13.0 (TID 945, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 110.0 in stage 13.0 (TID 927) in 880 ms on localhost (113/200)
15/08/09 15:27:34 INFO Executor: Finished task 114.0 in stage 13.0 (TID 931). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO Executor: Running task 128.0 in stage 13.0 (TID 945)
15/08/09 15:27:34 INFO TaskSetManager: Starting task 129.0 in stage 13.0 (TID 946, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 129.0 in stage 13.0 (TID 946)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 114.0 in stage 13.0 (TID 931) in 798 ms on localhost (114/200)
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00122 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:34 INFO Executor: Finished task 115.0 in stage 13.0 (TID 932). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 130.0 in stage 13.0 (TID 947, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 130.0 in stage 13.0 (TID 947)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 115.0 in stage 13.0 (TID 932) in 811 ms on localhost (115/200)
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00123 start: 0 end: 2482 length: 2482 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO Executor: Finished task 113.0 in stage 13.0 (TID 930). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 131.0 in stage 13.0 (TID 948, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 131.0 in stage 13.0 (TID 948)
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO TaskSetManager: Finished task 113.0 in stage 13.0 (TID 930) in 879 ms on localhost (116/200)
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 175
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 180 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 180
15/08/09 15:27:34 INFO Executor: Finished task 116.0 in stage 13.0 (TID 933). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 132.0 in stage 13.0 (TID 949, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 132.0 in stage 13.0 (TID 949)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 116.0 in stage 13.0 (TID 933) in 823 ms on localhost (117/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO Executor: Finished task 118.0 in stage 13.0 (TID 935). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 133.0 in stage 13.0 (TID 950, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 133.0 in stage 13.0 (TID 950)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 118.0 in stage 13.0 (TID 935) in 766 ms on localhost (118/200)
15/08/09 15:27:34 INFO Executor: Finished task 117.0 in stage 13.0 (TID 934). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO Executor: Finished task 119.0 in stage 13.0 (TID 936). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 134.0 in stage 13.0 (TID 951, localhost, ANY, 1816 bytes)
15/08/09 15:27:34 INFO Executor: Running task 134.0 in stage 13.0 (TID 951)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 117.0 in stage 13.0 (TID 934) in 782 ms on localhost (119/200)
15/08/09 15:27:34 INFO Executor: Finished task 120.0 in stage 13.0 (TID 937). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
15/08/09 15:27:34 INFO TaskSetManager: Starting task 135.0 in stage 13.0 (TID 952, localhost, ANY, 1816 bytes)
15/08/09 15:27:34 INFO Executor: Running task 135.0 in stage 13.0 (TID 952)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 119.0 in stage 13.0 (TID 936) in 349 ms on localhost (120/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO TaskSetManager: Starting task 136.0 in stage 13.0 (TID 953, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 136.0 in stage 13.0 (TID 953)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 120.0 in stage 13.0 (TID 937) in 307 ms on localhost (121/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:34 INFO Executor: Finished task 121.0 in stage 13.0 (TID 938). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 137.0 in stage 13.0 (TID 954, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 137.0 in stage 13.0 (TID 954)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 121.0 in stage 13.0 (TID 938) in 301 ms on localhost (122/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00124 start: 0 end: 2098 length: 2098 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 148 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 148
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00127 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00129 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 142
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00130 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00131 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00126 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00125 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 142
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 158
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 163
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00128 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 123
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 169
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00132 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00133 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 108
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00135 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 125
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00136 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00134 start: 0 end: 1990 length: 1990 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 139 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 139
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00137 start: 0 end: 1798 length: 1798 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO Executor: Finished task 122.0 in stage 13.0 (TID 939). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 138.0 in stage 13.0 (TID 955, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 138.0 in stage 13.0 (TID 955)
15/08/09 15:27:34 INFO Executor: Finished task 123.0 in stage 13.0 (TID 940). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO TaskSetManager: Finished task 122.0 in stage 13.0 (TID 939) in 392 ms on localhost (123/200)
15/08/09 15:27:34 INFO TaskSetManager: Starting task 139.0 in stage 13.0 (TID 956, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 139.0 in stage 13.0 (TID 956)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 123.0 in stage 13.0 (TID 940) in 386 ms on localhost (124/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 123 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 123
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00138 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00139 start: 0 end: 2014 length: 2014 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 113
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 141 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 141
15/08/09 15:27:34 INFO Executor: Finished task 127.0 in stage 13.0 (TID 944). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 140.0 in stage 13.0 (TID 957, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 140.0 in stage 13.0 (TID 957)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 127.0 in stage 13.0 (TID 944) in 441 ms on localhost (125/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO Executor: Finished task 124.0 in stage 13.0 (TID 941). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 141.0 in stage 13.0 (TID 958, localhost, ANY, 1813 bytes)
15/08/09 15:27:34 INFO Executor: Running task 141.0 in stage 13.0 (TID 958)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 124.0 in stage 13.0 (TID 941) in 497 ms on localhost (126/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00140 start: 0 end: 2494 length: 2494 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 181 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 181
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00141 start: 0 end: 1654 length: 1654 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 111 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 111
15/08/09 15:27:34 INFO Executor: Finished task 130.0 in stage 13.0 (TID 947). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO Executor: Finished task 126.0 in stage 13.0 (TID 943). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 142.0 in stage 13.0 (TID 959, localhost, ANY, 1812 bytes)
15/08/09 15:27:34 INFO Executor: Running task 142.0 in stage 13.0 (TID 959)
15/08/09 15:27:34 INFO Executor: Finished task 129.0 in stage 13.0 (TID 946). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Finished task 130.0 in stage 13.0 (TID 947) in 711 ms on localhost (127/200)
15/08/09 15:27:34 INFO TaskSetManager: Starting task 143.0 in stage 13.0 (TID 960, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 143.0 in stage 13.0 (TID 960)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 126.0 in stage 13.0 (TID 943) in 761 ms on localhost (128/200)
15/08/09 15:27:34 INFO TaskSetManager: Starting task 144.0 in stage 13.0 (TID 961, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 144.0 in stage 13.0 (TID 961)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 129.0 in stage 13.0 (TID 946) in 734 ms on localhost (129/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO Executor: Finished task 135.0 in stage 13.0 (TID 952). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 145.0 in stage 13.0 (TID 962, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 145.0 in stage 13.0 (TID 962)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 135.0 in stage 13.0 (TID 952) in 736 ms on localhost (130/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO Executor: Finished task 131.0 in stage 13.0 (TID 948). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 146.0 in stage 13.0 (TID 963, localhost, ANY, 1814 bytes)
15/08/09 15:27:34 INFO Executor: Running task 146.0 in stage 13.0 (TID 963)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 131.0 in stage 13.0 (TID 948) in 800 ms on localhost (131/200)
15/08/09 15:27:34 INFO Executor: Finished task 125.0 in stage 13.0 (TID 942). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 147.0 in stage 13.0 (TID 964, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 147.0 in stage 13.0 (TID 964)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 125.0 in stage 13.0 (TID 942) in 866 ms on localhost (132/200)
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00142 start: 0 end: 2350 length: 2350 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00143 start: 0 end: 1558 length: 1558 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO Executor: Finished task 128.0 in stage 13.0 (TID 945). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00144 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:34 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:34 INFO TaskSetManager: Starting task 148.0 in stage 13.0 (TID 965, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 148.0 in stage 13.0 (TID 965)
15/08/09 15:27:34 INFO Executor: Finished task 134.0 in stage 13.0 (TID 951). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Finished task 128.0 in stage 13.0 (TID 945) in 854 ms on localhost (133/200)
15/08/09 15:27:34 INFO TaskSetManager: Starting task 149.0 in stage 13.0 (TID 966, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO Executor: Running task 149.0 in stage 13.0 (TID 966)
15/08/09 15:27:34 INFO TaskSetManager: Finished task 134.0 in stage 13.0 (TID 951) in 794 ms on localhost (134/200)
15/08/09 15:27:34 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 169 records.
15/08/09 15:27:34 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:34 INFO Executor: Finished task 133.0 in stage 13.0 (TID 950). 2182 bytes result sent to driver
15/08/09 15:27:34 INFO TaskSetManager: Starting task 150.0 in stage 13.0 (TID 967, localhost, ANY, 1815 bytes)
15/08/09 15:27:34 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 169
15/08/09 15:27:35 INFO Executor: Running task 150.0 in stage 13.0 (TID 967)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 133.0 in stage 13.0 (TID 950) in 806 ms on localhost (135/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO Executor: Finished task 132.0 in stage 13.0 (TID 949). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:35 INFO TaskSetManager: Starting task 151.0 in stage 13.0 (TID 968, localhost, ANY, 1812 bytes)
15/08/09 15:27:35 INFO Executor: Running task 151.0 in stage 13.0 (TID 968)
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:35 INFO TaskSetManager: Finished task 132.0 in stage 13.0 (TID 949) in 908 ms on localhost (136/200)
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:35 INFO Executor: Finished task 137.0 in stage 13.0 (TID 954). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 152.0 in stage 13.0 (TID 969, localhost, ANY, 1816 bytes)
15/08/09 15:27:35 INFO Executor: Running task 152.0 in stage 13.0 (TID 969)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 137.0 in stage 13.0 (TID 954) in 854 ms on localhost (137/200)
15/08/09 15:27:35 INFO Executor: Finished task 136.0 in stage 13.0 (TID 953). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Finished task 139.0 in stage 13.0 (TID 956). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 153.0 in stage 13.0 (TID 970, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 103 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO Executor: Running task 153.0 in stage 13.0 (TID 970)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 136.0 in stage 13.0 (TID 953) in 886 ms on localhost (138/200)
15/08/09 15:27:35 INFO TaskSetManager: Starting task 154.0 in stage 13.0 (TID 971, localhost, ANY, 1813 bytes)
15/08/09 15:27:35 INFO Executor: Running task 154.0 in stage 13.0 (TID 971)
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 103
15/08/09 15:27:35 INFO Executor: Finished task 138.0 in stage 13.0 (TID 955). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Finished task 139.0 in stage 13.0 (TID 956) in 662 ms on localhost (139/200)
15/08/09 15:27:35 INFO TaskSetManager: Starting task 155.0 in stage 13.0 (TID 972, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Running task 155.0 in stage 13.0 (TID 972)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO TaskSetManager: Finished task 138.0 in stage 13.0 (TID 955) in 675 ms on localhost (140/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Finished task 140.0 in stage 13.0 (TID 957). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 156.0 in stage 13.0 (TID 973, localhost, ANY, 1816 bytes)
15/08/09 15:27:35 INFO Executor: Finished task 141.0 in stage 13.0 (TID 958). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO Executor: Running task 156.0 in stage 13.0 (TID 973)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 140.0 in stage 13.0 (TID 957) in 560 ms on localhost (141/200)
15/08/09 15:27:35 INFO TaskSetManager: Starting task 157.0 in stage 13.0 (TID 974, localhost, ANY, 1816 bytes)
15/08/09 15:27:35 INFO Executor: Running task 157.0 in stage 13.0 (TID 974)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 141.0 in stage 13.0 (TID 958) in 535 ms on localhost (142/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00145 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 130
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00146 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00147 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00153 start: 0 end: 1846 length: 1846 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00155 start: 0 end: 2110 length: 2110 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00151 start: 0 end: 2602 length: 2602 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00152 start: 0 end: 1678 length: 1678 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00148 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00149 start: 0 end: 1618 length: 1618 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO Executor: Finished task 142.0 in stage 13.0 (TID 959). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 127 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 127
15/08/09 15:27:35 INFO TaskSetManager: Starting task 158.0 in stage 13.0 (TID 975, localhost, ANY, 1813 bytes)
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00150 start: 0 end: 2278 length: 2278 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00156 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 INFO TaskSetManager: Finished task 142.0 in stage 13.0 (TID 959) in 402 ms on localhost (143/200)
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO Executor: Running task 158.0 in stage 13.0 (TID 975)
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 190 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 149 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 149
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 108 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 113 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 163 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 113
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 12 ms. row count = 190
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 108
15/08/09 15:27:35 INFO Executor: Finished task 152.0 in stage 13.0 (TID 969). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 163
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO TaskSetManager: Starting task 159.0 in stage 13.0 (TID 976, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO Executor: Running task 159.0 in stage 13.0 (TID 976)
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00154 start: 0 end: 2590 length: 2590 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 INFO TaskSetManager: Finished task 152.0 in stage 13.0 (TID 969) in 200 ms on localhost (144/200)
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 138
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Finished task 144.0 in stage 13.0 (TID 961). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 160.0 in stage 13.0 (TID 977, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO Executor: Running task 160.0 in stage 13.0 (TID 977)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 144.0 in stage 13.0 (TID 961) in 434 ms on localhost (145/200)
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00157 start: 0 end: 2230 length: 2230 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 189 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 189
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Finished task 143.0 in stage 13.0 (TID 960). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 161.0 in stage 13.0 (TID 978, localhost, ANY, 1816 bytes)
15/08/09 15:27:35 INFO Executor: Running task 161.0 in stage 13.0 (TID 978)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 143.0 in stage 13.0 (TID 960) in 444 ms on localhost (146/200)
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 159 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 159
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Finished task 145.0 in stage 13.0 (TID 962). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00159 start: 0 end: 2386 length: 2386 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 INFO TaskSetManager: Starting task 162.0 in stage 13.0 (TID 979, localhost, ANY, 1815 bytes)
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO Executor: Running task 162.0 in stage 13.0 (TID 979)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 145.0 in stage 13.0 (TID 962) in 467 ms on localhost (147/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 172 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 172
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00160 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00158 start: 0 end: 2338 length: 2338 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 147
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 168 records.
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00161 start: 0 end: 1714 length: 1714 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 168
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 116 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 116
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00162 start: 0 end: 1954 length: 1954 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 136 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 136
15/08/09 15:27:35 INFO Executor: Finished task 153.0 in stage 13.0 (TID 970). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO Executor: Finished task 146.0 in stage 13.0 (TID 963). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 163.0 in stage 13.0 (TID 980, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 153.0 in stage 13.0 (TID 970) in 592 ms on localhost (148/200)
15/08/09 15:27:35 INFO Executor: Running task 163.0 in stage 13.0 (TID 980)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO TaskSetManager: Starting task 164.0 in stage 13.0 (TID 981, localhost, ANY, 1815 bytes)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 146.0 in stage 13.0 (TID 963) in 741 ms on localhost (149/200)
15/08/09 15:27:35 INFO Executor: Running task 164.0 in stage 13.0 (TID 981)
15/08/09 15:27:35 INFO Executor: Finished task 147.0 in stage 13.0 (TID 964). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO Executor: Finished task 155.0 in stage 13.0 (TID 972). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO TaskSetManager: Starting task 165.0 in stage 13.0 (TID 982, localhost, ANY, 1816 bytes)
15/08/09 15:27:35 INFO Executor: Running task 165.0 in stage 13.0 (TID 982)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 147.0 in stage 13.0 (TID 964) in 754 ms on localhost (150/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO TaskSetManager: Starting task 166.0 in stage 13.0 (TID 983, localhost, ANY, 1815 bytes)
15/08/09 15:27:35 INFO Executor: Running task 166.0 in stage 13.0 (TID 983)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 155.0 in stage 13.0 (TID 972) in 654 ms on localhost (151/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Finished task 151.0 in stage 13.0 (TID 968). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00163 start: 0 end: 1642 length: 1642 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO Executor: Finished task 150.0 in stage 13.0 (TID 967). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 167.0 in stage 13.0 (TID 984, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO Executor: Running task 167.0 in stage 13.0 (TID 984)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 151.0 in stage 13.0 (TID 968) in 729 ms on localhost (152/200)
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00164 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO Executor: Finished task 156.0 in stage 13.0 (TID 973). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 110 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 110
15/08/09 15:27:35 INFO Executor: Finished task 163.0 in stage 13.0 (TID 980). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00165 start: 0 end: 2026 length: 2026 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO TaskSetManager: Starting task 168.0 in stage 13.0 (TID 985, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO Executor: Running task 168.0 in stage 13.0 (TID 985)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 150.0 in stage 13.0 (TID 967) in 841 ms on localhost (153/200)
15/08/09 15:27:35 INFO Executor: Finished task 154.0 in stage 13.0 (TID 971). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO Executor: Finished task 149.0 in stage 13.0 (TID 966). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 142 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 142
15/08/09 15:27:35 INFO TaskSetManager: Starting task 169.0 in stage 13.0 (TID 986, localhost, ANY, 1815 bytes)
15/08/09 15:27:35 INFO Executor: Running task 169.0 in stage 13.0 (TID 986)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 156.0 in stage 13.0 (TID 973) in 738 ms on localhost (154/200)
15/08/09 15:27:35 INFO Executor: Finished task 148.0 in stage 13.0 (TID 965). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Finished task 163.0 in stage 13.0 (TID 980) in 182 ms on localhost (155/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO TaskSetManager: Starting task 170.0 in stage 13.0 (TID 987, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO Executor: Running task 170.0 in stage 13.0 (TID 987)
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00166 start: 0 end: 1690 length: 1690 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:35 INFO TaskSetManager: Starting task 171.0 in stage 13.0 (TID 988, localhost, ANY, 1815 bytes)
15/08/09 15:27:35 INFO Executor: Running task 171.0 in stage 13.0 (TID 988)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 154.0 in stage 13.0 (TID 971) in 777 ms on localhost (156/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 114 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 114
15/08/09 15:27:35 INFO TaskSetManager: Starting task 172.0 in stage 13.0 (TID 989, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO Executor: Running task 172.0 in stage 13.0 (TID 989)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 149.0 in stage 13.0 (TID 966) in 894 ms on localhost (157/200)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 148.0 in stage 13.0 (TID 965) in 898 ms on localhost (158/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:35 INFO Executor: Finished task 157.0 in stage 13.0 (TID 974). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 173.0 in stage 13.0 (TID 990, localhost, ANY, 1812 bytes)
15/08/09 15:27:35 INFO Executor: Running task 173.0 in stage 13.0 (TID 990)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 157.0 in stage 13.0 (TID 974) in 782 ms on localhost (159/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Finished task 159.0 in stage 13.0 (TID 976). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 174.0 in stage 13.0 (TID 991, localhost, ANY, 1815 bytes)
15/08/09 15:27:35 INFO Executor: Running task 174.0 in stage 13.0 (TID 991)
15/08/09 15:27:35 INFO TaskSetManager: Starting task 175.0 in stage 13.0 (TID 992, localhost, ANY, 1812 bytes)
15/08/09 15:27:35 INFO Executor: Running task 175.0 in stage 13.0 (TID 992)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 159.0 in stage 13.0 (TID 976) in 626 ms on localhost (160/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO Executor: Finished task 158.0 in stage 13.0 (TID 975). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 176.0 in stage 13.0 (TID 993, localhost, ANY, 1813 bytes)
15/08/09 15:27:35 INFO Executor: Running task 176.0 in stage 13.0 (TID 993)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 158.0 in stage 13.0 (TID 975) in 665 ms on localhost (161/200)
15/08/09 15:27:35 INFO Executor: Finished task 160.0 in stage 13.0 (TID 977). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO Executor: Finished task 162.0 in stage 13.0 (TID 979). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 177.0 in stage 13.0 (TID 994, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO TaskSetManager: Starting task 178.0 in stage 13.0 (TID 995, localhost, ANY, 1815 bytes)
15/08/09 15:27:35 INFO Executor: Running task 178.0 in stage 13.0 (TID 995)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 160.0 in stage 13.0 (TID 977) in 634 ms on localhost (162/200)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:35 INFO Executor: Running task 177.0 in stage 13.0 (TID 994)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 162.0 in stage 13.0 (TID 979) in 543 ms on localhost (163/200)
15/08/09 15:27:35 INFO Executor: Finished task 161.0 in stage 13.0 (TID 978). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO TaskSetManager: Starting task 179.0 in stage 13.0 (TID 996, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO Executor: Running task 179.0 in stage 13.0 (TID 996)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:35 INFO TaskSetManager: Finished task 161.0 in stage 13.0 (TID 978) in 632 ms on localhost (164/200)
15/08/09 15:27:35 INFO Executor: Finished task 164.0 in stage 13.0 (TID 981). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO TaskSetManager: Starting task 180.0 in stage 13.0 (TID 997, localhost, ANY, 1815 bytes)
15/08/09 15:27:35 INFO TaskSetManager: Finished task 164.0 in stage 13.0 (TID 981) in 269 ms on localhost (165/200)
15/08/09 15:27:35 INFO Executor: Running task 180.0 in stage 13.0 (TID 997)
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00167 start: 0 end: 2902 length: 2902 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00168 start: 0 end: 1834 length: 1834 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 215 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 215
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00170 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00169 start: 0 end: 2182 length: 2182 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 126 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO Executor: Finished task 165.0 in stage 13.0 (TID 982). 2182 bytes result sent to driver
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 126
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO TaskSetManager: Starting task 181.0 in stage 13.0 (TID 998, localhost, ANY, 1814 bytes)
15/08/09 15:27:35 INFO Executor: Running task 181.0 in stage 13.0 (TID 998)
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 186
15/08/09 15:27:35 INFO TaskSetManager: Finished task 165.0 in stage 13.0 (TID 982) in 308 ms on localhost (166/200)
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00171 start: 0 end: 2506 length: 2506 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 155 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 155
15/08/09 15:27:35 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 182 records.
15/08/09 15:27:35 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:35 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00172 start: 0 end: 1870 length: 1870 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:35 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:35 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 182
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 129 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 9 ms. row count = 129
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00175 start: 0 end: 2674 length: 2674 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO Executor: Finished task 166.0 in stage 13.0 (TID 983). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Starting task 182.0 in stage 13.0 (TID 999, localhost, ANY, 1814 bytes)
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 196 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO Executor: Running task 182.0 in stage 13.0 (TID 999)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 166.0 in stage 13.0 (TID 983) in 349 ms on localhost (167/200)
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 108 ms. row count = 196
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00173 start: 0 end: 2314 length: 2314 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 166 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 166
15/08/09 15:27:36 INFO Executor: Finished task 173.0 in stage 13.0 (TID 990). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Starting task 183.0 in stage 13.0 (TID 1000, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 183.0 in stage 13.0 (TID 1000)
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00177 start: 0 end: 2554 length: 2554 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 INFO TaskSetManager: Finished task 173.0 in stage 13.0 (TID 990) in 310 ms on localhost (168/200)
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00178 start: 0 end: 3274 length: 3274 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00180 start: 0 end: 1978 length: 1978 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00179 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00174 start: 0 end: 3010 length: 3010 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00176 start: 0 end: 2686 length: 2686 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 186 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 246 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 138 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 186
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 246
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 138
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 133
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 197 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 197
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 224 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 224
15/08/09 15:27:36 INFO Executor: Finished task 167.0 in stage 13.0 (TID 984). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Starting task 184.0 in stage 13.0 (TID 1001, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 184.0 in stage 13.0 (TID 1001)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 167.0 in stage 13.0 (TID 984) in 454 ms on localhost (169/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:36 INFO Executor: Finished task 168.0 in stage 13.0 (TID 985). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00181 start: 0 end: 2434 length: 2434 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO TaskSetManager: Starting task 185.0 in stage 13.0 (TID 1002, localhost, ANY, 1814 bytes)
15/08/09 15:27:36 INFO Executor: Running task 185.0 in stage 13.0 (TID 1002)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 168.0 in stage 13.0 (TID 985) in 469 ms on localhost (170/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 176 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 176
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00182 start: 0 end: 2086 length: 2086 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00183 start: 0 end: 1942 length: 1942 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 147 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 147
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 135 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 135
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00184 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 184
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00185 start: 0 end: 1882 length: 1882 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO Executor: Finished task 169.0 in stage 13.0 (TID 986). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 130 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 130
15/08/09 15:27:36 INFO TaskSetManager: Starting task 186.0 in stage 13.0 (TID 1003, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 186.0 in stage 13.0 (TID 1003)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 169.0 in stage 13.0 (TID 986) in 593 ms on localhost (171/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00186 start: 0 end: 2458 length: 2458 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 178 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 178
15/08/09 15:27:36 INFO Executor: Finished task 171.0 in stage 13.0 (TID 988). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO Executor: Finished task 170.0 in stage 13.0 (TID 987). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Starting task 187.0 in stage 13.0 (TID 1004, localhost, ANY, 1812 bytes)
15/08/09 15:27:36 INFO Executor: Running task 187.0 in stage 13.0 (TID 1004)
15/08/09 15:27:36 INFO Executor: Finished task 172.0 in stage 13.0 (TID 989). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO TaskSetManager: Starting task 188.0 in stage 13.0 (TID 1005, localhost, ANY, 1814 bytes)
15/08/09 15:27:36 INFO Executor: Running task 188.0 in stage 13.0 (TID 1005)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 171.0 in stage 13.0 (TID 988) in 719 ms on localhost (172/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO TaskSetManager: Starting task 189.0 in stage 13.0 (TID 1006, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 189.0 in stage 13.0 (TID 1006)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 170.0 in stage 13.0 (TID 987) in 743 ms on localhost (173/200)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 172.0 in stage 13.0 (TID 989) in 722 ms on localhost (174/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00187 start: 0 end: 2650 length: 2650 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 194 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 194
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00188 start: 0 end: 2062 length: 2062 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00189 start: 0 end: 1930 length: 1930 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO Executor: Finished task 179.0 in stage 13.0 (TID 996). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO Executor: Finished task 175.0 in stage 13.0 (TID 992). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO Executor: Finished task 177.0 in stage 13.0 (TID 994). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 145 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 145
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 134 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 134
15/08/09 15:27:36 INFO TaskSetManager: Starting task 190.0 in stage 13.0 (TID 1007, localhost, ANY, 1814 bytes)
15/08/09 15:27:36 INFO Executor: Running task 190.0 in stage 13.0 (TID 1007)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 179.0 in stage 13.0 (TID 996) in 783 ms on localhost (175/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:36 INFO TaskSetManager: Starting task 191.0 in stage 13.0 (TID 1008, localhost, ANY, 1814 bytes)
15/08/09 15:27:36 INFO Executor: Running task 191.0 in stage 13.0 (TID 1008)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO Executor: Finished task 178.0 in stage 13.0 (TID 995). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Starting task 192.0 in stage 13.0 (TID 1009, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 192.0 in stage 13.0 (TID 1009)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 175.0 in stage 13.0 (TID 992) in 831 ms on localhost (176/200)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 177.0 in stage 13.0 (TID 994) in 811 ms on localhost (177/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO Executor: Finished task 176.0 in stage 13.0 (TID 993). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Starting task 193.0 in stage 13.0 (TID 1010, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 193.0 in stage 13.0 (TID 1010)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 178.0 in stage 13.0 (TID 995) in 821 ms on localhost (178/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO TaskSetManager: Starting task 194.0 in stage 13.0 (TID 1011, localhost, ANY, 1814 bytes)
15/08/09 15:27:36 INFO Executor: Running task 194.0 in stage 13.0 (TID 1011)
15/08/09 15:27:36 INFO Executor: Finished task 180.0 in stage 13.0 (TID 997). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Finished task 176.0 in stage 13.0 (TID 993) in 837 ms on localhost (179/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:36 INFO TaskSetManager: Starting task 195.0 in stage 13.0 (TID 1012, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 195.0 in stage 13.0 (TID 1012)
15/08/09 15:27:36 INFO Executor: Finished task 174.0 in stage 13.0 (TID 991). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Finished task 180.0 in stage 13.0 (TID 997) in 827 ms on localhost (180/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO TaskSetManager: Starting task 196.0 in stage 13.0 (TID 1013, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 196.0 in stage 13.0 (TID 1013)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 174.0 in stage 13.0 (TID 991) in 889 ms on localhost (181/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO Executor: Finished task 181.0 in stage 13.0 (TID 998). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Starting task 197.0 in stage 13.0 (TID 1014, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 197.0 in stage 13.0 (TID 1014)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 181.0 in stage 13.0 (TID 998) in 807 ms on localhost (182/200)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:36 INFO Executor: Finished task 183.0 in stage 13.0 (TID 1000). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO Executor: Finished task 184.0 in stage 13.0 (TID 1001). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Starting task 198.0 in stage 13.0 (TID 1015, localhost, ANY, 1813 bytes)
15/08/09 15:27:36 INFO Executor: Running task 198.0 in stage 13.0 (TID 1015)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 184.0 in stage 13.0 (TID 1001) in 596 ms on localhost (183/200)
15/08/09 15:27:36 INFO TaskSetManager: Finished task 183.0 in stage 13.0 (TID 1000) in 647 ms on localhost (184/200)
15/08/09 15:27:36 INFO TaskSetManager: Starting task 199.0 in stage 13.0 (TID 1016, localhost, ANY, 1815 bytes)
15/08/09 15:27:36 INFO Executor: Running task 199.0 in stage 13.0 (TID 1016)
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:36 INFO Executor: Finished task 182.0 in stage 13.0 (TID 999). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00191 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO TaskSetManager: Finished task 182.0 in stage 13.0 (TID 999) in 785 ms on localhost (185/200)
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00192 start: 0 end: 2038 length: 2038 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 INFO Executor: Finished task 185.0 in stage 13.0 (TID 1002). 2182 bytes result sent to driver
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00190 start: 0 end: 1918 length: 1918 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO TaskSetManager: Finished task 185.0 in stage 13.0 (TID 1002) in 581 ms on localhost (186/200)
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 158
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 143 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 133 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 3 ms. row count = 133
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 143
15/08/09 15:27:36 INFO Executor: Finished task 186.0 in stage 13.0 (TID 1003). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Finished task 186.0 in stage 13.0 (TID 1003) in 464 ms on localhost (187/200)
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00193 start: 0 end: 2530 length: 2530 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 184 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 184
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00194 start: 0 end: 1894 length: 1894 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00195 start: 0 end: 2122 length: 2122 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00196 start: 0 end: 1822 length: 1822 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 131 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 125 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 150 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 131
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 150
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 125
15/08/09 15:27:36 INFO Executor: Finished task 187.0 in stage 13.0 (TID 1004). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Finished task 187.0 in stage 13.0 (TID 1004) in 365 ms on localhost (188/200)
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00197 start: 0 end: 2218 length: 2218 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO Executor: Finished task 188.0 in stage 13.0 (TID 1005). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Finished task 188.0 in stage 13.0 (TID 1005) in 377 ms on localhost (189/200)
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 158 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 4 ms. row count = 158
15/08/09 15:27:36 INFO Executor: Finished task 189.0 in stage 13.0 (TID 1006). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Finished task 189.0 in stage 13.0 (TID 1006) in 363 ms on localhost (190/200)
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00199 start: 0 end: 2254 length: 2254 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 161 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 1 ms. row count = 161
15/08/09 15:27:36 INFO NewHadoopRDD: Input split: ParquetInputSplit{part: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_part_tmp_par_spark/part-00198 start: 0 end: 2422 length: 2422 hosts: [] requestedSchema: message root {
  optional int32 ps_partkey;
  optional double part_value;
}
 readSupportMetadata: {org.apache.spark.sql.parquet.row.metadata={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}, org.apache.spark.sql.parquet.row.requested_schema={"type":"struct","fields":[{"name":"ps_partkey","type":"integer","nullable":true,"metadata":{}},{"name":"part_value","type":"double","nullable":true,"metadata":{}}]}}}
15/08/09 15:27:36 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
15/08/09 15:27:36 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 175 records.
15/08/09 15:27:36 INFO InternalParquetRecordReader: at row 0. reading next block
15/08/09 15:27:36 INFO InternalParquetRecordReader: block read in memory in 2 ms. row count = 175
15/08/09 15:27:36 INFO Executor: Finished task 191.0 in stage 13.0 (TID 1008). 2182 bytes result sent to driver
15/08/09 15:27:36 INFO TaskSetManager: Finished task 191.0 in stage 13.0 (TID 1008) in 295 ms on localhost (191/200)
15/08/09 15:27:37 INFO Executor: Finished task 192.0 in stage 13.0 (TID 1009). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 192.0 in stage 13.0 (TID 1009) in 378 ms on localhost (192/200)
15/08/09 15:27:37 INFO Executor: Finished task 190.0 in stage 13.0 (TID 1007). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 190.0 in stage 13.0 (TID 1007) in 428 ms on localhost (193/200)
15/08/09 15:27:37 INFO Executor: Finished task 193.0 in stage 13.0 (TID 1010). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 193.0 in stage 13.0 (TID 1010) in 402 ms on localhost (194/200)
15/08/09 15:27:37 INFO Executor: Finished task 196.0 in stage 13.0 (TID 1013). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 196.0 in stage 13.0 (TID 1013) in 431 ms on localhost (195/200)
15/08/09 15:27:37 INFO Executor: Finished task 194.0 in stage 13.0 (TID 1011). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 194.0 in stage 13.0 (TID 1011) in 457 ms on localhost (196/200)
15/08/09 15:27:37 INFO Executor: Finished task 195.0 in stage 13.0 (TID 1012). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 195.0 in stage 13.0 (TID 1012) in 449 ms on localhost (197/200)
15/08/09 15:27:37 INFO Executor: Finished task 199.0 in stage 13.0 (TID 1016). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO Executor: Finished task 197.0 in stage 13.0 (TID 1014). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO Executor: Finished task 198.0 in stage 13.0 (TID 1015). 2182 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 199.0 in stage 13.0 (TID 1016) in 381 ms on localhost (198/200)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 197.0 in stage 13.0 (TID 1014) in 410 ms on localhost (199/200)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 198.0 in stage 13.0 (TID 1015) in 386 ms on localhost (200/200)
15/08/09 15:27:37 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/08/09 15:27:37 INFO DAGScheduler: Stage 13 (mapPartitions at Exchange.scala:77) finished in 8.543 s
15/08/09 15:27:37 INFO DAGScheduler: looking for newly runnable stages
15/08/09 15:27:37 INFO DAGScheduler: running: Set()
15/08/09 15:27:37 INFO DAGScheduler: waiting: Set(Stage 14)
15/08/09 15:27:37 INFO DAGScheduler: failed: Set()
15/08/09 15:27:37 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@22ec9353
15/08/09 15:27:37 INFO StatsReportListener: task runtime:(count: 200, mean: 678.425000, stdev: 225.077596, max: 1242.000000, min: 182.000000)
15/08/09 15:27:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:37 INFO StatsReportListener: 	182.0 ms	308.0 ms	378.0 ms	481.0 ms	738.0 ms	818.0 ms	889.0 ms	1.2 s	1.2 s
15/08/09 15:27:37 INFO DAGScheduler: Missing parents for Stage 14: List()
15/08/09 15:27:37 INFO StatsReportListener: shuffle bytes written:(count: 200, mean: 3352.795000, stdev: 425.781685, max: 3815.000000, min: 0.000000)
15/08/09 15:27:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:37 INFO DAGScheduler: Submitting Stage 14 (MapPartitionsRDD[80] at mapPartitions at basicOperators.scala:207), which is now runnable
15/08/09 15:27:37 INFO StatsReportListener: 	0.0 B	3.2 KB	3.2 KB	3.3 KB	3.3 KB	3.4 KB	3.5 KB	3.5 KB	3.7 KB
15/08/09 15:27:37 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.180000, stdev: 0.409390, max: 2.000000, min: 0.000000)
15/08/09 15:27:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:37 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	2.0 ms
15/08/09 15:27:37 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/09 15:27:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:37 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/09 15:27:37 INFO StatsReportListener: task result size:(count: 200, mean: 2182.000000, stdev: 0.000000, max: 2182.000000, min: 2182.000000)
15/08/09 15:27:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:37 INFO StatsReportListener: 	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB	2.1 KB
15/08/09 15:27:37 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 95.657880, stdev: 5.431288, max: 99.559471, min: 69.319114)
15/08/09 15:27:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:37 INFO StatsReportListener: 	69 %	86 %	92 %	95 %	97 %	98 %	99 %	99 %	100 %
15/08/09 15:27:37 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.033657, stdev: 0.090070, max: 0.573066, min: 0.000000)
15/08/09 15:27:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:37 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %
15/08/09 15:27:37 INFO StatsReportListener: other time pct: (count: 200, mean: 4.308462, stdev: 5.418396, max: 30.680886, min: 0.440529)
15/08/09 15:27:37 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:37 INFO StatsReportListener: 	 0 %	 1 %	 1 %	 2 %	 3 %	 5 %	 8 %	16 %	31 %
15/08/09 15:27:37 INFO MemoryStore: ensureFreeSpace(151248) called with curMem=1644420, maxMem=3333968363
15/08/09 15:27:37 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 147.7 KB, free 3.1 GB)
15/08/09 15:27:37 INFO MemoryStore: ensureFreeSpace(66817) called with curMem=1795668, maxMem=3333968363
15/08/09 15:27:37 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 65.3 KB, free 3.1 GB)
15/08/09 15:27:37 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:44535 (size: 65.3 KB, free: 3.1 GB)
15/08/09 15:27:37 INFO BlockManagerMaster: Updated info of block broadcast_19_piece0
15/08/09 15:27:37 INFO DefaultExecutionContext: Created broadcast 19 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:37 INFO DAGScheduler: Submitting 200 missing tasks from Stage 14 (MapPartitionsRDD[80] at mapPartitions at basicOperators.scala:207)
15/08/09 15:27:37 INFO TaskSchedulerImpl: Adding task set 14.0 with 200 tasks
15/08/09 15:27:37 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 1017, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 1018, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 1019, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 1020, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 1021, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 1022, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 1023, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 7.0 in stage 14.0 (TID 1024, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 8.0 in stage 14.0 (TID 1025, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 9.0 in stage 14.0 (TID 1026, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 10.0 in stage 14.0 (TID 1027, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 11.0 in stage 14.0 (TID 1028, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 12.0 in stage 14.0 (TID 1029, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 13.0 in stage 14.0 (TID 1030, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 14.0 in stage 14.0 (TID 1031, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 15.0 in stage 14.0 (TID 1032, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 0.0 in stage 14.0 (TID 1017)
15/08/09 15:27:37 INFO Executor: Running task 1.0 in stage 14.0 (TID 1018)
15/08/09 15:27:37 INFO Executor: Running task 3.0 in stage 14.0 (TID 1020)
15/08/09 15:27:37 INFO Executor: Running task 2.0 in stage 14.0 (TID 1019)
15/08/09 15:27:37 INFO Executor: Running task 4.0 in stage 14.0 (TID 1021)
15/08/09 15:27:37 INFO Executor: Running task 6.0 in stage 14.0 (TID 1023)
15/08/09 15:27:37 INFO Executor: Running task 7.0 in stage 14.0 (TID 1024)
15/08/09 15:27:37 INFO Executor: Running task 10.0 in stage 14.0 (TID 1027)
15/08/09 15:27:37 INFO Executor: Running task 8.0 in stage 14.0 (TID 1025)
15/08/09 15:27:37 INFO Executor: Running task 5.0 in stage 14.0 (TID 1022)
15/08/09 15:27:37 INFO Executor: Running task 9.0 in stage 14.0 (TID 1026)
15/08/09 15:27:37 INFO Executor: Running task 13.0 in stage 14.0 (TID 1030)
15/08/09 15:27:37 INFO Executor: Running task 14.0 in stage 14.0 (TID 1031)
15/08/09 15:27:37 INFO Executor: Running task 11.0 in stage 14.0 (TID 1028)
15/08/09 15:27:37 INFO Executor: Running task 15.0 in stage 14.0 (TID 1032)
15/08/09 15:27:37 INFO Executor: Running task 12.0 in stage 14.0 (TID 1029)
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3ed105c2
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000005_1022/part-00005
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@95219a
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000015_1032/part-00015
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c31216d
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000010_1027/part-00010
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@cd123c6
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3d8999fa
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000004_1021/part-00004
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000012_1029/part-00012
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@14dc0ba6
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000008_1025/part-00008
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2049b634
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000009_1026/part-00009
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4759207b
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c77ffb7
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000011_1028/part-00011
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69cb6f53
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2af9e868
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000013_1030/part-00013
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000000_1017/part-00000
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@392307d7
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000014_1031/part-00014
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@18ccf9c9
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ad7d18f
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5dd35c5d
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000007_1024/part-00007
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000003_1020/part-00003
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3797eefc
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@22d7cecb
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6865b649
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000001_1018/part-00001
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@ba1453
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2beef110
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b5f5e8
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@541467f8
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000006_1023/part-00006
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@cf6b5e9
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1920daa5
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32b36129
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1167a2cb
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6e7d15d6
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@598692ad
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5624cd45
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3f60eae6
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000002_1019/part-00002
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b1652b2
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000004_1021' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000004
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000004_1021: Committed
15/08/09 15:27:37 INFO Executor: Finished task 4.0 in stage 14.0 (TID 1021). 781 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Starting task 16.0 in stage 14.0 (TID 1033, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 16.0 in stage 14.0 (TID 1033)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 1021) in 374 ms on localhost (1/200)
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000012_1029' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000012
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000012_1029: Committed
15/08/09 15:27:37 INFO Executor: Finished task 12.0 in stage 14.0 (TID 1029). 781 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Starting task 17.0 in stage 14.0 (TID 1034, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 17.0 in stage 14.0 (TID 1034)
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72fe4064
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000010_1027' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000010
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000010_1027: Committed
15/08/09 15:27:37 INFO TaskSetManager: Finished task 12.0 in stage 14.0 (TID 1029) in 381 ms on localhost (2/200)
15/08/09 15:27:37 INFO Executor: Finished task 10.0 in stage 14.0 (TID 1027). 781 bytes result sent to driver
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000013_1030' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000013
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000013_1030: Committed
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO Executor: Finished task 13.0 in stage 14.0 (TID 1030). 781 bytes result sent to driver
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000007_1024' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000007
15/08/09 15:27:37 INFO TaskSetManager: Starting task 18.0 in stage 14.0 (TID 1035, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000007_1024: Committed
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000005_1022' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000005
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000005_1022: Committed
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000011_1028' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000011
15/08/09 15:27:37 INFO Executor: Running task 18.0 in stage 14.0 (TID 1035)
15/08/09 15:27:37 INFO Executor: Finished task 7.0 in stage 14.0 (TID 1024). 781 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 10.0 in stage 14.0 (TID 1027) in 386 ms on localhost (3/200)
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000008_1025' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000008
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000000_1017' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000000
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000008_1025: Committed
15/08/09 15:27:37 INFO Executor: Finished task 5.0 in stage 14.0 (TID 1022). 781 bytes result sent to driver
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000000_1017: Committed
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000015_1032' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000015
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000015_1032: Committed
15/08/09 15:27:37 INFO TaskSetManager: Starting task 19.0 in stage 14.0 (TID 1036, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000011_1028: Committed
15/08/09 15:27:37 INFO Executor: Finished task 8.0 in stage 14.0 (TID 1025). 781 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Starting task 20.0 in stage 14.0 (TID 1037, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 20.0 in stage 14.0 (TID 1037)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 21.0 in stage 14.0 (TID 1038, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Finished task 11.0 in stage 14.0 (TID 1028). 781 bytes result sent to driver
15/08/09 15:27:37 INFO Executor: Running task 21.0 in stage 14.0 (TID 1038)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 22.0 in stage 14.0 (TID 1039, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Finished task 15.0 in stage 14.0 (TID 1032). 781 bytes result sent to driver
15/08/09 15:27:37 INFO Executor: Running task 22.0 in stage 14.0 (TID 1039)
15/08/09 15:27:37 INFO Executor: Running task 19.0 in stage 14.0 (TID 1036)
15/08/09 15:27:37 INFO Executor: Finished task 0.0 in stage 14.0 (TID 1017). 781 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 13.0 in stage 14.0 (TID 1030) in 390 ms on localhost (4/200)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 23.0 in stage 14.0 (TID 1040, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 23.0 in stage 14.0 (TID 1040)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 5.0 in stage 14.0 (TID 1022) in 392 ms on localhost (5/200)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 8.0 in stage 14.0 (TID 1025) in 393 ms on localhost (6/200)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 7.0 in stage 14.0 (TID 1024) in 394 ms on localhost (7/200)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 24.0 in stage 14.0 (TID 1041, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 24.0 in stage 14.0 (TID 1041)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 25.0 in stage 14.0 (TID 1042, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 25.0 in stage 14.0 (TID 1042)
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000009_1026' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000009
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000009_1026: Committed
15/08/09 15:27:37 INFO TaskSetManager: Finished task 11.0 in stage 14.0 (TID 1028) in 395 ms on localhost (8/200)
15/08/09 15:27:37 INFO Executor: Finished task 9.0 in stage 14.0 (TID 1026). 781 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 1017) in 401 ms on localhost (9/200)
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000006_1023' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000006
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000006_1023: Committed
15/08/09 15:27:37 INFO TaskSetManager: Finished task 15.0 in stage 14.0 (TID 1032) in 399 ms on localhost (10/200)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 26.0 in stage 14.0 (TID 1043, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Finished task 6.0 in stage 14.0 (TID 1023). 781 bytes result sent to driver
15/08/09 15:27:37 INFO Executor: Running task 26.0 in stage 14.0 (TID 1043)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 9.0 in stage 14.0 (TID 1026) in 402 ms on localhost (11/200)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 27.0 in stage 14.0 (TID 1044, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 6.0 in stage 14.0 (TID 1023) in 404 ms on localhost (12/200)
15/08/09 15:27:37 INFO Executor: Running task 27.0 in stage 14.0 (TID 1044)
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000001_1018' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000001
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000001_1018: Committed
15/08/09 15:27:37 INFO Executor: Finished task 1.0 in stage 14.0 (TID 1018). 781 bytes result sent to driver
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000014_1031' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000014
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000014_1031: Committed
15/08/09 15:27:37 INFO Executor: Finished task 14.0 in stage 14.0 (TID 1031). 781 bytes result sent to driver
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000003_1020' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000003
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000003_1020: Committed
15/08/09 15:27:37 INFO TaskSetManager: Starting task 28.0 in stage 14.0 (TID 1045, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Finished task 3.0 in stage 14.0 (TID 1020). 781 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Starting task 29.0 in stage 14.0 (TID 1046, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 28.0 in stage 14.0 (TID 1045)
15/08/09 15:27:37 INFO TaskSetManager: Starting task 30.0 in stage 14.0 (TID 1047, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 30.0 in stage 14.0 (TID 1047)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 1020) in 412 ms on localhost (13/200)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 14.0 in stage 14.0 (TID 1031) in 410 ms on localhost (14/200)
15/08/09 15:27:37 INFO Executor: Running task 29.0 in stage 14.0 (TID 1046)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 1018) in 414 ms on localhost (15/200)
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000002_1019' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000002
15/08/09 15:27:37 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000002_1019: Committed
15/08/09 15:27:37 INFO Executor: Finished task 2.0 in stage 14.0 (TID 1019). 781 bytes result sent to driver
15/08/09 15:27:37 INFO TaskSetManager: Starting task 31.0 in stage 14.0 (TID 1048, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:37 INFO Executor: Running task 31.0 in stage 14.0 (TID 1048)
15/08/09 15:27:37 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 1019) in 438 ms on localhost (16/200)
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1816f7f3
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000021_1038/part-00021
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4af83c4f
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000018_1035/part-00018
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a0ef455
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000020_1037/part-00020
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@343b17aa
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000022_1039/part-00022
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bfa8bdd
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2df48123
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3fc6d46b
15/08/09 15:27:37 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6023cf11
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000025_1042/part-00025
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@70ecfebc
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000024_1041/part-00024
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5748228f
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000023_1040/part-00023
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3097836b
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000026_1043/part-00026
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:37 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@201c7296
15/08/09 15:27:37 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000017_1034/part-00017
15/08/09 15:27:37 INFO CodecConfig: Compression set to false
15/08/09 15:27:37 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:37 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:37 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:37 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1fef4912
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16a64d1f
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1139d7
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6aec5249
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000028_1045/part-00028
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000029_1046/part-00029
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1a347f58
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000019_1036/part-00019
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@17c40b8b
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000030_1047/part-00030
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4f414fdb
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000031_1048/part-00031
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6d836916
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c9477d8
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000016_1033/part-00016
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@61bfc8d5
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@761a4a15
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@37f5e546
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3da63bdd
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bb15e24
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ca8e15a
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000027_1044/part-00027
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000020_1037' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000020
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000020_1037: Committed
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@8ded619
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5dba07db
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000021_1038' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000021
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000021_1038: Committed
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO Executor: Finished task 20.0 in stage 14.0 (TID 1037). 781 bytes result sent to driver
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000018_1035' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000018
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000018_1035: Committed
15/08/09 15:27:38 INFO Executor: Finished task 21.0 in stage 14.0 (TID 1038). 781 bytes result sent to driver
15/08/09 15:27:38 INFO Executor: Finished task 18.0 in stage 14.0 (TID 1035). 781 bytes result sent to driver
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6bd60be0
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@552664ce
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO TaskSetManager: Starting task 32.0 in stage 14.0 (TID 1049, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000026_1043' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000026
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000026_1043: Committed
15/08/09 15:27:38 INFO Executor: Running task 32.0 in stage 14.0 (TID 1049)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 20.0 in stage 14.0 (TID 1037) in 419 ms on localhost (17/200)
15/08/09 15:27:38 INFO Executor: Finished task 26.0 in stage 14.0 (TID 1043). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 33.0 in stage 14.0 (TID 1050, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000025_1042' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000025
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000025_1042: Committed
15/08/09 15:27:38 INFO Executor: Running task 33.0 in stage 14.0 (TID 1050)
15/08/09 15:27:38 INFO Executor: Finished task 25.0 in stage 14.0 (TID 1042). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Finished task 21.0 in stage 14.0 (TID 1038) in 420 ms on localhost (18/200)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 34.0 in stage 14.0 (TID 1051, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 34.0 in stage 14.0 (TID 1051)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 18.0 in stage 14.0 (TID 1035) in 427 ms on localhost (19/200)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 35.0 in stage 14.0 (TID 1052, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 35.0 in stage 14.0 (TID 1052)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 26.0 in stage 14.0 (TID 1043) in 413 ms on localhost (20/200)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 36.0 in stage 14.0 (TID 1053, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 36.0 in stage 14.0 (TID 1053)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 25.0 in stage 14.0 (TID 1042) in 421 ms on localhost (21/200)
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12c58d50
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000023_1040' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000023
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000023_1040: Committed
15/08/09 15:27:38 INFO Executor: Finished task 23.0 in stage 14.0 (TID 1040). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 37.0 in stage 14.0 (TID 1054, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000028_1045' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000028
15/08/09 15:27:38 INFO Executor: Running task 37.0 in stage 14.0 (TID 1054)
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000028_1045: Committed
15/08/09 15:27:38 INFO TaskSetManager: Finished task 23.0 in stage 14.0 (TID 1040) in 435 ms on localhost (22/200)
15/08/09 15:27:38 INFO Executor: Finished task 28.0 in stage 14.0 (TID 1045). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 38.0 in stage 14.0 (TID 1055, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 38.0 in stage 14.0 (TID 1055)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000019_1036' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000019
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000019_1036: Committed
15/08/09 15:27:38 INFO Executor: Finished task 19.0 in stage 14.0 (TID 1036). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Finished task 28.0 in stage 14.0 (TID 1045) in 420 ms on localhost (23/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000031_1048' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000031
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000031_1048: Committed
15/08/09 15:27:38 INFO Executor: Finished task 31.0 in stage 14.0 (TID 1048). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 39.0 in stage 14.0 (TID 1056, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 39.0 in stage 14.0 (TID 1056)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 40.0 in stage 14.0 (TID 1057, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 19.0 in stage 14.0 (TID 1036) in 455 ms on localhost (24/200)
15/08/09 15:27:38 INFO Executor: Running task 40.0 in stage 14.0 (TID 1057)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 31.0 in stage 14.0 (TID 1048) in 408 ms on localhost (25/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000016_1033' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000016
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000016_1033: Committed
15/08/09 15:27:38 INFO Executor: Finished task 16.0 in stage 14.0 (TID 1033). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 41.0 in stage 14.0 (TID 1058, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 41.0 in stage 14.0 (TID 1058)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 16.0 in stage 14.0 (TID 1033) in 474 ms on localhost (26/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000027_1044' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000027
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000027_1044: Committed
15/08/09 15:27:38 INFO Executor: Finished task 27.0 in stage 14.0 (TID 1044). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 42.0 in stage 14.0 (TID 1059, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 42.0 in stage 14.0 (TID 1059)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 27.0 in stage 14.0 (TID 1044) in 450 ms on localhost (27/200)
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@26b0818a
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@e089ede
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000033_1050/part-00033
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000035_1052/part-00035
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@57f60804
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000032_1049/part-00032
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@59886bb9
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000034_1051/part-00034
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b586c9b
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000036_1053/part-00036
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5a4239ee
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000037_1054/part-00037
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31e9668d
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000038_1055/part-00038
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7631d543
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000039_1056/part-00039
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d1ced25
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@56152711
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b7f93e
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@184c58e4
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c5fed32
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000041_1058/part-00041
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000040_1057/part-00040
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3dcd8c95
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1c8a9e6
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@ae61f11
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6f521aa3
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@37c78434
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2a53c89e
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@57bfc591
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@510cad39
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000042_1059/part-00042
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000033_1050' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000033
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000033_1050: Committed
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000032_1049' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000032
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000032_1049: Committed
15/08/09 15:27:38 INFO Executor: Finished task 33.0 in stage 14.0 (TID 1050). 781 bytes result sent to driver
15/08/09 15:27:38 INFO Executor: Finished task 32.0 in stage 14.0 (TID 1049). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 43.0 in stage 14.0 (TID 1060, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 43.0 in stage 14.0 (TID 1060)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 33.0 in stage 14.0 (TID 1050) in 177 ms on localhost (28/200)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 44.0 in stage 14.0 (TID 1061, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 44.0 in stage 14.0 (TID 1061)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 32.0 in stage 14.0 (TID 1049) in 181 ms on localhost (29/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000039_1056' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000039
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000039_1056: Committed
15/08/09 15:27:38 INFO Executor: Finished task 39.0 in stage 14.0 (TID 1056). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 45.0 in stage 14.0 (TID 1062, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 45.0 in stage 14.0 (TID 1062)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 39.0 in stage 14.0 (TID 1056) in 152 ms on localhost (30/200)
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3767443d
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000040_1057' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000040
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000040_1057: Committed
15/08/09 15:27:38 INFO Executor: Finished task 40.0 in stage 14.0 (TID 1057). 781 bytes result sent to driver
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000041_1058' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000041
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000041_1058: Committed
15/08/09 15:27:38 INFO TaskSetManager: Starting task 46.0 in stage 14.0 (TID 1063, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 46.0 in stage 14.0 (TID 1063)
15/08/09 15:27:38 INFO Executor: Finished task 41.0 in stage 14.0 (TID 1058). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Finished task 40.0 in stage 14.0 (TID 1057) in 164 ms on localhost (31/200)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 47.0 in stage 14.0 (TID 1064, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 47.0 in stage 14.0 (TID 1064)
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO TaskSetManager: Finished task 41.0 in stage 14.0 (TID 1058) in 161 ms on localhost (32/200)
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000042_1059' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000042
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000042_1059: Committed
15/08/09 15:27:38 INFO Executor: Finished task 42.0 in stage 14.0 (TID 1059). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 48.0 in stage 14.0 (TID 1065, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO Executor: Running task 48.0 in stage 14.0 (TID 1065)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 42.0 in stage 14.0 (TID 1059) in 181 ms on localhost (33/200)
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@560f98e3
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000044_1061/part-00044
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16665884
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000043_1060/part-00043
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@193d4d99
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000045_1062/part-00045
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2f552592
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000047_1064/part-00047
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@138cb896
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000046_1063/part-00046
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@604cd72c
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@54c2bb18
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5aebc593
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d9e4e98
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@76cbc939
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000030_1047' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000030
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000030_1047: Committed
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000022_1039' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000022
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000022_1039: Committed
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000024_1041' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000024
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000024_1041: Committed
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000017_1034' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000017
15/08/09 15:27:38 INFO Executor: Finished task 22.0 in stage 14.0 (TID 1039). 781 bytes result sent to driver
15/08/09 15:27:38 INFO Executor: Finished task 24.0 in stage 14.0 (TID 1041). 781 bytes result sent to driver
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@72191935
15/08/09 15:27:38 INFO Executor: Finished task 30.0 in stage 14.0 (TID 1047). 781 bytes result sent to driver
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000017_1034: Committed
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000029_1046' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000029
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000048_1065/part-00048
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000029_1046: Committed
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO Executor: Finished task 17.0 in stage 14.0 (TID 1034). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 49.0 in stage 14.0 (TID 1066, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Finished task 29.0 in stage 14.0 (TID 1046). 781 bytes result sent to driver
15/08/09 15:27:38 INFO Executor: Running task 49.0 in stage 14.0 (TID 1066)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 24.0 in stage 14.0 (TID 1041) in 982 ms on localhost (34/200)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 50.0 in stage 14.0 (TID 1067, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 50.0 in stage 14.0 (TID 1067)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 22.0 in stage 14.0 (TID 1039) in 989 ms on localhost (35/200)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 30.0 in stage 14.0 (TID 1047) in 970 ms on localhost (36/200)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 51.0 in stage 14.0 (TID 1068, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 51.0 in stage 14.0 (TID 1068)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 52.0 in stage 14.0 (TID 1069, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 52.0 in stage 14.0 (TID 1069)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000044_1061' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000044
15/08/09 15:27:38 INFO TaskSetManager: Finished task 17.0 in stage 14.0 (TID 1034) in 1004 ms on localhost (37/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000045_1062' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000045
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000044_1061: Committed
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000045_1062: Committed
15/08/09 15:27:38 INFO Executor: Finished task 44.0 in stage 14.0 (TID 1061). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Finished task 29.0 in stage 14.0 (TID 1046) in 974 ms on localhost (38/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000035_1052' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000035
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000035_1052: Committed
15/08/09 15:27:38 INFO TaskSetManager: Starting task 53.0 in stage 14.0 (TID 1070, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 53.0 in stage 14.0 (TID 1070)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 54.0 in stage 14.0 (TID 1071, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Finished task 45.0 in stage 14.0 (TID 1062). 781 bytes result sent to driver
15/08/09 15:27:38 INFO Executor: Finished task 35.0 in stage 14.0 (TID 1052). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 55.0 in stage 14.0 (TID 1072, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 54.0 in stage 14.0 (TID 1071)
15/08/09 15:27:38 INFO Executor: Running task 55.0 in stage 14.0 (TID 1072)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 45.0 in stage 14.0 (TID 1062) in 391 ms on localhost (39/200)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 44.0 in stage 14.0 (TID 1061) in 398 ms on localhost (40/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000036_1053' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000036
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000036_1053: Committed
15/08/09 15:27:38 INFO TaskSetManager: Starting task 56.0 in stage 14.0 (TID 1073, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Finished task 36.0 in stage 14.0 (TID 1053). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Finished task 35.0 in stage 14.0 (TID 1052) in 575 ms on localhost (41/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000037_1054' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000037
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000037_1054: Committed
15/08/09 15:27:38 INFO TaskSetManager: Starting task 57.0 in stage 14.0 (TID 1074, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 57.0 in stage 14.0 (TID 1074)
15/08/09 15:27:38 INFO Executor: Running task 56.0 in stage 14.0 (TID 1073)
15/08/09 15:27:38 INFO Executor: Finished task 37.0 in stage 14.0 (TID 1054). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Finished task 36.0 in stage 14.0 (TID 1053) in 574 ms on localhost (42/200)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 58.0 in stage 14.0 (TID 1075, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 58.0 in stage 14.0 (TID 1075)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 37.0 in stage 14.0 (TID 1054) in 566 ms on localhost (43/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000034_1051' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000034
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000034_1051: Committed
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000038_1055' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000038
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@437de4d3
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000038_1055: Committed
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO Executor: Finished task 38.0 in stage 14.0 (TID 1055). 781 bytes result sent to driver
15/08/09 15:27:38 INFO Executor: Finished task 34.0 in stage 14.0 (TID 1051). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 59.0 in stage 14.0 (TID 1076, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 59.0 in stage 14.0 (TID 1076)
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO TaskSetManager: Finished task 38.0 in stage 14.0 (TID 1055) in 569 ms on localhost (44/200)
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO TaskSetManager: Starting task 60.0 in stage 14.0 (TID 1077, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 60.0 in stage 14.0 (TID 1077)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 34.0 in stage 14.0 (TID 1051) in 587 ms on localhost (45/200)
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000048_1065' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000048
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000048_1065: Committed
15/08/09 15:27:38 INFO Executor: Finished task 48.0 in stage 14.0 (TID 1065). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 61.0 in stage 14.0 (TID 1078, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 61.0 in stage 14.0 (TID 1078)
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:38 INFO TaskSetManager: Finished task 48.0 in stage 14.0 (TID 1065) in 394 ms on localhost (46/200)
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2e72544a
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000050_1067/part-00050
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5389901e
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000057_1074/part-00057
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@33387ab6
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000052_1069/part-00052
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7ed98aaf
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000049_1066/part-00049
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2330b432
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000060_1077/part-00060
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@59edc011
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7dcca37
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@67ad95ce
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000056_1073/part-00056
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@35463c8f
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000055_1072/part-00055
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@742705b0
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000058_1075/part-00058
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2de6a163
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@75738bdb
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67fb4be9
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@750468e2
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000051_1068/part-00051
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@392df88e
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000053_1070/part-00053
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@610002cb
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@24623f00
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@318c7be2
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000059_1076/part-00059
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@185ea893
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@60cce2ea
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@43ba59e
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000057_1074' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000057
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000057_1074: Committed
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO Executor: Finished task 57.0 in stage 14.0 (TID 1074). 781 bytes result sent to driver
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000052_1069' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000052
15/08/09 15:27:38 INFO TaskSetManager: Starting task 62.0 in stage 14.0 (TID 1079, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000052_1069: Committed
15/08/09 15:27:38 INFO Executor: Running task 62.0 in stage 14.0 (TID 1079)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 57.0 in stage 14.0 (TID 1074) in 179 ms on localhost (47/200)
15/08/09 15:27:38 INFO Executor: Finished task 52.0 in stage 14.0 (TID 1069). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 63.0 in stage 14.0 (TID 1080, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 52.0 in stage 14.0 (TID 1069) in 191 ms on localhost (48/200)
15/08/09 15:27:38 INFO Executor: Running task 63.0 in stage 14.0 (TID 1080)
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a1eeeea
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000060_1077' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000060
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000060_1077: Committed
15/08/09 15:27:38 INFO Executor: Finished task 60.0 in stage 14.0 (TID 1077). 781 bytes result sent to driver
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@59ab4523
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000054_1071/part-00054
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO TaskSetManager: Starting task 64.0 in stage 14.0 (TID 1081, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 64.0 in stage 14.0 (TID 1081)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000049_1066' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000049
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000049_1066: Committed
15/08/09 15:27:38 INFO TaskSetManager: Finished task 60.0 in stage 14.0 (TID 1077) in 180 ms on localhost (49/200)
15/08/09 15:27:38 INFO Executor: Finished task 49.0 in stage 14.0 (TID 1066). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 65.0 in stage 14.0 (TID 1082, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 65.0 in stage 14.0 (TID 1082)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 49.0 in stage 14.0 (TID 1066) in 211 ms on localhost (50/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000058_1075' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000058
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000058_1075: Committed
15/08/09 15:27:38 INFO Executor: Finished task 58.0 in stage 14.0 (TID 1075). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 66.0 in stage 14.0 (TID 1083, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 66.0 in stage 14.0 (TID 1083)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 58.0 in stage 14.0 (TID 1075) in 202 ms on localhost (51/200)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000055_1072' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000055
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000055_1072: Committed
15/08/09 15:27:38 INFO Executor: Finished task 55.0 in stage 14.0 (TID 1072). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 67.0 in stage 14.0 (TID 1084, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000056_1073' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000056
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000056_1073: Committed
15/08/09 15:27:38 INFO Executor: Finished task 56.0 in stage 14.0 (TID 1073). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Finished task 55.0 in stage 14.0 (TID 1072) in 212 ms on localhost (52/200)
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3545ebfb
15/08/09 15:27:38 INFO Executor: Running task 67.0 in stage 14.0 (TID 1084)
15/08/09 15:27:38 INFO TaskSetManager: Starting task 68.0 in stage 14.0 (TID 1085, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 68.0 in stage 14.0 (TID 1085)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 56.0 in stage 14.0 (TID 1073) in 211 ms on localhost (53/200)
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000059_1076' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000059
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000059_1076: Committed
15/08/09 15:27:38 INFO Executor: Finished task 59.0 in stage 14.0 (TID 1076). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 69.0 in stage 14.0 (TID 1086, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 69.0 in stage 14.0 (TID 1086)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 59.0 in stage 14.0 (TID 1076) in 216 ms on localhost (54/200)
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7e6a4bfd
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000061_1078/part-00061
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@516c7069
15/08/09 15:27:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000054_1071' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000054
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000054_1071: Committed
15/08/09 15:27:38 INFO Executor: Finished task 54.0 in stage 14.0 (TID 1071). 781 bytes result sent to driver
15/08/09 15:27:38 INFO TaskSetManager: Starting task 70.0 in stage 14.0 (TID 1087, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 70.0 in stage 14.0 (TID 1087)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 54.0 in stage 14.0 (TID 1071) in 265 ms on localhost (55/200)
15/08/09 15:27:38 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2b2eff34
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000063_1080/part-00063
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000061_1078' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000061
15/08/09 15:27:38 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000061_1078: Committed
15/08/09 15:27:38 INFO Executor: Finished task 61.0 in stage 14.0 (TID 1078). 781 bytes result sent to driver
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:38 INFO TaskSetManager: Starting task 71.0 in stage 14.0 (TID 1088, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:38 INFO Executor: Running task 71.0 in stage 14.0 (TID 1088)
15/08/09 15:27:38 INFO TaskSetManager: Finished task 61.0 in stage 14.0 (TID 1078) in 269 ms on localhost (56/200)
15/08/09 15:27:38 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e6ee282
15/08/09 15:27:38 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000068_1085/part-00068
15/08/09 15:27:38 INFO CodecConfig: Compression set to false
15/08/09 15:27:38 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:38 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:38 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@275676b0
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5b964131
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000066_1083/part-00066
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@659226f0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000064_1081/part-00064
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5e3a8aa8
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000065_1082/part-00065
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1a138757
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@240503b1
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@45356bf0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000062_1079/part-00062
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000067_1084/part-00067
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22b5526a
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000069_1086/part-00069
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@a79be6f
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000070_1087/part-00070
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c2f6da6
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000071_1088/part-00071
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@64b6b1e9
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@165a6ba7
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@a51ac68
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@780de522
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@583e58c1
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000046_1063' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000046
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000046_1063: Committed
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO Executor: Finished task 46.0 in stage 14.0 (TID 1063). 781 bytes result sent to driver
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3587a28e
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO TaskSetManager: Starting task 72.0 in stage 14.0 (TID 1089, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 72.0 in stage 14.0 (TID 1089)
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO TaskSetManager: Finished task 46.0 in stage 14.0 (TID 1063) in 1068 ms on localhost (57/200)
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d24bc7c
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@35707459
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000047_1064' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000047
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000047_1064: Committed
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000050_1067' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000050
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000050_1067: Committed
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000043_1060' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000043
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000043_1060: Committed
15/08/09 15:27:39 INFO Executor: Finished task 47.0 in stage 14.0 (TID 1064). 781 bytes result sent to driver
15/08/09 15:27:39 INFO Executor: Finished task 50.0 in stage 14.0 (TID 1067). 781 bytes result sent to driver
15/08/09 15:27:39 INFO Executor: Finished task 43.0 in stage 14.0 (TID 1060). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 73.0 in stage 14.0 (TID 1090, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 73.0 in stage 14.0 (TID 1090)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 50.0 in stage 14.0 (TID 1067) in 753 ms on localhost (58/200)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 74.0 in stage 14.0 (TID 1091, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 74.0 in stage 14.0 (TID 1091)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 47.0 in stage 14.0 (TID 1064) in 1127 ms on localhost (59/200)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 75.0 in stage 14.0 (TID 1092, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 75.0 in stage 14.0 (TID 1092)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 43.0 in stage 14.0 (TID 1060) in 1149 ms on localhost (60/200)
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000053_1070' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000053
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000053_1070: Committed
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000051_1068' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000051
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000051_1068: Committed
15/08/09 15:27:39 INFO Executor: Finished task 53.0 in stage 14.0 (TID 1070). 781 bytes result sent to driver
15/08/09 15:27:39 INFO Executor: Finished task 51.0 in stage 14.0 (TID 1068). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 76.0 in stage 14.0 (TID 1093, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 76.0 in stage 14.0 (TID 1093)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 77.0 in stage 14.0 (TID 1094, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 77.0 in stage 14.0 (TID 1094)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 53.0 in stage 14.0 (TID 1070) in 793 ms on localhost (61/200)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 51.0 in stage 14.0 (TID 1068) in 797 ms on localhost (62/200)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ff6ec30
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000072_1089/part-00072
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000063_1080' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000063
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000063_1080: Committed
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO Executor: Finished task 63.0 in stage 14.0 (TID 1080). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 78.0 in stage 14.0 (TID 1095, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 78.0 in stage 14.0 (TID 1095)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 63.0 in stage 14.0 (TID 1080) in 662 ms on localhost (63/200)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2679fd0c
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000073_1090/part-00073
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@377ea836
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c304cc5
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000075_1092/part-00075
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7003862
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000074_1091/part-00074
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000069_1086' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000069
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000069_1086: Committed
15/08/09 15:27:39 INFO Executor: Finished task 69.0 in stage 14.0 (TID 1086). 781 bytes result sent to driver
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000062_1079' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000062
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000064_1081' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000064
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000062_1079: Committed
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000064_1081: Committed
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000071_1088' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000071
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000071_1088: Committed
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000065_1082' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000065
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000065_1082: Committed
15/08/09 15:27:39 INFO TaskSetManager: Starting task 79.0 in stage 14.0 (TID 1096, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 79.0 in stage 14.0 (TID 1096)
15/08/09 15:27:39 INFO Executor: Finished task 71.0 in stage 14.0 (TID 1088). 781 bytes result sent to driver
15/08/09 15:27:39 INFO Executor: Finished task 65.0 in stage 14.0 (TID 1082). 781 bytes result sent to driver
15/08/09 15:27:39 INFO Executor: Finished task 64.0 in stage 14.0 (TID 1081). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Finished task 69.0 in stage 14.0 (TID 1086) in 670 ms on localhost (64/200)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 80.0 in stage 14.0 (TID 1097, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 80.0 in stage 14.0 (TID 1097)
15/08/09 15:27:39 INFO Executor: Finished task 62.0 in stage 14.0 (TID 1079). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Finished task 71.0 in stage 14.0 (TID 1088) in 591 ms on localhost (65/200)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 81.0 in stage 14.0 (TID 1098, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 81.0 in stage 14.0 (TID 1098)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 65.0 in stage 14.0 (TID 1082) in 706 ms on localhost (66/200)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 82.0 in stage 14.0 (TID 1099, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 82.0 in stage 14.0 (TID 1099)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6bbfc81b
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000076_1093/part-00076
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO TaskSetManager: Finished task 64.0 in stage 14.0 (TID 1081) in 710 ms on localhost (67/200)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 83.0 in stage 14.0 (TID 1100, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 83.0 in stage 14.0 (TID 1100)
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2dd09ca9
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO TaskSetManager: Finished task 62.0 in stage 14.0 (TID 1079) in 722 ms on localhost (68/200)
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77fd9b80
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32d25d91
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@9b00cbd
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000072_1089' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000072
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000072_1089: Committed
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO Executor: Finished task 72.0 in stage 14.0 (TID 1089). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 84.0 in stage 14.0 (TID 1101, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 84.0 in stage 14.0 (TID 1101)
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO TaskSetManager: Finished task 72.0 in stage 14.0 (TID 1089) in 239 ms on localhost (69/200)
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000075_1092' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000075
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000075_1092: Committed
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO Executor: Finished task 75.0 in stage 14.0 (TID 1092). 781 bytes result sent to driver
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000074_1091' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000074
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000074_1091: Committed
15/08/09 15:27:39 INFO TaskSetManager: Starting task 85.0 in stage 14.0 (TID 1102, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Finished task 74.0 in stage 14.0 (TID 1091). 781 bytes result sent to driver
15/08/09 15:27:39 INFO Executor: Running task 85.0 in stage 14.0 (TID 1102)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 75.0 in stage 14.0 (TID 1092) in 194 ms on localhost (70/200)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 86.0 in stage 14.0 (TID 1103, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 86.0 in stage 14.0 (TID 1103)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 74.0 in stage 14.0 (TID 1091) in 196 ms on localhost (71/200)
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000073_1090' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000073
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000073_1090: Committed
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@308c8323
15/08/09 15:27:39 INFO Executor: Finished task 73.0 in stage 14.0 (TID 1090). 781 bytes result sent to driver
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000077_1094/part-00077
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO TaskSetManager: Starting task 87.0 in stage 14.0 (TID 1104, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO Executor: Running task 87.0 in stage 14.0 (TID 1104)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 73.0 in stage 14.0 (TID 1090) in 207 ms on localhost (72/200)
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000076_1093' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000076
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000076_1093: Committed
15/08/09 15:27:39 INFO Executor: Finished task 76.0 in stage 14.0 (TID 1093). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 88.0 in stage 14.0 (TID 1105, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 88.0 in stage 14.0 (TID 1105)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 76.0 in stage 14.0 (TID 1093) in 167 ms on localhost (73/200)
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@28be97b6
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000077_1094' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000077
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000077_1094: Committed
15/08/09 15:27:39 INFO Executor: Finished task 77.0 in stage 14.0 (TID 1094). 781 bytes result sent to driver
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO TaskSetManager: Starting task 89.0 in stage 14.0 (TID 1106, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO Executor: Running task 89.0 in stage 14.0 (TID 1106)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 77.0 in stage 14.0 (TID 1094) in 193 ms on localhost (74/200)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2087c48e
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000078_1095/part-00078
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@31e5ebe6
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000079_1096/part-00079
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@29b97063
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4d8188fb
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000081_1098/part-00081
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24af8fc9
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000080_1097/part-00080
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51957d2
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14f8f464
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4e8f284e
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000082_1099/part-00082
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@118bab1a
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@69fbbdee
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000083_1100/part-00083
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000068_1085' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000068
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000068_1085: Committed
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@234c9868
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO Executor: Finished task 68.0 in stage 14.0 (TID 1085). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 90.0 in stage 14.0 (TID 1107, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 90.0 in stage 14.0 (TID 1107)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 68.0 in stage 14.0 (TID 1085) in 934 ms on localhost (75/200)
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7fb9820f
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7e71cb56
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000085_1102/part-00085
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000078_1095' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000078
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000078_1095: Committed
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000079_1096' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000079
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000079_1096: Committed
15/08/09 15:27:39 INFO Executor: Finished task 78.0 in stage 14.0 (TID 1095). 781 bytes result sent to driver
15/08/09 15:27:39 INFO Executor: Finished task 79.0 in stage 14.0 (TID 1096). 781 bytes result sent to driver
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000081_1098' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000081
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000081_1098: Committed
15/08/09 15:27:39 INFO TaskSetManager: Starting task 91.0 in stage 14.0 (TID 1108, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Finished task 81.0 in stage 14.0 (TID 1098). 781 bytes result sent to driver
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@552b7860
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000084_1101/part-00084
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO Executor: Running task 91.0 in stage 14.0 (TID 1108)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 78.0 in stage 14.0 (TID 1095) in 309 ms on localhost (76/200)
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO TaskSetManager: Starting task 92.0 in stage 14.0 (TID 1109, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3a322616
15/08/09 15:27:39 INFO TaskSetManager: Finished task 79.0 in stage 14.0 (TID 1096) in 263 ms on localhost (77/200)
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO TaskSetManager: Starting task 93.0 in stage 14.0 (TID 1110, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO TaskSetManager: Finished task 81.0 in stage 14.0 (TID 1098) in 260 ms on localhost (78/200)
15/08/09 15:27:39 INFO Executor: Running task 93.0 in stage 14.0 (TID 1110)
15/08/09 15:27:39 INFO Executor: Running task 92.0 in stage 14.0 (TID 1109)
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000080_1097' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000080
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000080_1097: Committed
15/08/09 15:27:39 INFO Executor: Finished task 80.0 in stage 14.0 (TID 1097). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 94.0 in stage 14.0 (TID 1111, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 94.0 in stage 14.0 (TID 1111)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 80.0 in stage 14.0 (TID 1097) in 269 ms on localhost (79/200)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1fc3538
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000086_1103/part-00086
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000082_1099' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000082
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000082_1099: Committed
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO Executor: Finished task 82.0 in stage 14.0 (TID 1099). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 95.0 in stage 14.0 (TID 1112, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 95.0 in stage 14.0 (TID 1112)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 82.0 in stage 14.0 (TID 1099) in 278 ms on localhost (80/200)
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000083_1100' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000083
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2dded34f
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000083_1100: Committed
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO Executor: Finished task 83.0 in stage 14.0 (TID 1100). 781 bytes result sent to driver
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO TaskSetManager: Starting task 96.0 in stage 14.0 (TID 1113, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 96.0 in stage 14.0 (TID 1113)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 83.0 in stage 14.0 (TID 1100) in 283 ms on localhost (81/200)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@ccb5690
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000087_1104/part-00087
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@417d0498
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000085_1102' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000085
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000085_1102: Committed
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO Executor: Finished task 85.0 in stage 14.0 (TID 1102). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 97.0 in stage 14.0 (TID 1114, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 97.0 in stage 14.0 (TID 1114)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 85.0 in stage 14.0 (TID 1102) in 262 ms on localhost (82/200)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42d7131
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000088_1105/part-00088
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000066_1083' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000066
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000066_1083: Committed
15/08/09 15:27:39 INFO Executor: Finished task 66.0 in stage 14.0 (TID 1083). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 98.0 in stage 14.0 (TID 1115, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 66.0 in stage 14.0 (TID 1083) in 1008 ms on localhost (83/200)
15/08/09 15:27:39 INFO Executor: Running task 98.0 in stage 14.0 (TID 1115)
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000070_1087' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000070
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000070_1087: Committed
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO Executor: Finished task 70.0 in stage 14.0 (TID 1087). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 99.0 in stage 14.0 (TID 1116, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 99.0 in stage 14.0 (TID 1116)
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@157b3aa2
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO TaskSetManager: Finished task 70.0 in stage 14.0 (TID 1087) in 955 ms on localhost (84/200)
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000067_1084' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000067
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000067_1084: Committed
15/08/09 15:27:39 INFO Executor: Finished task 67.0 in stage 14.0 (TID 1084). 781 bytes result sent to driver
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000086_1103' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000086
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000086_1103: Committed
15/08/09 15:27:39 INFO TaskSetManager: Starting task 100.0 in stage 14.0 (TID 1117, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 100.0 in stage 14.0 (TID 1117)
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000084_1101' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000084
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000084_1101: Committed
15/08/09 15:27:39 INFO Executor: Finished task 86.0 in stage 14.0 (TID 1103). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Finished task 67.0 in stage 14.0 (TID 1084) in 1014 ms on localhost (85/200)
15/08/09 15:27:39 INFO Executor: Finished task 84.0 in stage 14.0 (TID 1101). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 101.0 in stage 14.0 (TID 1118, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 101.0 in stage 14.0 (TID 1118)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 86.0 in stage 14.0 (TID 1103) in 282 ms on localhost (86/200)
15/08/09 15:27:39 INFO TaskSetManager: Starting task 102.0 in stage 14.0 (TID 1119, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 102.0 in stage 14.0 (TID 1119)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 84.0 in stage 14.0 (TID 1101) in 301 ms on localhost (87/200)
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1812f7b3
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000087_1104' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000087
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000087_1104: Committed
15/08/09 15:27:39 INFO Executor: Finished task 87.0 in stage 14.0 (TID 1104). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 103.0 in stage 14.0 (TID 1120, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 103.0 in stage 14.0 (TID 1120)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 87.0 in stage 14.0 (TID 1104) in 295 ms on localhost (88/200)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6b150c1
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000089_1106/part-00089
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72047d5f
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:39 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000088_1105' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000088
15/08/09 15:27:39 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000088_1105: Committed
15/08/09 15:27:39 INFO Executor: Finished task 88.0 in stage 14.0 (TID 1105). 781 bytes result sent to driver
15/08/09 15:27:39 INFO TaskSetManager: Starting task 104.0 in stage 14.0 (TID 1121, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:39 INFO Executor: Running task 104.0 in stage 14.0 (TID 1121)
15/08/09 15:27:39 INFO TaskSetManager: Finished task 88.0 in stage 14.0 (TID 1105) in 331 ms on localhost (89/200)
15/08/09 15:27:39 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@633c0e6f
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000090_1107/part-00090
15/08/09 15:27:39 INFO CodecConfig: Compression set to false
15/08/09 15:27:39 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:39 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:39 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:39 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c775143
15/08/09 15:27:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:39 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000089_1106' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000089
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000089_1106: Committed
15/08/09 15:27:40 INFO Executor: Finished task 89.0 in stage 14.0 (TID 1106). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 105.0 in stage 14.0 (TID 1122, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 89.0 in stage 14.0 (TID 1106) in 351 ms on localhost (90/200)
15/08/09 15:27:40 INFO Executor: Running task 105.0 in stage 14.0 (TID 1122)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000090_1107' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000090
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000090_1107: Committed
15/08/09 15:27:40 INFO Executor: Finished task 90.0 in stage 14.0 (TID 1107). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 106.0 in stage 14.0 (TID 1123, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 106.0 in stage 14.0 (TID 1123)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 90.0 in stage 14.0 (TID 1107) in 198 ms on localhost (91/200)
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a6dc9e6
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000093_1110/part-00093
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2fd6cae9
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000101_1118/part-00101
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3b27da72
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000096_1113/part-00096
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7482e9a9
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000091_1108/part-00091
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@77d916ae
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10b9566d
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000095_1112/part-00095
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3eba9201
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4ac79fd3
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2abc331
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@42ed9243
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000092_1109/part-00092
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53e67391
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000102_1119/part-00102
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000094_1111/part-00094
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4739da00
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4a536264
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2860b95b
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000099_1116/part-00099
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@669bef85
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000097_1114/part-00097
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78a3964
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000098_1115/part-00098
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000101_1118' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000101
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000101_1118: Committed
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO Executor: Finished task 101.0 in stage 14.0 (TID 1118). 781 bytes result sent to driver
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO TaskSetManager: Starting task 107.0 in stage 14.0 (TID 1124, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 107.0 in stage 14.0 (TID 1124)
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@48902250
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6ebcd767
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b9658ca
15/08/09 15:27:40 INFO TaskSetManager: Finished task 101.0 in stage 14.0 (TID 1118) in 174 ms on localhost (92/200)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000093_1110' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000093
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000093_1110: Committed
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000100_1117/part-00100
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2f68d13
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4e435a39
15/08/09 15:27:40 INFO Executor: Finished task 93.0 in stage 14.0 (TID 1110). 781 bytes result sent to driver
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO TaskSetManager: Starting task 108.0 in stage 14.0 (TID 1125, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO Executor: Running task 108.0 in stage 14.0 (TID 1125)
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO TaskSetManager: Finished task 93.0 in stage 14.0 (TID 1110) in 427 ms on localhost (93/200)
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@690c2ead
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6319a3e9
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ba0922
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000102_1119' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000102
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000102_1119: Committed
15/08/09 15:27:40 INFO Executor: Finished task 102.0 in stage 14.0 (TID 1119). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 109.0 in stage 14.0 (TID 1126, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 109.0 in stage 14.0 (TID 1126)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 102.0 in stage 14.0 (TID 1119) in 388 ms on localhost (94/200)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000098_1115' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000098
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000098_1115: Committed
15/08/09 15:27:40 INFO Executor: Finished task 98.0 in stage 14.0 (TID 1115). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 110.0 in stage 14.0 (TID 1127, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 110.0 in stage 14.0 (TID 1127)
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4b449730
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000103_1120/part-00103
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO TaskSetManager: Finished task 98.0 in stage 14.0 (TID 1115) in 407 ms on localhost (95/200)
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000100_1117' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000100
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000100_1117: Committed
15/08/09 15:27:40 INFO Executor: Finished task 100.0 in stage 14.0 (TID 1117). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 111.0 in stage 14.0 (TID 1128, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 111.0 in stage 14.0 (TID 1128)
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@b7f97ef
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000104_1121/part-00104
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO TaskSetManager: Finished task 100.0 in stage 14.0 (TID 1117) in 404 ms on localhost (96/200)
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2ead7ea1
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6bc8de6d
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000103_1120' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000103
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000103_1120: Committed
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000104_1121' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000104
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000104_1121: Committed
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO Executor: Finished task 103.0 in stage 14.0 (TID 1120). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 112.0 in stage 14.0 (TID 1129, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Finished task 104.0 in stage 14.0 (TID 1121). 781 bytes result sent to driver
15/08/09 15:27:40 INFO Executor: Running task 112.0 in stage 14.0 (TID 1129)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 103.0 in stage 14.0 (TID 1120) in 410 ms on localhost (97/200)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 113.0 in stage 14.0 (TID 1130, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 113.0 in stage 14.0 (TID 1130)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 104.0 in stage 14.0 (TID 1121) in 371 ms on localhost (98/200)
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2046c2d6
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000105_1122/part-00105
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4f79d07a
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000106_1123/part-00106
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d28b6be
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d296a28
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000106_1123' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000106
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000106_1123: Committed
15/08/09 15:27:40 INFO Executor: Finished task 106.0 in stage 14.0 (TID 1123). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 114.0 in stage 14.0 (TID 1131, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 114.0 in stage 14.0 (TID 1131)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 106.0 in stage 14.0 (TID 1123) in 359 ms on localhost (99/200)
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5f9c369f
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000107_1124/part-00107
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@356b4ea8
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000108_1125/part-00108
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@36c7a1af
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@565cdb0
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7841c28b
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000109_1126/part-00109
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000107_1124' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000107
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000107_1124: Committed
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6144eaf8
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@739e71ab
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000110_1127/part-00110
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000111_1128/part-00111
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO Executor: Finished task 107.0 in stage 14.0 (TID 1124). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 115.0 in stage 14.0 (TID 1132, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 115.0 in stage 14.0 (TID 1132)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 107.0 in stage 14.0 (TID 1124) in 343 ms on localhost (100/200)
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@421c4398
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000108_1125' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000108
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000108_1125: Committed
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2330cedd
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO Executor: Finished task 108.0 in stage 14.0 (TID 1125). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 116.0 in stage 14.0 (TID 1133, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f03cde9
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO Executor: Running task 116.0 in stage 14.0 (TID 1133)
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO TaskSetManager: Finished task 108.0 in stage 14.0 (TID 1125) in 351 ms on localhost (101/200)
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3977e263
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000113_1130/part-00113
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000109_1126' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000109
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000109_1126: Committed
15/08/09 15:27:40 INFO Executor: Finished task 109.0 in stage 14.0 (TID 1126). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 117.0 in stage 14.0 (TID 1134, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 117.0 in stage 14.0 (TID 1134)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000110_1127' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000110
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000110_1127: Committed
15/08/09 15:27:40 INFO TaskSetManager: Finished task 109.0 in stage 14.0 (TID 1126) in 163 ms on localhost (102/200)
15/08/09 15:27:40 INFO Executor: Finished task 110.0 in stage 14.0 (TID 1127). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 118.0 in stage 14.0 (TID 1135, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 118.0 in stage 14.0 (TID 1135)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 110.0 in stage 14.0 (TID 1127) in 160 ms on localhost (103/200)
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6aa77c72
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000112_1129/part-00112
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5c62528b
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4bf93196
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2957ad05
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000114_1131/part-00114
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000113_1130' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000113
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000113_1130: Committed
15/08/09 15:27:40 INFO Executor: Finished task 113.0 in stage 14.0 (TID 1130). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 119.0 in stage 14.0 (TID 1136, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 119.0 in stage 14.0 (TID 1136)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 113.0 in stage 14.0 (TID 1130) in 169 ms on localhost (104/200)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000112_1129' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000112
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000112_1129: Committed
15/08/09 15:27:40 INFO Executor: Finished task 112.0 in stage 14.0 (TID 1129). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 120.0 in stage 14.0 (TID 1137, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 120.0 in stage 14.0 (TID 1137)
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1b97884b
15/08/09 15:27:40 INFO TaskSetManager: Finished task 112.0 in stage 14.0 (TID 1129) in 174 ms on localhost (105/200)
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000114_1131' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000114
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000114_1131: Committed
15/08/09 15:27:40 INFO Executor: Finished task 114.0 in stage 14.0 (TID 1131). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 121.0 in stage 14.0 (TID 1138, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 114.0 in stage 14.0 (TID 1131) in 151 ms on localhost (106/200)
15/08/09 15:27:40 INFO Executor: Running task 121.0 in stage 14.0 (TID 1138)
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@15f532b0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000115_1132/part-00115
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3f008d83
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@10c52328
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000118_1135/part-00118
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@70f6b0d9
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000116_1133/part-00116
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000115_1132' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000115
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000115_1132: Committed
15/08/09 15:27:40 INFO Executor: Finished task 115.0 in stage 14.0 (TID 1132). 781 bytes result sent to driver
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25c7f94a
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000117_1134/part-00117
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO TaskSetManager: Starting task 122.0 in stage 14.0 (TID 1139, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 122.0 in stage 14.0 (TID 1139)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 115.0 in stage 14.0 (TID 1132) in 161 ms on localhost (107/200)
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@693a6618
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2db5ccd1
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@169ef753
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000117_1134' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000117
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000117_1134: Committed
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000118_1135' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000118
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000118_1135: Committed
15/08/09 15:27:40 INFO Executor: Finished task 117.0 in stage 14.0 (TID 1134). 781 bytes result sent to driver
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO TaskSetManager: Starting task 123.0 in stage 14.0 (TID 1140, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 123.0 in stage 14.0 (TID 1140)
15/08/09 15:27:40 INFO Executor: Finished task 118.0 in stage 14.0 (TID 1135). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Finished task 117.0 in stage 14.0 (TID 1134) in 155 ms on localhost (108/200)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 124.0 in stage 14.0 (TID 1141, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000116_1133' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000116
15/08/09 15:27:40 INFO Executor: Running task 124.0 in stage 14.0 (TID 1141)
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4243ae8f
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000119_1136/part-00119
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000116_1133: Committed
15/08/09 15:27:40 INFO TaskSetManager: Finished task 118.0 in stage 14.0 (TID 1135) in 155 ms on localhost (109/200)
15/08/09 15:27:40 INFO Executor: Finished task 116.0 in stage 14.0 (TID 1133). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 125.0 in stage 14.0 (TID 1142, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 125.0 in stage 14.0 (TID 1142)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 116.0 in stage 14.0 (TID 1133) in 183 ms on localhost (110/200)
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@56bf9cc4
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000120_1137/part-00120
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5a5bfc6c
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6eb02dc4
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000111_1128' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000111
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000111_1128: Committed
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000097_1114' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000097
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000097_1114: Committed
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22a52643
15/08/09 15:27:40 INFO Executor: Finished task 111.0 in stage 14.0 (TID 1128). 781 bytes result sent to driver
15/08/09 15:27:40 INFO Executor: Finished task 97.0 in stage 14.0 (TID 1114). 781 bytes result sent to driver
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000121_1138/part-00121
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO TaskSetManager: Starting task 126.0 in stage 14.0 (TID 1143, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 126.0 in stage 14.0 (TID 1143)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 127.0 in stage 14.0 (TID 1144, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 111.0 in stage 14.0 (TID 1128) in 607 ms on localhost (111/200)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 97.0 in stage 14.0 (TID 1114) in 1030 ms on localhost (112/200)
15/08/09 15:27:40 INFO Executor: Running task 127.0 in stage 14.0 (TID 1144)
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000105_1122' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000105
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000105_1122: Committed
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000094_1111' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000094
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000094_1111: Committed
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000091_1108' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000091
15/08/09 15:27:40 INFO Executor: Finished task 105.0 in stage 14.0 (TID 1122). 781 bytes result sent to driver
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000091_1108: Committed
15/08/09 15:27:40 INFO TaskSetManager: Starting task 128.0 in stage 14.0 (TID 1145, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 128.0 in stage 14.0 (TID 1145)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000099_1116' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000099
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000099_1116: Committed
15/08/09 15:27:40 INFO Executor: Finished task 91.0 in stage 14.0 (TID 1108). 781 bytes result sent to driver
15/08/09 15:27:40 INFO Executor: Finished task 94.0 in stage 14.0 (TID 1111). 781 bytes result sent to driver
15/08/09 15:27:40 INFO Executor: Finished task 99.0 in stage 14.0 (TID 1116). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Finished task 105.0 in stage 14.0 (TID 1122) in 905 ms on localhost (113/200)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 129.0 in stage 14.0 (TID 1146, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 129.0 in stage 14.0 (TID 1146)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 130.0 in stage 14.0 (TID 1147, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 130.0 in stage 14.0 (TID 1147)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 131.0 in stage 14.0 (TID 1148, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 131.0 in stage 14.0 (TID 1148)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000096_1113' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000096
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000096_1113: Committed
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000095_1112' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000095
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000095_1112: Committed
15/08/09 15:27:40 INFO TaskSetManager: Finished task 91.0 in stage 14.0 (TID 1108) in 1084 ms on localhost (114/200)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000092_1109' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000092
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000092_1109: Committed
15/08/09 15:27:40 INFO Executor: Finished task 96.0 in stage 14.0 (TID 1113). 781 bytes result sent to driver
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000119_1136' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000119
15/08/09 15:27:40 INFO Executor: Finished task 95.0 in stage 14.0 (TID 1112). 781 bytes result sent to driver
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000119_1136: Committed
15/08/09 15:27:40 INFO TaskSetManager: Finished task 94.0 in stage 14.0 (TID 1111) in 1074 ms on localhost (115/200)
15/08/09 15:27:40 INFO Executor: Finished task 92.0 in stage 14.0 (TID 1109). 781 bytes result sent to driver
15/08/09 15:27:40 INFO Executor: Finished task 119.0 in stage 14.0 (TID 1136). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Finished task 99.0 in stage 14.0 (TID 1116) in 1024 ms on localhost (116/200)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 132.0 in stage 14.0 (TID 1149, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 132.0 in stage 14.0 (TID 1149)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 96.0 in stage 14.0 (TID 1113) in 1056 ms on localhost (117/200)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 133.0 in stage 14.0 (TID 1150, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 133.0 in stage 14.0 (TID 1150)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 95.0 in stage 14.0 (TID 1112) in 1065 ms on localhost (118/200)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 134.0 in stage 14.0 (TID 1151, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 134.0 in stage 14.0 (TID 1151)
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2e9bd79c
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO TaskSetManager: Finished task 92.0 in stage 14.0 (TID 1109) in 1086 ms on localhost (119/200)
15/08/09 15:27:40 INFO TaskSetManager: Starting task 135.0 in stage 14.0 (TID 1152, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 135.0 in stage 14.0 (TID 1152)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 119.0 in stage 14.0 (TID 1136) in 425 ms on localhost (120/200)
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000120_1137' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000120
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000120_1137: Committed
15/08/09 15:27:40 INFO Executor: Finished task 120.0 in stage 14.0 (TID 1137). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 136.0 in stage 14.0 (TID 1153, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 136.0 in stage 14.0 (TID 1153)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 120.0 in stage 14.0 (TID 1137) in 429 ms on localhost (121/200)
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:40 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000121_1138' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000121
15/08/09 15:27:40 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000121_1138: Committed
15/08/09 15:27:40 INFO Executor: Finished task 121.0 in stage 14.0 (TID 1138). 781 bytes result sent to driver
15/08/09 15:27:40 INFO TaskSetManager: Starting task 137.0 in stage 14.0 (TID 1154, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:40 INFO Executor: Running task 137.0 in stage 14.0 (TID 1154)
15/08/09 15:27:40 INFO TaskSetManager: Finished task 121.0 in stage 14.0 (TID 1138) in 435 ms on localhost (122/200)
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/09 15:27:40 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19138dcd
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000122_1139/part-00122
15/08/09 15:27:40 INFO CodecConfig: Compression set to false
15/08/09 15:27:40 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:40 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:40 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:40 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3ef0dd35
15/08/09 15:27:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:40 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1573b439
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000125_1142/part-00125
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@23b70900
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000127_1144/part-00127
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@45aff151
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5dd4ee4b
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000123_1140/part-00123
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@16d12fb7
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000130_1147/part-00130
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d581836
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34b6fb81
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000122_1139' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000122
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000122_1139: Committed
15/08/09 15:27:41 INFO Executor: Finished task 122.0 in stage 14.0 (TID 1139). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 138.0 in stage 14.0 (TID 1155, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 138.0 in stage 14.0 (TID 1155)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 122.0 in stage 14.0 (TID 1139) in 482 ms on localhost (123/200)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@116fa838
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3f6efcf4
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000133_1150/part-00133
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6545c2f4
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000135_1152/part-00135
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2f8c46a4
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000129_1146/part-00129
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000125_1142' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000125
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000125_1142: Committed
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3c00136c
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000132_1149/part-00132
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO Executor: Finished task 125.0 in stage 14.0 (TID 1142). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 139.0 in stage 14.0 (TID 1156, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 139.0 in stage 14.0 (TID 1156)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 125.0 in stage 14.0 (TID 1142) in 462 ms on localhost (124/200)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@62dc1741
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000124_1141/part-00124
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000123_1140' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000123
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000127_1144' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000127
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000127_1144: Committed
15/08/09 15:27:41 INFO Executor: Finished task 127.0 in stage 14.0 (TID 1144). 781 bytes result sent to driver
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7606197f
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000134_1151/part-00134
15/08/09 15:27:41 INFO TaskSetManager: Starting task 140.0 in stage 14.0 (TID 1157, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000123_1140: Committed
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO TaskSetManager: Finished task 127.0 in stage 14.0 (TID 1144) in 177 ms on localhost (125/200)
15/08/09 15:27:41 INFO Executor: Running task 140.0 in stage 14.0 (TID 1157)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5fefb6ea
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@72d1edda
15/08/09 15:27:41 INFO Executor: Finished task 123.0 in stage 14.0 (TID 1140). 781 bytes result sent to driver
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO TaskSetManager: Starting task 141.0 in stage 14.0 (TID 1158, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 141.0 in stage 14.0 (TID 1158)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@678976cb
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000136_1153/part-00136
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO TaskSetManager: Finished task 123.0 in stage 14.0 (TID 1140) in 477 ms on localhost (126/200)
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5250332a
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@292a5f04
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@661d3389
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7229bbf
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000130_1147' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000130
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000130_1147: Committed
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@22c9c901
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@67b813d1
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000131_1148/part-00131
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@38e17f69
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000126_1143/part-00126
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO Executor: Finished task 130.0 in stage 14.0 (TID 1147). 781 bytes result sent to driver
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO TaskSetManager: Starting task 142.0 in stage 14.0 (TID 1159, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 142.0 in stage 14.0 (TID 1159)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 130.0 in stage 14.0 (TID 1147) in 197 ms on localhost (127/200)
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000129_1146' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000129
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000129_1146: Committed
15/08/09 15:27:41 INFO Executor: Finished task 129.0 in stage 14.0 (TID 1146). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 143.0 in stage 14.0 (TID 1160, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 143.0 in stage 14.0 (TID 1160)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 129.0 in stage 14.0 (TID 1146) in 212 ms on localhost (128/200)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7c9ffb44
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000128_1145/part-00128
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@104499ee
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7246b215
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000137_1154/part-00137
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ac0cb35
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@14b8ddeb
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5be94d60
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000132_1149' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000132
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000132_1149: Committed
15/08/09 15:27:41 INFO Executor: Finished task 132.0 in stage 14.0 (TID 1149). 781 bytes result sent to driver
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000133_1150' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000133
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000133_1150: Committed
15/08/09 15:27:41 INFO TaskSetManager: Starting task 144.0 in stage 14.0 (TID 1161, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Finished task 133.0 in stage 14.0 (TID 1150). 781 bytes result sent to driver
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000136_1153' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000136
15/08/09 15:27:41 INFO Executor: Running task 144.0 in stage 14.0 (TID 1161)
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000136_1153: Committed
15/08/09 15:27:41 INFO TaskSetManager: Finished task 132.0 in stage 14.0 (TID 1149) in 259 ms on localhost (129/200)
15/08/09 15:27:41 INFO Executor: Finished task 136.0 in stage 14.0 (TID 1153). 781 bytes result sent to driver
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000135_1152' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000135
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000135_1152: Committed
15/08/09 15:27:41 INFO TaskSetManager: Starting task 145.0 in stage 14.0 (TID 1162, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Finished task 135.0 in stage 14.0 (TID 1152). 781 bytes result sent to driver
15/08/09 15:27:41 INFO Executor: Running task 145.0 in stage 14.0 (TID 1162)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 133.0 in stage 14.0 (TID 1150) in 260 ms on localhost (130/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 146.0 in stage 14.0 (TID 1163, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 146.0 in stage 14.0 (TID 1163)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 136.0 in stage 14.0 (TID 1153) in 247 ms on localhost (131/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 147.0 in stage 14.0 (TID 1164, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 147.0 in stage 14.0 (TID 1164)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 135.0 in stage 14.0 (TID 1152) in 258 ms on localhost (132/200)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000128_1145' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000128
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000128_1145: Committed
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000124_1141' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000124
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000124_1141: Committed
15/08/09 15:27:41 INFO Executor: Finished task 124.0 in stage 14.0 (TID 1141). 781 bytes result sent to driver
15/08/09 15:27:41 INFO Executor: Finished task 128.0 in stage 14.0 (TID 1145). 781 bytes result sent to driver
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7d3dba5f
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000138_1155/part-00138
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO TaskSetManager: Starting task 148.0 in stage 14.0 (TID 1165, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 148.0 in stage 14.0 (TID 1165)
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO TaskSetManager: Finished task 124.0 in stage 14.0 (TID 1141) in 585 ms on localhost (133/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 149.0 in stage 14.0 (TID 1166, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 149.0 in stage 14.0 (TID 1166)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 128.0 in stage 14.0 (TID 1145) in 290 ms on localhost (134/200)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000134_1151' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000134
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000126_1143' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000126
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000131_1148' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000131
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000126_1143: Committed
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000131_1148: Committed
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000134_1151: Committed
15/08/09 15:27:41 INFO Executor: Finished task 126.0 in stage 14.0 (TID 1143). 781 bytes result sent to driver
15/08/09 15:27:41 INFO Executor: Finished task 131.0 in stage 14.0 (TID 1148). 781 bytes result sent to driver
15/08/09 15:27:41 INFO Executor: Finished task 134.0 in stage 14.0 (TID 1151). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Finished task 126.0 in stage 14.0 (TID 1143) in 300 ms on localhost (135/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 150.0 in stage 14.0 (TID 1167, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 131.0 in stage 14.0 (TID 1148) in 293 ms on localhost (136/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 151.0 in stage 14.0 (TID 1168, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 151.0 in stage 14.0 (TID 1168)
15/08/09 15:27:41 INFO Executor: Running task 150.0 in stage 14.0 (TID 1167)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 152.0 in stage 14.0 (TID 1169, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 152.0 in stage 14.0 (TID 1169)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 134.0 in stage 14.0 (TID 1151) in 289 ms on localhost (137/200)
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5189bcc0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000139_1156/part-00139
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@12d94c20
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3134567e
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000137_1154' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000137
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000137_1154: Committed
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO Executor: Finished task 137.0 in stage 14.0 (TID 1154). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 153.0 in stage 14.0 (TID 1170, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6c2bd990
15/08/09 15:27:41 INFO TaskSetManager: Finished task 137.0 in stage 14.0 (TID 1154) in 417 ms on localhost (138/200)
15/08/09 15:27:41 INFO Executor: Running task 153.0 in stage 14.0 (TID 1170)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000140_1157/part-00140
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@60dd4f2e
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@28cc3d8f
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000143_1160/part-00143
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@19202991
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78dd8c35
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000141_1158/part-00141
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000142_1159/part-00142
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000138_1155' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000138
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000138_1155: Committed
15/08/09 15:27:41 INFO Executor: Finished task 138.0 in stage 14.0 (TID 1155). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 154.0 in stage 14.0 (TID 1171, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 154.0 in stage 14.0 (TID 1171)
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO TaskSetManager: Finished task 138.0 in stage 14.0 (TID 1155) in 359 ms on localhost (139/200)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@592b53b0
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000140_1157' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000140
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000140_1157: Committed
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3504f642
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO Executor: Finished task 140.0 in stage 14.0 (TID 1157). 781 bytes result sent to driver
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO TaskSetManager: Starting task 155.0 in stage 14.0 (TID 1172, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 155.0 in stage 14.0 (TID 1172)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 140.0 in stage 14.0 (TID 1157) in 342 ms on localhost (140/200)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4d2aa984
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@24919142
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000144_1161/part-00144
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000143_1160' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000143
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000143_1160: Committed
15/08/09 15:27:41 INFO Executor: Finished task 143.0 in stage 14.0 (TID 1160). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 156.0 in stage 14.0 (TID 1173, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000142_1159' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000142
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000142_1159: Committed
15/08/09 15:27:41 INFO Executor: Running task 156.0 in stage 14.0 (TID 1173)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 143.0 in stage 14.0 (TID 1160) in 321 ms on localhost (141/200)
15/08/09 15:27:41 INFO Executor: Finished task 142.0 in stage 14.0 (TID 1159). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 157.0 in stage 14.0 (TID 1174, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 157.0 in stage 14.0 (TID 1174)
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO TaskSetManager: Finished task 142.0 in stage 14.0 (TID 1159) in 336 ms on localhost (142/200)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@66b46e6a
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000146_1163/part-00146
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1f87f19d
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53900c8
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61624d25
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000149_1166/part-00149
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000147_1164/part-00147
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000141_1158' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000141
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000141_1158: Committed
15/08/09 15:27:41 INFO Executor: Finished task 141.0 in stage 14.0 (TID 1158). 781 bytes result sent to driver
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO TaskSetManager: Starting task 158.0 in stage 14.0 (TID 1175, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 158.0 in stage 14.0 (TID 1175)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 141.0 in stage 14.0 (TID 1158) in 382 ms on localhost (143/200)
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1d6a6644
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@34600972
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1cbc622e
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000148_1165/part-00148
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@78863571
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000151_1168/part-00151
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@331ecb88
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000152_1169/part-00152
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@30a4c2fe
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000150_1167/part-00150
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5f970362
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@570eb5ae
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@41ae470e
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@63f72fbc
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2900b83
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000145_1162/part-00145
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000144_1161' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000144
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000144_1161: Committed
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b3f649d
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO Executor: Finished task 144.0 in stage 14.0 (TID 1161). 781 bytes result sent to driver
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO TaskSetManager: Starting task 159.0 in stage 14.0 (TID 1176, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 159.0 in stage 14.0 (TID 1176)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 144.0 in stage 14.0 (TID 1161) in 325 ms on localhost (144/200)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000146_1163' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000146
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000146_1163: Committed
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO Executor: Finished task 146.0 in stage 14.0 (TID 1163). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 160.0 in stage 14.0 (TID 1177, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 160.0 in stage 14.0 (TID 1177)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 146.0 in stage 14.0 (TID 1163) in 323 ms on localhost (145/200)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000149_1166' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000149
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000149_1166: Committed
15/08/09 15:27:41 INFO Executor: Finished task 149.0 in stage 14.0 (TID 1166). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 161.0 in stage 14.0 (TID 1178, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 161.0 in stage 14.0 (TID 1178)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@8bd50d4
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO TaskSetManager: Finished task 149.0 in stage 14.0 (TID 1166) in 316 ms on localhost (146/200)
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000147_1164' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000147
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000147_1164: Committed
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000152_1169' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000152
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000152_1169: Committed
15/08/09 15:27:41 INFO Executor: Finished task 152.0 in stage 14.0 (TID 1169). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 162.0 in stage 14.0 (TID 1179, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000151_1168' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000151
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000151_1168: Committed
15/08/09 15:27:41 INFO Executor: Running task 162.0 in stage 14.0 (TID 1179)
15/08/09 15:27:41 INFO Executor: Finished task 147.0 in stage 14.0 (TID 1164). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Finished task 152.0 in stage 14.0 (TID 1169) in 308 ms on localhost (147/200)
15/08/09 15:27:41 INFO Executor: Finished task 151.0 in stage 14.0 (TID 1168). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 163.0 in stage 14.0 (TID 1180, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 163.0 in stage 14.0 (TID 1180)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 147.0 in stage 14.0 (TID 1164) in 336 ms on localhost (148/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 164.0 in stage 14.0 (TID 1181, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 164.0 in stage 14.0 (TID 1181)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 151.0 in stage 14.0 (TID 1168) in 314 ms on localhost (149/200)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000150_1167' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000150
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000150_1167: Committed
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@55b2b945
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000153_1170/part-00153
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO Executor: Finished task 150.0 in stage 14.0 (TID 1167). 781 bytes result sent to driver
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000148_1165' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000148
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000148_1165: Committed
15/08/09 15:27:41 INFO TaskSetManager: Starting task 165.0 in stage 14.0 (TID 1182, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 165.0 in stage 14.0 (TID 1182)
15/08/09 15:27:41 INFO Executor: Finished task 148.0 in stage 14.0 (TID 1165). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Finished task 150.0 in stage 14.0 (TID 1167) in 319 ms on localhost (150/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 166.0 in stage 14.0 (TID 1183, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 166.0 in stage 14.0 (TID 1183)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 148.0 in stage 14.0 (TID 1165) in 332 ms on localhost (151/200)
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000145_1162' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000145
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000145_1162: Committed
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@770aa1cb
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO Executor: Finished task 145.0 in stage 14.0 (TID 1162). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 167.0 in stage 14.0 (TID 1184, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 167.0 in stage 14.0 (TID 1184)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 145.0 in stage 14.0 (TID 1162) in 363 ms on localhost (152/200)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53c7209
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000154_1171/part-00154
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1c4232ad
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000153_1170' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000153
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000153_1170: Committed
15/08/09 15:27:41 INFO Executor: Finished task 153.0 in stage 14.0 (TID 1170). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 168.0 in stage 14.0 (TID 1185, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 168.0 in stage 14.0 (TID 1185)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 153.0 in stage 14.0 (TID 1170) in 200 ms on localhost (153/200)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@18b4182b
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000156_1173/part-00156
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@73b2378b
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000157_1174/part-00157
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@377ce890
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000155_1172/part-00155
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000154_1171' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000154
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000154_1171: Committed
15/08/09 15:27:41 INFO Executor: Finished task 154.0 in stage 14.0 (TID 1171). 781 bytes result sent to driver
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@4e501e84
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO TaskSetManager: Starting task 169.0 in stage 14.0 (TID 1186, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 169.0 in stage 14.0 (TID 1186)
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO TaskSetManager: Finished task 154.0 in stage 14.0 (TID 1171) in 259 ms on localhost (154/200)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@38ca1f54
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3910d611
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5856746f
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000158_1175/part-00158
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@145f0c9e
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000156_1173' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000156
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000156_1173: Committed
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000155_1172' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000155
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000155_1172: Committed
15/08/09 15:27:41 INFO Executor: Finished task 156.0 in stage 14.0 (TID 1173). 781 bytes result sent to driver
15/08/09 15:27:41 INFO Executor: Finished task 155.0 in stage 14.0 (TID 1172). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 170.0 in stage 14.0 (TID 1187, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 170.0 in stage 14.0 (TID 1187)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 156.0 in stage 14.0 (TID 1173) in 260 ms on localhost (155/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 171.0 in stage 14.0 (TID 1188, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 171.0 in stage 14.0 (TID 1188)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 155.0 in stage 14.0 (TID 1172) in 283 ms on localhost (156/200)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@52819963
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000165_1182/part-00165
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ddd0e72
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@bdf75e4
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000160_1177/part-00160
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000161_1178/part-00161
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@294c032
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000163_1180/part-00163
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1659b05a
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000162_1179/part-00162
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000158_1175' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000158
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000158_1175: Committed
15/08/09 15:27:41 INFO Executor: Finished task 158.0 in stage 14.0 (TID 1175). 781 bytes result sent to driver
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@70446d01
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000166_1183/part-00166
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO TaskSetManager: Starting task 172.0 in stage 14.0 (TID 1189, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6eb74ba9
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO Executor: Running task 172.0 in stage 14.0 (TID 1189)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1f88bafc
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000164_1181/part-00164
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@49ac0690
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO TaskSetManager: Finished task 158.0 in stage 14.0 (TID 1175) in 262 ms on localhost (157/200)
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3ce2fdb9
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@adc4616
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000159_1176/part-00159
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@26862eaf
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7b880d43
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1c33cac4
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@29c12184
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@27395e45
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000165_1182' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000165
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000165_1182: Committed
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000161_1178' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000161
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000161_1178: Committed
15/08/09 15:27:41 INFO Executor: Finished task 165.0 in stage 14.0 (TID 1182). 781 bytes result sent to driver
15/08/09 15:27:41 INFO Executor: Finished task 161.0 in stage 14.0 (TID 1178). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 173.0 in stage 14.0 (TID 1190, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 174.0 in stage 14.0 (TID 1191, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 173.0 in stage 14.0 (TID 1190)
15/08/09 15:27:41 INFO Executor: Running task 174.0 in stage 14.0 (TID 1191)
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000160_1177' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000160
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000160_1177: Committed
15/08/09 15:27:41 INFO TaskSetManager: Finished task 165.0 in stage 14.0 (TID 1182) in 241 ms on localhost (158/200)
15/08/09 15:27:41 INFO Executor: Finished task 160.0 in stage 14.0 (TID 1177). 781 bytes result sent to driver
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000162_1179' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000162
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000162_1179: Committed
15/08/09 15:27:41 INFO TaskSetManager: Finished task 161.0 in stage 14.0 (TID 1178) in 257 ms on localhost (159/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 175.0 in stage 14.0 (TID 1192, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Finished task 162.0 in stage 14.0 (TID 1179). 781 bytes result sent to driver
15/08/09 15:27:41 INFO Executor: Running task 175.0 in stage 14.0 (TID 1192)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 160.0 in stage 14.0 (TID 1177) in 264 ms on localhost (160/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 176.0 in stage 14.0 (TID 1193, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 176.0 in stage 14.0 (TID 1193)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 162.0 in stage 14.0 (TID 1179) in 251 ms on localhost (161/200)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000166_1183' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000166
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000166_1183: Committed
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2c12d853
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000167_1184/part-00167
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO Executor: Finished task 166.0 in stage 14.0 (TID 1183). 781 bytes result sent to driver
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000164_1181' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000164
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000164_1181: Committed
15/08/09 15:27:41 INFO TaskSetManager: Starting task 177.0 in stage 14.0 (TID 1194, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 177.0 in stage 14.0 (TID 1194)
15/08/09 15:27:41 INFO Executor: Finished task 164.0 in stage 14.0 (TID 1181). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Finished task 166.0 in stage 14.0 (TID 1183) in 257 ms on localhost (162/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 178.0 in stage 14.0 (TID 1195, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 178.0 in stage 14.0 (TID 1195)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000159_1176' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000159
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000159_1176: Committed
15/08/09 15:27:41 INFO TaskSetManager: Finished task 164.0 in stage 14.0 (TID 1181) in 264 ms on localhost (163/200)
15/08/09 15:27:41 INFO Executor: Finished task 159.0 in stage 14.0 (TID 1176). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 179.0 in stage 14.0 (TID 1196, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 159.0 in stage 14.0 (TID 1176) in 286 ms on localhost (164/200)
15/08/09 15:27:41 INFO Executor: Running task 179.0 in stage 14.0 (TID 1196)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@43487e6b
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@6eb6f803
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000168_1185/part-00168
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3046a7eb
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1bb81ed0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000169_1186/part-00169
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7106f53f
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000139_1156' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000139
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000139_1156: Committed
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000171_1188/part-00171
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO Executor: Finished task 139.0 in stage 14.0 (TID 1156). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 180.0 in stage 14.0 (TID 1197, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 180.0 in stage 14.0 (TID 1197)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000167_1184' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000167
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000167_1184: Committed
15/08/09 15:27:41 INFO TaskSetManager: Finished task 139.0 in stage 14.0 (TID 1156) in 748 ms on localhost (165/200)
15/08/09 15:27:41 INFO Executor: Finished task 167.0 in stage 14.0 (TID 1184). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 181.0 in stage 14.0 (TID 1198, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 181.0 in stage 14.0 (TID 1198)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 167.0 in stage 14.0 (TID 1184) in 285 ms on localhost (166/200)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5ffc78c6
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1363228b
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5170dd9e
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000170_1187/part-00170
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7cb7e49f
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000172_1189/part-00172
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d02c2b1
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000169_1186' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000169
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000169_1186: Committed
15/08/09 15:27:41 INFO Executor: Finished task 169.0 in stage 14.0 (TID 1186). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 182.0 in stage 14.0 (TID 1199, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 182.0 in stage 14.0 (TID 1199)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 169.0 in stage 14.0 (TID 1186) in 191 ms on localhost (167/200)
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@45f04d27
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1e7f876c
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000176_1193/part-00176
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@409e3450
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000170_1187' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000170
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000170_1187: Committed
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000174_1191/part-00174
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO Executor: Finished task 170.0 in stage 14.0 (TID 1187). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Starting task 183.0 in stage 14.0 (TID 1200, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 183.0 in stage 14.0 (TID 1200)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 170.0 in stage 14.0 (TID 1187) in 210 ms on localhost (168/200)
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000172_1189' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000172
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000172_1189: Committed
15/08/09 15:27:41 INFO Executor: Finished task 172.0 in stage 14.0 (TID 1189). 781 bytes result sent to driver
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7509219f
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5985b20b
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO TaskSetManager: Starting task 184.0 in stage 14.0 (TID 1201, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 184.0 in stage 14.0 (TID 1201)
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2a7e89c0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000179_1196/part-00179
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO TaskSetManager: Finished task 172.0 in stage 14.0 (TID 1189) in 196 ms on localhost (169/200)
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4af5c4cd
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000175_1192/part-00175
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@71bc71f7
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000177_1194/part-00177
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2008ba2c
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000178_1195/part-00178
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@1ee07791
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6c966e32
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@81ccc50
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:41 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1aa99c59
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000173_1190/part-00173
15/08/09 15:27:41 INFO CodecConfig: Compression set to false
15/08/09 15:27:41 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:41 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:41 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3d5b2cda
15/08/09 15:27:41 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000174_1191' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000174
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000174_1191: Committed
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:41 INFO Executor: Finished task 174.0 in stage 14.0 (TID 1191). 781 bytes result sent to driver
15/08/09 15:27:41 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000176_1193' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000176
15/08/09 15:27:41 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000176_1193: Committed
15/08/09 15:27:41 INFO TaskSetManager: Starting task 185.0 in stage 14.0 (TID 1202, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 185.0 in stage 14.0 (TID 1202)
15/08/09 15:27:41 INFO Executor: Finished task 176.0 in stage 14.0 (TID 1193). 781 bytes result sent to driver
15/08/09 15:27:41 INFO TaskSetManager: Finished task 174.0 in stage 14.0 (TID 1191) in 198 ms on localhost (170/200)
15/08/09 15:27:41 INFO TaskSetManager: Starting task 186.0 in stage 14.0 (TID 1203, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:41 INFO Executor: Running task 186.0 in stage 14.0 (TID 1203)
15/08/09 15:27:41 INFO TaskSetManager: Finished task 176.0 in stage 14.0 (TID 1193) in 195 ms on localhost (171/200)
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3bfd1d5c
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3478dd33
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000181_1198/part-00181
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@25eea5cf
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000182_1199/part-00182
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000157_1174' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000157
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000157_1174: Committed
15/08/09 15:27:42 INFO Executor: Finished task 157.0 in stage 14.0 (TID 1174). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 187.0 in stage 14.0 (TID 1204, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 187.0 in stage 14.0 (TID 1204)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 157.0 in stage 14.0 (TID 1174) in 662 ms on localhost (172/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000173_1190' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000173
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000173_1190: Committed
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3963c623
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000180_1197/part-00180
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO Executor: Finished task 173.0 in stage 14.0 (TID 1190). 781 bytes result sent to driver
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO TaskSetManager: Starting task 188.0 in stage 14.0 (TID 1205, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 188.0 in stage 14.0 (TID 1205)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 173.0 in stage 14.0 (TID 1190) in 349 ms on localhost (173/200)
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@d54237e
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7d4ef8b6
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7629c9b7
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000182_1199' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000182
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000182_1199: Committed
15/08/09 15:27:42 INFO Executor: Finished task 182.0 in stage 14.0 (TID 1199). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 189.0 in stage 14.0 (TID 1206, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 189.0 in stage 14.0 (TID 1206)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 182.0 in stage 14.0 (TID 1199) in 294 ms on localhost (174/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000180_1197' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000180
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000180_1197: Committed
15/08/09 15:27:42 INFO Executor: Finished task 180.0 in stage 14.0 (TID 1197). 781 bytes result sent to driver
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000163_1180' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000163
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000163_1180: Committed
15/08/09 15:27:42 INFO TaskSetManager: Starting task 190.0 in stage 14.0 (TID 1207, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 190.0 in stage 14.0 (TID 1207)
15/08/09 15:27:42 INFO Executor: Finished task 163.0 in stage 14.0 (TID 1180). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 180.0 in stage 14.0 (TID 1197) in 346 ms on localhost (175/200)
15/08/09 15:27:42 INFO TaskSetManager: Starting task 191.0 in stage 14.0 (TID 1208, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 191.0 in stage 14.0 (TID 1208)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 163.0 in stage 14.0 (TID 1180) in 650 ms on localhost (176/200)
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@4e039d06
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000183_1200/part-00183
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2cb3fb2
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000184_1201/part-00184
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7f5a0c4a
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@50867128
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@5c2233e3
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000185_1202/part-00185
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000183_1200' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000183
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000183_1200: Committed
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3a6641c6
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000186_1203/part-00186
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO Executor: Finished task 183.0 in stage 14.0 (TID 1200). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 192.0 in stage 14.0 (TID 1209, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 192.0 in stage 14.0 (TID 1209)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 183.0 in stage 14.0 (TID 1200) in 297 ms on localhost (177/200)
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@297438b4
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@476d9db
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@61c3b434
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000187_1204/part-00187
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1428f637
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000188_1205/part-00188
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000168_1185' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000168
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000168_1185: Committed
15/08/09 15:27:42 INFO Executor: Finished task 168.0 in stage 14.0 (TID 1185). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 193.0 in stage 14.0 (TID 1210, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 193.0 in stage 14.0 (TID 1210)
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7595c468
15/08/09 15:27:42 INFO TaskSetManager: Finished task 168.0 in stage 14.0 (TID 1185) in 685 ms on localhost (178/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000185_1202' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000185
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000185_1202: Committed
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000186_1203' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000186
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000186_1203: Committed
15/08/09 15:27:42 INFO Executor: Finished task 185.0 in stage 14.0 (TID 1202). 781 bytes result sent to driver
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO Executor: Finished task 186.0 in stage 14.0 (TID 1203). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 194.0 in stage 14.0 (TID 1211, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000171_1188' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000171
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000171_1188: Committed
15/08/09 15:27:42 INFO Executor: Running task 194.0 in stage 14.0 (TID 1211)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 185.0 in stage 14.0 (TID 1202) in 302 ms on localhost (179/200)
15/08/09 15:27:42 INFO Executor: Finished task 171.0 in stage 14.0 (TID 1188). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 195.0 in stage 14.0 (TID 1212, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 195.0 in stage 14.0 (TID 1212)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 186.0 in stage 14.0 (TID 1203) in 303 ms on localhost (180/200)
15/08/09 15:27:42 INFO TaskSetManager: Starting task 196.0 in stage 14.0 (TID 1213, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 196.0 in stage 14.0 (TID 1213)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 171.0 in stage 14.0 (TID 1188) in 561 ms on localhost (181/200)
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@7e4214ba
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000187_1204' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000187
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000187_1204: Committed
15/08/09 15:27:42 INFO Executor: Finished task 187.0 in stage 14.0 (TID 1204). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 197.0 in stage 14.0 (TID 1214, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000188_1205' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000188
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000188_1205: Committed
15/08/09 15:27:42 INFO Executor: Running task 197.0 in stage 14.0 (TID 1214)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 187.0 in stage 14.0 (TID 1204) in 176 ms on localhost (182/200)
15/08/09 15:27:42 INFO Executor: Finished task 188.0 in stage 14.0 (TID 1205). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 198.0 in stage 14.0 (TID 1215, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 198.0 in stage 14.0 (TID 1215)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 188.0 in stage 14.0 (TID 1205) in 174 ms on localhost (183/200)
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@53766006
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000190_1207/part-00190
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@134fbd1e
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@58090679
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000191_1208/part-00191
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000189_1206/part-00189
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@70b0fa58
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@51ff985b
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@30fe3bcb
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000190_1207' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000190
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000190_1207: Committed
15/08/09 15:27:42 INFO Executor: Finished task 190.0 in stage 14.0 (TID 1207). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Starting task 199.0 in stage 14.0 (TID 1216, localhost, PROCESS_LOCAL, 1056 bytes)
15/08/09 15:27:42 INFO Executor: Running task 199.0 in stage 14.0 (TID 1216)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 190.0 in stage 14.0 (TID 1207) in 164 ms on localhost (184/200)
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Getting 197 non-empty blocks out of 200 blocks
15/08/09 15:27:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@551d59a0
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000192_1209/part-00192
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@79e74307
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000193_1210/part-00193
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@3de1bf17
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000195_1212/part-00195
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@1c2de474
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000194_1211/part-00194
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@6b0879d2
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@797600e3
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@32347561
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@12064d07
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@5bab0fcf
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000196_1213/part-00196
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@505f9e81
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000198_1215/part-00198
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2635a095
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@ac1a9cc
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000197_1214/part-00197
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000194_1211' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000194
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000194_1211: Committed
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000192_1209' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000192
15/08/09 15:27:42 INFO Executor: Finished task 194.0 in stage 14.0 (TID 1211). 781 bytes result sent to driver
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000192_1209: Committed
15/08/09 15:27:42 INFO Executor: Finished task 192.0 in stage 14.0 (TID 1209). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 194.0 in stage 14.0 (TID 1211) in 137 ms on localhost (185/200)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 192.0 in stage 14.0 (TID 1209) in 193 ms on localhost (186/200)
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@2fe56671
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,376
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 59B for [ps_partkey] INT32: 5 values, 26B raw, 26B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 87B for [value] DOUBLE: 5 values, 46B raw, 46B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3a4ccbe5
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000198_1215' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000198
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000198_1215: Committed
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000197_1214' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000197
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000197_1214: Committed
15/08/09 15:27:42 INFO Executor: Finished task 198.0 in stage 14.0 (TID 1215). 781 bytes result sent to driver
15/08/09 15:27:42 INFO Executor: Finished task 197.0 in stage 14.0 (TID 1214). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 198.0 in stage 14.0 (TID 1215) in 143 ms on localhost (187/200)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 197.0 in stage 14.0 (TID 1214) in 146 ms on localhost (188/200)
15/08/09 15:27:42 INFO MapredParquetOutputFormat: creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@9d0d221
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: creating real writer to write at hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/_temporary/attempt_201508091527_0014_m_000199_1216/part-00199
15/08/09 15:27:42 INFO CodecConfig: Compression set to false
15/08/09 15:27:42 INFO CodecConfig: Compression: UNCOMPRESSED
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet block size to 134217728
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
15/08/09 15:27:42 INFO ParquetOutputFormat: Dictionary is on
15/08/09 15:27:42 INFO ParquetOutputFormat: Validation is off
15/08/09 15:27:42 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
15/08/09 15:27:42 INFO ParquetRecordWriterWrapper: real writer: parquet.hadoop.ParquetRecordWriter@3c31fd5f
15/08/09 15:27:42 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 31,457,356
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 55B for [ps_partkey] INT32: 4 values, 22B raw, 22B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO ColumnChunkPageWriteStore: written 79B for [value] DOUBLE: 4 values, 38B raw, 38B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000199_1216' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000199
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000199_1216: Committed
15/08/09 15:27:42 INFO Executor: Finished task 199.0 in stage 14.0 (TID 1216). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 199.0 in stage 14.0 (TID 1216) in 152 ms on localhost (189/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000177_1194' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000177
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000177_1194: Committed
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000175_1192' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000175
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000179_1196' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000179
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000179_1196: Committed
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000175_1192: Committed
15/08/09 15:27:42 INFO Executor: Finished task 177.0 in stage 14.0 (TID 1194). 781 bytes result sent to driver
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000178_1195' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000178
15/08/09 15:27:42 INFO Executor: Finished task 175.0 in stage 14.0 (TID 1192). 781 bytes result sent to driver
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000178_1195: Committed
15/08/09 15:27:42 INFO Executor: Finished task 178.0 in stage 14.0 (TID 1195). 781 bytes result sent to driver
15/08/09 15:27:42 INFO Executor: Finished task 179.0 in stage 14.0 (TID 1196). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 177.0 in stage 14.0 (TID 1194) in 724 ms on localhost (190/200)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 175.0 in stage 14.0 (TID 1192) in 742 ms on localhost (191/200)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 178.0 in stage 14.0 (TID 1195) in 726 ms on localhost (192/200)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 179.0 in stage 14.0 (TID 1196) in 725 ms on localhost (193/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000181_1198' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000181
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000181_1198: Committed
15/08/09 15:27:42 INFO Executor: Finished task 181.0 in stage 14.0 (TID 1198). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 181.0 in stage 14.0 (TID 1198) in 715 ms on localhost (194/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000184_1201' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000184
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000184_1201: Committed
15/08/09 15:27:42 INFO Executor: Finished task 184.0 in stage 14.0 (TID 1201). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 184.0 in stage 14.0 (TID 1201) in 707 ms on localhost (195/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000191_1208' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000191
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000191_1208: Committed
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000189_1206' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000189
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000189_1206: Committed
15/08/09 15:27:42 INFO Executor: Finished task 191.0 in stage 14.0 (TID 1208). 781 bytes result sent to driver
15/08/09 15:27:42 INFO Executor: Finished task 189.0 in stage 14.0 (TID 1206). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 191.0 in stage 14.0 (TID 1208) in 589 ms on localhost (196/200)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 189.0 in stage 14.0 (TID 1206) in 603 ms on localhost (197/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000195_1212' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000195
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000195_1212: Committed
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000193_1210' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000193
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000193_1210: Committed
15/08/09 15:27:42 INFO Executor: Finished task 195.0 in stage 14.0 (TID 1212). 781 bytes result sent to driver
15/08/09 15:27:42 INFO Executor: Finished task 193.0 in stage 14.0 (TID 1210). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 195.0 in stage 14.0 (TID 1212) in 542 ms on localhost (198/200)
15/08/09 15:27:42 INFO TaskSetManager: Finished task 193.0 in stage 14.0 (TID 1210) in 549 ms on localhost (199/200)
15/08/09 15:27:42 INFO FileOutputCommitter: Saved output of task 'attempt_201508091527_0014_m_000196_1213' to hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_temporary/0/task_201508091527_0014_m_000196
15/08/09 15:27:42 INFO SparkHiveWriterContainer: attempt_201508091527_0014_m_000196_1213: Committed
15/08/09 15:27:42 INFO Executor: Finished task 196.0 in stage 14.0 (TID 1213). 781 bytes result sent to driver
15/08/09 15:27:42 INFO TaskSetManager: Finished task 196.0 in stage 14.0 (TID 1213) in 555 ms on localhost (200/200)
15/08/09 15:27:42 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/08/09 15:27:42 INFO DAGScheduler: Stage 14 (runJob at InsertIntoHiveTable.scala:93) finished in 5.536 s
15/08/09 15:27:42 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@34d68411
15/08/09 15:27:42 INFO DAGScheduler: Job 8 finished: runJob at InsertIntoHiveTable.scala:93, took 14.272894 s
15/08/09 15:27:42 INFO StatsReportListener: task runtime:(count: 200, mean: 425.065000, stdev: 255.374550, max: 1149.000000, min: 137.000000)
15/08/09 15:27:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:42 INFO StatsReportListener: 	137.0 ms	161.0 ms	177.0 ms	251.0 ms	351.0 ms	555.0 ms	905.0 ms	1.0 s	1.1 s
15/08/09 15:27:42 INFO StatsReportListener: fetch wait time:(count: 200, mean: 0.295000, stdev: 1.173872, max: 15.000000, min: 0.000000)
15/08/09 15:27:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:42 INFO StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	1.0 ms	1.0 ms	15.0 ms
15/08/09 15:27:42 INFO StatsReportListener: remote bytes read:(count: 200, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/08/09 15:27:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:42 INFO StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/08/09 15:27:42 INFO StatsReportListener: task result size:(count: 200, mean: 781.000000, stdev: 0.000000, max: 781.000000, min: 781.000000)
15/08/09 15:27:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:42 INFO StatsReportListener: 	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B	781.0 B
15/08/09 15:27:42 INFO StatsReportListener: executor (non-fetch) time pct: (count: 200, mean: 97.769837, stdev: 5.193605, max: 99.747793, min: 47.008547)
15/08/09 15:27:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:42 INFO StatsReportListener: 	47 %	96 %	97 %	98 %	99 %	99 %	99 %	100 %	100 %
15/08/09 15:27:42 INFO StatsReportListener: fetch wait time pct: (count: 200, mean: 0.096676, stdev: 0.371031, max: 4.531722, min: 0.000000)
15/08/09 15:27:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:42 INFO StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 1 %	 5 %
15/08/09 15:27:42 INFO StatsReportListener: other time pct: (count: 200, mean: 2.133487, stdev: 5.193520, max: 52.991453, min: 0.252207)
15/08/09 15:27:42 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:42 INFO StatsReportListener: 	 0 %	 0 %	 1 %	 1 %	 1 %	 2 %	 3 %	 4 %	53 %
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/_SUCCESS;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/_SUCCESS;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00000;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00000;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00001;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00001;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00002;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00002;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00003;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00003;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00004;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00004;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00005;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00005;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00006;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00006;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00007;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00007;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00008;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00008;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00009;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00009;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00010;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00010;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00011;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00011;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00012;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00012;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00013;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00013;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00014;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00014;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00015;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00015;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00016;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00016;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00017;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00017;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00018;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00018;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00019;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00019;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00020;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00020;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00021;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00021;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00022;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00022;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00023;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00023;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00024;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00024;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00025;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00025;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00026;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00026;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00027;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00027;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00028;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00028;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00029;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00029;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00030;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00030;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00031;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00031;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00032;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00032;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00033;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00033;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00034;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00034;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00035;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00035;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00036;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00036;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00037;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00037;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00038;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00038;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00039;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00039;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00040;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00040;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00041;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00041;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00042;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00042;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00043;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00043;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00044;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00044;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00045;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00045;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00046;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00046;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00047;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00047;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00048;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00048;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00049;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00049;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00050;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00050;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00051;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00051;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00052;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00052;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00053;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00053;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00054;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00054;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00055;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00055;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00056;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00056;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00057;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00057;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00058;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00058;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00059;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00059;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00060;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00060;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00061;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00061;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00062;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00062;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00063;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00063;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00064;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00064;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00065;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00065;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00066;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00066;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00067;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00067;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00068;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00068;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00069;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00069;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00070;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00070;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00071;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00071;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00072;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00072;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00073;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00073;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00074;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00074;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00075;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00075;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00076;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00076;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00077;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00077;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00078;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00078;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00079;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00079;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00080;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00080;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00081;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00081;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00082;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00082;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00083;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00083;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00084;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00084;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00085;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00085;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00086;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00086;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00087;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00087;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00088;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00088;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00089;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00089;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00090;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00090;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00091;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00091;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00092;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00092;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00093;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00093;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00094;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00094;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00095;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00095;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00096;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00096;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00097;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00097;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00098;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00098;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00099;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00099;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00100;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00100;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00101;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00101;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00102;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00102;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00103;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00103;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00104;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00104;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00105;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00105;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00106;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00106;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00107;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00107;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00108;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00108;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00109;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00109;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00110;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00110;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00111;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00111;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00112;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00112;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00113;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00113;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00114;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00114;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00115;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00115;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00116;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00116;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00117;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00117;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00118;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00118;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00119;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00119;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00120;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00120;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00121;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00121;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00122;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00122;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00123;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00123;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00124;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00124;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00125;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00125;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00126;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00126;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00127;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00127;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00128;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00128;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00129;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00129;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00130;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00130;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00131;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00131;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00132;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00132;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00133;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00133;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00134;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00134;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00135;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00135;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00136;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00136;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00137;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00137;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00138;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00138;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00139;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00139;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00140;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00140;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00141;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00141;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00142;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00142;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00143;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00143;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00144;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00144;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00145;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00145;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00146;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00146;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00147;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00147;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00148;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00148;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00149;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00149;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00150;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00150;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00151;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00151;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00152;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00152;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00153;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00153;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00154;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00154;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00155;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00155;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00156;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00156;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00157;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00157;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00158;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00158;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00159;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00159;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00160;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00160;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00161;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00161;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00162;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00162;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00163;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00163;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00164;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00164;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00165;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00165;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00166;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00166;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00167;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00167;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00168;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00168;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00169;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00169;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00170;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00170;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00171;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00171;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00172;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00172;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00173;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00173;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00174;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00174;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00175;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00175;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00176;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00176;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00177;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00177;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00178;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00178;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00179;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00179;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00180;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00180;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00181;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00181;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00182;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00182;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00183;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00183;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00184;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00184;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00185;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00185;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00186;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00186;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00187;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00187;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00188;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00188;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00189;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00189;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00190;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00190;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00191;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00191;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00192;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00192;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00193;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00193;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00194;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00194;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00195;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00195;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00196;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00196;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00197;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00197;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00198;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00198;Status:true
15/08/09 15:27:44 INFO Hive: Renaming src:hdfs://sandbox.hortonworks.com:8020/tmp/hive-hive/hive_2015-08-09_15-27-24_026_1133259343709698739-1/-ext-10000/part-00199;dest: hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/q11_important_stock_par_spark/part-00199;Status:true
15/08/09 15:27:44 INFO DefaultExecutionContext: Starting job: collect at SparkPlan.scala:84
15/08/09 15:27:44 INFO DAGScheduler: Got job 9 (collect at SparkPlan.scala:84) with 1 output partitions (allowLocal=false)
15/08/09 15:27:44 INFO DAGScheduler: Final stage: Stage 15(collect at SparkPlan.scala:84)
15/08/09 15:27:44 INFO DAGScheduler: Parents of final stage: List()
15/08/09 15:27:44 INFO DAGScheduler: Missing parents: List()
15/08/09 15:27:44 INFO DAGScheduler: Submitting Stage 15 (MappedRDD[82] at map at SparkPlan.scala:84), which has no missing parents
15/08/09 15:27:44 INFO MemoryStore: ensureFreeSpace(3256) called with curMem=1862485, maxMem=3333968363
15/08/09 15:27:44 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 3.2 KB, free 3.1 GB)
15/08/09 15:27:44 INFO MemoryStore: ensureFreeSpace(1958) called with curMem=1865741, maxMem=3333968363
15/08/09 15:27:44 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 1958.0 B, free 3.1 GB)
15/08/09 15:27:44 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:44535 (size: 1958.0 B, free: 3.1 GB)
15/08/09 15:27:44 INFO BlockManagerMaster: Updated info of block broadcast_20_piece0
15/08/09 15:27:44 INFO DefaultExecutionContext: Created broadcast 20 from broadcast at DAGScheduler.scala:838
15/08/09 15:27:44 INFO DAGScheduler: Submitting 1 missing tasks from Stage 15 (MappedRDD[82] at map at SparkPlan.scala:84)
15/08/09 15:27:44 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
15/08/09 15:27:44 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 1217, localhost, PROCESS_LOCAL, 1249 bytes)
15/08/09 15:27:44 INFO Executor: Running task 0.0 in stage 15.0 (TID 1217)
15/08/09 15:27:44 INFO Executor: Finished task 0.0 in stage 15.0 (TID 1217). 618 bytes result sent to driver
15/08/09 15:27:44 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 1217) in 6 ms on localhost (1/1)
15/08/09 15:27:44 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/08/09 15:27:44 INFO DAGScheduler: Stage 15 (collect at SparkPlan.scala:84) finished in 0.006 s
15/08/09 15:27:44 INFO DAGScheduler: Job 9 finished: collect at SparkPlan.scala:84, took 0.018011 s
15/08/09 15:27:44 INFO StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@727b8066
Time taken: 21.154 seconds
15/08/09 15:27:44 INFO CliDriver: Time taken: 21.154 seconds
15/08/09 15:27:44 INFO PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:44 INFO StatsReportListener: task runtime:(count: 1, mean: 6.000000, stdev: 0.000000, max: 6.000000, min: 6.000000)
15/08/09 15:27:44 INFO PerfLogger: </PERFLOG method=releaseLocks start=1439134064823 end=1439134064823 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/08/09 15:27:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:44 INFO StatsReportListener: 	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms	6.0 ms
15/08/09 15:27:44 INFO StatsReportListener: task result size:(count: 1, mean: 618.000000, stdev: 0.000000, max: 618.000000, min: 618.000000)
15/08/09 15:27:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:44 INFO StatsReportListener: 	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B	618.0 B
15/08/09 15:27:44 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 16.666667, stdev: 0.000000, max: 16.666667, min: 16.666667)
15/08/09 15:27:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:44 INFO StatsReportListener: 	17 %	17 %	17 %	17 %	17 %	17 %	17 %	17 %	17 %
15/08/09 15:27:44 INFO StatsReportListener: other time pct: (count: 1, mean: 83.333333, stdev: 0.000000, max: 83.333333, min: 83.333333)
15/08/09 15:27:44 INFO StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/08/09 15:27:44 INFO StatsReportListener: 	83 %	83 %	83 %	83 %	83 %	83 %	83 %	83 %	83 %
15/08/09 15:27:45 INFO SparkUI: Stopped Spark web UI at http://sandbox.hortonworks.com:4040
15/08/09 15:27:45 INFO DAGScheduler: Stopping DAGScheduler
15/08/09 15:27:45 INFO BlockManager: Removing broadcast 20
15/08/09 15:27:45 INFO BlockManager: Removing block broadcast_20
15/08/09 15:27:45 INFO MemoryStore: Block broadcast_20 of size 3256 dropped from memory (free 3332103920)
15/08/09 15:27:45 INFO BlockManager: Removing block broadcast_20_piece0
15/08/09 15:27:45 INFO MemoryStore: Block broadcast_20_piece0 of size 1958 dropped from memory (free 3332105878)
15/08/09 15:27:45 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:44535 in memory (size: 1958.0 B, free: 3.1 GB)
15/08/09 15:27:45 INFO BlockManagerMaster: Updated info of block broadcast_20_piece0
15/08/09 15:27:45 INFO ContextCleaner: Cleaned broadcast 20
15/08/09 15:27:46 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
15/08/09 15:27:46 INFO MemoryStore: MemoryStore cleared
15/08/09 15:27:46 INFO BlockManager: BlockManager stopped
15/08/09 15:27:46 INFO BlockManagerMaster: BlockManagerMaster stopped
15/08/09 15:27:46 INFO SparkContext: Successfully stopped SparkContext
